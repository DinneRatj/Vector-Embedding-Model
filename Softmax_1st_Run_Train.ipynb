{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DinneRatj/Vector-Embedding-Model/blob/main/Softmax_1st_Run_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da85_POQktm5",
        "outputId": "f9606f66-6c23-48dd-d9e0-71a0f5568e98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb25014e930>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Imporing libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Added so that the random numbers are always the same when the program is run, so the results are always the same\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting to Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXHrQD4awV5F",
        "outputId": "9138e78b-8fce-47cc-ec70-a07fd663f2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "btHd-1nfktm_",
        "outputId": "b218a2f8-7318-4711-eb1b-9a64792c5d86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     state  account length  area code phone number international plan  \\\n",
              "0       KS             128        415     382-4657                 no   \n",
              "1       OH             107        415     371-7191                 no   \n",
              "2       NJ             137        415     358-1921                 no   \n",
              "3       OH              84        408     375-9999                yes   \n",
              "4       OK              75        415     330-6626                yes   \n",
              "...    ...             ...        ...          ...                ...   \n",
              "3328    AZ             192        415     414-4276                 no   \n",
              "3329    WV              68        415     370-3271                 no   \n",
              "3330    RI              28        510     328-8230                 no   \n",
              "3331    CT             184        510     364-6381                yes   \n",
              "3332    TN              74        415     400-4344                 no   \n",
              "\n",
              "     voice mail plan  number vmail messages  total day minutes  \\\n",
              "0                yes                     25              265.1   \n",
              "1                yes                     26              161.6   \n",
              "2                 no                      0              243.4   \n",
              "3                 no                      0              299.4   \n",
              "4                 no                      0              166.7   \n",
              "...              ...                    ...                ...   \n",
              "3328             yes                     36              156.2   \n",
              "3329              no                      0              231.1   \n",
              "3330              no                      0              180.8   \n",
              "3331              no                      0              213.8   \n",
              "3332             yes                     25              234.4   \n",
              "\n",
              "      total day calls  total day charge  total eve minutes  total eve calls  \\\n",
              "0                 110             45.07              197.4               99   \n",
              "1                 123             27.47              195.5              103   \n",
              "2                 114             41.38              121.2              110   \n",
              "3                  71             50.90               61.9               88   \n",
              "4                 113             28.34              148.3              122   \n",
              "...               ...               ...                ...              ...   \n",
              "3328               77             26.55              215.5              126   \n",
              "3329               57             39.29              153.4               55   \n",
              "3330              109             30.74              288.8               58   \n",
              "3331              105             36.35              159.6               84   \n",
              "3332              113             39.85              265.9               82   \n",
              "\n",
              "      total eve charge  total night minutes  total night calls  \\\n",
              "0                16.78                244.7                 91   \n",
              "1                16.62                254.4                103   \n",
              "2                10.30                162.6                104   \n",
              "3                 5.26                196.9                 89   \n",
              "4                12.61                186.9                121   \n",
              "...                ...                  ...                ...   \n",
              "3328             18.32                279.1                 83   \n",
              "3329             13.04                191.3                123   \n",
              "3330             24.55                191.9                 91   \n",
              "3331             13.57                139.2                137   \n",
              "3332             22.60                241.4                 77   \n",
              "\n",
              "      total night charge  total intl minutes  total intl calls  \\\n",
              "0                  11.01                10.0                 3   \n",
              "1                  11.45                13.7                 3   \n",
              "2                   7.32                12.2                 5   \n",
              "3                   8.86                 6.6                 7   \n",
              "4                   8.41                10.1                 3   \n",
              "...                  ...                 ...               ...   \n",
              "3328               12.56                 9.9                 6   \n",
              "3329                8.61                 9.6                 4   \n",
              "3330                8.64                14.1                 6   \n",
              "3331                6.26                 5.0                10   \n",
              "3332               10.86                13.7                 4   \n",
              "\n",
              "      total intl charge  customer service calls  churn  \n",
              "0                  2.70                       1  False  \n",
              "1                  3.70                       1  False  \n",
              "2                  3.29                       0  False  \n",
              "3                  1.78                       2  False  \n",
              "4                  2.73                       3  False  \n",
              "...                 ...                     ...    ...  \n",
              "3328               2.67                       2  False  \n",
              "3329               2.59                       3  False  \n",
              "3330               3.81                       2  False  \n",
              "3331               1.35                       2  False  \n",
              "3332               3.70                       0  False  \n",
              "\n",
              "[3333 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-195418da-dce3-4219-8f74-6837677aa422\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>account length</th>\n",
              "      <th>area code</th>\n",
              "      <th>phone number</th>\n",
              "      <th>international plan</th>\n",
              "      <th>voice mail plan</th>\n",
              "      <th>number vmail messages</th>\n",
              "      <th>total day minutes</th>\n",
              "      <th>total day calls</th>\n",
              "      <th>total day charge</th>\n",
              "      <th>total eve minutes</th>\n",
              "      <th>total eve calls</th>\n",
              "      <th>total eve charge</th>\n",
              "      <th>total night minutes</th>\n",
              "      <th>total night calls</th>\n",
              "      <th>total night charge</th>\n",
              "      <th>total intl minutes</th>\n",
              "      <th>total intl calls</th>\n",
              "      <th>total intl charge</th>\n",
              "      <th>customer service calls</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KS</td>\n",
              "      <td>128</td>\n",
              "      <td>415</td>\n",
              "      <td>382-4657</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>265.1</td>\n",
              "      <td>110</td>\n",
              "      <td>45.07</td>\n",
              "      <td>197.4</td>\n",
              "      <td>99</td>\n",
              "      <td>16.78</td>\n",
              "      <td>244.7</td>\n",
              "      <td>91</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OH</td>\n",
              "      <td>107</td>\n",
              "      <td>415</td>\n",
              "      <td>371-7191</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>26</td>\n",
              "      <td>161.6</td>\n",
              "      <td>123</td>\n",
              "      <td>27.47</td>\n",
              "      <td>195.5</td>\n",
              "      <td>103</td>\n",
              "      <td>16.62</td>\n",
              "      <td>254.4</td>\n",
              "      <td>103</td>\n",
              "      <td>11.45</td>\n",
              "      <td>13.7</td>\n",
              "      <td>3</td>\n",
              "      <td>3.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NJ</td>\n",
              "      <td>137</td>\n",
              "      <td>415</td>\n",
              "      <td>358-1921</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>243.4</td>\n",
              "      <td>114</td>\n",
              "      <td>41.38</td>\n",
              "      <td>121.2</td>\n",
              "      <td>110</td>\n",
              "      <td>10.30</td>\n",
              "      <td>162.6</td>\n",
              "      <td>104</td>\n",
              "      <td>7.32</td>\n",
              "      <td>12.2</td>\n",
              "      <td>5</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OH</td>\n",
              "      <td>84</td>\n",
              "      <td>408</td>\n",
              "      <td>375-9999</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>299.4</td>\n",
              "      <td>71</td>\n",
              "      <td>50.90</td>\n",
              "      <td>61.9</td>\n",
              "      <td>88</td>\n",
              "      <td>5.26</td>\n",
              "      <td>196.9</td>\n",
              "      <td>89</td>\n",
              "      <td>8.86</td>\n",
              "      <td>6.6</td>\n",
              "      <td>7</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OK</td>\n",
              "      <td>75</td>\n",
              "      <td>415</td>\n",
              "      <td>330-6626</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>166.7</td>\n",
              "      <td>113</td>\n",
              "      <td>28.34</td>\n",
              "      <td>148.3</td>\n",
              "      <td>122</td>\n",
              "      <td>12.61</td>\n",
              "      <td>186.9</td>\n",
              "      <td>121</td>\n",
              "      <td>8.41</td>\n",
              "      <td>10.1</td>\n",
              "      <td>3</td>\n",
              "      <td>2.73</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>AZ</td>\n",
              "      <td>192</td>\n",
              "      <td>415</td>\n",
              "      <td>414-4276</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>36</td>\n",
              "      <td>156.2</td>\n",
              "      <td>77</td>\n",
              "      <td>26.55</td>\n",
              "      <td>215.5</td>\n",
              "      <td>126</td>\n",
              "      <td>18.32</td>\n",
              "      <td>279.1</td>\n",
              "      <td>83</td>\n",
              "      <td>12.56</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "      <td>2.67</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3329</th>\n",
              "      <td>WV</td>\n",
              "      <td>68</td>\n",
              "      <td>415</td>\n",
              "      <td>370-3271</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>231.1</td>\n",
              "      <td>57</td>\n",
              "      <td>39.29</td>\n",
              "      <td>153.4</td>\n",
              "      <td>55</td>\n",
              "      <td>13.04</td>\n",
              "      <td>191.3</td>\n",
              "      <td>123</td>\n",
              "      <td>8.61</td>\n",
              "      <td>9.6</td>\n",
              "      <td>4</td>\n",
              "      <td>2.59</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>RI</td>\n",
              "      <td>28</td>\n",
              "      <td>510</td>\n",
              "      <td>328-8230</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>180.8</td>\n",
              "      <td>109</td>\n",
              "      <td>30.74</td>\n",
              "      <td>288.8</td>\n",
              "      <td>58</td>\n",
              "      <td>24.55</td>\n",
              "      <td>191.9</td>\n",
              "      <td>91</td>\n",
              "      <td>8.64</td>\n",
              "      <td>14.1</td>\n",
              "      <td>6</td>\n",
              "      <td>3.81</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3331</th>\n",
              "      <td>CT</td>\n",
              "      <td>184</td>\n",
              "      <td>510</td>\n",
              "      <td>364-6381</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>213.8</td>\n",
              "      <td>105</td>\n",
              "      <td>36.35</td>\n",
              "      <td>159.6</td>\n",
              "      <td>84</td>\n",
              "      <td>13.57</td>\n",
              "      <td>139.2</td>\n",
              "      <td>137</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3332</th>\n",
              "      <td>TN</td>\n",
              "      <td>74</td>\n",
              "      <td>415</td>\n",
              "      <td>400-4344</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>234.4</td>\n",
              "      <td>113</td>\n",
              "      <td>39.85</td>\n",
              "      <td>265.9</td>\n",
              "      <td>82</td>\n",
              "      <td>22.60</td>\n",
              "      <td>241.4</td>\n",
              "      <td>77</td>\n",
              "      <td>10.86</td>\n",
              "      <td>13.7</td>\n",
              "      <td>4</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3333 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-195418da-dce3-4219-8f74-6837677aa422')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-195418da-dce3-4219-8f74-6837677aa422 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-195418da-dce3-4219-8f74-6837677aa422');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Reading data\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/bigml_59c28831336c6604c800002a.csv\")\n",
        "pd.options.display.max_columns = None\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc2pn_TAktnA"
      },
      "outputs": [],
      "source": [
        "#Defining columns\n",
        "numerical_columns = ['number vmail messages', 'total day minutes', 'total day calls',\n",
        "                     'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes',\n",
        "                     'total night calls', 'total night charge', 'total intl minutes', 'total intl calls',\n",
        "                     'total intl charge', 'customer service calls']\n",
        "categorical_columns = ['state', 'international plan', 'voice mail plan','area code']\n",
        "outputs = ['churn']\n",
        "\n",
        "#Input >14 Numerical coloums and 4 Categorical coloums\n",
        "#Output > 1 Categorical coloum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka5ZzJi5ktnB"
      },
      "outputs": [],
      "source": [
        "churn_data = dataset[dataset['churn'] == 'True']\n",
        "notchurn_data = dataset[dataset['churn'] == 'False']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYZfXbl7ktnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb26a5f-6131-4dc3-8cc8-a0972d674c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2349,  1.5668,  0.4766,  ..., -0.6012, -0.0857, -0.4279],\n",
            "        [ 1.3079, -0.3337,  1.1245,  ..., -0.6012,  1.2412, -0.4279],\n",
            "        [-0.5918,  1.1683,  0.6760,  ...,  0.2115,  0.6972, -1.1882],\n",
            "        ...,\n",
            "        [-0.5918,  0.0188,  0.4268,  ...,  0.6179,  1.3871,  0.3324],\n",
            "        [-0.5918,  0.6248,  0.2275,  ...,  2.2434, -1.8770,  0.3324],\n",
            "        [ 1.2349,  1.0030,  0.6261,  ..., -0.1948,  1.2412, -1.1882]])\n",
            "torch.float32\n",
            "torch.Size([3333, 14])\n",
            "___________________________________________________________________________\n",
            "tensor([[16,  0,  1,  1],\n",
            "        [35,  0,  1,  1],\n",
            "        [31,  0,  0,  1],\n",
            "        ...,\n",
            "        [39,  0,  0,  2],\n",
            "        [ 6,  1,  0,  2],\n",
            "        [42,  0,  1,  1]])\n",
            "torch.int64\n",
            "torch.Size([3333, 4])\n",
            "___________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Processing columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Numerical\n",
        "#Convert our numerical columns to tensors\n",
        "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
        "\n",
        "#Fixed how to use scaler\n",
        "numerical_data = scaler.fit_transform(numerical_data)\n",
        "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
        "\n",
        "#Categorical\n",
        "#Convert the types for categorical columns to category\n",
        "for category in categorical_columns:\n",
        "    dataset[category] = dataset[category].astype('category')\n",
        "\n",
        "#Convert data in the four categorical columns into numpy arrays and then stack all the columns horizontally \n",
        "st = dataset['state'].cat.codes.values\n",
        "ip = dataset['international plan'].cat.codes.values\n",
        "vm = dataset['voice mail plan'].cat.codes.values\n",
        "ac = dataset['area code'].cat.codes.values\n",
        "\n",
        "categorical_data = np.stack([st, ip, vm, ac], 1)\n",
        "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
        "\n",
        "#Outputs\n",
        "#Convert the output numpy array into a tensor object\n",
        "dataset[outputs] = dataset[outputs].astype(int)\n",
        "outputs = torch.tensor(dataset[outputs].values).flatten()\n",
        "outputs = outputs.long()\n",
        "\n",
        "#Print Outputs\n",
        "print(numerical_data)\n",
        "print(numerical_data.dtype)\n",
        "print(numerical_data.shape)\n",
        "print('_' * 75)\n",
        "\n",
        "print(categorical_data)\n",
        "print(categorical_data.dtype)\n",
        "print(categorical_data.shape)\n",
        "print('_' * 75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZGgQzxCktnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469ab420-a15f-45d7-a201-ff808dbf924a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999\n",
            "666\n",
            "666\n"
          ]
        }
      ],
      "source": [
        "#Dividing dataset into Training, Valid and Test\n",
        "total_records = 3333\n",
        "\n",
        "train_records = int(total_records * .6)\n",
        "valid_records = int(total_records * .2)\n",
        "test_records = int(total_records * .2)\n",
        "\n",
        "numerical_train_data = numerical_data[:train_records]\n",
        "numerical_valid_data = numerical_data[train_records:train_records+valid_records]\n",
        "numerical_test_data = numerical_data[train_records+valid_records:total_records]\n",
        "\n",
        "categorical_train_data = categorical_data[:train_records]\n",
        "categorical_valid_data = categorical_data[train_records:train_records+valid_records]\n",
        "categorical_test_data = categorical_data[train_records+valid_records:total_records]\n",
        "\n",
        "train_outputs = outputs[:train_records]\n",
        "valid_outputs = outputs[train_records:train_records+valid_records]\n",
        "test_outputs = outputs[train_records+valid_records:total_records]\n",
        "\n",
        "#Print divide dataset\n",
        "print(train_records)\n",
        "print(valid_records)\n",
        "print(test_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G19CQZ-LktnE"
      },
      "outputs": [],
      "source": [
        "#Define a class named Model, which will be used to train the model\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "#Creating the Neural Network\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(14, 100) #Numerical\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm1d(100)\n",
        "        self.weights1 = Parameter(torch.Tensor(1, 120))\n",
        "        init.kaiming_uniform_(self.weights1, a=math.sqrt(5))\n",
        "        \n",
        "        self.weights2 = Parameter(torch.Tensor(1, 120))\n",
        "        init.kaiming_uniform_(self.weights2, a=math.sqrt(5))\n",
        "        \n",
        "        self.bias1 = Parameter(torch.Tensor(1))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weights1)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias1, -bound, bound)\n",
        "        \n",
        "        self.bias2 = Parameter(torch.Tensor(1))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weights2)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias2, -bound, bound)\n",
        "        \n",
        "        #Categorical\n",
        "        self.layer1_1 = nn.Embedding(51, 5) #51 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_1 = nn.BatchNorm1d(5)\n",
        "        self.layer1_2 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_2 = nn.BatchNorm1d(5)\n",
        "        self.layer1_3 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_3 = nn.BatchNorm1d(5)\n",
        "        self.layer1_4 = nn.Embedding(3, 5) #3 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_4 = nn.BatchNorm1d(5)\n",
        "        \n",
        "        self.layer2 = nn.Linear(120, 120)\n",
        "        self.bn2 = nn.BatchNorm1d(120)\n",
        "        \n",
        "    def forward(self, x_numerical, x_categorical):\n",
        "        x1 = self.layer1(x_numerical)\n",
        "        x1 = self.relu(x1)\n",
        "        x1 = self.bn1(x1)\n",
        "        \n",
        "        #Decoder\n",
        "        x1_embedding = self.layer1_1(x_categorical[:,0])\n",
        "        x1_embedding = self.relu(x1_embedding)\n",
        "        x1_embedding = self.bn1_1(x1_embedding)\n",
        "        \n",
        "        x2_embedding = self.layer1_2(x_categorical[:,1])\n",
        "        x2_embedding = self.relu(x2_embedding)\n",
        "        x2_embedding = self.bn1_2(x2_embedding)\n",
        "        \n",
        "        x3_embedding = self.layer1_3(x_categorical[:,2])\n",
        "        x3_embedding = self.relu(x3_embedding)\n",
        "        x3_embedding = self.bn1_3(x3_embedding)\n",
        "        \n",
        "        x4_embedding = self.layer1_4(x_categorical[:,3])\n",
        "        x4_embedding = self.relu(x4_embedding)\n",
        "        x4_embedding = self.bn1_4(x4_embedding)\n",
        "        \n",
        "        x_embedding = torch.cat([x1_embedding,x2_embedding,x3_embedding,x4_embedding], 1)\n",
        "                \n",
        "        x1 = torch.cat([x1, x_embedding], 1)\n",
        "        \n",
        "        #Decoder\n",
        "        x2 = self.layer2(x1)        \n",
        "        emb = self.relu(x2)\n",
        "        x2 = self.bn2(emb)\n",
        "        \n",
        "        x2_weights1 = torch.mm(x2, self.weights1.t()) + self.bias1\n",
        "        x2_weights2 = torch.mm(x2, self.weights2.t()) + self.bias2\n",
        "        \n",
        "        x3 = torch.cat([x2_weights1, x2_weights2], 1)\n",
        "        \n",
        "        return emb, self.weights1, self.weights2, x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQSGMWRVktnG"
      },
      "outputs": [],
      "source": [
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYV5jKKqktnH"
      },
      "outputs": [],
      "source": [
        "#===============================================================================================\n",
        "#Defining churn:loyal weight ratio. churn_percentage=0.7 means churn:loyal weight ratio of 7:3.\n",
        "#===============================================================================================\n",
        "churn_percentage = 0.7\n",
        "\n",
        "#Defining loss function\n",
        "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1-churn_percentage, churn_percentage]))\n",
        "\n",
        "#Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Added learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vU05KkcQktnI",
        "outputId": "651f237a-bbb3-496e-b492-71af88bcb8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration:  50 loss: 0.05420018\n",
            "iteration:  51 loss: 0.04229215\n",
            "iteration:  52 loss: 0.02643572\n",
            "iteration:  53 loss: 0.54420060\n",
            "iteration:  54 loss: 0.07964019\n",
            "iteration:  55 loss: 0.05025754\n",
            "iteration:  56 loss: 0.05168339\n",
            "iteration:  57 loss: 0.08518010\n",
            "iteration:  58 loss: 0.11534101\n",
            "iteration:  59 loss: 0.13436039\n",
            "iteration:  60 loss: 0.33338377\n",
            "iteration:  61 loss: 0.36237624\n",
            "iteration:  62 loss: 0.09694417\n",
            "iteration:  63 loss: 0.08063716\n",
            "iteration:  64 loss: 0.06274779\n",
            "iteration:  65 loss: 0.00948846\n",
            "iteration:  66 loss: 0.01627511\n",
            "iteration:  67 loss: 0.04708342\n",
            "iteration:  68 loss: 0.43406197\n",
            "iteration:  69 loss: 0.11002510\n",
            "iteration:  70 loss: 0.19200875\n",
            "iteration:  71 loss: 0.32954696\n",
            "iteration:  72 loss: 0.07440836\n",
            "iteration:  73 loss: 0.05052375\n",
            "iteration:  74 loss: 0.04412610\n",
            "iteration:  75 loss: 0.02072336\n",
            "iteration:  76 loss: 0.05353837\n",
            "iteration:  77 loss: 0.15795296\n",
            "iteration:  78 loss: 0.14455889\n",
            "iteration:  79 loss: 0.05181881\n",
            "iteration:  80 loss: 0.18992907\n",
            "iteration:  81 loss: 0.14804165\n",
            "iteration:  82 loss: 0.09148099\n",
            "iteration:  83 loss: 0.06214458\n",
            "iteration:  84 loss: 0.05346106\n",
            "iteration:  85 loss: 0.04737476\n",
            "iteration:  86 loss: 0.09331909\n",
            "iteration:  87 loss: 0.05757222\n",
            "iteration:  88 loss: 0.05451102\n",
            "iteration:  89 loss: 0.03044651\n",
            "iteration:  90 loss: 0.05722240\n",
            "iteration:  91 loss: 0.46677047\n",
            "iteration:  92 loss: 0.18157777\n",
            "iteration:  93 loss: 0.04649769\n",
            "iteration:  94 loss: 0.04778275\n",
            "iteration:  95 loss: 0.15832499\n",
            "iteration:  96 loss: 0.23179708\n",
            "iteration:  97 loss: 0.03689771\n",
            "iteration:  98 loss: 0.31002384\n",
            "iteration:  99 loss: 0.07758287\n",
            "iteration: 100 loss: 0.07480361\n",
            "iteration: 101 loss: 0.09712486\n",
            "iteration: 102 loss: 0.08212493\n",
            "iteration: 103 loss: 0.08613208\n",
            "iteration: 104 loss: 0.12267476\n",
            "iteration: 105 loss: 0.09505846\n",
            "iteration: 106 loss: 0.27404386\n",
            "iteration: 107 loss: 0.12634403\n",
            "iteration: 108 loss: 0.07116775\n",
            "iteration: 109 loss: 0.17296562\n",
            "iteration: 110 loss: 0.25320736\n",
            "iteration: 111 loss: 0.08283802\n",
            "iteration: 112 loss: 0.07788686\n",
            "iteration: 113 loss: 0.35803694\n",
            "iteration: 114 loss: 0.13447960\n",
            "iteration: 115 loss: 0.01734379\n",
            "iteration: 116 loss: 0.04794905\n",
            "iteration: 117 loss: 0.13358122\n",
            "iteration: 118 loss: 0.20461027\n",
            "iteration: 119 loss: 0.20087177\n",
            "iteration: 120 loss: 0.02371272\n",
            "iteration: 121 loss: 0.13089938\n",
            "iteration: 122 loss: 0.82099921\n",
            "iteration: 123 loss: 0.10315699\n",
            "iteration: 124 loss: 0.21167549\n",
            "iteration: 125 loss: 0.06574688\n",
            "iteration: 126 loss: 0.03795892\n",
            "iteration: 127 loss: 0.09181464\n",
            "iteration: 128 loss: 0.34151670\n",
            "iteration: 129 loss: 0.12271653\n",
            "iteration: 130 loss: 0.12511697\n",
            "iteration: 131 loss: 0.24680099\n",
            "iteration: 132 loss: 0.07610959\n",
            "iteration: 133 loss: 0.05934380\n",
            "iteration: 134 loss: 0.55086583\n",
            "iteration: 135 loss: 0.17576441\n",
            "iteration: 136 loss: 0.18473813\n",
            "iteration: 137 loss: 0.02725796\n",
            "iteration: 138 loss: 0.10294995\n",
            "iteration: 139 loss: 0.05086755\n",
            "iteration: 140 loss: 0.10948026\n",
            "iteration: 141 loss: 0.05138760\n",
            "iteration: 142 loss: 0.10309809\n",
            "iteration: 143 loss: 0.04634310\n",
            "iteration: 144 loss: 0.08035772\n",
            "iteration: 145 loss: 0.09953712\n",
            "iteration: 146 loss: 0.03537576\n",
            "iteration: 147 loss: 0.22019838\n",
            "iteration: 148 loss: 0.05161737\n",
            "iteration: 149 loss: 0.30675548\n",
            "iteration: 150 loss: 0.31091869\n",
            "iteration: 151 loss: 0.02615553\n",
            "iteration: 152 loss: 0.06849280\n",
            "iteration: 153 loss: 0.08591561\n",
            "iteration: 154 loss: 0.11450276\n",
            "iteration: 155 loss: 0.02626262\n",
            "iteration: 156 loss: 0.05733352\n",
            "iteration: 157 loss: 0.11602449\n",
            "iteration: 158 loss: 0.05977860\n",
            "iteration: 159 loss: 0.04160259\n",
            "iteration: 160 loss: 0.48631766\n",
            "iteration: 161 loss: 0.10127308\n",
            "iteration: 162 loss: 0.05255662\n",
            "iteration: 163 loss: 0.21598554\n",
            "iteration: 164 loss: 0.19763988\n",
            "iteration: 165 loss: 0.06740195\n",
            "iteration: 166 loss: 0.03845555\n",
            "iteration: 167 loss: 0.12302407\n",
            "iteration: 168 loss: 0.12861717\n",
            "iteration: 169 loss: 0.03627503\n",
            "iteration: 170 loss: 0.05402857\n",
            "iteration: 171 loss: 0.34692562\n",
            "iteration: 172 loss: 0.09363893\n",
            "iteration: 173 loss: 0.12698519\n",
            "iteration: 174 loss: 0.09931327\n",
            "iteration: 175 loss: 0.11662668\n",
            "iteration: 176 loss: 0.04256224\n",
            "iteration: 177 loss: 0.03598874\n",
            "iteration: 178 loss: 0.08168934\n",
            "iteration: 179 loss: 0.08132004\n",
            "iteration: 180 loss: 0.02935943\n",
            "iteration: 181 loss: 0.14721398\n",
            "iteration: 182 loss: 0.04316666\n",
            "iteration: 183 loss: 0.03685020\n",
            "iteration: 184 loss: 0.22602546\n",
            "iteration: 185 loss: 0.03683527\n",
            "iteration: 186 loss: 0.03559710\n",
            "iteration: 187 loss: 0.04510614\n",
            "iteration: 188 loss: 0.03257861\n",
            "iteration: 189 loss: 0.10648558\n",
            "iteration: 190 loss: 0.21367393\n",
            "iteration: 191 loss: 0.03962244\n",
            "iteration: 192 loss: 0.03457604\n",
            "iteration: 193 loss: 0.02592634\n",
            "iteration: 194 loss: 0.08711617\n",
            "iteration: 195 loss: 0.11345326\n",
            "iteration: 196 loss: 0.02966883\n",
            "iteration: 197 loss: 0.46163487\n",
            "iteration: 198 loss: 0.09663586\n",
            "iteration: 199 loss: 0.07584698\n",
            "epoch:  76 mean loss training: 0.13099982\n",
            "epoch:  76 mean loss validation: 0.37011179\n",
            "iteration:   0 loss: 0.31570569\n",
            "iteration:   1 loss: 0.16169125\n",
            "iteration:   2 loss: 0.03840658\n",
            "iteration:   3 loss: 0.10776597\n",
            "iteration:   4 loss: 0.35456908\n",
            "iteration:   5 loss: 0.12104127\n",
            "iteration:   6 loss: 0.04497658\n",
            "iteration:   7 loss: 0.06770050\n",
            "iteration:   8 loss: 0.03173925\n",
            "iteration:   9 loss: 0.15283105\n",
            "iteration:  10 loss: 0.04560333\n",
            "iteration:  11 loss: 0.07449839\n",
            "iteration:  12 loss: 0.19811848\n",
            "iteration:  13 loss: 0.64494914\n",
            "iteration:  14 loss: 0.18297958\n",
            "iteration:  15 loss: 0.04294346\n",
            "iteration:  16 loss: 0.02722060\n",
            "iteration:  17 loss: 0.03310579\n",
            "iteration:  18 loss: 0.02019664\n",
            "iteration:  19 loss: 0.20195667\n",
            "iteration:  20 loss: 0.27929944\n",
            "iteration:  21 loss: 0.18511248\n",
            "iteration:  22 loss: 0.14617757\n",
            "iteration:  23 loss: 0.16251110\n",
            "iteration:  24 loss: 0.21394929\n",
            "iteration:  25 loss: 0.03804141\n",
            "iteration:  26 loss: 0.22127439\n",
            "iteration:  27 loss: 0.07722843\n",
            "iteration:  28 loss: 0.10467489\n",
            "iteration:  29 loss: 0.30435428\n",
            "iteration:  30 loss: 0.04878884\n",
            "iteration:  31 loss: 0.02434880\n",
            "iteration:  32 loss: 0.01360569\n",
            "iteration:  33 loss: 0.08213422\n",
            "iteration:  34 loss: 0.06422012\n",
            "iteration:  35 loss: 0.17442083\n",
            "iteration:  36 loss: 0.11888523\n",
            "iteration:  37 loss: 0.06732802\n",
            "iteration:  38 loss: 0.06311072\n",
            "iteration:  39 loss: 0.09284496\n",
            "iteration:  40 loss: 0.04726264\n",
            "iteration:  41 loss: 0.20570843\n",
            "iteration:  42 loss: 0.07002258\n",
            "iteration:  43 loss: 0.11608422\n",
            "iteration:  44 loss: 0.14312539\n",
            "iteration:  45 loss: 0.01082039\n",
            "iteration:  46 loss: 0.09087675\n",
            "iteration:  47 loss: 0.29684830\n",
            "iteration:  48 loss: 0.02081962\n",
            "iteration:  49 loss: 0.05032556\n",
            "iteration:  50 loss: 0.13811755\n",
            "iteration:  51 loss: 0.03809724\n",
            "iteration:  52 loss: 0.01448958\n",
            "iteration:  53 loss: 0.05071202\n",
            "iteration:  54 loss: 0.02697387\n",
            "iteration:  55 loss: 0.18127729\n",
            "iteration:  56 loss: 0.26072243\n",
            "iteration:  57 loss: 0.21269268\n",
            "iteration:  58 loss: 0.05736884\n",
            "iteration:  59 loss: 0.03919472\n",
            "iteration:  60 loss: 0.07616326\n",
            "iteration:  61 loss: 0.09610577\n",
            "iteration:  62 loss: 0.15540868\n",
            "iteration:  63 loss: 0.18863590\n",
            "iteration:  64 loss: 0.03716995\n",
            "iteration:  65 loss: 0.67950982\n",
            "iteration:  66 loss: 0.13299608\n",
            "iteration:  67 loss: 0.02638336\n",
            "iteration:  68 loss: 0.02017958\n",
            "iteration:  69 loss: 0.03052720\n",
            "iteration:  70 loss: 0.06159912\n",
            "iteration:  71 loss: 0.28912142\n",
            "iteration:  72 loss: 0.03961331\n",
            "iteration:  73 loss: 0.03270405\n",
            "iteration:  74 loss: 0.05741194\n",
            "iteration:  75 loss: 0.04101373\n",
            "iteration:  76 loss: 0.28128710\n",
            "iteration:  77 loss: 0.04988635\n",
            "iteration:  78 loss: 0.37723979\n",
            "iteration:  79 loss: 0.10554539\n",
            "iteration:  80 loss: 0.03505435\n",
            "iteration:  81 loss: 0.02628129\n",
            "iteration:  82 loss: 0.02329554\n",
            "iteration:  83 loss: 0.02402800\n",
            "iteration:  84 loss: 0.00490340\n",
            "iteration:  85 loss: 0.08386062\n",
            "iteration:  86 loss: 0.09935651\n",
            "iteration:  87 loss: 0.03541496\n",
            "iteration:  88 loss: 0.11872699\n",
            "iteration:  89 loss: 0.03904368\n",
            "iteration:  90 loss: 0.31493792\n",
            "iteration:  91 loss: 0.07591264\n",
            "iteration:  92 loss: 0.08240579\n",
            "iteration:  93 loss: 0.07707853\n",
            "iteration:  94 loss: 0.09132443\n",
            "iteration:  95 loss: 0.04998108\n",
            "iteration:  96 loss: 0.04917510\n",
            "iteration:  97 loss: 0.04690647\n",
            "iteration:  98 loss: 0.04394492\n",
            "iteration:  99 loss: 0.17668016\n",
            "iteration: 100 loss: 0.12480798\n",
            "iteration: 101 loss: 0.03956558\n",
            "iteration: 102 loss: 0.04347098\n",
            "iteration: 103 loss: 0.09491221\n",
            "iteration: 104 loss: 0.59637284\n",
            "iteration: 105 loss: 0.59715611\n",
            "iteration: 106 loss: 0.23784089\n",
            "iteration: 107 loss: 0.02565182\n",
            "iteration: 108 loss: 0.04935502\n",
            "iteration: 109 loss: 0.07743761\n",
            "iteration: 110 loss: 0.20750770\n",
            "iteration: 111 loss: 0.53795797\n",
            "iteration: 112 loss: 0.23170815\n",
            "iteration: 113 loss: 0.07235340\n",
            "iteration: 114 loss: 0.03314171\n",
            "iteration: 115 loss: 0.04111416\n",
            "iteration: 116 loss: 0.05433021\n",
            "iteration: 117 loss: 0.11154990\n",
            "iteration: 118 loss: 0.01428101\n",
            "iteration: 119 loss: 0.02604819\n",
            "iteration: 120 loss: 0.03389304\n",
            "iteration: 121 loss: 0.06594085\n",
            "iteration: 122 loss: 0.32121140\n",
            "iteration: 123 loss: 0.04355388\n",
            "iteration: 124 loss: 0.16357444\n",
            "iteration: 125 loss: 0.65311891\n",
            "iteration: 126 loss: 0.09993766\n",
            "iteration: 127 loss: 0.30940995\n",
            "iteration: 128 loss: 0.06432563\n",
            "iteration: 129 loss: 0.12268858\n",
            "iteration: 130 loss: 0.45422658\n",
            "iteration: 131 loss: 0.28715974\n",
            "iteration: 132 loss: 0.08862127\n",
            "iteration: 133 loss: 0.18799068\n",
            "iteration: 134 loss: 0.16200279\n",
            "iteration: 135 loss: 0.06908121\n",
            "iteration: 136 loss: 0.07990932\n",
            "iteration: 137 loss: 0.25391206\n",
            "iteration: 138 loss: 0.09747074\n",
            "iteration: 139 loss: 0.04462168\n",
            "iteration: 140 loss: 0.04569460\n",
            "iteration: 141 loss: 0.07890889\n",
            "iteration: 142 loss: 1.46154547\n",
            "iteration: 143 loss: 0.03717697\n",
            "iteration: 144 loss: 0.35418028\n",
            "iteration: 145 loss: 0.03786638\n",
            "iteration: 146 loss: 0.22918212\n",
            "iteration: 147 loss: 0.02967272\n",
            "iteration: 148 loss: 0.09901336\n",
            "iteration: 149 loss: 0.08602840\n",
            "iteration: 150 loss: 0.05553001\n",
            "iteration: 151 loss: 0.17853066\n",
            "iteration: 152 loss: 0.11770716\n",
            "iteration: 153 loss: 0.13790679\n",
            "iteration: 154 loss: 0.18081282\n",
            "iteration: 155 loss: 0.11513621\n",
            "iteration: 156 loss: 0.09266353\n",
            "iteration: 157 loss: 0.03045131\n",
            "iteration: 158 loss: 0.04267897\n",
            "iteration: 159 loss: 0.14552781\n",
            "iteration: 160 loss: 0.09352991\n",
            "iteration: 161 loss: 0.01336678\n",
            "iteration: 162 loss: 0.27746570\n",
            "iteration: 163 loss: 0.01449409\n",
            "iteration: 164 loss: 0.13297896\n",
            "iteration: 165 loss: 0.13674635\n",
            "iteration: 166 loss: 0.34099069\n",
            "iteration: 167 loss: 0.04835370\n",
            "iteration: 168 loss: 0.04351145\n",
            "iteration: 169 loss: 0.17782761\n",
            "iteration: 170 loss: 0.04505745\n",
            "iteration: 171 loss: 0.54220778\n",
            "iteration: 172 loss: 0.09836955\n",
            "iteration: 173 loss: 0.08713666\n",
            "iteration: 174 loss: 0.24118647\n",
            "iteration: 175 loss: 0.10599883\n",
            "iteration: 176 loss: 0.21193793\n",
            "iteration: 177 loss: 0.21840972\n",
            "iteration: 178 loss: 0.19574112\n",
            "iteration: 179 loss: 0.59686106\n",
            "iteration: 180 loss: 0.09165610\n",
            "iteration: 181 loss: 0.08620587\n",
            "iteration: 182 loss: 0.22828329\n",
            "iteration: 183 loss: 0.20059423\n",
            "iteration: 184 loss: 0.08097343\n",
            "iteration: 185 loss: 0.24271932\n",
            "iteration: 186 loss: 0.07186341\n",
            "iteration: 187 loss: 0.11140978\n",
            "iteration: 188 loss: 0.04595175\n",
            "iteration: 189 loss: 0.08278871\n",
            "iteration: 190 loss: 0.05481978\n",
            "iteration: 191 loss: 0.04218010\n",
            "iteration: 192 loss: 0.13797654\n",
            "iteration: 193 loss: 0.15681113\n",
            "iteration: 194 loss: 0.16470824\n",
            "iteration: 195 loss: 0.14396568\n",
            "iteration: 196 loss: 0.05279946\n",
            "iteration: 197 loss: 0.06143396\n",
            "iteration: 198 loss: 0.08404513\n",
            "iteration: 199 loss: 0.44800758\n",
            "epoch:  77 mean loss training: 0.14142729\n",
            "epoch:  77 mean loss validation: 0.37391576\n",
            "iteration:   0 loss: 0.13105251\n",
            "iteration:   1 loss: 0.09454164\n",
            "iteration:   2 loss: 0.07992838\n",
            "iteration:   3 loss: 0.16471432\n",
            "iteration:   4 loss: 0.09382762\n",
            "iteration:   5 loss: 0.04593442\n",
            "iteration:   6 loss: 0.08698368\n",
            "iteration:   7 loss: 0.08703030\n",
            "iteration:   8 loss: 0.05581824\n",
            "iteration:   9 loss: 0.40682346\n",
            "iteration:  10 loss: 0.03288975\n",
            "iteration:  11 loss: 0.04499081\n",
            "iteration:  12 loss: 0.12614177\n",
            "iteration:  13 loss: 0.14809100\n",
            "iteration:  14 loss: 0.11503175\n",
            "iteration:  15 loss: 0.07086901\n",
            "iteration:  16 loss: 0.02519298\n",
            "iteration:  17 loss: 0.09895325\n",
            "iteration:  18 loss: 0.06288556\n",
            "iteration:  19 loss: 0.13544960\n",
            "iteration:  20 loss: 0.01015182\n",
            "iteration:  21 loss: 0.06310574\n",
            "iteration:  22 loss: 0.06003731\n",
            "iteration:  23 loss: 0.10423223\n",
            "iteration:  24 loss: 0.02643888\n",
            "iteration:  25 loss: 0.08602537\n",
            "iteration:  26 loss: 0.14639343\n",
            "iteration:  27 loss: 0.07283998\n",
            "iteration:  28 loss: 0.12115283\n",
            "iteration:  29 loss: 0.06933377\n",
            "iteration:  30 loss: 0.09205516\n",
            "iteration:  31 loss: 0.03445745\n",
            "iteration:  32 loss: 0.16582124\n",
            "iteration:  33 loss: 0.13616070\n",
            "iteration:  34 loss: 0.08471439\n",
            "iteration:  35 loss: 0.01538670\n",
            "iteration:  36 loss: 0.09840861\n",
            "iteration:  37 loss: 0.08003491\n",
            "iteration:  38 loss: 0.02859195\n",
            "iteration:  39 loss: 0.44277552\n",
            "iteration:  40 loss: 0.06838904\n",
            "iteration:  41 loss: 0.07731736\n",
            "iteration:  42 loss: 0.18427230\n",
            "iteration:  43 loss: 0.03055401\n",
            "iteration:  44 loss: 0.36701083\n",
            "iteration:  45 loss: 0.02625990\n",
            "iteration:  46 loss: 0.06491683\n",
            "iteration:  47 loss: 0.04967508\n",
            "iteration:  48 loss: 0.07028057\n",
            "iteration:  49 loss: 0.17316926\n",
            "iteration:  50 loss: 0.30021423\n",
            "iteration:  51 loss: 0.56255656\n",
            "iteration:  52 loss: 0.15040968\n",
            "iteration:  53 loss: 0.10349438\n",
            "iteration:  54 loss: 0.14510301\n",
            "iteration:  55 loss: 0.04055946\n",
            "iteration:  56 loss: 0.05736630\n",
            "iteration:  57 loss: 0.09098456\n",
            "iteration:  58 loss: 0.05647227\n",
            "iteration:  59 loss: 0.48559475\n",
            "iteration:  60 loss: 0.09576668\n",
            "iteration:  61 loss: 0.09092604\n",
            "iteration:  62 loss: 0.29297858\n",
            "iteration:  63 loss: 0.03925779\n",
            "iteration:  64 loss: 0.10370781\n",
            "iteration:  65 loss: 0.36599344\n",
            "iteration:  66 loss: 0.04842544\n",
            "iteration:  67 loss: 0.17650899\n",
            "iteration:  68 loss: 0.34308690\n",
            "iteration:  69 loss: 0.09327585\n",
            "iteration:  70 loss: 0.10242665\n",
            "iteration:  71 loss: 0.04534484\n",
            "iteration:  72 loss: 0.26095054\n",
            "iteration:  73 loss: 0.43229404\n",
            "iteration:  74 loss: 1.16787779\n",
            "iteration:  75 loss: 0.07086828\n",
            "iteration:  76 loss: 0.12337598\n",
            "iteration:  77 loss: 0.04442560\n",
            "iteration:  78 loss: 0.29003039\n",
            "iteration:  79 loss: 0.08489315\n",
            "iteration:  80 loss: 0.11689164\n",
            "iteration:  81 loss: 0.05255452\n",
            "iteration:  82 loss: 0.11366837\n",
            "iteration:  83 loss: 0.05126516\n",
            "iteration:  84 loss: 0.04113012\n",
            "iteration:  85 loss: 0.10473443\n",
            "iteration:  86 loss: 0.63363218\n",
            "iteration:  87 loss: 0.07751665\n",
            "iteration:  88 loss: 0.25963950\n",
            "iteration:  89 loss: 0.11084332\n",
            "iteration:  90 loss: 0.12499596\n",
            "iteration:  91 loss: 0.08193377\n",
            "iteration:  92 loss: 0.18877445\n",
            "iteration:  93 loss: 0.34077227\n",
            "iteration:  94 loss: 0.08192960\n",
            "iteration:  95 loss: 0.11895605\n",
            "iteration:  96 loss: 0.09254611\n",
            "iteration:  97 loss: 0.33661321\n",
            "iteration:  98 loss: 0.06189076\n",
            "iteration:  99 loss: 0.10787228\n",
            "iteration: 100 loss: 0.16144000\n",
            "iteration: 101 loss: 0.19176531\n",
            "iteration: 102 loss: 0.08331881\n",
            "iteration: 103 loss: 0.13837416\n",
            "iteration: 104 loss: 0.06770100\n",
            "iteration: 105 loss: 0.04029866\n",
            "iteration: 106 loss: 0.05838146\n",
            "iteration: 107 loss: 0.33696130\n",
            "iteration: 108 loss: 0.08321865\n",
            "iteration: 109 loss: 0.24880803\n",
            "iteration: 110 loss: 0.03385829\n",
            "iteration: 111 loss: 0.15050569\n",
            "iteration: 112 loss: 0.28020528\n",
            "iteration: 113 loss: 0.07989109\n",
            "iteration: 114 loss: 0.06797475\n",
            "iteration: 115 loss: 0.08938706\n",
            "iteration: 116 loss: 0.10265210\n",
            "iteration: 117 loss: 0.05440422\n",
            "iteration: 118 loss: 0.04086315\n",
            "iteration: 119 loss: 0.02623456\n",
            "iteration: 120 loss: 0.06690456\n",
            "iteration: 121 loss: 0.15493323\n",
            "iteration: 122 loss: 0.10028881\n",
            "iteration: 123 loss: 0.20485018\n",
            "iteration: 124 loss: 0.10577241\n",
            "iteration: 125 loss: 0.14779620\n",
            "iteration: 126 loss: 0.36093584\n",
            "iteration: 127 loss: 0.11624337\n",
            "iteration: 128 loss: 0.14825696\n",
            "iteration: 129 loss: 0.10537288\n",
            "iteration: 130 loss: 0.28068158\n",
            "iteration: 131 loss: 0.10182083\n",
            "iteration: 132 loss: 0.46721900\n",
            "iteration: 133 loss: 0.24228576\n",
            "iteration: 134 loss: 0.07374052\n",
            "iteration: 135 loss: 0.05602134\n",
            "iteration: 136 loss: 0.02095461\n",
            "iteration: 137 loss: 0.02847222\n",
            "iteration: 138 loss: 0.08811504\n",
            "iteration: 139 loss: 0.08469043\n",
            "iteration: 140 loss: 0.08622517\n",
            "iteration: 141 loss: 0.04118268\n",
            "iteration: 142 loss: 0.10523292\n",
            "iteration: 143 loss: 0.05693019\n",
            "iteration: 144 loss: 0.21900453\n",
            "iteration: 145 loss: 0.02030583\n",
            "iteration: 146 loss: 0.12068567\n",
            "iteration: 147 loss: 0.11068881\n",
            "iteration: 148 loss: 0.39851865\n",
            "iteration: 149 loss: 0.03096473\n",
            "iteration: 150 loss: 0.03029315\n",
            "iteration: 151 loss: 0.03882002\n",
            "iteration: 152 loss: 0.08628345\n",
            "iteration: 153 loss: 0.37864572\n",
            "iteration: 154 loss: 0.03754492\n",
            "iteration: 155 loss: 0.09828738\n",
            "iteration: 156 loss: 0.30891791\n",
            "iteration: 157 loss: 0.06552882\n",
            "iteration: 158 loss: 0.19851397\n",
            "iteration: 159 loss: 0.06442286\n",
            "iteration: 160 loss: 0.12279226\n",
            "iteration: 161 loss: 0.14155106\n",
            "iteration: 162 loss: 0.05733247\n",
            "iteration: 163 loss: 0.23994839\n",
            "iteration: 164 loss: 0.07353698\n",
            "iteration: 165 loss: 0.43308747\n",
            "iteration: 166 loss: 0.32530957\n",
            "iteration: 167 loss: 0.08715376\n",
            "iteration: 168 loss: 0.39907902\n",
            "iteration: 169 loss: 0.24332863\n",
            "iteration: 170 loss: 0.04476861\n",
            "iteration: 171 loss: 0.12349812\n",
            "iteration: 172 loss: 0.18238921\n",
            "iteration: 173 loss: 0.02644852\n",
            "iteration: 174 loss: 0.06994468\n",
            "iteration: 175 loss: 0.35169563\n",
            "iteration: 176 loss: 0.02238549\n",
            "iteration: 177 loss: 0.06955541\n",
            "iteration: 178 loss: 0.19836727\n",
            "iteration: 179 loss: 0.06136896\n",
            "iteration: 180 loss: 0.06313333\n",
            "iteration: 181 loss: 0.32971716\n",
            "iteration: 182 loss: 0.02317341\n",
            "iteration: 183 loss: 0.05216548\n",
            "iteration: 184 loss: 0.02455636\n",
            "iteration: 185 loss: 0.26944095\n",
            "iteration: 186 loss: 0.08945460\n",
            "iteration: 187 loss: 0.08068528\n",
            "iteration: 188 loss: 0.06316929\n",
            "iteration: 189 loss: 0.04393310\n",
            "iteration: 190 loss: 0.06991117\n",
            "iteration: 191 loss: 0.03878408\n",
            "iteration: 192 loss: 0.11195754\n",
            "iteration: 193 loss: 0.07124155\n",
            "iteration: 194 loss: 0.08559831\n",
            "iteration: 195 loss: 0.01057865\n",
            "iteration: 196 loss: 0.09545191\n",
            "iteration: 197 loss: 0.10177112\n",
            "iteration: 198 loss: 0.22112398\n",
            "iteration: 199 loss: 0.07082263\n",
            "epoch:  78 mean loss training: 0.13749625\n",
            "epoch:  78 mean loss validation: 0.38366237\n",
            "iteration:   0 loss: 0.05241095\n",
            "iteration:   1 loss: 0.17638402\n",
            "iteration:   2 loss: 0.17566162\n",
            "iteration:   3 loss: 0.38377300\n",
            "iteration:   4 loss: 0.12831931\n",
            "iteration:   5 loss: 0.08159936\n",
            "iteration:   6 loss: 0.04443883\n",
            "iteration:   7 loss: 0.05557318\n",
            "iteration:   8 loss: 0.01809888\n",
            "iteration:   9 loss: 0.30573633\n",
            "iteration:  10 loss: 0.22347356\n",
            "iteration:  11 loss: 0.07994051\n",
            "iteration:  12 loss: 0.27286452\n",
            "iteration:  13 loss: 0.33089295\n",
            "iteration:  14 loss: 0.05705793\n",
            "iteration:  15 loss: 0.03076549\n",
            "iteration:  16 loss: 0.13752705\n",
            "iteration:  17 loss: 0.06866630\n",
            "iteration:  18 loss: 0.22962587\n",
            "iteration:  19 loss: 0.10084431\n",
            "iteration:  20 loss: 0.07716382\n",
            "iteration:  21 loss: 0.25058845\n",
            "iteration:  22 loss: 0.09041384\n",
            "iteration:  23 loss: 0.13161708\n",
            "iteration:  24 loss: 0.03601782\n",
            "iteration:  25 loss: 0.58180082\n",
            "iteration:  26 loss: 0.31627107\n",
            "iteration:  27 loss: 0.18453230\n",
            "iteration:  28 loss: 0.03319230\n",
            "iteration:  29 loss: 0.02242688\n",
            "iteration:  30 loss: 0.13163821\n",
            "iteration:  31 loss: 0.04248786\n",
            "iteration:  32 loss: 0.06646625\n",
            "iteration:  33 loss: 0.03596210\n",
            "iteration:  34 loss: 0.01972580\n",
            "iteration:  35 loss: 0.07288970\n",
            "iteration:  36 loss: 0.03125028\n",
            "iteration:  37 loss: 0.04632301\n",
            "iteration:  38 loss: 0.08081654\n",
            "iteration:  39 loss: 0.12242802\n",
            "iteration:  40 loss: 0.13712664\n",
            "iteration:  41 loss: 0.09120979\n",
            "iteration:  42 loss: 0.02590461\n",
            "iteration:  43 loss: 0.08191922\n",
            "iteration:  44 loss: 0.13472000\n",
            "iteration:  45 loss: 0.00964739\n",
            "iteration:  46 loss: 0.05024595\n",
            "iteration:  47 loss: 0.19043535\n",
            "iteration:  48 loss: 0.33234513\n",
            "iteration:  49 loss: 0.01988823\n",
            "iteration:  50 loss: 0.07697123\n",
            "iteration:  51 loss: 0.27118585\n",
            "iteration:  52 loss: 0.16343014\n",
            "iteration:  53 loss: 0.26478958\n",
            "iteration:  54 loss: 0.42760903\n",
            "iteration:  55 loss: 0.12050236\n",
            "iteration:  56 loss: 0.04326231\n",
            "iteration:  57 loss: 0.05008700\n",
            "iteration:  58 loss: 0.04991263\n",
            "iteration:  59 loss: 0.07849455\n",
            "iteration:  60 loss: 0.06519622\n",
            "iteration:  61 loss: 0.01939722\n",
            "iteration:  62 loss: 0.07212515\n",
            "iteration:  63 loss: 0.08251286\n",
            "iteration:  64 loss: 0.15688881\n",
            "iteration:  65 loss: 0.03943766\n",
            "iteration:  66 loss: 0.02281380\n",
            "iteration:  67 loss: 0.47636497\n",
            "iteration:  68 loss: 0.10863758\n",
            "iteration:  69 loss: 0.09914226\n",
            "iteration:  70 loss: 0.11802762\n",
            "iteration:  71 loss: 0.02756855\n",
            "iteration:  72 loss: 0.03479988\n",
            "iteration:  73 loss: 0.07195292\n",
            "iteration:  74 loss: 0.13434885\n",
            "iteration:  75 loss: 0.20536172\n",
            "iteration:  76 loss: 0.08264927\n",
            "iteration:  77 loss: 0.01919947\n",
            "iteration:  78 loss: 0.05072229\n",
            "iteration:  79 loss: 0.11567621\n",
            "iteration:  80 loss: 0.03578927\n",
            "iteration:  81 loss: 0.09927516\n",
            "iteration:  82 loss: 0.06392465\n",
            "iteration:  83 loss: 0.07434443\n",
            "iteration:  84 loss: 0.12299208\n",
            "iteration:  85 loss: 0.13131765\n",
            "iteration:  86 loss: 0.06639493\n",
            "iteration:  87 loss: 0.11662031\n",
            "iteration:  88 loss: 0.18774146\n",
            "iteration:  89 loss: 0.10233194\n",
            "iteration:  90 loss: 0.69002604\n",
            "iteration:  91 loss: 0.05767010\n",
            "iteration:  92 loss: 0.18719180\n",
            "iteration:  93 loss: 0.03938724\n",
            "iteration:  94 loss: 0.38636574\n",
            "iteration:  95 loss: 0.12540019\n",
            "iteration:  96 loss: 0.13231029\n",
            "iteration:  97 loss: 0.09370574\n",
            "iteration:  98 loss: 0.49300316\n",
            "iteration:  99 loss: 0.02115201\n",
            "iteration: 100 loss: 0.03385399\n",
            "iteration: 101 loss: 0.06331964\n",
            "iteration: 102 loss: 0.29198277\n",
            "iteration: 103 loss: 0.04912279\n",
            "iteration: 104 loss: 0.04138262\n",
            "iteration: 105 loss: 0.04369865\n",
            "iteration: 106 loss: 0.09871104\n",
            "iteration: 107 loss: 0.04565072\n",
            "iteration: 108 loss: 0.24656318\n",
            "iteration: 109 loss: 0.09335467\n",
            "iteration: 110 loss: 0.19534804\n",
            "iteration: 111 loss: 0.02247588\n",
            "iteration: 112 loss: 0.05997966\n",
            "iteration: 113 loss: 0.10309935\n",
            "iteration: 114 loss: 0.08307831\n",
            "iteration: 115 loss: 0.19464958\n",
            "iteration: 116 loss: 0.13854878\n",
            "iteration: 117 loss: 0.13109604\n",
            "iteration: 118 loss: 0.15092717\n",
            "iteration: 119 loss: 0.02462407\n",
            "iteration: 120 loss: 0.05025280\n",
            "iteration: 121 loss: 0.09392059\n",
            "iteration: 122 loss: 0.20305428\n",
            "iteration: 123 loss: 0.06567517\n",
            "iteration: 124 loss: 0.10598232\n",
            "iteration: 125 loss: 0.07095242\n",
            "iteration: 126 loss: 0.14220278\n",
            "iteration: 127 loss: 0.23992981\n",
            "iteration: 128 loss: 0.06666590\n",
            "iteration: 129 loss: 0.01735595\n",
            "iteration: 130 loss: 0.43889648\n",
            "iteration: 131 loss: 0.12438639\n",
            "iteration: 132 loss: 0.07708405\n",
            "iteration: 133 loss: 0.15169139\n",
            "iteration: 134 loss: 0.01638938\n",
            "iteration: 135 loss: 0.08760783\n",
            "iteration: 136 loss: 0.03475193\n",
            "iteration: 137 loss: 0.05477512\n",
            "iteration: 138 loss: 0.03842554\n",
            "iteration: 139 loss: 0.05121983\n",
            "iteration: 140 loss: 0.04589134\n",
            "iteration: 141 loss: 0.22215791\n",
            "iteration: 142 loss: 0.07088651\n",
            "iteration: 143 loss: 0.06642414\n",
            "iteration: 144 loss: 0.42133781\n",
            "iteration: 145 loss: 0.02497396\n",
            "iteration: 146 loss: 0.22559367\n",
            "iteration: 147 loss: 0.17659338\n",
            "iteration: 148 loss: 0.07192749\n",
            "iteration: 149 loss: 0.01410656\n",
            "iteration: 150 loss: 0.09415670\n",
            "iteration: 151 loss: 0.02515452\n",
            "iteration: 152 loss: 0.06495215\n",
            "iteration: 153 loss: 0.10269291\n",
            "iteration: 154 loss: 0.03108206\n",
            "iteration: 155 loss: 0.50233436\n",
            "iteration: 156 loss: 0.07271878\n",
            "iteration: 157 loss: 0.04941953\n",
            "iteration: 158 loss: 0.02782656\n",
            "iteration: 159 loss: 0.16288039\n",
            "iteration: 160 loss: 0.11423476\n",
            "iteration: 161 loss: 0.11189499\n",
            "iteration: 162 loss: 0.10867657\n",
            "iteration: 163 loss: 0.06276750\n",
            "iteration: 164 loss: 0.06971858\n",
            "iteration: 165 loss: 0.07552886\n",
            "iteration: 166 loss: 0.24179566\n",
            "iteration: 167 loss: 0.09270564\n",
            "iteration: 168 loss: 0.08362869\n",
            "iteration: 169 loss: 0.11217131\n",
            "iteration: 170 loss: 0.12020491\n",
            "iteration: 171 loss: 0.03325450\n",
            "iteration: 172 loss: 0.16268419\n",
            "iteration: 173 loss: 0.15271690\n",
            "iteration: 174 loss: 0.04741502\n",
            "iteration: 175 loss: 0.19710371\n",
            "iteration: 176 loss: 0.11053243\n",
            "iteration: 177 loss: 0.04098279\n",
            "iteration: 178 loss: 0.09844246\n",
            "iteration: 179 loss: 0.08350712\n",
            "iteration: 180 loss: 0.06685042\n",
            "iteration: 181 loss: 0.05928085\n",
            "iteration: 182 loss: 0.02080766\n",
            "iteration: 183 loss: 0.07300371\n",
            "iteration: 184 loss: 0.10499229\n",
            "iteration: 185 loss: 0.11143082\n",
            "iteration: 186 loss: 0.17306738\n",
            "iteration: 187 loss: 0.05438495\n",
            "iteration: 188 loss: 0.38139287\n",
            "iteration: 189 loss: 0.18988834\n",
            "iteration: 190 loss: 0.42362455\n",
            "iteration: 191 loss: 0.14537631\n",
            "iteration: 192 loss: 0.01638641\n",
            "iteration: 193 loss: 0.02274027\n",
            "iteration: 194 loss: 0.03166925\n",
            "iteration: 195 loss: 0.09408058\n",
            "iteration: 196 loss: 0.09228816\n",
            "iteration: 197 loss: 0.10143211\n",
            "iteration: 198 loss: 0.07946023\n",
            "iteration: 199 loss: 0.83320361\n",
            "epoch:  79 mean loss training: 0.12544122\n",
            "epoch:  79 mean loss validation: 0.39093488\n",
            "iteration:   0 loss: 0.05314103\n",
            "iteration:   1 loss: 0.15700509\n",
            "iteration:   2 loss: 0.10122553\n",
            "iteration:   3 loss: 0.12398519\n",
            "iteration:   4 loss: 0.07400789\n",
            "iteration:   5 loss: 0.50914264\n",
            "iteration:   6 loss: 0.16193473\n",
            "iteration:   7 loss: 0.02171353\n",
            "iteration:   8 loss: 0.09264255\n",
            "iteration:   9 loss: 0.05062282\n",
            "iteration:  10 loss: 0.03243398\n",
            "iteration:  11 loss: 0.04211266\n",
            "iteration:  12 loss: 0.16119871\n",
            "iteration:  13 loss: 0.12811814\n",
            "iteration:  14 loss: 0.02712498\n",
            "iteration:  15 loss: 0.09144404\n",
            "iteration:  16 loss: 0.08650056\n",
            "iteration:  17 loss: 0.08955803\n",
            "iteration:  18 loss: 0.17406467\n",
            "iteration:  19 loss: 0.09341136\n",
            "iteration:  20 loss: 0.23080699\n",
            "iteration:  21 loss: 0.01918572\n",
            "iteration:  22 loss: 0.12994340\n",
            "iteration:  23 loss: 0.40998662\n",
            "iteration:  24 loss: 0.06461548\n",
            "iteration:  25 loss: 0.04887316\n",
            "iteration:  26 loss: 0.01853205\n",
            "iteration:  27 loss: 0.02302388\n",
            "iteration:  28 loss: 0.32111612\n",
            "iteration:  29 loss: 0.23858656\n",
            "iteration:  30 loss: 0.19527976\n",
            "iteration:  31 loss: 0.34043634\n",
            "iteration:  32 loss: 0.03098831\n",
            "iteration:  33 loss: 0.08167686\n",
            "iteration:  34 loss: 0.22956946\n",
            "iteration:  35 loss: 0.00874172\n",
            "iteration:  36 loss: 0.14243634\n",
            "iteration:  37 loss: 0.02992628\n",
            "iteration:  38 loss: 0.03284644\n",
            "iteration:  39 loss: 0.01925644\n",
            "iteration:  40 loss: 0.08498438\n",
            "iteration:  41 loss: 0.15276241\n",
            "iteration:  42 loss: 0.08126441\n",
            "iteration:  43 loss: 0.38376543\n",
            "iteration:  44 loss: 0.09693091\n",
            "iteration:  45 loss: 0.03391337\n",
            "iteration:  46 loss: 0.05798781\n",
            "iteration:  47 loss: 0.04318768\n",
            "iteration:  48 loss: 0.52638364\n",
            "iteration:  49 loss: 0.06946245\n",
            "iteration:  50 loss: 0.01759229\n",
            "iteration:  51 loss: 0.12172610\n",
            "iteration:  52 loss: 0.04546665\n",
            "iteration:  53 loss: 0.59113652\n",
            "iteration:  54 loss: 0.03210620\n",
            "iteration:  55 loss: 0.06316145\n",
            "iteration:  56 loss: 0.13235934\n",
            "iteration:  57 loss: 0.48705703\n",
            "iteration:  58 loss: 0.09907140\n",
            "iteration:  59 loss: 0.15362257\n",
            "iteration:  60 loss: 0.08814052\n",
            "iteration:  61 loss: 0.07605791\n",
            "iteration:  62 loss: 0.02460455\n",
            "iteration:  63 loss: 0.06818003\n",
            "iteration:  64 loss: 0.12249368\n",
            "iteration:  65 loss: 0.05159168\n",
            "iteration:  66 loss: 0.03150470\n",
            "iteration:  67 loss: 0.47309652\n",
            "iteration:  68 loss: 0.12139878\n",
            "iteration:  69 loss: 0.24181890\n",
            "iteration:  70 loss: 0.12119358\n",
            "iteration:  71 loss: 0.04674990\n",
            "iteration:  72 loss: 0.17892446\n",
            "iteration:  73 loss: 0.52745569\n",
            "iteration:  74 loss: 0.16286121\n",
            "iteration:  75 loss: 0.25011182\n",
            "iteration:  76 loss: 0.06794318\n",
            "iteration:  77 loss: 0.08201451\n",
            "iteration:  78 loss: 0.06836775\n",
            "iteration:  79 loss: 0.05333635\n",
            "iteration:  80 loss: 0.44112706\n",
            "iteration:  81 loss: 0.02679391\n",
            "iteration:  82 loss: 0.07048035\n",
            "iteration:  83 loss: 0.17842904\n",
            "iteration:  84 loss: 0.17291644\n",
            "iteration:  85 loss: 0.40507627\n",
            "iteration:  86 loss: 0.03437613\n",
            "iteration:  87 loss: 0.07339358\n",
            "iteration:  88 loss: 0.03924998\n",
            "iteration:  89 loss: 0.05754332\n",
            "iteration:  90 loss: 0.24193411\n",
            "iteration:  91 loss: 0.03674626\n",
            "iteration:  92 loss: 0.11941034\n",
            "iteration:  93 loss: 0.05753756\n",
            "iteration:  94 loss: 0.05744946\n",
            "iteration:  95 loss: 0.12041729\n",
            "iteration:  96 loss: 0.09445508\n",
            "iteration:  97 loss: 0.14415081\n",
            "iteration:  98 loss: 0.02661083\n",
            "iteration:  99 loss: 0.13392706\n",
            "iteration: 100 loss: 0.06551210\n",
            "iteration: 101 loss: 0.01630673\n",
            "iteration: 102 loss: 0.35918993\n",
            "iteration: 103 loss: 0.18895923\n",
            "iteration: 104 loss: 0.11706463\n",
            "iteration: 105 loss: 0.11934470\n",
            "iteration: 106 loss: 0.10641958\n",
            "iteration: 107 loss: 0.01306891\n",
            "iteration: 108 loss: 0.06810131\n",
            "iteration: 109 loss: 0.03591375\n",
            "iteration: 110 loss: 0.12377277\n",
            "iteration: 111 loss: 0.17580748\n",
            "iteration: 112 loss: 0.02569982\n",
            "iteration: 113 loss: 0.04610727\n",
            "iteration: 114 loss: 0.60465652\n",
            "iteration: 115 loss: 0.28300518\n",
            "iteration: 116 loss: 1.07005811\n",
            "iteration: 117 loss: 0.54066038\n",
            "iteration: 118 loss: 0.20325080\n",
            "iteration: 119 loss: 0.57431239\n",
            "iteration: 120 loss: 0.18978122\n",
            "iteration: 121 loss: 0.43168792\n",
            "iteration: 122 loss: 0.01625407\n",
            "iteration: 123 loss: 0.07715835\n",
            "iteration: 124 loss: 0.13474281\n",
            "iteration: 125 loss: 0.15749532\n",
            "iteration: 126 loss: 0.11692130\n",
            "iteration: 127 loss: 0.01977117\n",
            "iteration: 128 loss: 0.10688429\n",
            "iteration: 129 loss: 0.05818247\n",
            "iteration: 130 loss: 0.08397456\n",
            "iteration: 131 loss: 0.07443837\n",
            "iteration: 132 loss: 0.27853963\n",
            "iteration: 133 loss: 0.02622129\n",
            "iteration: 134 loss: 0.06382244\n",
            "iteration: 135 loss: 0.03905092\n",
            "iteration: 136 loss: 0.31084067\n",
            "iteration: 137 loss: 0.33415890\n",
            "iteration: 138 loss: 0.05326911\n",
            "iteration: 139 loss: 0.08949038\n",
            "iteration: 140 loss: 0.02114483\n",
            "iteration: 141 loss: 0.02530154\n",
            "iteration: 142 loss: 0.12991345\n",
            "iteration: 143 loss: 0.05591582\n",
            "iteration: 144 loss: 0.07908629\n",
            "iteration: 145 loss: 0.14046088\n",
            "iteration: 146 loss: 0.30302516\n",
            "iteration: 147 loss: 0.06065655\n",
            "iteration: 148 loss: 0.30323157\n",
            "iteration: 149 loss: 0.02534449\n",
            "iteration: 150 loss: 0.05024939\n",
            "iteration: 151 loss: 0.12222081\n",
            "iteration: 152 loss: 0.20540898\n",
            "iteration: 153 loss: 0.23716210\n",
            "iteration: 154 loss: 0.02452219\n",
            "iteration: 155 loss: 0.11219142\n",
            "iteration: 156 loss: 0.13650225\n",
            "iteration: 157 loss: 0.16387394\n",
            "iteration: 158 loss: 0.19282751\n",
            "iteration: 159 loss: 0.29903844\n",
            "iteration: 160 loss: 0.08950443\n",
            "iteration: 161 loss: 0.07188179\n",
            "iteration: 162 loss: 0.36015138\n",
            "iteration: 163 loss: 0.03540570\n",
            "iteration: 164 loss: 0.08013636\n",
            "iteration: 165 loss: 0.16401353\n",
            "iteration: 166 loss: 0.08490460\n",
            "iteration: 167 loss: 0.13656718\n",
            "iteration: 168 loss: 0.01930666\n",
            "iteration: 169 loss: 0.09797087\n",
            "iteration: 170 loss: 0.07784508\n",
            "iteration: 171 loss: 0.04416328\n",
            "iteration: 172 loss: 0.12311263\n",
            "iteration: 173 loss: 0.31054771\n",
            "iteration: 174 loss: 0.54852682\n",
            "iteration: 175 loss: 0.02522380\n",
            "iteration: 176 loss: 0.08079365\n",
            "iteration: 177 loss: 0.11825652\n",
            "iteration: 178 loss: 0.13222785\n",
            "iteration: 179 loss: 0.05534081\n",
            "iteration: 180 loss: 0.03521076\n",
            "iteration: 181 loss: 0.35721722\n",
            "iteration: 182 loss: 0.02607445\n",
            "iteration: 183 loss: 0.06169861\n",
            "iteration: 184 loss: 0.02618055\n",
            "iteration: 185 loss: 0.02830547\n",
            "iteration: 186 loss: 0.08833668\n",
            "iteration: 187 loss: 0.05301793\n",
            "iteration: 188 loss: 0.26601487\n",
            "iteration: 189 loss: 0.03002534\n",
            "iteration: 190 loss: 0.06444444\n",
            "iteration: 191 loss: 0.08070511\n",
            "iteration: 192 loss: 0.03397014\n",
            "iteration: 193 loss: 0.05301278\n",
            "iteration: 194 loss: 0.06086421\n",
            "iteration: 195 loss: 0.42432618\n",
            "iteration: 196 loss: 0.02129432\n",
            "iteration: 197 loss: 0.01897279\n",
            "iteration: 198 loss: 0.01685043\n",
            "iteration: 199 loss: 0.09150891\n",
            "epoch:  80 mean loss training: 0.14066458\n",
            "epoch:  80 mean loss validation: 0.35754108\n",
            "iteration:   0 loss: 0.13192756\n",
            "iteration:   1 loss: 0.04627772\n",
            "iteration:   2 loss: 0.70965570\n",
            "iteration:   3 loss: 0.05378599\n",
            "iteration:   4 loss: 0.06306297\n",
            "iteration:   5 loss: 0.15882543\n",
            "iteration:   6 loss: 0.09337255\n",
            "iteration:   7 loss: 0.04899207\n",
            "iteration:   8 loss: 0.04942208\n",
            "iteration:   9 loss: 0.37636894\n",
            "iteration:  10 loss: 0.11750492\n",
            "iteration:  11 loss: 0.10607622\n",
            "iteration:  12 loss: 0.02241025\n",
            "iteration:  13 loss: 0.14948405\n",
            "iteration:  14 loss: 0.90054762\n",
            "iteration:  15 loss: 0.02480247\n",
            "iteration:  16 loss: 0.12155739\n",
            "iteration:  17 loss: 0.13197421\n",
            "iteration:  18 loss: 0.03076708\n",
            "iteration:  19 loss: 0.20116559\n",
            "iteration:  20 loss: 0.54558921\n",
            "iteration:  21 loss: 0.12993930\n",
            "iteration:  22 loss: 0.14909372\n",
            "iteration:  23 loss: 0.22599554\n",
            "iteration:  24 loss: 0.05952812\n",
            "iteration:  25 loss: 0.03051693\n",
            "iteration:  26 loss: 0.11179668\n",
            "iteration:  27 loss: 0.10370637\n",
            "iteration:  28 loss: 0.20681262\n",
            "iteration:  29 loss: 0.04968560\n",
            "iteration:  30 loss: 0.55973077\n",
            "iteration:  31 loss: 0.08116990\n",
            "iteration:  32 loss: 0.03410271\n",
            "iteration:  33 loss: 0.15963168\n",
            "iteration:  34 loss: 0.11454337\n",
            "iteration:  35 loss: 0.37164384\n",
            "iteration:  36 loss: 0.08555736\n",
            "iteration:  37 loss: 0.06261615\n",
            "iteration:  38 loss: 0.04975457\n",
            "iteration:  39 loss: 0.05770478\n",
            "iteration:  40 loss: 0.10974137\n",
            "iteration:  41 loss: 0.18846054\n",
            "iteration:  42 loss: 0.02769233\n",
            "iteration:  43 loss: 0.04754544\n",
            "iteration:  44 loss: 0.05128422\n",
            "iteration:  45 loss: 0.03189504\n",
            "iteration:  46 loss: 0.01234444\n",
            "iteration:  47 loss: 0.02373856\n",
            "iteration:  48 loss: 0.13208540\n",
            "iteration:  49 loss: 0.06368504\n",
            "iteration:  50 loss: 0.02832780\n",
            "iteration:  51 loss: 0.06979344\n",
            "iteration:  52 loss: 0.03891161\n",
            "iteration:  53 loss: 0.04737668\n",
            "iteration:  54 loss: 0.04461409\n",
            "iteration:  55 loss: 0.16985826\n",
            "iteration:  56 loss: 0.07049681\n",
            "iteration:  57 loss: 0.12599151\n",
            "iteration:  58 loss: 0.18386611\n",
            "iteration:  59 loss: 0.07074302\n",
            "iteration:  60 loss: 0.22247159\n",
            "iteration:  61 loss: 0.05583185\n",
            "iteration:  62 loss: 0.22592176\n",
            "iteration:  63 loss: 0.13374731\n",
            "iteration:  64 loss: 0.09239520\n",
            "iteration:  65 loss: 0.10087760\n",
            "iteration:  66 loss: 0.08246330\n",
            "iteration:  67 loss: 0.02982550\n",
            "iteration:  68 loss: 0.06478136\n",
            "iteration:  69 loss: 0.02282067\n",
            "iteration:  70 loss: 0.14551063\n",
            "iteration:  71 loss: 0.03121582\n",
            "iteration:  72 loss: 0.24400231\n",
            "iteration:  73 loss: 0.03391811\n",
            "iteration:  74 loss: 1.00737512\n",
            "iteration:  75 loss: 0.04676326\n",
            "iteration:  76 loss: 0.07714710\n",
            "iteration:  77 loss: 0.12617747\n",
            "iteration:  78 loss: 0.21316826\n",
            "iteration:  79 loss: 0.08205829\n",
            "iteration:  80 loss: 0.06237397\n",
            "iteration:  81 loss: 0.03196593\n",
            "iteration:  82 loss: 0.17929514\n",
            "iteration:  83 loss: 0.16137615\n",
            "iteration:  84 loss: 0.06664855\n",
            "iteration:  85 loss: 0.18152779\n",
            "iteration:  86 loss: 0.02208710\n",
            "iteration:  87 loss: 0.13094054\n",
            "iteration:  88 loss: 0.01596756\n",
            "iteration:  89 loss: 0.14088272\n",
            "iteration:  90 loss: 0.12294526\n",
            "iteration:  91 loss: 0.12935193\n",
            "iteration:  92 loss: 0.18434152\n",
            "iteration:  93 loss: 0.09964072\n",
            "iteration:  94 loss: 0.13595429\n",
            "iteration:  95 loss: 0.09113324\n",
            "iteration:  96 loss: 0.04869830\n",
            "iteration:  97 loss: 0.17138495\n",
            "iteration:  98 loss: 0.02017959\n",
            "iteration:  99 loss: 0.13071685\n",
            "iteration: 100 loss: 0.03210562\n",
            "iteration: 101 loss: 0.66135859\n",
            "iteration: 102 loss: 0.23132324\n",
            "iteration: 103 loss: 0.07100356\n",
            "iteration: 104 loss: 0.10029707\n",
            "iteration: 105 loss: 0.12972274\n",
            "iteration: 106 loss: 0.08395105\n",
            "iteration: 107 loss: 0.08105522\n",
            "iteration: 108 loss: 0.13831873\n",
            "iteration: 109 loss: 0.03941191\n",
            "iteration: 110 loss: 0.04579500\n",
            "iteration: 111 loss: 0.06971098\n",
            "iteration: 112 loss: 0.03561083\n",
            "iteration: 113 loss: 0.06091021\n",
            "iteration: 114 loss: 0.03251822\n",
            "iteration: 115 loss: 0.12340987\n",
            "iteration: 116 loss: 0.12008633\n",
            "iteration: 117 loss: 0.06411919\n",
            "iteration: 118 loss: 0.09462986\n",
            "iteration: 119 loss: 0.07630370\n",
            "iteration: 120 loss: 0.09559856\n",
            "iteration: 121 loss: 0.05492741\n",
            "iteration: 122 loss: 0.60738152\n",
            "iteration: 123 loss: 0.03894620\n",
            "iteration: 124 loss: 0.04574962\n",
            "iteration: 125 loss: 0.02633681\n",
            "iteration: 126 loss: 0.07925357\n",
            "iteration: 127 loss: 0.04912074\n",
            "iteration: 128 loss: 0.12944588\n",
            "iteration: 129 loss: 0.04794952\n",
            "iteration: 130 loss: 0.07762353\n",
            "iteration: 131 loss: 0.35290536\n",
            "iteration: 132 loss: 0.07031973\n",
            "iteration: 133 loss: 0.03955070\n",
            "iteration: 134 loss: 0.09641011\n",
            "iteration: 135 loss: 0.07416624\n",
            "iteration: 136 loss: 0.05339806\n",
            "iteration: 137 loss: 0.54832429\n",
            "iteration: 138 loss: 0.68502551\n",
            "iteration: 139 loss: 0.02019971\n",
            "iteration: 140 loss: 0.79683739\n",
            "iteration: 141 loss: 0.47455883\n",
            "iteration: 142 loss: 0.06651030\n",
            "iteration: 143 loss: 0.03747571\n",
            "iteration: 144 loss: 0.06107828\n",
            "iteration: 145 loss: 0.03941666\n",
            "iteration: 146 loss: 0.05996603\n",
            "iteration: 147 loss: 0.03687936\n",
            "iteration: 148 loss: 0.06255073\n",
            "iteration: 149 loss: 0.18468705\n",
            "iteration: 150 loss: 0.36219960\n",
            "iteration: 151 loss: 0.32244721\n",
            "iteration: 152 loss: 0.03688632\n",
            "iteration: 153 loss: 0.34704962\n",
            "iteration: 154 loss: 0.04102786\n",
            "iteration: 155 loss: 0.03093147\n",
            "iteration: 156 loss: 0.20416574\n",
            "iteration: 157 loss: 0.11121348\n",
            "iteration: 158 loss: 0.05550419\n",
            "iteration: 159 loss: 0.06108586\n",
            "iteration: 160 loss: 0.04364994\n",
            "iteration: 161 loss: 0.16599604\n",
            "iteration: 162 loss: 0.01497565\n",
            "iteration: 163 loss: 0.13134083\n",
            "iteration: 164 loss: 0.15757269\n",
            "iteration: 165 loss: 0.04010040\n",
            "iteration: 166 loss: 0.02878130\n",
            "iteration: 167 loss: 0.62177753\n",
            "iteration: 168 loss: 0.07039624\n",
            "iteration: 169 loss: 0.06926643\n",
            "iteration: 170 loss: 0.03552708\n",
            "iteration: 171 loss: 0.05544686\n",
            "iteration: 172 loss: 0.10053991\n",
            "iteration: 173 loss: 0.04824645\n",
            "iteration: 174 loss: 0.26466313\n",
            "iteration: 175 loss: 0.12545228\n",
            "iteration: 176 loss: 0.04869013\n",
            "iteration: 177 loss: 0.02938805\n",
            "iteration: 178 loss: 0.21675028\n",
            "iteration: 179 loss: 0.09491052\n",
            "iteration: 180 loss: 0.30290461\n",
            "iteration: 181 loss: 0.13980223\n",
            "iteration: 182 loss: 0.05344595\n",
            "iteration: 183 loss: 0.08113765\n",
            "iteration: 184 loss: 0.06388113\n",
            "iteration: 185 loss: 0.18781218\n",
            "iteration: 186 loss: 0.03428225\n",
            "iteration: 187 loss: 0.10661599\n",
            "iteration: 188 loss: 0.06038595\n",
            "iteration: 189 loss: 0.08885378\n",
            "iteration: 190 loss: 0.02200489\n",
            "iteration: 191 loss: 0.08461289\n",
            "iteration: 192 loss: 0.06097959\n",
            "iteration: 193 loss: 0.47232828\n",
            "iteration: 194 loss: 0.04948423\n",
            "iteration: 195 loss: 0.03045334\n",
            "iteration: 196 loss: 0.04025847\n",
            "iteration: 197 loss: 0.05510607\n",
            "iteration: 198 loss: 0.08946357\n",
            "iteration: 199 loss: 0.04843925\n",
            "epoch:  81 mean loss training: 0.13371955\n",
            "epoch:  81 mean loss validation: 0.37681606\n",
            "iteration:   0 loss: 0.20416273\n",
            "iteration:   1 loss: 0.03400838\n",
            "iteration:   2 loss: 0.26372558\n",
            "iteration:   3 loss: 0.35082567\n",
            "iteration:   4 loss: 0.81269729\n",
            "iteration:   5 loss: 0.13462301\n",
            "iteration:   6 loss: 0.13644998\n",
            "iteration:   7 loss: 0.03764525\n",
            "iteration:   8 loss: 0.02888264\n",
            "iteration:   9 loss: 0.06991647\n",
            "iteration:  10 loss: 0.07745965\n",
            "iteration:  11 loss: 0.11040773\n",
            "iteration:  12 loss: 0.02906065\n",
            "iteration:  13 loss: 0.08928078\n",
            "iteration:  14 loss: 0.06021293\n",
            "iteration:  15 loss: 0.30938762\n",
            "iteration:  16 loss: 0.42236042\n",
            "iteration:  17 loss: 0.05740150\n",
            "iteration:  18 loss: 0.01326692\n",
            "iteration:  19 loss: 0.01570235\n",
            "iteration:  20 loss: 0.04701059\n",
            "iteration:  21 loss: 0.07719005\n",
            "iteration:  22 loss: 0.06599049\n",
            "iteration:  23 loss: 0.07939800\n",
            "iteration:  24 loss: 0.05642029\n",
            "iteration:  25 loss: 0.23414080\n",
            "iteration:  26 loss: 0.06028409\n",
            "iteration:  27 loss: 0.19758958\n",
            "iteration:  28 loss: 0.12555002\n",
            "iteration:  29 loss: 0.05363154\n",
            "iteration:  30 loss: 0.15855208\n",
            "iteration:  31 loss: 0.13493900\n",
            "iteration:  32 loss: 0.07070708\n",
            "iteration:  33 loss: 0.08483858\n",
            "iteration:  34 loss: 0.07900347\n",
            "iteration:  35 loss: 0.02447537\n",
            "iteration:  36 loss: 0.03800058\n",
            "iteration:  37 loss: 0.10298891\n",
            "iteration:  38 loss: 0.16048884\n",
            "iteration:  39 loss: 0.01367626\n",
            "iteration:  40 loss: 0.24120122\n",
            "iteration:  41 loss: 0.16251993\n",
            "iteration:  42 loss: 0.04243283\n",
            "iteration:  43 loss: 0.03352906\n",
            "iteration:  44 loss: 0.08478272\n",
            "iteration:  45 loss: 0.07555673\n",
            "iteration:  46 loss: 0.14277090\n",
            "iteration:  47 loss: 0.05357283\n",
            "iteration:  48 loss: 0.04517052\n",
            "iteration:  49 loss: 0.02471257\n",
            "iteration:  50 loss: 0.26898548\n",
            "iteration:  51 loss: 0.28413320\n",
            "iteration:  52 loss: 0.10544992\n",
            "iteration:  53 loss: 0.06782465\n",
            "iteration:  54 loss: 0.18715389\n",
            "iteration:  55 loss: 0.11061031\n",
            "iteration:  56 loss: 0.09728531\n",
            "iteration:  57 loss: 0.05322738\n",
            "iteration:  58 loss: 0.12616700\n",
            "iteration:  59 loss: 0.17235056\n",
            "iteration:  60 loss: 0.58576566\n",
            "iteration:  61 loss: 0.29966891\n",
            "iteration:  62 loss: 0.02636409\n",
            "iteration:  63 loss: 0.02128872\n",
            "iteration:  64 loss: 0.05306705\n",
            "iteration:  65 loss: 0.03323632\n",
            "iteration:  66 loss: 0.01388270\n",
            "iteration:  67 loss: 0.05762848\n",
            "iteration:  68 loss: 0.20465268\n",
            "iteration:  69 loss: 0.12828530\n",
            "iteration:  70 loss: 0.59102911\n",
            "iteration:  71 loss: 0.04659162\n",
            "iteration:  72 loss: 0.07919538\n",
            "iteration:  73 loss: 0.11773577\n",
            "iteration:  74 loss: 0.05615629\n",
            "iteration:  75 loss: 0.14034893\n",
            "iteration:  76 loss: 0.12679705\n",
            "iteration:  77 loss: 0.06011491\n",
            "iteration:  78 loss: 0.25089419\n",
            "iteration:  79 loss: 0.13712384\n",
            "iteration:  80 loss: 0.09873606\n",
            "iteration:  81 loss: 0.03606914\n",
            "iteration:  82 loss: 0.03046857\n",
            "iteration:  83 loss: 0.03171455\n",
            "iteration:  84 loss: 0.11146723\n",
            "iteration:  85 loss: 0.34152508\n",
            "iteration:  86 loss: 0.03121175\n",
            "iteration:  87 loss: 0.08826883\n",
            "iteration:  88 loss: 0.16287442\n",
            "iteration:  89 loss: 0.05203158\n",
            "iteration:  90 loss: 0.10473763\n",
            "iteration:  91 loss: 0.16646320\n",
            "iteration:  92 loss: 0.15881081\n",
            "iteration:  93 loss: 0.05485779\n",
            "iteration:  94 loss: 0.09070604\n",
            "iteration:  95 loss: 0.06012738\n",
            "iteration:  96 loss: 0.19081479\n",
            "iteration:  97 loss: 0.16042143\n",
            "iteration:  98 loss: 0.07135454\n",
            "iteration:  99 loss: 0.06959522\n",
            "iteration: 100 loss: 0.10927075\n",
            "iteration: 101 loss: 0.02178650\n",
            "iteration: 102 loss: 0.09082742\n",
            "iteration: 103 loss: 0.11590076\n",
            "iteration: 104 loss: 0.15622899\n",
            "iteration: 105 loss: 0.02778058\n",
            "iteration: 106 loss: 0.04895294\n",
            "iteration: 107 loss: 0.16851616\n",
            "iteration: 108 loss: 0.06708913\n",
            "iteration: 109 loss: 0.07497229\n",
            "iteration: 110 loss: 0.11051561\n",
            "iteration: 111 loss: 0.06719112\n",
            "iteration: 112 loss: 0.14105166\n",
            "iteration: 113 loss: 0.06975202\n",
            "iteration: 114 loss: 0.16906515\n",
            "iteration: 115 loss: 0.18933049\n",
            "iteration: 116 loss: 0.12807752\n",
            "iteration: 117 loss: 0.03713696\n",
            "iteration: 118 loss: 0.05769293\n",
            "iteration: 119 loss: 0.46343443\n",
            "iteration: 120 loss: 0.00998061\n",
            "iteration: 121 loss: 0.25458184\n",
            "iteration: 122 loss: 0.04507771\n",
            "iteration: 123 loss: 0.04506431\n",
            "iteration: 124 loss: 0.10148338\n",
            "iteration: 125 loss: 0.18531884\n",
            "iteration: 126 loss: 0.08388638\n",
            "iteration: 127 loss: 0.11463697\n",
            "iteration: 128 loss: 0.04728209\n",
            "iteration: 129 loss: 0.24493803\n",
            "iteration: 130 loss: 0.12835862\n",
            "iteration: 131 loss: 0.37761986\n",
            "iteration: 132 loss: 0.21816446\n",
            "iteration: 133 loss: 0.06562491\n",
            "iteration: 134 loss: 0.04289931\n",
            "iteration: 135 loss: 0.08584113\n",
            "iteration: 136 loss: 0.16141170\n",
            "iteration: 137 loss: 0.03518442\n",
            "iteration: 138 loss: 0.49590930\n",
            "iteration: 139 loss: 0.20979579\n",
            "iteration: 140 loss: 0.08917733\n",
            "iteration: 141 loss: 0.03054032\n",
            "iteration: 142 loss: 0.14269871\n",
            "iteration: 143 loss: 0.10265288\n",
            "iteration: 144 loss: 0.07920279\n",
            "iteration: 145 loss: 0.18432435\n",
            "iteration: 146 loss: 0.04665933\n",
            "iteration: 147 loss: 0.06236996\n",
            "iteration: 148 loss: 0.25556016\n",
            "iteration: 149 loss: 0.23533198\n",
            "iteration: 150 loss: 0.11577620\n",
            "iteration: 151 loss: 0.04641260\n",
            "iteration: 152 loss: 0.02404010\n",
            "iteration: 153 loss: 0.08263197\n",
            "iteration: 154 loss: 0.10353769\n",
            "iteration: 155 loss: 0.09475183\n",
            "iteration: 156 loss: 0.12070741\n",
            "iteration: 157 loss: 0.06872835\n",
            "iteration: 158 loss: 0.04989100\n",
            "iteration: 159 loss: 0.05128412\n",
            "iteration: 160 loss: 0.56185722\n",
            "iteration: 161 loss: 0.05293852\n",
            "iteration: 162 loss: 0.06414314\n",
            "iteration: 163 loss: 0.17952996\n",
            "iteration: 164 loss: 0.02294059\n",
            "iteration: 165 loss: 0.12543102\n",
            "iteration: 166 loss: 0.03595766\n",
            "iteration: 167 loss: 0.15199831\n",
            "iteration: 168 loss: 0.02165044\n",
            "iteration: 169 loss: 0.05366659\n",
            "iteration: 170 loss: 0.05525844\n",
            "iteration: 171 loss: 0.02128443\n",
            "iteration: 172 loss: 0.29883891\n",
            "iteration: 173 loss: 0.53533477\n",
            "iteration: 174 loss: 0.05398726\n",
            "iteration: 175 loss: 0.03709619\n",
            "iteration: 176 loss: 0.06415966\n",
            "iteration: 177 loss: 0.22155528\n",
            "iteration: 178 loss: 0.03568241\n",
            "iteration: 179 loss: 0.03133547\n",
            "iteration: 180 loss: 0.05910464\n",
            "iteration: 181 loss: 0.05818884\n",
            "iteration: 182 loss: 0.19624580\n",
            "iteration: 183 loss: 0.67798197\n",
            "iteration: 184 loss: 0.04420410\n",
            "iteration: 185 loss: 0.01906535\n",
            "iteration: 186 loss: 0.07219240\n",
            "iteration: 187 loss: 0.27501142\n",
            "iteration: 188 loss: 0.34967154\n",
            "iteration: 189 loss: 0.14020683\n",
            "iteration: 190 loss: 0.10984791\n",
            "iteration: 191 loss: 0.04930504\n",
            "iteration: 192 loss: 0.14025439\n",
            "iteration: 193 loss: 0.08129559\n",
            "iteration: 194 loss: 0.20910707\n",
            "iteration: 195 loss: 0.59178829\n",
            "iteration: 196 loss: 0.03703823\n",
            "iteration: 197 loss: 0.54782474\n",
            "iteration: 198 loss: 0.18664652\n",
            "iteration: 199 loss: 0.09517556\n",
            "epoch:  82 mean loss training: 0.13231294\n",
            "epoch:  82 mean loss validation: 0.36649862\n",
            "iteration:   0 loss: 0.12136938\n",
            "iteration:   1 loss: 0.01934182\n",
            "iteration:   2 loss: 0.02835536\n",
            "iteration:   3 loss: 0.03784212\n",
            "iteration:   4 loss: 0.02832552\n",
            "iteration:   5 loss: 0.24390788\n",
            "iteration:   6 loss: 0.05737768\n",
            "iteration:   7 loss: 0.46991831\n",
            "iteration:   8 loss: 0.10494999\n",
            "iteration:   9 loss: 0.00804446\n",
            "iteration:  10 loss: 0.05902108\n",
            "iteration:  11 loss: 0.08337189\n",
            "iteration:  12 loss: 0.06888054\n",
            "iteration:  13 loss: 0.06553908\n",
            "iteration:  14 loss: 0.08648872\n",
            "iteration:  15 loss: 0.16100611\n",
            "iteration:  16 loss: 0.03810230\n",
            "iteration:  17 loss: 0.25774834\n",
            "iteration:  18 loss: 0.04049579\n",
            "iteration:  19 loss: 0.61006129\n",
            "iteration:  20 loss: 0.05394202\n",
            "iteration:  21 loss: 0.07912570\n",
            "iteration:  22 loss: 0.10552981\n",
            "iteration:  23 loss: 0.08046800\n",
            "iteration:  24 loss: 0.02442814\n",
            "iteration:  25 loss: 0.04244217\n",
            "iteration:  26 loss: 0.04322348\n",
            "iteration:  27 loss: 0.06602471\n",
            "iteration:  28 loss: 0.10476365\n",
            "iteration:  29 loss: 0.04153838\n",
            "iteration:  30 loss: 0.04307185\n",
            "iteration:  31 loss: 0.09662527\n",
            "iteration:  32 loss: 0.02109628\n",
            "iteration:  33 loss: 0.13327298\n",
            "iteration:  34 loss: 0.15620059\n",
            "iteration:  35 loss: 0.02620986\n",
            "iteration:  36 loss: 0.28910023\n",
            "iteration:  37 loss: 0.04604962\n",
            "iteration:  38 loss: 0.03362677\n",
            "iteration:  39 loss: 0.10161000\n",
            "iteration:  40 loss: 0.05827031\n",
            "iteration:  41 loss: 0.07352378\n",
            "iteration:  42 loss: 0.04090685\n",
            "iteration:  43 loss: 0.05555589\n",
            "iteration:  44 loss: 0.03302588\n",
            "iteration:  45 loss: 0.09855437\n",
            "iteration:  46 loss: 0.61867493\n",
            "iteration:  47 loss: 0.03206722\n",
            "iteration:  48 loss: 0.08198410\n",
            "iteration:  49 loss: 0.01541263\n",
            "iteration:  50 loss: 0.03407528\n",
            "iteration:  51 loss: 0.04117653\n",
            "iteration:  52 loss: 0.17047197\n",
            "iteration:  53 loss: 0.07424387\n",
            "iteration:  54 loss: 0.21069174\n",
            "iteration:  55 loss: 0.02764088\n",
            "iteration:  56 loss: 0.15136491\n",
            "iteration:  57 loss: 0.07963018\n",
            "iteration:  58 loss: 0.17503795\n",
            "iteration:  59 loss: 0.05389369\n",
            "iteration:  60 loss: 0.22424348\n",
            "iteration:  61 loss: 0.07793500\n",
            "iteration:  62 loss: 0.16122271\n",
            "iteration:  63 loss: 0.50793719\n",
            "iteration:  64 loss: 0.05202648\n",
            "iteration:  65 loss: 0.14645727\n",
            "iteration:  66 loss: 0.14080167\n",
            "iteration:  67 loss: 0.04585153\n",
            "iteration:  68 loss: 0.07612203\n",
            "iteration:  69 loss: 0.17488982\n",
            "iteration:  70 loss: 0.02903386\n",
            "iteration:  71 loss: 0.13831654\n",
            "iteration:  72 loss: 0.07753789\n",
            "iteration:  73 loss: 0.08721158\n",
            "iteration:  74 loss: 0.15306474\n",
            "iteration:  75 loss: 0.14155582\n",
            "iteration:  76 loss: 0.17941912\n",
            "iteration:  77 loss: 0.16178614\n",
            "iteration:  78 loss: 0.03159674\n",
            "iteration:  79 loss: 0.03496892\n",
            "iteration:  80 loss: 0.12509295\n",
            "iteration:  81 loss: 0.04248877\n",
            "iteration:  82 loss: 0.24093361\n",
            "iteration:  83 loss: 0.06895794\n",
            "iteration:  84 loss: 0.14890693\n",
            "iteration:  85 loss: 0.13172650\n",
            "iteration:  86 loss: 0.63211167\n",
            "iteration:  87 loss: 0.15049587\n",
            "iteration:  88 loss: 0.12085257\n",
            "iteration:  89 loss: 0.02104548\n",
            "iteration:  90 loss: 0.09001794\n",
            "iteration:  91 loss: 0.03637115\n",
            "iteration:  92 loss: 0.10973529\n",
            "iteration:  93 loss: 0.06175928\n",
            "iteration:  94 loss: 0.05897565\n",
            "iteration:  95 loss: 0.22923090\n",
            "iteration:  96 loss: 0.08123501\n",
            "iteration:  97 loss: 0.22383538\n",
            "iteration:  98 loss: 0.04736722\n",
            "iteration:  99 loss: 0.03967646\n",
            "iteration: 100 loss: 0.53827018\n",
            "iteration: 101 loss: 0.13646448\n",
            "iteration: 102 loss: 0.09947024\n",
            "iteration: 103 loss: 0.05261874\n",
            "iteration: 104 loss: 0.01592484\n",
            "iteration: 105 loss: 0.23950981\n",
            "iteration: 106 loss: 0.07765017\n",
            "iteration: 107 loss: 0.08416648\n",
            "iteration: 108 loss: 0.13028918\n",
            "iteration: 109 loss: 0.16288042\n",
            "iteration: 110 loss: 0.12391308\n",
            "iteration: 111 loss: 0.13613261\n",
            "iteration: 112 loss: 0.11068592\n",
            "iteration: 113 loss: 0.07794947\n",
            "iteration: 114 loss: 0.11134752\n",
            "iteration: 115 loss: 0.08701214\n",
            "iteration: 116 loss: 0.01384152\n",
            "iteration: 117 loss: 0.12460869\n",
            "iteration: 118 loss: 0.01668983\n",
            "iteration: 119 loss: 0.19183707\n",
            "iteration: 120 loss: 0.14119852\n",
            "iteration: 121 loss: 0.11794945\n",
            "iteration: 122 loss: 0.15732834\n",
            "iteration: 123 loss: 0.02499882\n",
            "iteration: 124 loss: 0.05503461\n",
            "iteration: 125 loss: 0.32273316\n",
            "iteration: 126 loss: 0.05430958\n",
            "iteration: 127 loss: 0.25224656\n",
            "iteration: 128 loss: 0.13009131\n",
            "iteration: 129 loss: 0.07139938\n",
            "iteration: 130 loss: 0.05335326\n",
            "iteration: 131 loss: 0.03499112\n",
            "iteration: 132 loss: 0.11384124\n",
            "iteration: 133 loss: 0.05467095\n",
            "iteration: 134 loss: 0.01849951\n",
            "iteration: 135 loss: 0.10362613\n",
            "iteration: 136 loss: 0.09163002\n",
            "iteration: 137 loss: 0.07187705\n",
            "iteration: 138 loss: 0.07577097\n",
            "iteration: 139 loss: 0.13990742\n",
            "iteration: 140 loss: 0.13094632\n",
            "iteration: 141 loss: 0.07473782\n",
            "iteration: 142 loss: 0.07461912\n",
            "iteration: 143 loss: 0.17694888\n",
            "iteration: 144 loss: 0.24214914\n",
            "iteration: 145 loss: 0.22226648\n",
            "iteration: 146 loss: 0.10857970\n",
            "iteration: 147 loss: 0.26446041\n",
            "iteration: 148 loss: 0.01689638\n",
            "iteration: 149 loss: 0.73350370\n",
            "iteration: 150 loss: 0.19264850\n",
            "iteration: 151 loss: 0.04280857\n",
            "iteration: 152 loss: 0.05671629\n",
            "iteration: 153 loss: 0.12710588\n",
            "iteration: 154 loss: 0.17621097\n",
            "iteration: 155 loss: 0.11733717\n",
            "iteration: 156 loss: 0.02347842\n",
            "iteration: 157 loss: 0.18274349\n",
            "iteration: 158 loss: 0.14242128\n",
            "iteration: 159 loss: 0.15665078\n",
            "iteration: 160 loss: 0.13675664\n",
            "iteration: 161 loss: 0.27217749\n",
            "iteration: 162 loss: 0.12891552\n",
            "iteration: 163 loss: 0.08770330\n",
            "iteration: 164 loss: 0.04769357\n",
            "iteration: 165 loss: 0.02539561\n",
            "iteration: 166 loss: 0.13689674\n",
            "iteration: 167 loss: 0.17416790\n",
            "iteration: 168 loss: 0.07932796\n",
            "iteration: 169 loss: 0.05600607\n",
            "iteration: 170 loss: 0.35234195\n",
            "iteration: 171 loss: 0.08372597\n",
            "iteration: 172 loss: 0.12386838\n",
            "iteration: 173 loss: 0.28522778\n",
            "iteration: 174 loss: 0.25450978\n",
            "iteration: 175 loss: 0.06670856\n",
            "iteration: 176 loss: 0.13962501\n",
            "iteration: 177 loss: 0.05787984\n",
            "iteration: 178 loss: 0.14091419\n",
            "iteration: 179 loss: 0.04765506\n",
            "iteration: 180 loss: 0.03542075\n",
            "iteration: 181 loss: 0.18912637\n",
            "iteration: 182 loss: 0.05191605\n",
            "iteration: 183 loss: 0.31306556\n",
            "iteration: 184 loss: 0.58265817\n",
            "iteration: 185 loss: 0.39633340\n",
            "iteration: 186 loss: 0.04503779\n",
            "iteration: 187 loss: 0.05582448\n",
            "iteration: 188 loss: 0.05450090\n",
            "iteration: 189 loss: 0.19379772\n",
            "iteration: 190 loss: 0.12512851\n",
            "iteration: 191 loss: 0.23198856\n",
            "iteration: 192 loss: 0.16371371\n",
            "iteration: 193 loss: 0.02495859\n",
            "iteration: 194 loss: 0.03696897\n",
            "iteration: 195 loss: 0.18710484\n",
            "iteration: 196 loss: 0.34884229\n",
            "iteration: 197 loss: 0.10916281\n",
            "iteration: 198 loss: 0.07344631\n",
            "iteration: 199 loss: 0.08792443\n",
            "epoch:  83 mean loss training: 0.12745932\n",
            "epoch:  83 mean loss validation: 0.37066320\n",
            "iteration:   0 loss: 0.14067650\n",
            "iteration:   1 loss: 0.10475028\n",
            "iteration:   2 loss: 0.03313382\n",
            "iteration:   3 loss: 0.10187143\n",
            "iteration:   4 loss: 0.04005874\n",
            "iteration:   5 loss: 0.03816329\n",
            "iteration:   6 loss: 0.09669165\n",
            "iteration:   7 loss: 0.02512057\n",
            "iteration:   8 loss: 0.04900166\n",
            "iteration:   9 loss: 0.03461201\n",
            "iteration:  10 loss: 0.07340883\n",
            "iteration:  11 loss: 0.39419007\n",
            "iteration:  12 loss: 0.15474699\n",
            "iteration:  13 loss: 0.05194625\n",
            "iteration:  14 loss: 0.06538342\n",
            "iteration:  15 loss: 0.02917530\n",
            "iteration:  16 loss: 0.17422011\n",
            "iteration:  17 loss: 0.21261716\n",
            "iteration:  18 loss: 0.27082667\n",
            "iteration:  19 loss: 0.11621338\n",
            "iteration:  20 loss: 0.34320846\n",
            "iteration:  21 loss: 0.20618102\n",
            "iteration:  22 loss: 0.32656139\n",
            "iteration:  23 loss: 0.05880882\n",
            "iteration:  24 loss: 0.10936353\n",
            "iteration:  25 loss: 0.06913399\n",
            "iteration:  26 loss: 0.02009924\n",
            "iteration:  27 loss: 0.14289473\n",
            "iteration:  28 loss: 0.19557627\n",
            "iteration:  29 loss: 0.05365927\n",
            "iteration:  30 loss: 0.02744552\n",
            "iteration:  31 loss: 0.19919035\n",
            "iteration:  32 loss: 0.10813723\n",
            "iteration:  33 loss: 0.07253066\n",
            "iteration:  34 loss: 0.02319685\n",
            "iteration:  35 loss: 0.18334413\n",
            "iteration:  36 loss: 0.33626306\n",
            "iteration:  37 loss: 0.02716280\n",
            "iteration:  38 loss: 0.21787235\n",
            "iteration:  39 loss: 0.04780209\n",
            "iteration:  40 loss: 0.21265432\n",
            "iteration:  41 loss: 0.19745255\n",
            "iteration:  42 loss: 0.25211573\n",
            "iteration:  43 loss: 0.04165115\n",
            "iteration:  44 loss: 0.09563950\n",
            "iteration:  45 loss: 0.09836686\n",
            "iteration:  46 loss: 0.23004757\n",
            "iteration:  47 loss: 0.03816321\n",
            "iteration:  48 loss: 0.04084844\n",
            "iteration:  49 loss: 0.65236038\n",
            "iteration:  50 loss: 0.03346586\n",
            "iteration:  51 loss: 0.06051204\n",
            "iteration:  52 loss: 0.02000438\n",
            "iteration:  53 loss: 0.04222190\n",
            "iteration:  54 loss: 0.02581803\n",
            "iteration:  55 loss: 0.29661462\n",
            "iteration:  56 loss: 0.41970959\n",
            "iteration:  57 loss: 0.17806292\n",
            "iteration:  58 loss: 0.01830034\n",
            "iteration:  59 loss: 0.57615364\n",
            "iteration:  60 loss: 0.02815380\n",
            "iteration:  61 loss: 0.59521949\n",
            "iteration:  62 loss: 0.08025863\n",
            "iteration:  63 loss: 0.07479524\n",
            "iteration:  64 loss: 0.11384777\n",
            "iteration:  65 loss: 0.49134246\n",
            "iteration:  66 loss: 0.05508691\n",
            "iteration:  67 loss: 0.08981696\n",
            "iteration:  68 loss: 0.10731074\n",
            "iteration:  69 loss: 0.02726121\n",
            "iteration:  70 loss: 0.04128920\n",
            "iteration:  71 loss: 0.13481002\n",
            "iteration:  72 loss: 0.06983379\n",
            "iteration:  73 loss: 0.06620014\n",
            "iteration:  74 loss: 0.20325424\n",
            "iteration:  75 loss: 0.16157579\n",
            "iteration:  76 loss: 0.03157011\n",
            "iteration:  77 loss: 0.04920239\n",
            "iteration:  78 loss: 0.34321809\n",
            "iteration:  79 loss: 0.17018305\n",
            "iteration:  80 loss: 0.21653700\n",
            "iteration:  81 loss: 0.16928595\n",
            "iteration:  82 loss: 0.08433730\n",
            "iteration:  83 loss: 0.08773491\n",
            "iteration:  84 loss: 0.13446701\n",
            "iteration:  85 loss: 0.28112561\n",
            "iteration:  86 loss: 0.04300850\n",
            "iteration:  87 loss: 0.06246434\n",
            "iteration:  88 loss: 0.15199564\n",
            "iteration:  89 loss: 0.10855059\n",
            "iteration:  90 loss: 0.06130972\n",
            "iteration:  91 loss: 0.03990469\n",
            "iteration:  92 loss: 0.23335958\n",
            "iteration:  93 loss: 0.03961917\n",
            "iteration:  94 loss: 0.07941624\n",
            "iteration:  95 loss: 0.45353457\n",
            "iteration:  96 loss: 0.10382337\n",
            "iteration:  97 loss: 0.04107901\n",
            "iteration:  98 loss: 0.04377446\n",
            "iteration:  99 loss: 0.13966814\n",
            "iteration: 100 loss: 0.08476249\n",
            "iteration: 101 loss: 0.16123739\n",
            "iteration: 102 loss: 0.39053011\n",
            "iteration: 103 loss: 0.05299400\n",
            "iteration: 104 loss: 0.04550868\n",
            "iteration: 105 loss: 0.05627985\n",
            "iteration: 106 loss: 0.03509472\n",
            "iteration: 107 loss: 0.06748080\n",
            "iteration: 108 loss: 0.05418354\n",
            "iteration: 109 loss: 0.08898292\n",
            "iteration: 110 loss: 0.03706521\n",
            "iteration: 111 loss: 0.12427427\n",
            "iteration: 112 loss: 0.20843050\n",
            "iteration: 113 loss: 0.36986151\n",
            "iteration: 114 loss: 0.10574788\n",
            "iteration: 115 loss: 0.09430265\n",
            "iteration: 116 loss: 0.19405502\n",
            "iteration: 117 loss: 0.13896044\n",
            "iteration: 118 loss: 0.07357620\n",
            "iteration: 119 loss: 0.05556463\n",
            "iteration: 120 loss: 0.32738966\n",
            "iteration: 121 loss: 0.01449030\n",
            "iteration: 122 loss: 0.07757905\n",
            "iteration: 123 loss: 0.06016025\n",
            "iteration: 124 loss: 0.14635430\n",
            "iteration: 125 loss: 0.04512277\n",
            "iteration: 126 loss: 0.09568347\n",
            "iteration: 127 loss: 0.04723298\n",
            "iteration: 128 loss: 0.36482197\n",
            "iteration: 129 loss: 0.13694814\n",
            "iteration: 130 loss: 0.10559360\n",
            "iteration: 131 loss: 0.02333729\n",
            "iteration: 132 loss: 0.10997881\n",
            "iteration: 133 loss: 0.13724150\n",
            "iteration: 134 loss: 0.09480346\n",
            "iteration: 135 loss: 0.08061313\n",
            "iteration: 136 loss: 0.04384477\n",
            "iteration: 137 loss: 0.05865585\n",
            "iteration: 138 loss: 0.10897196\n",
            "iteration: 139 loss: 0.05295291\n",
            "iteration: 140 loss: 0.13273479\n",
            "iteration: 141 loss: 0.16657877\n",
            "iteration: 142 loss: 0.13050219\n",
            "iteration: 143 loss: 0.04454792\n",
            "iteration: 144 loss: 0.21851641\n",
            "iteration: 145 loss: 0.09273919\n",
            "iteration: 146 loss: 0.23933038\n",
            "iteration: 147 loss: 0.06096359\n",
            "iteration: 148 loss: 0.03273953\n",
            "iteration: 149 loss: 0.22510223\n",
            "iteration: 150 loss: 0.44040531\n",
            "iteration: 151 loss: 0.08391143\n",
            "iteration: 152 loss: 0.68027335\n",
            "iteration: 153 loss: 0.08082359\n",
            "iteration: 154 loss: 0.17888243\n",
            "iteration: 155 loss: 0.04337323\n",
            "iteration: 156 loss: 0.04616749\n",
            "iteration: 157 loss: 0.05295750\n",
            "iteration: 158 loss: 0.13146795\n",
            "iteration: 159 loss: 0.04041916\n",
            "iteration: 160 loss: 0.08103070\n",
            "iteration: 161 loss: 0.31092572\n",
            "iteration: 162 loss: 0.51158702\n",
            "iteration: 163 loss: 0.17382330\n",
            "iteration: 164 loss: 0.07395232\n",
            "iteration: 165 loss: 0.19293872\n",
            "iteration: 166 loss: 0.06265534\n",
            "iteration: 167 loss: 0.03628433\n",
            "iteration: 168 loss: 0.31604919\n",
            "iteration: 169 loss: 0.05569590\n",
            "iteration: 170 loss: 0.06597292\n",
            "iteration: 171 loss: 0.09356869\n",
            "iteration: 172 loss: 0.02250998\n",
            "iteration: 173 loss: 0.04276250\n",
            "iteration: 174 loss: 0.05896668\n",
            "iteration: 175 loss: 0.11276048\n",
            "iteration: 176 loss: 0.06978009\n",
            "iteration: 177 loss: 0.09439147\n",
            "iteration: 178 loss: 0.07221060\n",
            "iteration: 179 loss: 0.33363855\n",
            "iteration: 180 loss: 0.03433313\n",
            "iteration: 181 loss: 0.07730239\n",
            "iteration: 182 loss: 0.06602319\n",
            "iteration: 183 loss: 0.03475371\n",
            "iteration: 184 loss: 0.09250012\n",
            "iteration: 185 loss: 0.25317773\n",
            "iteration: 186 loss: 0.01448623\n",
            "iteration: 187 loss: 0.12061601\n",
            "iteration: 188 loss: 0.13583882\n",
            "iteration: 189 loss: 0.10945485\n",
            "iteration: 190 loss: 0.03549241\n",
            "iteration: 191 loss: 0.07135247\n",
            "iteration: 192 loss: 0.04243531\n",
            "iteration: 193 loss: 0.04594978\n",
            "iteration: 194 loss: 0.41046727\n",
            "iteration: 195 loss: 0.18025279\n",
            "iteration: 196 loss: 0.15322164\n",
            "iteration: 197 loss: 0.05697064\n",
            "iteration: 198 loss: 0.04103592\n",
            "iteration: 199 loss: 0.07500344\n",
            "epoch:  84 mean loss training: 0.13269953\n",
            "epoch:  84 mean loss validation: 0.35328832\n",
            "iteration:   0 loss: 0.11658292\n",
            "iteration:   1 loss: 0.12050248\n",
            "iteration:   2 loss: 0.23350926\n",
            "iteration:   3 loss: 0.25437906\n",
            "iteration:   4 loss: 0.10835413\n",
            "iteration:   5 loss: 0.12467457\n",
            "iteration:   6 loss: 0.47260550\n",
            "iteration:   7 loss: 0.06214339\n",
            "iteration:   8 loss: 0.06663642\n",
            "iteration:   9 loss: 0.07772929\n",
            "iteration:  10 loss: 0.48605025\n",
            "iteration:  11 loss: 0.09809279\n",
            "iteration:  12 loss: 0.19515182\n",
            "iteration:  13 loss: 0.03007739\n",
            "iteration:  14 loss: 0.10567962\n",
            "iteration:  15 loss: 0.21297926\n",
            "iteration:  16 loss: 0.13005133\n",
            "iteration:  17 loss: 0.18197896\n",
            "iteration:  18 loss: 0.03235169\n",
            "iteration:  19 loss: 0.04826491\n",
            "iteration:  20 loss: 0.05043905\n",
            "iteration:  21 loss: 0.06124650\n",
            "iteration:  22 loss: 0.12718913\n",
            "iteration:  23 loss: 0.40722793\n",
            "iteration:  24 loss: 0.05069910\n",
            "iteration:  25 loss: 0.02179440\n",
            "iteration:  26 loss: 0.04823990\n",
            "iteration:  27 loss: 0.07787203\n",
            "iteration:  28 loss: 0.36923525\n",
            "iteration:  29 loss: 0.01539675\n",
            "iteration:  30 loss: 0.04616144\n",
            "iteration:  31 loss: 0.29031190\n",
            "iteration:  32 loss: 0.09084596\n",
            "iteration:  33 loss: 0.05181856\n",
            "iteration:  34 loss: 0.11115622\n",
            "iteration:  35 loss: 0.09137432\n",
            "iteration:  36 loss: 0.05740020\n",
            "iteration:  37 loss: 0.06849117\n",
            "iteration:  38 loss: 0.14096877\n",
            "iteration:  39 loss: 0.02991971\n",
            "iteration:  40 loss: 0.08663565\n",
            "iteration:  41 loss: 0.08415833\n",
            "iteration:  42 loss: 0.03329963\n",
            "iteration:  43 loss: 0.04281520\n",
            "iteration:  44 loss: 0.09609133\n",
            "iteration:  45 loss: 0.16069354\n",
            "iteration:  46 loss: 0.04723328\n",
            "iteration:  47 loss: 0.10692759\n",
            "iteration:  48 loss: 0.03485409\n",
            "iteration:  49 loss: 0.23054254\n",
            "iteration:  50 loss: 0.02689812\n",
            "iteration:  51 loss: 0.02516063\n",
            "iteration:  52 loss: 0.07092207\n",
            "iteration:  53 loss: 0.08222924\n",
            "iteration:  54 loss: 0.40792370\n",
            "iteration:  55 loss: 0.23273920\n",
            "iteration:  56 loss: 0.16795130\n",
            "iteration:  57 loss: 0.18307973\n",
            "iteration:  58 loss: 0.16011797\n",
            "iteration:  59 loss: 0.13396035\n",
            "iteration:  60 loss: 0.12355799\n",
            "iteration:  61 loss: 0.74071348\n",
            "iteration:  62 loss: 0.06280510\n",
            "iteration:  63 loss: 0.08806192\n",
            "iteration:  64 loss: 0.05570986\n",
            "iteration:  65 loss: 0.07595579\n",
            "iteration:  66 loss: 0.22663838\n",
            "iteration:  67 loss: 0.07604200\n",
            "iteration:  68 loss: 0.05045295\n",
            "iteration:  69 loss: 0.46756580\n",
            "iteration:  70 loss: 0.08337372\n",
            "iteration:  71 loss: 0.04780079\n",
            "iteration:  72 loss: 0.08945800\n",
            "iteration:  73 loss: 0.11590087\n",
            "iteration:  74 loss: 0.06058140\n",
            "iteration:  75 loss: 0.19324896\n",
            "iteration:  76 loss: 0.17218222\n",
            "iteration:  77 loss: 0.06090291\n",
            "iteration:  78 loss: 0.26565832\n",
            "iteration:  79 loss: 0.07503671\n",
            "iteration:  80 loss: 0.04305788\n",
            "iteration:  81 loss: 0.01880440\n",
            "iteration:  82 loss: 0.07453198\n",
            "iteration:  83 loss: 0.35656402\n",
            "iteration:  84 loss: 0.21321253\n",
            "iteration:  85 loss: 0.09717185\n",
            "iteration:  86 loss: 0.11918459\n",
            "iteration:  87 loss: 0.04497962\n",
            "iteration:  88 loss: 0.02089753\n",
            "iteration:  89 loss: 0.33612421\n",
            "iteration:  90 loss: 0.15081616\n",
            "iteration:  91 loss: 0.24819468\n",
            "iteration:  92 loss: 0.04814300\n",
            "iteration:  93 loss: 0.16058026\n",
            "iteration:  94 loss: 0.07448399\n",
            "iteration:  95 loss: 0.13729168\n",
            "iteration:  96 loss: 0.05759166\n",
            "iteration:  97 loss: 0.14810869\n",
            "iteration:  98 loss: 0.06576438\n",
            "iteration:  99 loss: 0.10626855\n",
            "iteration: 100 loss: 0.07059166\n",
            "iteration: 101 loss: 0.10912064\n",
            "iteration: 102 loss: 0.12245411\n",
            "iteration: 103 loss: 0.07196276\n",
            "iteration: 104 loss: 0.03817812\n",
            "iteration: 105 loss: 0.01712036\n",
            "iteration: 106 loss: 0.10572437\n",
            "iteration: 107 loss: 0.02975276\n",
            "iteration: 108 loss: 0.06146459\n",
            "iteration: 109 loss: 0.08669359\n",
            "iteration: 110 loss: 0.12157048\n",
            "iteration: 111 loss: 0.12027205\n",
            "iteration: 112 loss: 0.07086256\n",
            "iteration: 113 loss: 0.09869583\n",
            "iteration: 114 loss: 0.04784565\n",
            "iteration: 115 loss: 0.04678321\n",
            "iteration: 116 loss: 0.02244082\n",
            "iteration: 117 loss: 0.23193286\n",
            "iteration: 118 loss: 0.33758110\n",
            "iteration: 119 loss: 0.26003686\n",
            "iteration: 120 loss: 0.19925548\n",
            "iteration: 121 loss: 0.02104806\n",
            "iteration: 122 loss: 0.06326630\n",
            "iteration: 123 loss: 0.07125313\n",
            "iteration: 124 loss: 0.28493294\n",
            "iteration: 125 loss: 0.15344745\n",
            "iteration: 126 loss: 0.27420324\n",
            "iteration: 127 loss: 0.07890832\n",
            "iteration: 128 loss: 0.04575757\n",
            "iteration: 129 loss: 0.06370916\n",
            "iteration: 130 loss: 0.03434898\n",
            "iteration: 131 loss: 0.16469769\n",
            "iteration: 132 loss: 0.08419920\n",
            "iteration: 133 loss: 0.17889228\n",
            "iteration: 134 loss: 0.03122889\n",
            "iteration: 135 loss: 0.41472873\n",
            "iteration: 136 loss: 0.14371911\n",
            "iteration: 137 loss: 0.14079696\n",
            "iteration: 138 loss: 0.19227464\n",
            "iteration: 139 loss: 0.09392756\n",
            "iteration: 140 loss: 0.45187354\n",
            "iteration: 141 loss: 0.05461089\n",
            "iteration: 142 loss: 0.07744927\n",
            "iteration: 143 loss: 0.16724157\n",
            "iteration: 144 loss: 0.11958705\n",
            "iteration: 145 loss: 0.04146428\n",
            "iteration: 146 loss: 0.02621911\n",
            "iteration: 147 loss: 0.19751413\n",
            "iteration: 148 loss: 0.20669986\n",
            "iteration: 149 loss: 0.04448770\n",
            "iteration: 150 loss: 0.10184152\n",
            "iteration: 151 loss: 0.19483049\n",
            "iteration: 152 loss: 0.25748497\n",
            "iteration: 153 loss: 0.09896546\n",
            "iteration: 154 loss: 0.24856187\n",
            "iteration: 155 loss: 0.04819662\n",
            "iteration: 156 loss: 0.13938616\n",
            "iteration: 157 loss: 0.32341447\n",
            "iteration: 158 loss: 0.06789842\n",
            "iteration: 159 loss: 0.09304699\n",
            "iteration: 160 loss: 0.01894931\n",
            "iteration: 161 loss: 0.12244605\n",
            "iteration: 162 loss: 0.01743670\n",
            "iteration: 163 loss: 0.06956997\n",
            "iteration: 164 loss: 0.04456962\n",
            "iteration: 165 loss: 0.26696593\n",
            "iteration: 166 loss: 0.04437097\n",
            "iteration: 167 loss: 0.09867115\n",
            "iteration: 168 loss: 0.14939666\n",
            "iteration: 169 loss: 0.03687538\n",
            "iteration: 170 loss: 0.10538736\n",
            "iteration: 171 loss: 0.19250706\n",
            "iteration: 172 loss: 0.10379560\n",
            "iteration: 173 loss: 0.12752759\n",
            "iteration: 174 loss: 0.08675262\n",
            "iteration: 175 loss: 0.64840907\n",
            "iteration: 176 loss: 0.08989948\n",
            "iteration: 177 loss: 0.15494147\n",
            "iteration: 178 loss: 0.45674330\n",
            "iteration: 179 loss: 0.02786913\n",
            "iteration: 180 loss: 0.11434462\n",
            "iteration: 181 loss: 0.07147450\n",
            "iteration: 182 loss: 0.15454359\n",
            "iteration: 183 loss: 0.04424951\n",
            "iteration: 184 loss: 0.16037044\n",
            "iteration: 185 loss: 0.05352442\n",
            "iteration: 186 loss: 0.07582937\n",
            "iteration: 187 loss: 0.07112882\n",
            "iteration: 188 loss: 0.02444008\n",
            "iteration: 189 loss: 0.11925556\n",
            "iteration: 190 loss: 1.02837217\n",
            "iteration: 191 loss: 0.07688872\n",
            "iteration: 192 loss: 0.27959639\n",
            "iteration: 193 loss: 0.32870996\n",
            "iteration: 194 loss: 0.39987665\n",
            "iteration: 195 loss: 0.07427719\n",
            "iteration: 196 loss: 0.07382176\n",
            "iteration: 197 loss: 0.05875268\n",
            "iteration: 198 loss: 0.13866194\n",
            "iteration: 199 loss: 0.02743373\n",
            "epoch:  85 mean loss training: 0.13719164\n",
            "epoch:  85 mean loss validation: 0.40092814\n",
            "iteration:   0 loss: 0.04893794\n",
            "iteration:   1 loss: 0.17604107\n",
            "iteration:   2 loss: 0.14522855\n",
            "iteration:   3 loss: 0.10547297\n",
            "iteration:   4 loss: 0.07515626\n",
            "iteration:   5 loss: 0.06529353\n",
            "iteration:   6 loss: 0.16596898\n",
            "iteration:   7 loss: 0.09814730\n",
            "iteration:   8 loss: 0.61246955\n",
            "iteration:   9 loss: 0.04564835\n",
            "iteration:  10 loss: 0.06415553\n",
            "iteration:  11 loss: 0.05312887\n",
            "iteration:  12 loss: 0.01878265\n",
            "iteration:  13 loss: 0.03152309\n",
            "iteration:  14 loss: 0.13876209\n",
            "iteration:  15 loss: 0.23346691\n",
            "iteration:  16 loss: 0.02046076\n",
            "iteration:  17 loss: 0.09723684\n",
            "iteration:  18 loss: 0.41815671\n",
            "iteration:  19 loss: 0.47436073\n",
            "iteration:  20 loss: 0.08898118\n",
            "iteration:  21 loss: 0.04491390\n",
            "iteration:  22 loss: 0.14121294\n",
            "iteration:  23 loss: 0.07340922\n",
            "iteration:  24 loss: 0.34160444\n",
            "iteration:  25 loss: 0.03730504\n",
            "iteration:  26 loss: 0.66880202\n",
            "iteration:  27 loss: 0.17668517\n",
            "iteration:  28 loss: 0.06480325\n",
            "iteration:  29 loss: 0.18565616\n",
            "iteration:  30 loss: 0.12101056\n",
            "iteration:  31 loss: 0.04658351\n",
            "iteration:  32 loss: 0.07745706\n",
            "iteration:  33 loss: 0.62959975\n",
            "iteration:  34 loss: 0.04384751\n",
            "iteration:  35 loss: 0.31536400\n",
            "iteration:  36 loss: 0.50519580\n",
            "iteration:  37 loss: 0.06934611\n",
            "iteration:  38 loss: 0.32633162\n",
            "iteration:  39 loss: 0.03190346\n",
            "iteration:  40 loss: 0.03874545\n",
            "iteration:  41 loss: 0.01747400\n",
            "iteration:  42 loss: 0.16031612\n",
            "iteration:  43 loss: 0.45351997\n",
            "iteration:  44 loss: 0.03677351\n",
            "iteration:  45 loss: 0.01790338\n",
            "iteration:  46 loss: 0.37025583\n",
            "iteration:  47 loss: 0.09409057\n",
            "iteration:  48 loss: 0.10372476\n",
            "iteration:  49 loss: 0.09959043\n",
            "iteration:  50 loss: 0.08251708\n",
            "iteration:  51 loss: 0.12553628\n",
            "iteration:  52 loss: 0.24277505\n",
            "iteration:  53 loss: 0.06871539\n",
            "iteration:  54 loss: 0.10674848\n",
            "iteration:  55 loss: 0.10621107\n",
            "iteration:  56 loss: 0.16008763\n",
            "iteration:  57 loss: 0.05194186\n",
            "iteration:  58 loss: 0.08045593\n",
            "iteration:  59 loss: 0.11868033\n",
            "iteration:  60 loss: 0.14005433\n",
            "iteration:  61 loss: 0.08724819\n",
            "iteration:  62 loss: 0.42091709\n",
            "iteration:  63 loss: 0.14278282\n",
            "iteration:  64 loss: 0.02646935\n",
            "iteration:  65 loss: 0.13078056\n",
            "iteration:  66 loss: 0.03320135\n",
            "iteration:  67 loss: 0.07333190\n",
            "iteration:  68 loss: 0.05514263\n",
            "iteration:  69 loss: 0.01854439\n",
            "iteration:  70 loss: 0.07231512\n",
            "iteration:  71 loss: 0.05536553\n",
            "iteration:  72 loss: 0.33840126\n",
            "iteration:  73 loss: 0.09540560\n",
            "iteration:  74 loss: 0.29793099\n",
            "iteration:  75 loss: 0.23334585\n",
            "iteration:  76 loss: 0.31741700\n",
            "iteration:  77 loss: 0.02393008\n",
            "iteration:  78 loss: 0.06351341\n",
            "iteration:  79 loss: 0.40519035\n",
            "iteration:  80 loss: 0.02752306\n",
            "iteration:  81 loss: 0.03581606\n",
            "iteration:  82 loss: 0.14582552\n",
            "iteration:  83 loss: 0.19389072\n",
            "iteration:  84 loss: 0.15815274\n",
            "iteration:  85 loss: 0.22272877\n",
            "iteration:  86 loss: 0.18972281\n",
            "iteration:  87 loss: 0.22043057\n",
            "iteration:  88 loss: 0.05453783\n",
            "iteration:  89 loss: 0.00821273\n",
            "iteration:  90 loss: 0.10862780\n",
            "iteration:  91 loss: 0.03107220\n",
            "iteration:  92 loss: 0.13385655\n",
            "iteration:  93 loss: 0.10467692\n",
            "iteration:  94 loss: 0.07673682\n",
            "iteration:  95 loss: 0.06833884\n",
            "iteration:  96 loss: 0.08519080\n",
            "iteration:  97 loss: 0.02195123\n",
            "iteration:  98 loss: 0.01135551\n",
            "iteration:  99 loss: 0.33538717\n",
            "iteration: 100 loss: 0.09751222\n",
            "iteration: 101 loss: 0.03129328\n",
            "iteration: 102 loss: 0.02022742\n",
            "iteration: 103 loss: 0.15428862\n",
            "iteration: 104 loss: 0.10283460\n",
            "iteration: 105 loss: 0.05612873\n",
            "iteration: 106 loss: 0.36042184\n",
            "iteration: 107 loss: 0.13071777\n",
            "iteration: 108 loss: 0.12878157\n",
            "iteration: 109 loss: 0.15947518\n",
            "iteration: 110 loss: 0.01245751\n",
            "iteration: 111 loss: 0.08157460\n",
            "iteration: 112 loss: 0.23920541\n",
            "iteration: 113 loss: 0.04855143\n",
            "iteration: 114 loss: 0.14487931\n",
            "iteration: 115 loss: 0.14237536\n",
            "iteration: 116 loss: 0.07993990\n",
            "iteration: 117 loss: 0.06046806\n",
            "iteration: 118 loss: 0.11372391\n",
            "iteration: 119 loss: 0.07415076\n",
            "iteration: 120 loss: 0.14424379\n",
            "iteration: 121 loss: 0.08676631\n",
            "iteration: 122 loss: 0.09871200\n",
            "iteration: 123 loss: 0.05777611\n",
            "iteration: 124 loss: 0.11176894\n",
            "iteration: 125 loss: 0.03068504\n",
            "iteration: 126 loss: 0.03791888\n",
            "iteration: 127 loss: 0.14383535\n",
            "iteration: 128 loss: 0.26003590\n",
            "iteration: 129 loss: 0.05340661\n",
            "iteration: 130 loss: 0.08742234\n",
            "iteration: 131 loss: 0.42175758\n",
            "iteration: 132 loss: 0.01997729\n",
            "iteration: 133 loss: 0.20507188\n",
            "iteration: 134 loss: 0.04470735\n",
            "iteration: 135 loss: 0.06777623\n",
            "iteration: 136 loss: 0.13744546\n",
            "iteration: 137 loss: 0.07166559\n",
            "iteration: 138 loss: 0.21402265\n",
            "iteration: 139 loss: 0.10908705\n",
            "iteration: 140 loss: 0.39424866\n",
            "iteration: 141 loss: 0.12021071\n",
            "iteration: 142 loss: 0.05783780\n",
            "iteration: 143 loss: 0.06444999\n",
            "iteration: 144 loss: 0.01810287\n",
            "iteration: 145 loss: 0.06020362\n",
            "iteration: 146 loss: 0.08836728\n",
            "iteration: 147 loss: 0.05577609\n",
            "iteration: 148 loss: 0.03626256\n",
            "iteration: 149 loss: 0.04089077\n",
            "iteration: 150 loss: 0.16768099\n",
            "iteration: 151 loss: 0.40851039\n",
            "iteration: 152 loss: 0.01316431\n",
            "iteration: 153 loss: 0.08196706\n",
            "iteration: 154 loss: 0.06921232\n",
            "iteration: 155 loss: 0.21221039\n",
            "iteration: 156 loss: 0.04090750\n",
            "iteration: 157 loss: 0.18116020\n",
            "iteration: 158 loss: 0.04222685\n",
            "iteration: 159 loss: 0.04692592\n",
            "iteration: 160 loss: 0.08418272\n",
            "iteration: 161 loss: 0.07592023\n",
            "iteration: 162 loss: 0.05342432\n",
            "iteration: 163 loss: 0.09228244\n",
            "iteration: 164 loss: 0.39722422\n",
            "iteration: 165 loss: 0.18111122\n",
            "iteration: 166 loss: 0.19644654\n",
            "iteration: 167 loss: 0.23522855\n",
            "iteration: 168 loss: 0.07004856\n",
            "iteration: 169 loss: 0.04978947\n",
            "iteration: 170 loss: 0.09679035\n",
            "iteration: 171 loss: 0.16425921\n",
            "iteration: 172 loss: 0.05332021\n",
            "iteration: 173 loss: 0.07171956\n",
            "iteration: 174 loss: 0.13424024\n",
            "iteration: 175 loss: 0.19861597\n",
            "iteration: 176 loss: 0.08231596\n",
            "iteration: 177 loss: 0.02253694\n",
            "iteration: 178 loss: 0.16424015\n",
            "iteration: 179 loss: 0.02041798\n",
            "iteration: 180 loss: 0.07425500\n",
            "iteration: 181 loss: 0.08501551\n",
            "iteration: 182 loss: 0.05038643\n",
            "iteration: 183 loss: 0.10694134\n",
            "iteration: 184 loss: 0.06663794\n",
            "iteration: 185 loss: 0.06635107\n",
            "iteration: 186 loss: 0.24292925\n",
            "iteration: 187 loss: 0.13506785\n",
            "iteration: 188 loss: 0.25220102\n",
            "iteration: 189 loss: 0.17951521\n",
            "iteration: 190 loss: 0.17671525\n",
            "iteration: 191 loss: 0.07834661\n",
            "iteration: 192 loss: 0.06478006\n",
            "iteration: 193 loss: 0.14903794\n",
            "iteration: 194 loss: 0.04221584\n",
            "iteration: 195 loss: 0.09957036\n",
            "iteration: 196 loss: 0.04422013\n",
            "iteration: 197 loss: 0.12374769\n",
            "iteration: 198 loss: 0.10703710\n",
            "iteration: 199 loss: 0.07853166\n",
            "epoch:  86 mean loss training: 0.13195412\n",
            "epoch:  86 mean loss validation: 0.36980459\n",
            "iteration:   0 loss: 0.02327944\n",
            "iteration:   1 loss: 0.01965523\n",
            "iteration:   2 loss: 0.75248325\n",
            "iteration:   3 loss: 0.09049413\n",
            "iteration:   4 loss: 0.06345003\n",
            "iteration:   5 loss: 0.05457893\n",
            "iteration:   6 loss: 0.45417404\n",
            "iteration:   7 loss: 0.31943235\n",
            "iteration:   8 loss: 0.93658644\n",
            "iteration:   9 loss: 0.46906957\n",
            "iteration:  10 loss: 0.01907413\n",
            "iteration:  11 loss: 0.13435079\n",
            "iteration:  12 loss: 0.11094702\n",
            "iteration:  13 loss: 0.07150747\n",
            "iteration:  14 loss: 0.03155957\n",
            "iteration:  15 loss: 0.07585367\n",
            "iteration:  16 loss: 0.10868701\n",
            "iteration:  17 loss: 0.05319387\n",
            "iteration:  18 loss: 0.10583986\n",
            "iteration:  19 loss: 0.15945446\n",
            "iteration:  20 loss: 0.13212974\n",
            "iteration:  21 loss: 0.03453159\n",
            "iteration:  22 loss: 0.11704992\n",
            "iteration:  23 loss: 0.03822540\n",
            "iteration:  24 loss: 0.09526174\n",
            "iteration:  25 loss: 0.01944110\n",
            "iteration:  26 loss: 0.01252474\n",
            "iteration:  27 loss: 0.20061696\n",
            "iteration:  28 loss: 0.10575947\n",
            "iteration:  29 loss: 0.02823843\n",
            "iteration:  30 loss: 0.16297661\n",
            "iteration:  31 loss: 0.01749808\n",
            "iteration:  32 loss: 0.41179070\n",
            "iteration:  33 loss: 0.10382620\n",
            "iteration:  34 loss: 0.25551564\n",
            "iteration:  35 loss: 0.11380779\n",
            "iteration:  36 loss: 0.13555461\n",
            "iteration:  37 loss: 0.22514722\n",
            "iteration:  38 loss: 0.02504266\n",
            "iteration:  39 loss: 0.06841428\n",
            "iteration:  40 loss: 0.27753854\n",
            "iteration:  41 loss: 0.17230856\n",
            "iteration:  42 loss: 0.04231247\n",
            "iteration:  43 loss: 0.08689384\n",
            "iteration:  44 loss: 0.76196945\n",
            "iteration:  45 loss: 0.03141039\n",
            "iteration:  46 loss: 0.08063374\n",
            "iteration:  47 loss: 0.47337705\n",
            "iteration:  48 loss: 0.18023404\n",
            "iteration:  49 loss: 0.13837133\n",
            "iteration:  50 loss: 0.47092882\n",
            "iteration:  51 loss: 0.60183877\n",
            "iteration:  52 loss: 0.01088242\n",
            "iteration:  53 loss: 0.07192940\n",
            "iteration:  54 loss: 0.07314730\n",
            "iteration:  55 loss: 0.03665802\n",
            "iteration:  56 loss: 0.22857249\n",
            "iteration:  57 loss: 0.04803964\n",
            "iteration:  58 loss: 0.03885566\n",
            "iteration:  59 loss: 0.26551774\n",
            "iteration:  60 loss: 0.25423771\n",
            "iteration:  61 loss: 0.08060345\n",
            "iteration:  62 loss: 0.05940324\n",
            "iteration:  63 loss: 0.04362121\n",
            "iteration:  64 loss: 0.11458197\n",
            "iteration:  65 loss: 0.04350015\n",
            "iteration:  66 loss: 0.02135031\n",
            "iteration:  67 loss: 0.17227179\n",
            "iteration:  68 loss: 0.20695710\n",
            "iteration:  69 loss: 0.09788698\n",
            "iteration:  70 loss: 0.03171438\n",
            "iteration:  71 loss: 0.07183443\n",
            "iteration:  72 loss: 0.17358413\n",
            "iteration:  73 loss: 0.04848825\n",
            "iteration:  74 loss: 0.13960332\n",
            "iteration:  75 loss: 1.35406208\n",
            "iteration:  76 loss: 0.17070544\n",
            "iteration:  77 loss: 0.35239309\n",
            "iteration:  78 loss: 0.03971177\n",
            "iteration:  79 loss: 0.02691536\n",
            "iteration:  80 loss: 0.07590642\n",
            "iteration:  81 loss: 0.03630345\n",
            "iteration:  82 loss: 0.05784887\n",
            "iteration:  83 loss: 0.04946693\n",
            "iteration:  84 loss: 0.06228973\n",
            "iteration:  85 loss: 0.12358771\n",
            "iteration:  86 loss: 0.19193161\n",
            "iteration:  87 loss: 0.03299349\n",
            "iteration:  88 loss: 0.21228445\n",
            "iteration:  89 loss: 0.03883915\n",
            "iteration:  90 loss: 0.03467301\n",
            "iteration:  91 loss: 0.05586654\n",
            "iteration:  92 loss: 0.08596824\n",
            "iteration:  93 loss: 0.06704746\n",
            "iteration:  94 loss: 0.02480773\n",
            "iteration:  95 loss: 0.10950336\n",
            "iteration:  96 loss: 0.18645501\n",
            "iteration:  97 loss: 0.10588406\n",
            "iteration:  98 loss: 0.14426632\n",
            "iteration:  99 loss: 0.06446201\n",
            "iteration: 100 loss: 0.09177439\n",
            "iteration: 101 loss: 0.17264745\n",
            "iteration: 102 loss: 0.04334681\n",
            "iteration: 103 loss: 0.16976054\n",
            "iteration: 104 loss: 0.12071653\n",
            "iteration: 105 loss: 0.31006846\n",
            "iteration: 106 loss: 0.05873065\n",
            "iteration: 107 loss: 0.08605359\n",
            "iteration: 108 loss: 0.15483864\n",
            "iteration: 109 loss: 0.10332215\n",
            "iteration: 110 loss: 0.13790646\n",
            "iteration: 111 loss: 0.31297201\n",
            "iteration: 112 loss: 0.00859737\n",
            "iteration: 113 loss: 0.02025099\n",
            "iteration: 114 loss: 0.02296272\n",
            "iteration: 115 loss: 0.04668363\n",
            "iteration: 116 loss: 0.21428937\n",
            "iteration: 117 loss: 0.06655574\n",
            "iteration: 118 loss: 0.03059588\n",
            "iteration: 119 loss: 0.01901535\n",
            "iteration: 120 loss: 0.03770769\n",
            "iteration: 121 loss: 0.06257375\n",
            "iteration: 122 loss: 0.11543292\n",
            "iteration: 123 loss: 0.04576047\n",
            "iteration: 124 loss: 0.05637977\n",
            "iteration: 125 loss: 0.62096024\n",
            "iteration: 126 loss: 0.07234804\n",
            "iteration: 127 loss: 0.04493107\n",
            "iteration: 128 loss: 0.12256666\n",
            "iteration: 129 loss: 0.14178188\n",
            "iteration: 130 loss: 0.12789461\n",
            "iteration: 131 loss: 0.01679760\n",
            "iteration: 132 loss: 0.12285855\n",
            "iteration: 133 loss: 0.05776002\n",
            "iteration: 134 loss: 0.05963502\n",
            "iteration: 135 loss: 0.09521231\n",
            "iteration: 136 loss: 0.04999116\n",
            "iteration: 137 loss: 0.16327852\n",
            "iteration: 138 loss: 0.06336720\n",
            "iteration: 139 loss: 0.01822813\n",
            "iteration: 140 loss: 0.05353640\n",
            "iteration: 141 loss: 0.01621594\n",
            "iteration: 142 loss: 0.08974555\n",
            "iteration: 143 loss: 0.07962207\n",
            "iteration: 144 loss: 0.02542684\n",
            "iteration: 145 loss: 0.09351884\n",
            "iteration: 146 loss: 0.12708519\n",
            "iteration: 147 loss: 0.07976960\n",
            "iteration: 148 loss: 0.29115728\n",
            "iteration: 149 loss: 1.07481360\n",
            "iteration: 150 loss: 0.04899473\n",
            "iteration: 151 loss: 0.05678006\n",
            "iteration: 152 loss: 0.07709460\n",
            "iteration: 153 loss: 0.11753106\n",
            "iteration: 154 loss: 0.40371433\n",
            "iteration: 155 loss: 0.09056301\n",
            "iteration: 156 loss: 0.12585594\n",
            "iteration: 157 loss: 0.04240365\n",
            "iteration: 158 loss: 0.10614756\n",
            "iteration: 159 loss: 0.09160987\n",
            "iteration: 160 loss: 0.05439820\n",
            "iteration: 161 loss: 0.20345582\n",
            "iteration: 162 loss: 0.19297141\n",
            "iteration: 163 loss: 0.07035805\n",
            "iteration: 164 loss: 0.12177704\n",
            "iteration: 165 loss: 0.10879689\n",
            "iteration: 166 loss: 0.09155018\n",
            "iteration: 167 loss: 0.05681738\n",
            "iteration: 168 loss: 0.11690179\n",
            "iteration: 169 loss: 0.09302899\n",
            "iteration: 170 loss: 0.02508398\n",
            "iteration: 171 loss: 0.14251512\n",
            "iteration: 172 loss: 0.08147718\n",
            "iteration: 173 loss: 0.07869034\n",
            "iteration: 174 loss: 0.54868460\n",
            "iteration: 175 loss: 0.05896468\n",
            "iteration: 176 loss: 0.10173163\n",
            "iteration: 177 loss: 0.41980195\n",
            "iteration: 178 loss: 0.30122158\n",
            "iteration: 179 loss: 0.13750555\n",
            "iteration: 180 loss: 0.08188521\n",
            "iteration: 181 loss: 0.06001436\n",
            "iteration: 182 loss: 0.18772878\n",
            "iteration: 183 loss: 0.07490221\n",
            "iteration: 184 loss: 0.08010190\n",
            "iteration: 185 loss: 0.10014558\n",
            "iteration: 186 loss: 0.06831079\n",
            "iteration: 187 loss: 0.02826246\n",
            "iteration: 188 loss: 0.08612385\n",
            "iteration: 189 loss: 0.04675797\n",
            "iteration: 190 loss: 0.74041677\n",
            "iteration: 191 loss: 1.03296709\n",
            "iteration: 192 loss: 0.15500398\n",
            "iteration: 193 loss: 0.03972577\n",
            "iteration: 194 loss: 0.05185209\n",
            "iteration: 195 loss: 0.11125039\n",
            "iteration: 196 loss: 0.14524977\n",
            "iteration: 197 loss: 0.10430156\n",
            "iteration: 198 loss: 0.11353367\n",
            "iteration: 199 loss: 0.20005681\n",
            "epoch:  87 mean loss training: 0.14802359\n",
            "epoch:  87 mean loss validation: 0.36246094\n",
            "iteration:   0 loss: 0.09996135\n",
            "iteration:   1 loss: 0.31940392\n",
            "iteration:   2 loss: 0.03112579\n",
            "iteration:   3 loss: 0.05101429\n",
            "iteration:   4 loss: 0.25513658\n",
            "iteration:   5 loss: 0.22214398\n",
            "iteration:   6 loss: 0.15253207\n",
            "iteration:   7 loss: 0.09071670\n",
            "iteration:   8 loss: 0.04809094\n",
            "iteration:   9 loss: 0.11734712\n",
            "iteration:  10 loss: 0.24064657\n",
            "iteration:  11 loss: 0.46535739\n",
            "iteration:  12 loss: 0.05846041\n",
            "iteration:  13 loss: 0.06004900\n",
            "iteration:  14 loss: 0.04347872\n",
            "iteration:  15 loss: 0.04422256\n",
            "iteration:  16 loss: 0.05779979\n",
            "iteration:  17 loss: 0.05003948\n",
            "iteration:  18 loss: 0.04545230\n",
            "iteration:  19 loss: 0.14847943\n",
            "iteration:  20 loss: 0.20515445\n",
            "iteration:  21 loss: 0.01947534\n",
            "iteration:  22 loss: 0.47067034\n",
            "iteration:  23 loss: 0.67336601\n",
            "iteration:  24 loss: 0.03627524\n",
            "iteration:  25 loss: 0.00930526\n",
            "iteration:  26 loss: 0.13757096\n",
            "iteration:  27 loss: 0.04862167\n",
            "iteration:  28 loss: 0.09441643\n",
            "iteration:  29 loss: 0.05544296\n",
            "iteration:  30 loss: 0.16075805\n",
            "iteration:  31 loss: 0.05883461\n",
            "iteration:  32 loss: 0.04208077\n",
            "iteration:  33 loss: 0.09935724\n",
            "iteration:  34 loss: 0.15621628\n",
            "iteration:  35 loss: 0.15606749\n",
            "iteration:  36 loss: 0.80919194\n",
            "iteration:  37 loss: 0.04917061\n",
            "iteration:  38 loss: 0.04307352\n",
            "iteration:  39 loss: 0.11813418\n",
            "iteration:  40 loss: 0.02960729\n",
            "iteration:  41 loss: 0.15337738\n",
            "iteration:  42 loss: 0.09512705\n",
            "iteration:  43 loss: 0.06459532\n",
            "iteration:  44 loss: 0.13722725\n",
            "iteration:  45 loss: 0.02449161\n",
            "iteration:  46 loss: 0.02851385\n",
            "iteration:  47 loss: 0.07824596\n",
            "iteration:  48 loss: 0.06041706\n",
            "iteration:  49 loss: 0.18548271\n",
            "iteration:  50 loss: 0.05726954\n",
            "iteration:  51 loss: 0.08156782\n",
            "iteration:  52 loss: 0.19918554\n",
            "iteration:  53 loss: 0.02807045\n",
            "iteration:  54 loss: 0.07121573\n",
            "iteration:  55 loss: 0.03406288\n",
            "iteration:  56 loss: 0.61866397\n",
            "iteration:  57 loss: 0.16147904\n",
            "iteration:  58 loss: 0.03559550\n",
            "iteration:  59 loss: 0.08009671\n",
            "iteration:  60 loss: 0.20011857\n",
            "iteration:  61 loss: 0.18270841\n",
            "iteration:  62 loss: 0.03347241\n",
            "iteration:  63 loss: 0.00881231\n",
            "iteration:  64 loss: 0.20710137\n",
            "iteration:  65 loss: 0.19183946\n",
            "iteration:  66 loss: 0.05755059\n",
            "iteration:  67 loss: 0.25375748\n",
            "iteration:  68 loss: 0.01731189\n",
            "iteration:  69 loss: 0.03079234\n",
            "iteration:  70 loss: 0.04239180\n",
            "iteration:  71 loss: 0.65217847\n",
            "iteration:  72 loss: 0.07635845\n",
            "iteration:  73 loss: 0.13123448\n",
            "iteration:  74 loss: 0.05563988\n",
            "iteration:  75 loss: 0.07239693\n",
            "iteration:  76 loss: 0.07242695\n",
            "iteration:  77 loss: 0.01711611\n",
            "iteration:  78 loss: 0.06835371\n",
            "iteration:  79 loss: 0.11718012\n",
            "iteration:  80 loss: 0.15684603\n",
            "iteration:  81 loss: 0.03225737\n",
            "iteration:  82 loss: 0.18791723\n",
            "iteration:  83 loss: 0.04975672\n",
            "iteration:  84 loss: 0.04070542\n",
            "iteration:  85 loss: 0.31315315\n",
            "iteration:  86 loss: 0.06579445\n",
            "iteration:  87 loss: 0.34522775\n",
            "iteration:  88 loss: 0.16091648\n",
            "iteration:  89 loss: 0.08014859\n",
            "iteration:  90 loss: 0.15597068\n",
            "iteration:  91 loss: 0.01279127\n",
            "iteration:  92 loss: 0.05378612\n",
            "iteration:  93 loss: 0.10846087\n",
            "iteration:  94 loss: 0.03855335\n",
            "iteration:  95 loss: 0.29904139\n",
            "iteration:  96 loss: 0.06218106\n",
            "iteration:  97 loss: 0.10018282\n",
            "iteration:  98 loss: 0.07648570\n",
            "iteration:  99 loss: 0.02333081\n",
            "iteration: 100 loss: 0.56155169\n",
            "iteration: 101 loss: 0.09041028\n",
            "iteration: 102 loss: 0.05748521\n",
            "iteration: 103 loss: 0.17080994\n",
            "iteration: 104 loss: 0.03687745\n",
            "iteration: 105 loss: 0.07746895\n",
            "iteration: 106 loss: 0.18357821\n",
            "iteration: 107 loss: 0.09755270\n",
            "iteration: 108 loss: 0.10768677\n",
            "iteration: 109 loss: 0.02046452\n",
            "iteration: 110 loss: 0.13439709\n",
            "iteration: 111 loss: 0.17622089\n",
            "iteration: 112 loss: 0.11074328\n",
            "iteration: 113 loss: 0.10278548\n",
            "iteration: 114 loss: 0.16572645\n",
            "iteration: 115 loss: 0.08643674\n",
            "iteration: 116 loss: 0.04524232\n",
            "iteration: 117 loss: 0.11097165\n",
            "iteration: 118 loss: 0.43012404\n",
            "iteration: 119 loss: 0.10028467\n",
            "iteration: 120 loss: 0.16222912\n",
            "iteration: 121 loss: 0.13841006\n",
            "iteration: 122 loss: 0.08980108\n",
            "iteration: 123 loss: 0.01494119\n",
            "iteration: 124 loss: 0.22688627\n",
            "iteration: 125 loss: 0.03545205\n",
            "iteration: 126 loss: 0.03028611\n",
            "iteration: 127 loss: 0.35542235\n",
            "iteration: 128 loss: 0.11674613\n",
            "iteration: 129 loss: 0.18095836\n",
            "iteration: 130 loss: 0.15504934\n",
            "iteration: 131 loss: 0.23045206\n",
            "iteration: 132 loss: 0.05997771\n",
            "iteration: 133 loss: 0.07974762\n",
            "iteration: 134 loss: 0.20085099\n",
            "iteration: 135 loss: 0.05695767\n",
            "iteration: 136 loss: 0.07870641\n",
            "iteration: 137 loss: 0.03919663\n",
            "iteration: 138 loss: 0.03834658\n",
            "iteration: 139 loss: 0.07112880\n",
            "iteration: 140 loss: 0.08474207\n",
            "iteration: 141 loss: 0.04142047\n",
            "iteration: 142 loss: 0.10275197\n",
            "iteration: 143 loss: 0.04684450\n",
            "iteration: 144 loss: 0.05120995\n",
            "iteration: 145 loss: 0.11008067\n",
            "iteration: 146 loss: 0.19611476\n",
            "iteration: 147 loss: 0.45706379\n",
            "iteration: 148 loss: 0.03546727\n",
            "iteration: 149 loss: 0.17956433\n",
            "iteration: 150 loss: 0.36553147\n",
            "iteration: 151 loss: 0.02195371\n",
            "iteration: 152 loss: 0.28589538\n",
            "iteration: 153 loss: 0.14945410\n",
            "iteration: 154 loss: 0.11668884\n",
            "iteration: 155 loss: 0.02750816\n",
            "iteration: 156 loss: 0.16424522\n",
            "iteration: 157 loss: 0.18085751\n",
            "iteration: 158 loss: 0.85568792\n",
            "iteration: 159 loss: 0.20705374\n",
            "iteration: 160 loss: 0.08807524\n",
            "iteration: 161 loss: 0.07668266\n",
            "iteration: 162 loss: 0.15548742\n",
            "iteration: 163 loss: 0.06564037\n",
            "iteration: 164 loss: 0.05609068\n",
            "iteration: 165 loss: 0.07032651\n",
            "iteration: 166 loss: 0.01581925\n",
            "iteration: 167 loss: 0.10177337\n",
            "iteration: 168 loss: 0.15496898\n",
            "iteration: 169 loss: 0.02206682\n",
            "iteration: 170 loss: 0.05926586\n",
            "iteration: 171 loss: 0.04947612\n",
            "iteration: 172 loss: 0.07472821\n",
            "iteration: 173 loss: 0.38763934\n",
            "iteration: 174 loss: 0.13945006\n",
            "iteration: 175 loss: 0.04582603\n",
            "iteration: 176 loss: 0.08229832\n",
            "iteration: 177 loss: 0.08659253\n",
            "iteration: 178 loss: 0.70671988\n",
            "iteration: 179 loss: 0.05384308\n",
            "iteration: 180 loss: 0.28818107\n",
            "iteration: 181 loss: 0.25099707\n",
            "iteration: 182 loss: 0.20324667\n",
            "iteration: 183 loss: 0.02821925\n",
            "iteration: 184 loss: 0.04248837\n",
            "iteration: 185 loss: 0.03548601\n",
            "iteration: 186 loss: 0.03978754\n",
            "iteration: 187 loss: 0.02138256\n",
            "iteration: 188 loss: 0.02876702\n",
            "iteration: 189 loss: 0.06499686\n",
            "iteration: 190 loss: 0.03848305\n",
            "iteration: 191 loss: 0.06360230\n",
            "iteration: 192 loss: 0.04993601\n",
            "iteration: 193 loss: 0.04472591\n",
            "iteration: 194 loss: 0.06093937\n",
            "iteration: 195 loss: 0.23397851\n",
            "iteration: 196 loss: 0.10502367\n",
            "iteration: 197 loss: 0.03378016\n",
            "iteration: 198 loss: 0.09398106\n",
            "iteration: 199 loss: 0.05541200\n",
            "epoch:  88 mean loss training: 0.13152935\n",
            "epoch:  88 mean loss validation: 0.37567773\n",
            "iteration:   0 loss: 0.06638669\n",
            "iteration:   1 loss: 0.01119743\n",
            "iteration:   2 loss: 0.12677509\n",
            "iteration:   3 loss: 0.43827274\n",
            "iteration:   4 loss: 0.36676145\n",
            "iteration:   5 loss: 0.26532015\n",
            "iteration:   6 loss: 0.07439851\n",
            "iteration:   7 loss: 0.15676223\n",
            "iteration:   8 loss: 0.05978948\n",
            "iteration:   9 loss: 0.03014961\n",
            "iteration:  10 loss: 0.05492243\n",
            "iteration:  11 loss: 0.16526097\n",
            "iteration:  12 loss: 0.03228111\n",
            "iteration:  13 loss: 0.07005770\n",
            "iteration:  14 loss: 0.07005856\n",
            "iteration:  15 loss: 0.06164341\n",
            "iteration:  16 loss: 0.06673490\n",
            "iteration:  17 loss: 0.12377473\n",
            "iteration:  18 loss: 0.44368950\n",
            "iteration:  19 loss: 0.07716697\n",
            "iteration:  20 loss: 0.08254085\n",
            "iteration:  21 loss: 0.11249615\n",
            "iteration:  22 loss: 0.74561894\n",
            "iteration:  23 loss: 0.18854555\n",
            "iteration:  24 loss: 0.20104913\n",
            "iteration:  25 loss: 0.34110695\n",
            "iteration:  26 loss: 0.10446854\n",
            "iteration:  27 loss: 0.08459052\n",
            "iteration:  28 loss: 0.20279254\n",
            "iteration:  29 loss: 0.09319872\n",
            "iteration:  30 loss: 0.48155275\n",
            "iteration:  31 loss: 0.04719098\n",
            "iteration:  32 loss: 0.34353709\n",
            "iteration:  33 loss: 0.06491707\n",
            "iteration:  34 loss: 0.26134679\n",
            "iteration:  35 loss: 0.27958354\n",
            "iteration:  36 loss: 0.21047106\n",
            "iteration:  37 loss: 0.12457657\n",
            "iteration:  38 loss: 0.02941320\n",
            "iteration:  39 loss: 0.01326253\n",
            "iteration:  40 loss: 0.11962092\n",
            "iteration:  41 loss: 0.05763111\n",
            "iteration:  42 loss: 0.06794912\n",
            "iteration:  43 loss: 0.09668887\n",
            "iteration:  44 loss: 0.03746541\n",
            "iteration:  45 loss: 0.06059879\n",
            "iteration:  46 loss: 0.05408552\n",
            "iteration:  47 loss: 0.06524777\n",
            "iteration:  48 loss: 0.17518969\n",
            "iteration:  49 loss: 0.19677272\n",
            "iteration:  50 loss: 0.10877191\n",
            "iteration:  51 loss: 0.03131169\n",
            "iteration:  52 loss: 0.15755622\n",
            "iteration:  53 loss: 0.13712960\n",
            "iteration:  54 loss: 0.10941979\n",
            "iteration:  55 loss: 0.15157066\n",
            "iteration:  56 loss: 0.04407638\n",
            "iteration:  57 loss: 0.09114499\n",
            "iteration:  58 loss: 0.10731161\n",
            "iteration:  59 loss: 0.34083772\n",
            "iteration:  60 loss: 0.09914923\n",
            "iteration:  61 loss: 0.02412956\n",
            "iteration:  62 loss: 0.14106630\n",
            "iteration:  63 loss: 0.11097424\n",
            "iteration:  64 loss: 0.14870952\n",
            "iteration:  65 loss: 0.12403655\n",
            "iteration:  66 loss: 0.05103006\n",
            "iteration:  67 loss: 0.05767565\n",
            "iteration:  68 loss: 0.07748934\n",
            "iteration:  69 loss: 0.20468836\n",
            "iteration:  70 loss: 0.02669117\n",
            "iteration:  71 loss: 0.03472327\n",
            "iteration:  72 loss: 0.06677579\n",
            "iteration:  73 loss: 0.12514794\n",
            "iteration:  74 loss: 0.76008898\n",
            "iteration:  75 loss: 0.03302107\n",
            "iteration:  76 loss: 0.15059917\n",
            "iteration:  77 loss: 0.11307859\n",
            "iteration:  78 loss: 0.07073071\n",
            "iteration:  79 loss: 0.05382592\n",
            "iteration:  80 loss: 0.18063831\n",
            "iteration:  81 loss: 0.03702016\n",
            "iteration:  82 loss: 0.22959654\n",
            "iteration:  83 loss: 0.07721088\n",
            "iteration:  84 loss: 0.18598735\n",
            "iteration:  85 loss: 0.07785532\n",
            "iteration:  86 loss: 0.12571406\n",
            "iteration:  87 loss: 0.06090850\n",
            "iteration:  88 loss: 0.10899217\n",
            "iteration:  89 loss: 0.07619464\n",
            "iteration:  90 loss: 0.08987831\n",
            "iteration:  91 loss: 0.09472573\n",
            "iteration:  92 loss: 0.27759197\n",
            "iteration:  93 loss: 0.04964346\n",
            "iteration:  94 loss: 0.04594525\n",
            "iteration:  95 loss: 0.08174171\n",
            "iteration:  96 loss: 0.04206429\n",
            "iteration:  97 loss: 0.17332420\n",
            "iteration:  98 loss: 0.12626129\n",
            "iteration:  99 loss: 0.03848775\n",
            "iteration: 100 loss: 0.10884003\n",
            "iteration: 101 loss: 0.16202949\n",
            "iteration: 102 loss: 0.10335777\n",
            "iteration: 103 loss: 0.06437308\n",
            "iteration: 104 loss: 0.03085237\n",
            "iteration: 105 loss: 0.14125457\n",
            "iteration: 106 loss: 0.38291660\n",
            "iteration: 107 loss: 0.04344049\n",
            "iteration: 108 loss: 0.02810995\n",
            "iteration: 109 loss: 0.13845111\n",
            "iteration: 110 loss: 0.13229527\n",
            "iteration: 111 loss: 0.01437945\n",
            "iteration: 112 loss: 0.03926304\n",
            "iteration: 113 loss: 0.27676955\n",
            "iteration: 114 loss: 0.09330940\n",
            "iteration: 115 loss: 0.08390514\n",
            "iteration: 116 loss: 0.04527696\n",
            "iteration: 117 loss: 0.06628836\n",
            "iteration: 118 loss: 0.04536359\n",
            "iteration: 119 loss: 0.08148724\n",
            "iteration: 120 loss: 0.34291473\n",
            "iteration: 121 loss: 0.04056088\n",
            "iteration: 122 loss: 0.09049316\n",
            "iteration: 123 loss: 0.40032265\n",
            "iteration: 124 loss: 0.40326038\n",
            "iteration: 125 loss: 0.12681733\n",
            "iteration: 126 loss: 0.11113252\n",
            "iteration: 127 loss: 0.47731167\n",
            "iteration: 128 loss: 0.06510800\n",
            "iteration: 129 loss: 0.11924047\n",
            "iteration: 130 loss: 0.25421390\n",
            "iteration: 131 loss: 0.13045874\n",
            "iteration: 132 loss: 0.05903888\n",
            "iteration: 133 loss: 0.12667917\n",
            "iteration: 134 loss: 0.07764274\n",
            "iteration: 135 loss: 0.10259779\n",
            "iteration: 136 loss: 0.19114868\n",
            "iteration: 137 loss: 0.15917277\n",
            "iteration: 138 loss: 0.40618232\n",
            "iteration: 139 loss: 0.13426943\n",
            "iteration: 140 loss: 0.07683900\n",
            "iteration: 141 loss: 0.33018064\n",
            "iteration: 142 loss: 0.13455437\n",
            "iteration: 143 loss: 0.04971235\n",
            "iteration: 144 loss: 0.22689624\n",
            "iteration: 145 loss: 0.15452516\n",
            "iteration: 146 loss: 0.05045981\n",
            "iteration: 147 loss: 0.40460649\n",
            "iteration: 148 loss: 0.16325702\n",
            "iteration: 149 loss: 0.03388831\n",
            "iteration: 150 loss: 0.04903306\n",
            "iteration: 151 loss: 0.11955190\n",
            "iteration: 152 loss: 0.02005061\n",
            "iteration: 153 loss: 0.15457138\n",
            "iteration: 154 loss: 0.05972750\n",
            "iteration: 155 loss: 0.31451261\n",
            "iteration: 156 loss: 0.23618810\n",
            "iteration: 157 loss: 0.06727812\n",
            "iteration: 158 loss: 0.20866750\n",
            "iteration: 159 loss: 0.08798996\n",
            "iteration: 160 loss: 0.05117507\n",
            "iteration: 161 loss: 0.04733508\n",
            "iteration: 162 loss: 0.30558521\n",
            "iteration: 163 loss: 0.08585169\n",
            "iteration: 164 loss: 0.20748386\n",
            "iteration: 165 loss: 0.13794661\n",
            "iteration: 166 loss: 0.13162261\n",
            "iteration: 167 loss: 0.06877205\n",
            "iteration: 168 loss: 0.11511144\n",
            "iteration: 169 loss: 0.15721644\n",
            "iteration: 170 loss: 0.09831829\n",
            "iteration: 171 loss: 0.07148435\n",
            "iteration: 172 loss: 0.21633649\n",
            "iteration: 173 loss: 0.33162066\n",
            "iteration: 174 loss: 0.05207695\n",
            "iteration: 175 loss: 0.20543090\n",
            "iteration: 176 loss: 0.16013996\n",
            "iteration: 177 loss: 0.09990117\n",
            "iteration: 178 loss: 0.54785907\n",
            "iteration: 179 loss: 0.08501364\n",
            "iteration: 180 loss: 0.02559352\n",
            "iteration: 181 loss: 0.19553275\n",
            "iteration: 182 loss: 0.14739427\n",
            "iteration: 183 loss: 0.62920636\n",
            "iteration: 184 loss: 0.30644429\n",
            "iteration: 185 loss: 0.10933527\n",
            "iteration: 186 loss: 0.17885889\n",
            "iteration: 187 loss: 0.06926283\n",
            "iteration: 188 loss: 0.48268729\n",
            "iteration: 189 loss: 0.02818656\n",
            "iteration: 190 loss: 0.04386082\n",
            "iteration: 191 loss: 0.06481425\n",
            "iteration: 192 loss: 0.45285553\n",
            "iteration: 193 loss: 0.16064411\n",
            "iteration: 194 loss: 0.04730419\n",
            "iteration: 195 loss: 0.05545953\n",
            "iteration: 196 loss: 0.09234341\n",
            "iteration: 197 loss: 0.04827318\n",
            "iteration: 198 loss: 0.04475547\n",
            "iteration: 199 loss: 0.01899114\n",
            "epoch:  89 mean loss training: 0.14429280\n",
            "epoch:  89 mean loss validation: 0.36644053\n",
            "iteration:   0 loss: 0.22786379\n",
            "iteration:   1 loss: 0.06895285\n",
            "iteration:   2 loss: 0.09026676\n",
            "iteration:   3 loss: 0.16747893\n",
            "iteration:   4 loss: 0.07801676\n",
            "iteration:   5 loss: 0.08542720\n",
            "iteration:   6 loss: 0.16442302\n",
            "iteration:   7 loss: 0.08515561\n",
            "iteration:   8 loss: 0.15399875\n",
            "iteration:   9 loss: 0.10346875\n",
            "iteration:  10 loss: 0.21758702\n",
            "iteration:  11 loss: 0.34184307\n",
            "iteration:  12 loss: 0.09098533\n",
            "iteration:  13 loss: 0.38612631\n",
            "iteration:  14 loss: 0.19970173\n",
            "iteration:  15 loss: 0.22477221\n",
            "iteration:  16 loss: 0.19036812\n",
            "iteration:  17 loss: 0.18599966\n",
            "iteration:  18 loss: 0.04747893\n",
            "iteration:  19 loss: 0.10023531\n",
            "iteration:  20 loss: 0.15034987\n",
            "iteration:  21 loss: 0.11888152\n",
            "iteration:  22 loss: 0.11806955\n",
            "iteration:  23 loss: 0.10232414\n",
            "iteration:  24 loss: 0.16857386\n",
            "iteration:  25 loss: 0.07291871\n",
            "iteration:  26 loss: 0.03151515\n",
            "iteration:  27 loss: 0.10971621\n",
            "iteration:  28 loss: 0.03340788\n",
            "iteration:  29 loss: 0.12120787\n",
            "iteration:  30 loss: 0.05351302\n",
            "iteration:  31 loss: 0.62647825\n",
            "iteration:  32 loss: 0.33181533\n",
            "iteration:  33 loss: 0.38211846\n",
            "iteration:  34 loss: 0.05109471\n",
            "iteration:  35 loss: 0.01213077\n",
            "iteration:  36 loss: 0.14686523\n",
            "iteration:  37 loss: 0.06575157\n",
            "iteration:  38 loss: 0.97269762\n",
            "iteration:  39 loss: 0.01795823\n",
            "iteration:  40 loss: 0.08142451\n",
            "iteration:  41 loss: 0.04435159\n",
            "iteration:  42 loss: 0.24915880\n",
            "iteration:  43 loss: 0.13235028\n",
            "iteration:  44 loss: 0.16255072\n",
            "iteration:  45 loss: 0.06125852\n",
            "iteration:  46 loss: 0.14509565\n",
            "iteration:  47 loss: 0.61227256\n",
            "iteration:  48 loss: 0.04477485\n",
            "iteration:  49 loss: 0.16511716\n",
            "iteration:  50 loss: 0.09365153\n",
            "iteration:  51 loss: 0.43218705\n",
            "iteration:  52 loss: 0.00906848\n",
            "iteration:  53 loss: 0.06830046\n",
            "iteration:  54 loss: 0.05308311\n",
            "iteration:  55 loss: 0.04054055\n",
            "iteration:  56 loss: 0.08971198\n",
            "iteration:  57 loss: 0.07422335\n",
            "iteration:  58 loss: 0.11416758\n",
            "iteration:  59 loss: 0.04328787\n",
            "iteration:  60 loss: 0.02716427\n",
            "iteration:  61 loss: 0.19836588\n",
            "iteration:  62 loss: 0.03047353\n",
            "iteration:  63 loss: 0.02192428\n",
            "iteration:  64 loss: 0.66828084\n",
            "iteration:  65 loss: 0.12062735\n",
            "iteration:  66 loss: 0.03512129\n",
            "iteration:  67 loss: 0.07777462\n",
            "iteration:  68 loss: 0.06784023\n",
            "iteration:  69 loss: 0.08708747\n",
            "iteration:  70 loss: 0.04527615\n",
            "iteration:  71 loss: 0.02609271\n",
            "iteration:  72 loss: 0.04540642\n",
            "iteration:  73 loss: 0.04688628\n",
            "iteration:  74 loss: 0.53721672\n",
            "iteration:  75 loss: 0.13202688\n",
            "iteration:  76 loss: 0.04838663\n",
            "iteration:  77 loss: 0.04461836\n",
            "iteration:  78 loss: 0.04192866\n",
            "iteration:  79 loss: 0.06030653\n",
            "iteration:  80 loss: 0.56906974\n",
            "iteration:  81 loss: 0.04734461\n",
            "iteration:  82 loss: 0.00693227\n",
            "iteration:  83 loss: 0.14225614\n",
            "iteration:  84 loss: 0.16338798\n",
            "iteration:  85 loss: 0.09336166\n",
            "iteration:  86 loss: 0.02795243\n",
            "iteration:  87 loss: 0.12407463\n",
            "iteration:  88 loss: 0.03968794\n",
            "iteration:  89 loss: 0.16129224\n",
            "iteration:  90 loss: 0.05980763\n",
            "iteration:  91 loss: 0.07079179\n",
            "iteration:  92 loss: 0.15557082\n",
            "iteration:  93 loss: 0.06163865\n",
            "iteration:  94 loss: 0.19442843\n",
            "iteration:  95 loss: 0.25670516\n",
            "iteration:  96 loss: 0.03910071\n",
            "iteration:  97 loss: 0.24539641\n",
            "iteration:  98 loss: 0.03261255\n",
            "iteration:  99 loss: 0.09550764\n",
            "iteration: 100 loss: 0.23525341\n",
            "iteration: 101 loss: 0.39787477\n",
            "iteration: 102 loss: 0.05249651\n",
            "iteration: 103 loss: 0.18463276\n",
            "iteration: 104 loss: 0.10254471\n",
            "iteration: 105 loss: 0.12610142\n",
            "iteration: 106 loss: 0.07533514\n",
            "iteration: 107 loss: 0.40262839\n",
            "iteration: 108 loss: 0.12589128\n",
            "iteration: 109 loss: 0.01986537\n",
            "iteration: 110 loss: 0.02485954\n",
            "iteration: 111 loss: 0.02770467\n",
            "iteration: 112 loss: 0.06265470\n",
            "iteration: 113 loss: 0.06636450\n",
            "iteration: 114 loss: 0.23329507\n",
            "iteration: 115 loss: 0.09382739\n",
            "iteration: 116 loss: 0.05483812\n",
            "iteration: 117 loss: 0.14005737\n",
            "iteration: 118 loss: 0.26047793\n",
            "iteration: 119 loss: 0.03718869\n",
            "iteration: 120 loss: 0.11104794\n",
            "iteration: 121 loss: 0.10278574\n",
            "iteration: 122 loss: 0.23296621\n",
            "iteration: 123 loss: 0.10090949\n",
            "iteration: 124 loss: 0.12951846\n",
            "iteration: 125 loss: 0.30692434\n",
            "iteration: 126 loss: 0.21474338\n",
            "iteration: 127 loss: 0.18390356\n",
            "iteration: 128 loss: 0.05900022\n",
            "iteration: 129 loss: 0.01468793\n",
            "iteration: 130 loss: 0.10757694\n",
            "iteration: 131 loss: 0.32175776\n",
            "iteration: 132 loss: 0.03413445\n",
            "iteration: 133 loss: 0.08947287\n",
            "iteration: 134 loss: 0.14282562\n",
            "iteration: 135 loss: 0.04500838\n",
            "iteration: 136 loss: 0.08002843\n",
            "iteration: 137 loss: 0.04066861\n",
            "iteration: 138 loss: 0.02190314\n",
            "iteration: 139 loss: 0.13478114\n",
            "iteration: 140 loss: 0.19045258\n",
            "iteration: 141 loss: 0.08136609\n",
            "iteration: 142 loss: 0.03325842\n",
            "iteration: 143 loss: 0.48128736\n",
            "iteration: 144 loss: 0.01722195\n",
            "iteration: 145 loss: 0.17610012\n",
            "iteration: 146 loss: 0.13963667\n",
            "iteration: 147 loss: 0.26951754\n",
            "iteration: 148 loss: 0.08364116\n",
            "iteration: 149 loss: 0.05895219\n",
            "iteration: 150 loss: 0.20506020\n",
            "iteration: 151 loss: 0.12559654\n",
            "iteration: 152 loss: 0.01954745\n",
            "iteration: 153 loss: 0.07850350\n",
            "iteration: 154 loss: 0.11502042\n",
            "iteration: 155 loss: 0.14281207\n",
            "iteration: 156 loss: 0.12970708\n",
            "iteration: 157 loss: 0.04808572\n",
            "iteration: 158 loss: 0.18986708\n",
            "iteration: 159 loss: 0.26947027\n",
            "iteration: 160 loss: 0.08974352\n",
            "iteration: 161 loss: 0.04474204\n",
            "iteration: 162 loss: 0.08944434\n",
            "iteration: 163 loss: 0.13254139\n",
            "iteration: 164 loss: 0.11982828\n",
            "iteration: 165 loss: 0.31952989\n",
            "iteration: 166 loss: 0.17829193\n",
            "iteration: 167 loss: 0.02146034\n",
            "iteration: 168 loss: 0.07235753\n",
            "iteration: 169 loss: 0.21778888\n",
            "iteration: 170 loss: 0.08341002\n",
            "iteration: 171 loss: 0.15768570\n",
            "iteration: 172 loss: 0.04291862\n",
            "iteration: 173 loss: 0.04603149\n",
            "iteration: 174 loss: 0.15290561\n",
            "iteration: 175 loss: 0.11938569\n",
            "iteration: 176 loss: 0.08114512\n",
            "iteration: 177 loss: 1.02942383\n",
            "iteration: 178 loss: 0.53174740\n",
            "iteration: 179 loss: 0.02679914\n",
            "iteration: 180 loss: 0.28641206\n",
            "iteration: 181 loss: 0.53419238\n",
            "iteration: 182 loss: 0.08723678\n",
            "iteration: 183 loss: 0.15320987\n",
            "iteration: 184 loss: 0.03622481\n",
            "iteration: 185 loss: 0.04667727\n",
            "iteration: 186 loss: 0.14677078\n",
            "iteration: 187 loss: 0.13368799\n",
            "iteration: 188 loss: 0.06691087\n",
            "iteration: 189 loss: 0.07778274\n",
            "iteration: 190 loss: 0.06529962\n",
            "iteration: 191 loss: 0.04694403\n",
            "iteration: 192 loss: 0.17128722\n",
            "iteration: 193 loss: 0.04913348\n",
            "iteration: 194 loss: 0.03102088\n",
            "iteration: 195 loss: 0.07618369\n",
            "iteration: 196 loss: 0.03774403\n",
            "iteration: 197 loss: 0.04904579\n",
            "iteration: 198 loss: 0.04396745\n",
            "iteration: 199 loss: 0.12756886\n",
            "epoch:  90 mean loss training: 0.14220981\n",
            "epoch:  90 mean loss validation: 0.37889257\n",
            "iteration:   0 loss: 0.37871757\n",
            "iteration:   1 loss: 0.06665971\n",
            "iteration:   2 loss: 0.08551452\n",
            "iteration:   3 loss: 0.06325546\n",
            "iteration:   4 loss: 0.08396438\n",
            "iteration:   5 loss: 0.50312233\n",
            "iteration:   6 loss: 0.62131602\n",
            "iteration:   7 loss: 0.06161304\n",
            "iteration:   8 loss: 0.05842748\n",
            "iteration:   9 loss: 0.03115650\n",
            "iteration:  10 loss: 0.05073215\n",
            "iteration:  11 loss: 0.07858730\n",
            "iteration:  12 loss: 0.46636012\n",
            "iteration:  13 loss: 0.32139242\n",
            "iteration:  14 loss: 0.05578062\n",
            "iteration:  15 loss: 0.04096078\n",
            "iteration:  16 loss: 0.09114704\n",
            "iteration:  17 loss: 0.06027292\n",
            "iteration:  18 loss: 0.13564165\n",
            "iteration:  19 loss: 0.13622585\n",
            "iteration:  20 loss: 0.13074557\n",
            "iteration:  21 loss: 0.02993411\n",
            "iteration:  22 loss: 0.14748508\n",
            "iteration:  23 loss: 0.05325975\n",
            "iteration:  24 loss: 0.13878612\n",
            "iteration:  25 loss: 0.03170456\n",
            "iteration:  26 loss: 0.20603098\n",
            "iteration:  27 loss: 0.08954159\n",
            "iteration:  28 loss: 0.19241801\n",
            "iteration:  29 loss: 0.06016496\n",
            "iteration:  30 loss: 0.02390538\n",
            "iteration:  31 loss: 0.19624048\n",
            "iteration:  32 loss: 0.14112654\n",
            "iteration:  33 loss: 0.16554470\n",
            "iteration:  34 loss: 0.10977150\n",
            "iteration:  35 loss: 0.12470153\n",
            "iteration:  36 loss: 0.09314698\n",
            "iteration:  37 loss: 0.10752311\n",
            "iteration:  38 loss: 0.21997015\n",
            "iteration:  39 loss: 0.02993518\n",
            "iteration:  40 loss: 0.45178357\n",
            "iteration:  41 loss: 0.02928056\n",
            "iteration:  42 loss: 0.03615354\n",
            "iteration:  43 loss: 0.53900272\n",
            "iteration:  44 loss: 0.29942721\n",
            "iteration:  45 loss: 0.05704267\n",
            "iteration:  46 loss: 0.06331107\n",
            "iteration:  47 loss: 0.29187688\n",
            "iteration:  48 loss: 0.37886754\n",
            "iteration:  49 loss: 0.15827830\n",
            "iteration:  50 loss: 0.12453240\n",
            "iteration:  51 loss: 0.02790489\n",
            "iteration:  52 loss: 0.06401421\n",
            "iteration:  53 loss: 0.47422937\n",
            "iteration:  54 loss: 0.05383375\n",
            "iteration:  55 loss: 0.19413288\n",
            "iteration:  56 loss: 0.16301233\n",
            "iteration:  57 loss: 0.05469956\n",
            "iteration:  58 loss: 0.11397710\n",
            "iteration:  59 loss: 0.57264179\n",
            "iteration:  60 loss: 0.08468574\n",
            "iteration:  61 loss: 0.17151137\n",
            "iteration:  62 loss: 0.01759692\n",
            "iteration:  63 loss: 0.03720046\n",
            "iteration:  64 loss: 0.44539309\n",
            "iteration:  65 loss: 0.07545529\n",
            "iteration:  66 loss: 0.01954760\n",
            "iteration:  67 loss: 0.04612077\n",
            "iteration:  68 loss: 0.08847493\n",
            "iteration:  69 loss: 0.05866047\n",
            "iteration:  70 loss: 0.14216928\n",
            "iteration:  71 loss: 0.09894978\n",
            "iteration:  72 loss: 0.11006212\n",
            "iteration:  73 loss: 0.11311626\n",
            "iteration:  74 loss: 0.13540815\n",
            "iteration:  75 loss: 0.41914284\n",
            "iteration:  76 loss: 0.22010893\n",
            "iteration:  77 loss: 0.12089523\n",
            "iteration:  78 loss: 0.10512318\n",
            "iteration:  79 loss: 0.14928503\n",
            "iteration:  80 loss: 0.10206275\n",
            "iteration:  81 loss: 0.04940875\n",
            "iteration:  82 loss: 0.15602398\n",
            "iteration:  83 loss: 0.02097662\n",
            "iteration:  84 loss: 0.07712552\n",
            "iteration:  85 loss: 0.03047528\n",
            "iteration:  86 loss: 0.29676363\n",
            "iteration:  87 loss: 0.06569391\n",
            "iteration:  88 loss: 0.28081781\n",
            "iteration:  89 loss: 0.00991823\n",
            "iteration:  90 loss: 0.28107321\n",
            "iteration:  91 loss: 0.20125842\n",
            "iteration:  92 loss: 0.09207098\n",
            "iteration:  93 loss: 0.03769930\n",
            "iteration:  94 loss: 0.18577667\n",
            "iteration:  95 loss: 0.02909764\n",
            "iteration:  96 loss: 0.28537136\n",
            "iteration:  97 loss: 0.28013286\n",
            "iteration:  98 loss: 0.06803261\n",
            "iteration:  99 loss: 0.14017099\n",
            "iteration: 100 loss: 0.05138827\n",
            "iteration: 101 loss: 0.04436813\n",
            "iteration: 102 loss: 0.17623790\n",
            "iteration: 103 loss: 0.12727335\n",
            "iteration: 104 loss: 0.05745006\n",
            "iteration: 105 loss: 0.03911282\n",
            "iteration: 106 loss: 0.03149295\n",
            "iteration: 107 loss: 0.02162561\n",
            "iteration: 108 loss: 0.14641351\n",
            "iteration: 109 loss: 0.18634194\n",
            "iteration: 110 loss: 0.04788079\n",
            "iteration: 111 loss: 0.13152100\n",
            "iteration: 112 loss: 0.06893074\n",
            "iteration: 113 loss: 0.11944161\n",
            "iteration: 114 loss: 0.04001265\n",
            "iteration: 115 loss: 0.11733573\n",
            "iteration: 116 loss: 0.06307063\n",
            "iteration: 117 loss: 0.07246015\n",
            "iteration: 118 loss: 0.19906558\n",
            "iteration: 119 loss: 0.10314036\n",
            "iteration: 120 loss: 0.22441638\n",
            "iteration: 121 loss: 0.05213993\n",
            "iteration: 122 loss: 0.07774175\n",
            "iteration: 123 loss: 0.10422951\n",
            "iteration: 124 loss: 0.06105382\n",
            "iteration: 125 loss: 0.05966145\n",
            "iteration: 126 loss: 0.09637903\n",
            "iteration: 127 loss: 0.02932411\n",
            "iteration: 128 loss: 0.07370336\n",
            "iteration: 129 loss: 0.10655573\n",
            "iteration: 130 loss: 0.02116397\n",
            "iteration: 131 loss: 0.04807965\n",
            "iteration: 132 loss: 0.09679226\n",
            "iteration: 133 loss: 0.02899634\n",
            "iteration: 134 loss: 0.06568918\n",
            "iteration: 135 loss: 0.18532945\n",
            "iteration: 136 loss: 0.08628033\n",
            "iteration: 137 loss: 0.08755786\n",
            "iteration: 138 loss: 0.14259532\n",
            "iteration: 139 loss: 0.22741598\n",
            "iteration: 140 loss: 0.33512911\n",
            "iteration: 141 loss: 0.09412275\n",
            "iteration: 142 loss: 0.03091983\n",
            "iteration: 143 loss: 0.15617283\n",
            "iteration: 144 loss: 0.08095004\n",
            "iteration: 145 loss: 0.02387477\n",
            "iteration: 146 loss: 0.50866920\n",
            "iteration: 147 loss: 0.04744492\n",
            "iteration: 148 loss: 0.07072411\n",
            "iteration: 149 loss: 0.14248995\n",
            "iteration: 150 loss: 0.06256805\n",
            "iteration: 151 loss: 0.13832115\n",
            "iteration: 152 loss: 0.07218084\n",
            "iteration: 153 loss: 0.02839866\n",
            "iteration: 154 loss: 0.36078730\n",
            "iteration: 155 loss: 0.11317093\n",
            "iteration: 156 loss: 0.06520785\n",
            "iteration: 157 loss: 0.19441807\n",
            "iteration: 158 loss: 0.10273137\n",
            "iteration: 159 loss: 0.16043247\n",
            "iteration: 160 loss: 0.02225023\n",
            "iteration: 161 loss: 0.13207886\n",
            "iteration: 162 loss: 0.08133926\n",
            "iteration: 163 loss: 0.35580677\n",
            "iteration: 164 loss: 0.05525697\n",
            "iteration: 165 loss: 0.80638993\n",
            "iteration: 166 loss: 0.06248794\n",
            "iteration: 167 loss: 0.11401695\n",
            "iteration: 168 loss: 0.04649270\n",
            "iteration: 169 loss: 0.18544890\n",
            "iteration: 170 loss: 0.15125640\n",
            "iteration: 171 loss: 0.06286409\n",
            "iteration: 172 loss: 0.08622470\n",
            "iteration: 173 loss: 0.04009609\n",
            "iteration: 174 loss: 0.06280571\n",
            "iteration: 175 loss: 0.09326793\n",
            "iteration: 176 loss: 0.08202346\n",
            "iteration: 177 loss: 0.17401728\n",
            "iteration: 178 loss: 0.16640325\n",
            "iteration: 179 loss: 0.07713312\n",
            "iteration: 180 loss: 0.36392850\n",
            "iteration: 181 loss: 0.02777567\n",
            "iteration: 182 loss: 0.24525426\n",
            "iteration: 183 loss: 0.24256028\n",
            "iteration: 184 loss: 0.19666772\n",
            "iteration: 185 loss: 0.19919521\n",
            "iteration: 186 loss: 0.11520312\n",
            "iteration: 187 loss: 0.06791782\n",
            "iteration: 188 loss: 0.15385842\n",
            "iteration: 189 loss: 0.05902375\n",
            "iteration: 190 loss: 0.07971373\n",
            "iteration: 191 loss: 0.08065601\n",
            "iteration: 192 loss: 0.07889485\n",
            "iteration: 193 loss: 0.06798350\n",
            "iteration: 194 loss: 0.38773477\n",
            "iteration: 195 loss: 0.06453754\n",
            "iteration: 196 loss: 0.12744913\n",
            "iteration: 197 loss: 0.04457998\n",
            "iteration: 198 loss: 0.11093618\n",
            "iteration: 199 loss: 0.07296203\n",
            "epoch:  91 mean loss training: 0.13666260\n",
            "epoch:  91 mean loss validation: 0.36051229\n",
            "iteration:   0 loss: 0.21218552\n",
            "iteration:   1 loss: 0.01703360\n",
            "iteration:   2 loss: 0.02833951\n",
            "iteration:   3 loss: 0.03336751\n",
            "iteration:   4 loss: 0.09267469\n",
            "iteration:   5 loss: 0.18883340\n",
            "iteration:   6 loss: 0.26156479\n",
            "iteration:   7 loss: 0.01614195\n",
            "iteration:   8 loss: 0.04789785\n",
            "iteration:   9 loss: 0.08966292\n",
            "iteration:  10 loss: 0.15570943\n",
            "iteration:  11 loss: 0.12842341\n",
            "iteration:  12 loss: 0.98353946\n",
            "iteration:  13 loss: 0.06301666\n",
            "iteration:  14 loss: 0.10397249\n",
            "iteration:  15 loss: 0.35578126\n",
            "iteration:  16 loss: 0.03284606\n",
            "iteration:  17 loss: 0.07936020\n",
            "iteration:  18 loss: 0.20125869\n",
            "iteration:  19 loss: 0.34603983\n",
            "iteration:  20 loss: 0.02029544\n",
            "iteration:  21 loss: 0.07332335\n",
            "iteration:  22 loss: 0.12142719\n",
            "iteration:  23 loss: 0.13092814\n",
            "iteration:  24 loss: 0.03264807\n",
            "iteration:  25 loss: 0.11280908\n",
            "iteration:  26 loss: 0.32176626\n",
            "iteration:  27 loss: 0.87165123\n",
            "iteration:  28 loss: 0.04451567\n",
            "iteration:  29 loss: 0.16234607\n",
            "iteration:  30 loss: 0.13726465\n",
            "iteration:  31 loss: 0.03545709\n",
            "iteration:  32 loss: 0.06254250\n",
            "iteration:  33 loss: 0.01637844\n",
            "iteration:  34 loss: 0.11477678\n",
            "iteration:  35 loss: 0.06807884\n",
            "iteration:  36 loss: 0.03402996\n",
            "iteration:  37 loss: 0.25586629\n",
            "iteration:  38 loss: 0.06093623\n",
            "iteration:  39 loss: 0.03121214\n",
            "iteration:  40 loss: 0.03393624\n",
            "iteration:  41 loss: 0.07661266\n",
            "iteration:  42 loss: 0.06671152\n",
            "iteration:  43 loss: 0.22053379\n",
            "iteration:  44 loss: 0.01721635\n",
            "iteration:  45 loss: 0.16828460\n",
            "iteration:  46 loss: 0.18164344\n",
            "iteration:  47 loss: 0.08515271\n",
            "iteration:  48 loss: 0.10531148\n",
            "iteration:  49 loss: 0.02537506\n",
            "iteration:  50 loss: 0.07707401\n",
            "iteration:  51 loss: 0.12034144\n",
            "iteration:  52 loss: 0.12113842\n",
            "iteration:  53 loss: 0.08009784\n",
            "iteration:  54 loss: 0.07230496\n",
            "iteration:  55 loss: 0.08893273\n",
            "iteration:  56 loss: 0.19319156\n",
            "iteration:  57 loss: 0.02795458\n",
            "iteration:  58 loss: 0.06841870\n",
            "iteration:  59 loss: 0.09468338\n",
            "iteration:  60 loss: 0.11389980\n",
            "iteration:  61 loss: 0.01788362\n",
            "iteration:  62 loss: 0.04430916\n",
            "iteration:  63 loss: 0.10003483\n",
            "iteration:  64 loss: 0.02322197\n",
            "iteration:  65 loss: 0.08789232\n",
            "iteration:  66 loss: 0.04871439\n",
            "iteration:  67 loss: 0.18852177\n",
            "iteration:  68 loss: 0.01598109\n",
            "iteration:  69 loss: 0.18993393\n",
            "iteration:  70 loss: 0.06997072\n",
            "iteration:  71 loss: 0.01724998\n",
            "iteration:  72 loss: 0.04352807\n",
            "iteration:  73 loss: 0.03578922\n",
            "iteration:  74 loss: 0.08581594\n",
            "iteration:  75 loss: 0.07085419\n",
            "iteration:  76 loss: 0.14806014\n",
            "iteration:  77 loss: 0.02796109\n",
            "iteration:  78 loss: 0.17007008\n",
            "iteration:  79 loss: 0.19574225\n",
            "iteration:  80 loss: 0.09771194\n",
            "iteration:  81 loss: 0.14522807\n",
            "iteration:  82 loss: 0.05388037\n",
            "iteration:  83 loss: 0.05747757\n",
            "iteration:  84 loss: 0.09371085\n",
            "iteration:  85 loss: 0.08762563\n",
            "iteration:  86 loss: 0.36279047\n",
            "iteration:  87 loss: 0.08658698\n",
            "iteration:  88 loss: 0.32379949\n",
            "iteration:  89 loss: 0.05872114\n",
            "iteration:  90 loss: 0.19042936\n",
            "iteration:  91 loss: 0.03725471\n",
            "iteration:  92 loss: 0.04226588\n",
            "iteration:  93 loss: 0.13668348\n",
            "iteration:  94 loss: 0.02267007\n",
            "iteration:  95 loss: 0.06089423\n",
            "iteration:  96 loss: 0.50968897\n",
            "iteration:  97 loss: 0.05727838\n",
            "iteration:  98 loss: 0.01288546\n",
            "iteration:  99 loss: 0.07015158\n",
            "iteration: 100 loss: 0.05741079\n",
            "iteration: 101 loss: 0.07769129\n",
            "iteration: 102 loss: 0.01492905\n",
            "iteration: 103 loss: 0.09930038\n",
            "iteration: 104 loss: 0.05969318\n",
            "iteration: 105 loss: 0.20480731\n",
            "iteration: 106 loss: 0.16073784\n",
            "iteration: 107 loss: 0.07097805\n",
            "iteration: 108 loss: 0.26777118\n",
            "iteration: 109 loss: 0.06816893\n",
            "iteration: 110 loss: 0.08508738\n",
            "iteration: 111 loss: 0.10594222\n",
            "iteration: 112 loss: 0.02396374\n",
            "iteration: 113 loss: 0.21368715\n",
            "iteration: 114 loss: 0.16938838\n",
            "iteration: 115 loss: 0.35742968\n",
            "iteration: 116 loss: 0.02361641\n",
            "iteration: 117 loss: 0.22702348\n",
            "iteration: 118 loss: 0.08755628\n",
            "iteration: 119 loss: 0.03479676\n",
            "iteration: 120 loss: 0.10829781\n",
            "iteration: 121 loss: 0.02556035\n",
            "iteration: 122 loss: 0.01320156\n",
            "iteration: 123 loss: 0.11787114\n",
            "iteration: 124 loss: 0.04983388\n",
            "iteration: 125 loss: 0.14867128\n",
            "iteration: 126 loss: 0.62875479\n",
            "iteration: 127 loss: 0.18886842\n",
            "iteration: 128 loss: 0.05058924\n",
            "iteration: 129 loss: 0.07478845\n",
            "iteration: 130 loss: 0.03017869\n",
            "iteration: 131 loss: 0.01864314\n",
            "iteration: 132 loss: 0.03593386\n",
            "iteration: 133 loss: 0.70856243\n",
            "iteration: 134 loss: 0.19807412\n",
            "iteration: 135 loss: 0.05501024\n",
            "iteration: 136 loss: 0.02186432\n",
            "iteration: 137 loss: 0.04589149\n",
            "iteration: 138 loss: 0.02064425\n",
            "iteration: 139 loss: 0.04737394\n",
            "iteration: 140 loss: 0.17476892\n",
            "iteration: 141 loss: 0.13690461\n",
            "iteration: 142 loss: 0.14431278\n",
            "iteration: 143 loss: 0.08733293\n",
            "iteration: 144 loss: 0.30082479\n",
            "iteration: 145 loss: 0.03476210\n",
            "iteration: 146 loss: 0.22183512\n",
            "iteration: 147 loss: 0.07072789\n",
            "iteration: 148 loss: 0.11237849\n",
            "iteration: 149 loss: 0.03629654\n",
            "iteration: 150 loss: 0.13339511\n",
            "iteration: 151 loss: 0.02840791\n",
            "iteration: 152 loss: 0.10503034\n",
            "iteration: 153 loss: 0.10676295\n",
            "iteration: 154 loss: 0.13730951\n",
            "iteration: 155 loss: 0.22494760\n",
            "iteration: 156 loss: 0.09221213\n",
            "iteration: 157 loss: 0.02897139\n",
            "iteration: 158 loss: 0.04237155\n",
            "iteration: 159 loss: 0.14036657\n",
            "iteration: 160 loss: 0.13932540\n",
            "iteration: 161 loss: 0.13643864\n",
            "iteration: 162 loss: 0.12525603\n",
            "iteration: 163 loss: 0.10251540\n",
            "iteration: 164 loss: 0.49229202\n",
            "iteration: 165 loss: 0.04396447\n",
            "iteration: 166 loss: 0.08469275\n",
            "iteration: 167 loss: 0.65000558\n",
            "iteration: 168 loss: 0.12862027\n",
            "iteration: 169 loss: 0.02903071\n",
            "iteration: 170 loss: 0.07646664\n",
            "iteration: 171 loss: 0.04780884\n",
            "iteration: 172 loss: 0.15334252\n",
            "iteration: 173 loss: 0.05017774\n",
            "iteration: 174 loss: 0.27367643\n",
            "iteration: 175 loss: 0.07220756\n",
            "iteration: 176 loss: 0.13640060\n",
            "iteration: 177 loss: 0.25059822\n",
            "iteration: 178 loss: 0.14166228\n",
            "iteration: 179 loss: 0.13556543\n",
            "iteration: 180 loss: 0.09461217\n",
            "iteration: 181 loss: 0.02627661\n",
            "iteration: 182 loss: 0.01150806\n",
            "iteration: 183 loss: 0.11622076\n",
            "iteration: 184 loss: 0.14163935\n",
            "iteration: 185 loss: 0.16201837\n",
            "iteration: 186 loss: 0.09957397\n",
            "iteration: 187 loss: 0.05241923\n",
            "iteration: 188 loss: 0.04055748\n",
            "iteration: 189 loss: 0.03172646\n",
            "iteration: 190 loss: 0.09643611\n",
            "iteration: 191 loss: 0.49980801\n",
            "iteration: 192 loss: 0.11786388\n",
            "iteration: 193 loss: 0.24575049\n",
            "iteration: 194 loss: 0.05989362\n",
            "iteration: 195 loss: 0.22213146\n",
            "iteration: 196 loss: 0.10439214\n",
            "iteration: 197 loss: 0.13685131\n",
            "iteration: 198 loss: 0.16379245\n",
            "iteration: 199 loss: 0.12387922\n",
            "epoch:  92 mean loss training: 0.12724949\n",
            "epoch:  92 mean loss validation: 0.36327952\n",
            "iteration:   0 loss: 0.05921035\n",
            "iteration:   1 loss: 0.02986284\n",
            "iteration:   2 loss: 0.07071653\n",
            "iteration:   3 loss: 0.35391715\n",
            "iteration:   4 loss: 0.14979158\n",
            "iteration:   5 loss: 0.17072222\n",
            "iteration:   6 loss: 0.08916017\n",
            "iteration:   7 loss: 0.12003829\n",
            "iteration:   8 loss: 0.03495920\n",
            "iteration:   9 loss: 0.40237364\n",
            "iteration:  10 loss: 0.04223799\n",
            "iteration:  11 loss: 0.04915192\n",
            "iteration:  12 loss: 0.23696452\n",
            "iteration:  13 loss: 0.10887105\n",
            "iteration:  14 loss: 0.17888135\n",
            "iteration:  15 loss: 0.11144903\n",
            "iteration:  16 loss: 0.09645767\n",
            "iteration:  17 loss: 0.04156014\n",
            "iteration:  18 loss: 0.02773514\n",
            "iteration:  19 loss: 0.05003741\n",
            "iteration:  20 loss: 0.45536473\n",
            "iteration:  21 loss: 0.12355141\n",
            "iteration:  22 loss: 0.14324637\n",
            "iteration:  23 loss: 0.05039478\n",
            "iteration:  24 loss: 0.05950109\n",
            "iteration:  25 loss: 0.04040755\n",
            "iteration:  26 loss: 0.13653551\n",
            "iteration:  27 loss: 0.11804724\n",
            "iteration:  28 loss: 0.08729927\n",
            "iteration:  29 loss: 0.04915537\n",
            "iteration:  30 loss: 0.07911667\n",
            "iteration:  31 loss: 0.05838938\n",
            "iteration:  32 loss: 0.08269347\n",
            "iteration:  33 loss: 0.08988990\n",
            "iteration:  34 loss: 0.10751305\n",
            "iteration:  35 loss: 0.21490751\n",
            "iteration:  36 loss: 0.66810399\n",
            "iteration:  37 loss: 0.01572730\n",
            "iteration:  38 loss: 0.03645511\n",
            "iteration:  39 loss: 0.02654474\n",
            "iteration:  40 loss: 0.09728090\n",
            "iteration:  41 loss: 0.05475189\n",
            "iteration:  42 loss: 0.11671576\n",
            "iteration:  43 loss: 0.06725908\n",
            "iteration:  44 loss: 0.13256189\n",
            "iteration:  45 loss: 0.02816100\n",
            "iteration:  46 loss: 0.12334738\n",
            "iteration:  47 loss: 0.12792139\n",
            "iteration:  48 loss: 0.01615856\n",
            "iteration:  49 loss: 0.10888194\n",
            "iteration:  50 loss: 0.15026924\n",
            "iteration:  51 loss: 0.23697318\n",
            "iteration:  52 loss: 0.40627581\n",
            "iteration:  53 loss: 0.10383386\n",
            "iteration:  54 loss: 0.02056414\n",
            "iteration:  55 loss: 0.05398636\n",
            "iteration:  56 loss: 0.34092909\n",
            "iteration:  57 loss: 0.04591655\n",
            "iteration:  58 loss: 0.19278839\n",
            "iteration:  59 loss: 0.02293102\n",
            "iteration:  60 loss: 0.01557868\n",
            "iteration:  61 loss: 0.06431980\n",
            "iteration:  62 loss: 0.11631732\n",
            "iteration:  63 loss: 0.03006375\n",
            "iteration:  64 loss: 0.16841272\n",
            "iteration:  65 loss: 0.03432031\n",
            "iteration:  66 loss: 0.13563383\n",
            "iteration:  67 loss: 0.03408807\n",
            "iteration:  68 loss: 0.15833119\n",
            "iteration:  69 loss: 0.14386158\n",
            "iteration:  70 loss: 0.15993284\n",
            "iteration:  71 loss: 0.07163674\n",
            "iteration:  72 loss: 0.14211209\n",
            "iteration:  73 loss: 0.16854231\n",
            "iteration:  74 loss: 0.17472322\n",
            "iteration:  75 loss: 0.15242966\n",
            "iteration:  76 loss: 0.06538559\n",
            "iteration:  77 loss: 0.40826279\n",
            "iteration:  78 loss: 0.11597841\n",
            "iteration:  79 loss: 0.07685724\n",
            "iteration:  80 loss: 0.05911262\n",
            "iteration:  81 loss: 0.12537931\n",
            "iteration:  82 loss: 0.04475443\n",
            "iteration:  83 loss: 0.12894975\n",
            "iteration:  84 loss: 0.07399046\n",
            "iteration:  85 loss: 0.56309539\n",
            "iteration:  86 loss: 0.11649787\n",
            "iteration:  87 loss: 0.10085968\n",
            "iteration:  88 loss: 0.10398968\n",
            "iteration:  89 loss: 0.11744111\n",
            "iteration:  90 loss: 0.10140149\n",
            "iteration:  91 loss: 0.13178398\n",
            "iteration:  92 loss: 0.05868118\n",
            "iteration:  93 loss: 0.12322686\n",
            "iteration:  94 loss: 0.04429007\n",
            "iteration:  95 loss: 0.08219354\n",
            "iteration:  96 loss: 0.24894585\n",
            "iteration:  97 loss: 0.05773287\n",
            "iteration:  98 loss: 0.06495029\n",
            "iteration:  99 loss: 0.13419272\n",
            "iteration: 100 loss: 0.44275090\n",
            "iteration: 101 loss: 0.08606575\n",
            "iteration: 102 loss: 0.01732958\n",
            "iteration: 103 loss: 0.06787267\n",
            "iteration: 104 loss: 0.04664722\n",
            "iteration: 105 loss: 0.05065251\n",
            "iteration: 106 loss: 0.04454604\n",
            "iteration: 107 loss: 0.05278309\n",
            "iteration: 108 loss: 0.18113250\n",
            "iteration: 109 loss: 0.56914324\n",
            "iteration: 110 loss: 0.12482052\n",
            "iteration: 111 loss: 0.20710930\n",
            "iteration: 112 loss: 0.71016163\n",
            "iteration: 113 loss: 0.02102761\n",
            "iteration: 114 loss: 0.95140857\n",
            "iteration: 115 loss: 0.06387252\n",
            "iteration: 116 loss: 0.13031885\n",
            "iteration: 117 loss: 0.84352136\n",
            "iteration: 118 loss: 0.01994097\n",
            "iteration: 119 loss: 0.08477918\n",
            "iteration: 120 loss: 0.10216216\n",
            "iteration: 121 loss: 0.18315607\n",
            "iteration: 122 loss: 0.12970380\n",
            "iteration: 123 loss: 0.16785574\n",
            "iteration: 124 loss: 0.24378642\n",
            "iteration: 125 loss: 0.02860967\n",
            "iteration: 126 loss: 0.03918184\n",
            "iteration: 127 loss: 0.38594097\n",
            "iteration: 128 loss: 0.17711887\n",
            "iteration: 129 loss: 0.06737004\n",
            "iteration: 130 loss: 0.10609487\n",
            "iteration: 131 loss: 0.04509165\n",
            "iteration: 132 loss: 0.38600868\n",
            "iteration: 133 loss: 0.10963988\n",
            "iteration: 134 loss: 0.84639591\n",
            "iteration: 135 loss: 0.09718272\n",
            "iteration: 136 loss: 0.15953608\n",
            "iteration: 137 loss: 0.04903111\n",
            "iteration: 138 loss: 0.12471948\n",
            "iteration: 139 loss: 0.39223546\n",
            "iteration: 140 loss: 0.07908081\n",
            "iteration: 141 loss: 0.06100876\n",
            "iteration: 142 loss: 0.37172610\n",
            "iteration: 143 loss: 0.06555475\n",
            "iteration: 144 loss: 0.13778618\n",
            "iteration: 145 loss: 0.21173674\n",
            "iteration: 146 loss: 0.10268309\n",
            "iteration: 147 loss: 0.04211476\n",
            "iteration: 148 loss: 0.09130277\n",
            "iteration: 149 loss: 0.04352358\n",
            "iteration: 150 loss: 0.10152228\n",
            "iteration: 151 loss: 0.02788689\n",
            "iteration: 152 loss: 0.12969251\n",
            "iteration: 153 loss: 0.03642878\n",
            "iteration: 154 loss: 0.07428260\n",
            "iteration: 155 loss: 0.13860856\n",
            "iteration: 156 loss: 0.06596074\n",
            "iteration: 157 loss: 0.12726457\n",
            "iteration: 158 loss: 0.14507668\n",
            "iteration: 159 loss: 0.15258406\n",
            "iteration: 160 loss: 0.23232745\n",
            "iteration: 161 loss: 0.03129551\n",
            "iteration: 162 loss: 0.03034366\n",
            "iteration: 163 loss: 0.04975258\n",
            "iteration: 164 loss: 0.19785711\n",
            "iteration: 165 loss: 0.02586841\n",
            "iteration: 166 loss: 0.38251176\n",
            "iteration: 167 loss: 0.08530286\n",
            "iteration: 168 loss: 0.11370247\n",
            "iteration: 169 loss: 0.08942190\n",
            "iteration: 170 loss: 0.05921545\n",
            "iteration: 171 loss: 0.07819641\n",
            "iteration: 172 loss: 0.14844440\n",
            "iteration: 173 loss: 0.28296873\n",
            "iteration: 174 loss: 0.07488220\n",
            "iteration: 175 loss: 0.06419103\n",
            "iteration: 176 loss: 0.01875059\n",
            "iteration: 177 loss: 0.05254510\n",
            "iteration: 178 loss: 0.03797394\n",
            "iteration: 179 loss: 0.03534554\n",
            "iteration: 180 loss: 0.14215305\n",
            "iteration: 181 loss: 0.07377286\n",
            "iteration: 182 loss: 0.03121378\n",
            "iteration: 183 loss: 0.14077041\n",
            "iteration: 184 loss: 0.01551951\n",
            "iteration: 185 loss: 0.19847831\n",
            "iteration: 186 loss: 0.29140574\n",
            "iteration: 187 loss: 0.02746953\n",
            "iteration: 188 loss: 0.11898635\n",
            "iteration: 189 loss: 0.14964445\n",
            "iteration: 190 loss: 0.09550714\n",
            "iteration: 191 loss: 0.05717742\n",
            "iteration: 192 loss: 0.19231117\n",
            "iteration: 193 loss: 0.30228040\n",
            "iteration: 194 loss: 0.14413209\n",
            "iteration: 195 loss: 0.07315055\n",
            "iteration: 196 loss: 0.35229596\n",
            "iteration: 197 loss: 0.10306861\n",
            "iteration: 198 loss: 0.01504650\n",
            "iteration: 199 loss: 0.12567781\n",
            "epoch:  93 mean loss training: 0.13968098\n",
            "epoch:  93 mean loss validation: 0.38586459\n",
            "iteration:   0 loss: 0.08641250\n",
            "iteration:   1 loss: 0.04684512\n",
            "iteration:   2 loss: 0.02207752\n",
            "iteration:   3 loss: 0.13568421\n",
            "iteration:   4 loss: 0.09036677\n",
            "iteration:   5 loss: 0.10393515\n",
            "iteration:   6 loss: 0.14053521\n",
            "iteration:   7 loss: 0.36632210\n",
            "iteration:   8 loss: 0.05005202\n",
            "iteration:   9 loss: 0.17139827\n",
            "iteration:  10 loss: 0.05256581\n",
            "iteration:  11 loss: 0.10634057\n",
            "iteration:  12 loss: 0.08552958\n",
            "iteration:  13 loss: 0.02615053\n",
            "iteration:  14 loss: 0.27636260\n",
            "iteration:  15 loss: 1.24164379\n",
            "iteration:  16 loss: 0.19518286\n",
            "iteration:  17 loss: 0.04997667\n",
            "iteration:  18 loss: 0.02637636\n",
            "iteration:  19 loss: 0.03082959\n",
            "iteration:  20 loss: 0.13782011\n",
            "iteration:  21 loss: 0.03198050\n",
            "iteration:  22 loss: 0.30820754\n",
            "iteration:  23 loss: 0.03808413\n",
            "iteration:  24 loss: 0.08053485\n",
            "iteration:  25 loss: 0.55235761\n",
            "iteration:  26 loss: 0.11963987\n",
            "iteration:  27 loss: 0.02035521\n",
            "iteration:  28 loss: 0.45877746\n",
            "iteration:  29 loss: 0.35147041\n",
            "iteration:  30 loss: 0.25138530\n",
            "iteration:  31 loss: 0.04877059\n",
            "iteration:  32 loss: 0.15900496\n",
            "iteration:  33 loss: 0.07769843\n",
            "iteration:  34 loss: 0.32098436\n",
            "iteration:  35 loss: 0.11591920\n",
            "iteration:  36 loss: 0.22740053\n",
            "iteration:  37 loss: 0.02946549\n",
            "iteration:  38 loss: 0.01193831\n",
            "iteration:  39 loss: 0.04738532\n",
            "iteration:  40 loss: 0.03370663\n",
            "iteration:  41 loss: 0.23315148\n",
            "iteration:  42 loss: 0.09019436\n",
            "iteration:  43 loss: 0.06461304\n",
            "iteration:  44 loss: 0.17869103\n",
            "iteration:  45 loss: 0.26124334\n",
            "iteration:  46 loss: 0.03283822\n",
            "iteration:  47 loss: 0.04167462\n",
            "iteration:  48 loss: 0.05881995\n",
            "iteration:  49 loss: 0.03264585\n",
            "iteration:  50 loss: 0.14037992\n",
            "iteration:  51 loss: 0.10282189\n",
            "iteration:  52 loss: 0.06122087\n",
            "iteration:  53 loss: 0.03291935\n",
            "iteration:  54 loss: 0.12482562\n",
            "iteration:  55 loss: 0.03308777\n",
            "iteration:  56 loss: 0.01865127\n",
            "iteration:  57 loss: 0.12653850\n",
            "iteration:  58 loss: 0.02442025\n",
            "iteration:  59 loss: 0.03523676\n",
            "iteration:  60 loss: 0.05963211\n",
            "iteration:  61 loss: 0.11137927\n",
            "iteration:  62 loss: 0.07688829\n",
            "iteration:  63 loss: 0.12055288\n",
            "iteration:  64 loss: 0.03037309\n",
            "iteration:  65 loss: 0.03470115\n",
            "iteration:  66 loss: 0.03423343\n",
            "iteration:  67 loss: 0.02582029\n",
            "iteration:  68 loss: 0.15131257\n",
            "iteration:  69 loss: 0.04901840\n",
            "iteration:  70 loss: 0.04136041\n",
            "iteration:  71 loss: 0.25028655\n",
            "iteration:  72 loss: 0.16722578\n",
            "iteration:  73 loss: 0.03025637\n",
            "iteration:  74 loss: 0.12933969\n",
            "iteration:  75 loss: 0.08498674\n",
            "iteration:  76 loss: 0.06067523\n",
            "iteration:  77 loss: 0.15198210\n",
            "iteration:  78 loss: 0.07525899\n",
            "iteration:  79 loss: 0.10165732\n",
            "iteration:  80 loss: 0.14226551\n",
            "iteration:  81 loss: 0.04081407\n",
            "iteration:  82 loss: 0.32725254\n",
            "iteration:  83 loss: 0.13564706\n",
            "iteration:  84 loss: 0.06792802\n",
            "iteration:  85 loss: 0.03939528\n",
            "iteration:  86 loss: 0.07416885\n",
            "iteration:  87 loss: 0.27705729\n",
            "iteration:  88 loss: 0.22567302\n",
            "iteration:  89 loss: 0.13651581\n",
            "iteration:  90 loss: 0.09356797\n",
            "iteration:  91 loss: 0.16178817\n",
            "iteration:  92 loss: 0.07733364\n",
            "iteration:  93 loss: 0.06291360\n",
            "iteration:  94 loss: 0.01885606\n",
            "iteration:  95 loss: 0.09727322\n",
            "iteration:  96 loss: 0.02501199\n",
            "iteration:  97 loss: 0.17929295\n",
            "iteration:  98 loss: 0.25176752\n",
            "iteration:  99 loss: 0.31681338\n",
            "iteration: 100 loss: 0.10502399\n",
            "iteration: 101 loss: 0.01939559\n",
            "iteration: 102 loss: 0.46229962\n",
            "iteration: 103 loss: 0.23894180\n",
            "iteration: 104 loss: 0.07535496\n",
            "iteration: 105 loss: 0.01413037\n",
            "iteration: 106 loss: 0.03785106\n",
            "iteration: 107 loss: 0.09416095\n",
            "iteration: 108 loss: 0.10553957\n",
            "iteration: 109 loss: 0.02204306\n",
            "iteration: 110 loss: 0.06797010\n",
            "iteration: 111 loss: 0.14655256\n",
            "iteration: 112 loss: 0.27269569\n",
            "iteration: 113 loss: 0.36562693\n",
            "iteration: 114 loss: 0.06235305\n",
            "iteration: 115 loss: 0.06379978\n",
            "iteration: 116 loss: 0.08471388\n",
            "iteration: 117 loss: 0.55678052\n",
            "iteration: 118 loss: 0.55536103\n",
            "iteration: 119 loss: 0.67780858\n",
            "iteration: 120 loss: 0.13985614\n",
            "iteration: 121 loss: 0.17320196\n",
            "iteration: 122 loss: 0.25965613\n",
            "iteration: 123 loss: 0.15892103\n",
            "iteration: 124 loss: 0.08282548\n",
            "iteration: 125 loss: 0.04583198\n",
            "iteration: 126 loss: 0.29993820\n",
            "iteration: 127 loss: 0.21609046\n",
            "iteration: 128 loss: 0.07785240\n",
            "iteration: 129 loss: 0.25945637\n",
            "iteration: 130 loss: 0.15024783\n",
            "iteration: 131 loss: 0.03030256\n",
            "iteration: 132 loss: 0.08112524\n",
            "iteration: 133 loss: 0.21316898\n",
            "iteration: 134 loss: 0.02491005\n",
            "iteration: 135 loss: 0.06746259\n",
            "iteration: 136 loss: 0.23603205\n",
            "iteration: 137 loss: 0.02738470\n",
            "iteration: 138 loss: 0.07347247\n",
            "iteration: 139 loss: 0.09816859\n",
            "iteration: 140 loss: 0.10245608\n",
            "iteration: 141 loss: 0.07684577\n",
            "iteration: 142 loss: 0.08661143\n",
            "iteration: 143 loss: 0.08932514\n",
            "iteration: 144 loss: 0.20657746\n",
            "iteration: 145 loss: 0.06762530\n",
            "iteration: 146 loss: 1.32313633\n",
            "iteration: 147 loss: 0.08065934\n",
            "iteration: 148 loss: 0.24785100\n",
            "iteration: 149 loss: 0.07996531\n",
            "iteration: 150 loss: 0.04063678\n",
            "iteration: 151 loss: 0.07838688\n",
            "iteration: 152 loss: 0.12638156\n",
            "iteration: 153 loss: 0.05672278\n",
            "iteration: 154 loss: 0.02868548\n",
            "iteration: 155 loss: 0.07902595\n",
            "iteration: 156 loss: 0.06617813\n",
            "iteration: 157 loss: 0.07028242\n",
            "iteration: 158 loss: 0.16587307\n",
            "iteration: 159 loss: 0.16502368\n",
            "iteration: 160 loss: 0.12581782\n",
            "iteration: 161 loss: 0.10562745\n",
            "iteration: 162 loss: 0.05311440\n",
            "iteration: 163 loss: 0.04576520\n",
            "iteration: 164 loss: 1.07388091\n",
            "iteration: 165 loss: 0.15582135\n",
            "iteration: 166 loss: 0.01700731\n",
            "iteration: 167 loss: 0.07152193\n",
            "iteration: 168 loss: 0.13502416\n",
            "iteration: 169 loss: 0.10939468\n",
            "iteration: 170 loss: 0.12281600\n",
            "iteration: 171 loss: 0.44930205\n",
            "iteration: 172 loss: 0.09214613\n",
            "iteration: 173 loss: 0.23981664\n",
            "iteration: 174 loss: 0.39086926\n",
            "iteration: 175 loss: 0.43948942\n",
            "iteration: 176 loss: 0.09647531\n",
            "iteration: 177 loss: 0.20901185\n",
            "iteration: 178 loss: 0.18271005\n",
            "iteration: 179 loss: 0.05084383\n",
            "iteration: 180 loss: 0.10811552\n",
            "iteration: 181 loss: 0.12770244\n",
            "iteration: 182 loss: 0.05625617\n",
            "iteration: 183 loss: 0.10703377\n",
            "iteration: 184 loss: 0.10462541\n",
            "iteration: 185 loss: 0.18346927\n",
            "iteration: 186 loss: 0.09102800\n",
            "iteration: 187 loss: 0.03469802\n",
            "iteration: 188 loss: 0.07840075\n",
            "iteration: 189 loss: 0.12368193\n",
            "iteration: 190 loss: 0.06631318\n",
            "iteration: 191 loss: 0.32333726\n",
            "iteration: 192 loss: 0.03438931\n",
            "iteration: 193 loss: 0.12871149\n",
            "iteration: 194 loss: 0.13795631\n",
            "iteration: 195 loss: 0.04342157\n",
            "iteration: 196 loss: 0.07937549\n",
            "iteration: 197 loss: 0.26184043\n",
            "iteration: 198 loss: 0.11270448\n",
            "iteration: 199 loss: 0.06438134\n",
            "epoch:  94 mean loss training: 0.14688089\n",
            "epoch:  94 mean loss validation: 0.37257734\n",
            "iteration:   0 loss: 0.25762025\n",
            "iteration:   1 loss: 0.06447946\n",
            "iteration:   2 loss: 0.02210099\n",
            "iteration:   3 loss: 0.05760333\n",
            "iteration:   4 loss: 0.19457622\n",
            "iteration:   5 loss: 0.05473736\n",
            "iteration:   6 loss: 0.10729650\n",
            "iteration:   7 loss: 0.04587472\n",
            "iteration:   8 loss: 0.08167882\n",
            "iteration:   9 loss: 0.08261541\n",
            "iteration:  10 loss: 0.28299025\n",
            "iteration:  11 loss: 0.11620605\n",
            "iteration:  12 loss: 0.18226817\n",
            "iteration:  13 loss: 0.26858699\n",
            "iteration:  14 loss: 0.20696598\n",
            "iteration:  15 loss: 0.04360400\n",
            "iteration:  16 loss: 0.11134490\n",
            "iteration:  17 loss: 0.08317906\n",
            "iteration:  18 loss: 0.14470954\n",
            "iteration:  19 loss: 0.03989248\n",
            "iteration:  20 loss: 0.09182120\n",
            "iteration:  21 loss: 0.09577539\n",
            "iteration:  22 loss: 0.03658863\n",
            "iteration:  23 loss: 0.33486867\n",
            "iteration:  24 loss: 0.02998099\n",
            "iteration:  25 loss: 0.18766522\n",
            "iteration:  26 loss: 0.72320306\n",
            "iteration:  27 loss: 0.11021985\n",
            "iteration:  28 loss: 0.13070045\n",
            "iteration:  29 loss: 0.08163906\n",
            "iteration:  30 loss: 0.04861121\n",
            "iteration:  31 loss: 0.05016566\n",
            "iteration:  32 loss: 0.24697119\n",
            "iteration:  33 loss: 0.04806018\n",
            "iteration:  34 loss: 0.10822324\n",
            "iteration:  35 loss: 0.01144698\n",
            "iteration:  36 loss: 0.05885159\n",
            "iteration:  37 loss: 0.05053202\n",
            "iteration:  38 loss: 0.03790854\n",
            "iteration:  39 loss: 0.11657804\n",
            "iteration:  40 loss: 0.02258361\n",
            "iteration:  41 loss: 0.05082225\n",
            "iteration:  42 loss: 0.05339209\n",
            "iteration:  43 loss: 0.02853600\n",
            "iteration:  44 loss: 0.11451561\n",
            "iteration:  45 loss: 0.06423033\n",
            "iteration:  46 loss: 0.06043483\n",
            "iteration:  47 loss: 0.03370097\n",
            "iteration:  48 loss: 0.09635478\n",
            "iteration:  49 loss: 0.10846730\n",
            "iteration:  50 loss: 0.04712734\n",
            "iteration:  51 loss: 0.03554612\n",
            "iteration:  52 loss: 0.03959914\n",
            "iteration:  53 loss: 0.01923388\n",
            "iteration:  54 loss: 0.08967006\n",
            "iteration:  55 loss: 0.11474216\n",
            "iteration:  56 loss: 0.01960456\n",
            "iteration:  57 loss: 0.33165717\n",
            "iteration:  58 loss: 0.16105838\n",
            "iteration:  59 loss: 0.14019018\n",
            "iteration:  60 loss: 0.16641106\n",
            "iteration:  61 loss: 0.13496469\n",
            "iteration:  62 loss: 0.19395138\n",
            "iteration:  63 loss: 0.09626660\n",
            "iteration:  64 loss: 0.04944161\n",
            "iteration:  65 loss: 0.02935444\n",
            "iteration:  66 loss: 0.13626038\n",
            "iteration:  67 loss: 0.03984402\n",
            "iteration:  68 loss: 0.08514917\n",
            "iteration:  69 loss: 0.13968690\n",
            "iteration:  70 loss: 0.04107846\n",
            "iteration:  71 loss: 0.43928227\n",
            "iteration:  72 loss: 0.15801148\n",
            "iteration:  73 loss: 0.04691160\n",
            "iteration:  74 loss: 0.09167778\n",
            "iteration:  75 loss: 0.08375455\n",
            "iteration:  76 loss: 0.20918620\n",
            "iteration:  77 loss: 0.10240584\n",
            "iteration:  78 loss: 0.02752348\n",
            "iteration:  79 loss: 0.04083689\n",
            "iteration:  80 loss: 0.11143712\n",
            "iteration:  81 loss: 0.10023472\n",
            "iteration:  82 loss: 0.06905977\n",
            "iteration:  83 loss: 0.06579980\n",
            "iteration:  84 loss: 0.08824255\n",
            "iteration:  85 loss: 0.09063411\n",
            "iteration:  86 loss: 0.10620656\n",
            "iteration:  87 loss: 0.05493723\n",
            "iteration:  88 loss: 0.06677616\n",
            "iteration:  89 loss: 0.15078476\n",
            "iteration:  90 loss: 0.16790853\n",
            "iteration:  91 loss: 0.03923764\n",
            "iteration:  92 loss: 0.19684678\n",
            "iteration:  93 loss: 0.07447195\n",
            "iteration:  94 loss: 0.10403853\n",
            "iteration:  95 loss: 0.25887913\n",
            "iteration:  96 loss: 0.20478471\n",
            "iteration:  97 loss: 0.32999682\n",
            "iteration:  98 loss: 0.06246312\n",
            "iteration:  99 loss: 0.17803544\n",
            "iteration: 100 loss: 0.05157931\n",
            "iteration: 101 loss: 0.13894907\n",
            "iteration: 102 loss: 0.04945945\n",
            "iteration: 103 loss: 0.14841449\n",
            "iteration: 104 loss: 0.47634444\n",
            "iteration: 105 loss: 0.14340149\n",
            "iteration: 106 loss: 0.19242440\n",
            "iteration: 107 loss: 0.04946408\n",
            "iteration: 108 loss: 0.07414529\n",
            "iteration: 109 loss: 0.48474747\n",
            "iteration: 110 loss: 0.25419566\n",
            "iteration: 111 loss: 0.04904485\n",
            "iteration: 112 loss: 0.11007480\n",
            "iteration: 113 loss: 0.09240001\n",
            "iteration: 114 loss: 0.33991855\n",
            "iteration: 115 loss: 0.12918612\n",
            "iteration: 116 loss: 0.23033760\n",
            "iteration: 117 loss: 0.10852937\n",
            "iteration: 118 loss: 0.04669914\n",
            "iteration: 119 loss: 0.05493034\n",
            "iteration: 120 loss: 0.03604076\n",
            "iteration: 121 loss: 0.08068874\n",
            "iteration: 122 loss: 0.10630310\n",
            "iteration: 123 loss: 0.17753871\n",
            "iteration: 124 loss: 0.05081873\n",
            "iteration: 125 loss: 0.06632007\n",
            "iteration: 126 loss: 0.03776024\n",
            "iteration: 127 loss: 0.06157089\n",
            "iteration: 128 loss: 0.15839188\n",
            "iteration: 129 loss: 0.18307032\n",
            "iteration: 130 loss: 0.13146996\n",
            "iteration: 131 loss: 0.01405324\n",
            "iteration: 132 loss: 0.15636054\n",
            "iteration: 133 loss: 0.14255762\n",
            "iteration: 134 loss: 0.04420148\n",
            "iteration: 135 loss: 0.14281398\n",
            "iteration: 136 loss: 0.12062948\n",
            "iteration: 137 loss: 0.22369015\n",
            "iteration: 138 loss: 0.16290025\n",
            "iteration: 139 loss: 0.07446470\n",
            "iteration: 140 loss: 0.02427996\n",
            "iteration: 141 loss: 0.04970976\n",
            "iteration: 142 loss: 0.07177048\n",
            "iteration: 143 loss: 0.03190141\n",
            "iteration: 144 loss: 0.08517672\n",
            "iteration: 145 loss: 0.01477075\n",
            "iteration: 146 loss: 0.02691219\n",
            "iteration: 147 loss: 0.13066758\n",
            "iteration: 148 loss: 0.26994902\n",
            "iteration: 149 loss: 0.02586973\n",
            "iteration: 150 loss: 0.03023169\n",
            "iteration: 151 loss: 0.04391377\n",
            "iteration: 152 loss: 0.11458066\n",
            "iteration: 153 loss: 0.04721655\n",
            "iteration: 154 loss: 0.02325243\n",
            "iteration: 155 loss: 0.11824699\n",
            "iteration: 156 loss: 0.17276482\n",
            "iteration: 157 loss: 0.35066965\n",
            "iteration: 158 loss: 0.35901031\n",
            "iteration: 159 loss: 0.04673321\n",
            "iteration: 160 loss: 0.07962885\n",
            "iteration: 161 loss: 0.26430705\n",
            "iteration: 162 loss: 0.14479546\n",
            "iteration: 163 loss: 0.02842788\n",
            "iteration: 164 loss: 0.49662307\n",
            "iteration: 165 loss: 0.02785401\n",
            "iteration: 166 loss: 0.06809691\n",
            "iteration: 167 loss: 0.03885077\n",
            "iteration: 168 loss: 0.15521105\n",
            "iteration: 169 loss: 0.08909699\n",
            "iteration: 170 loss: 0.05789568\n",
            "iteration: 171 loss: 0.09676215\n",
            "iteration: 172 loss: 0.03688458\n",
            "iteration: 173 loss: 0.13032320\n",
            "iteration: 174 loss: 0.05447160\n",
            "iteration: 175 loss: 0.14786868\n",
            "iteration: 176 loss: 0.22450776\n",
            "iteration: 177 loss: 0.09950167\n",
            "iteration: 178 loss: 0.07744490\n",
            "iteration: 179 loss: 0.22682564\n",
            "iteration: 180 loss: 0.06609503\n",
            "iteration: 181 loss: 0.02273470\n",
            "iteration: 182 loss: 0.17504440\n",
            "iteration: 183 loss: 0.11870918\n",
            "iteration: 184 loss: 0.14200921\n",
            "iteration: 185 loss: 0.18087386\n",
            "iteration: 186 loss: 0.37635976\n",
            "iteration: 187 loss: 0.14769512\n",
            "iteration: 188 loss: 0.12826385\n",
            "iteration: 189 loss: 0.12251828\n",
            "iteration: 190 loss: 0.24060504\n",
            "iteration: 191 loss: 0.05797756\n",
            "iteration: 192 loss: 0.40137461\n",
            "iteration: 193 loss: 0.05157359\n",
            "iteration: 194 loss: 0.18098947\n",
            "iteration: 195 loss: 0.03437724\n",
            "iteration: 196 loss: 0.07339203\n",
            "iteration: 197 loss: 0.12046481\n",
            "iteration: 198 loss: 0.04298390\n",
            "iteration: 199 loss: 0.02217913\n",
            "epoch:  95 mean loss training: 0.12098750\n",
            "epoch:  95 mean loss validation: 0.36802316\n",
            "iteration:   0 loss: 0.23491798\n",
            "iteration:   1 loss: 0.11881443\n",
            "iteration:   2 loss: 0.25316766\n",
            "iteration:   3 loss: 0.18287188\n",
            "iteration:   4 loss: 0.40605283\n",
            "iteration:   5 loss: 0.35879296\n",
            "iteration:   6 loss: 0.11378583\n",
            "iteration:   7 loss: 0.48055393\n",
            "iteration:   8 loss: 0.27394351\n",
            "iteration:   9 loss: 0.19005920\n",
            "iteration:  10 loss: 0.15547287\n",
            "iteration:  11 loss: 0.28525418\n",
            "iteration:  12 loss: 0.20015952\n",
            "iteration:  13 loss: 0.10386492\n",
            "iteration:  14 loss: 0.04258103\n",
            "iteration:  15 loss: 0.16064319\n",
            "iteration:  16 loss: 0.05325659\n",
            "iteration:  17 loss: 0.06352694\n",
            "iteration:  18 loss: 0.02440874\n",
            "iteration:  19 loss: 0.32692686\n",
            "iteration:  20 loss: 0.04748312\n",
            "iteration:  21 loss: 0.14895205\n",
            "iteration:  22 loss: 0.19128011\n",
            "iteration:  23 loss: 0.03452608\n",
            "iteration:  24 loss: 0.04738965\n",
            "iteration:  25 loss: 0.27870190\n",
            "iteration:  26 loss: 0.10479364\n",
            "iteration:  27 loss: 0.10713384\n",
            "iteration:  28 loss: 0.06161142\n",
            "iteration:  29 loss: 0.04516567\n",
            "iteration:  30 loss: 0.07147408\n",
            "iteration:  31 loss: 0.08243064\n",
            "iteration:  32 loss: 0.37206915\n",
            "iteration:  33 loss: 0.09541310\n",
            "iteration:  34 loss: 0.13190532\n",
            "iteration:  35 loss: 0.06840628\n",
            "iteration:  36 loss: 0.13063532\n",
            "iteration:  37 loss: 0.14450401\n",
            "iteration:  38 loss: 0.07374098\n",
            "iteration:  39 loss: 0.01519947\n",
            "iteration:  40 loss: 0.03287489\n",
            "iteration:  41 loss: 0.17446488\n",
            "iteration:  42 loss: 0.20422335\n",
            "iteration:  43 loss: 0.23902993\n",
            "iteration:  44 loss: 0.05054213\n",
            "iteration:  45 loss: 0.22586817\n",
            "iteration:  46 loss: 0.11921360\n",
            "iteration:  47 loss: 0.24916553\n",
            "iteration:  48 loss: 0.22941111\n",
            "iteration:  49 loss: 0.14091466\n",
            "iteration:  50 loss: 0.05807429\n",
            "iteration:  51 loss: 0.06158528\n",
            "iteration:  52 loss: 0.24776812\n",
            "iteration:  53 loss: 0.36156657\n",
            "iteration:  54 loss: 0.04867357\n",
            "iteration:  55 loss: 0.01715694\n",
            "iteration:  56 loss: 0.04595954\n",
            "iteration:  57 loss: 0.33703434\n",
            "iteration:  58 loss: 0.03401698\n",
            "iteration:  59 loss: 0.04332738\n",
            "iteration:  60 loss: 0.11433280\n",
            "iteration:  61 loss: 0.03955339\n",
            "iteration:  62 loss: 0.13043119\n",
            "iteration:  63 loss: 0.09364934\n",
            "iteration:  64 loss: 0.09171769\n",
            "iteration:  65 loss: 0.22779919\n",
            "iteration:  66 loss: 0.31664577\n",
            "iteration:  67 loss: 0.01619207\n",
            "iteration:  68 loss: 0.02735912\n",
            "iteration:  69 loss: 0.16669756\n",
            "iteration:  70 loss: 0.07550362\n",
            "iteration:  71 loss: 0.09363498\n",
            "iteration:  72 loss: 0.15031435\n",
            "iteration:  73 loss: 0.06893548\n",
            "iteration:  74 loss: 0.24223305\n",
            "iteration:  75 loss: 0.08662971\n",
            "iteration:  76 loss: 0.06785630\n",
            "iteration:  77 loss: 0.06544217\n",
            "iteration:  78 loss: 0.03803539\n",
            "iteration:  79 loss: 0.12825584\n",
            "iteration:  80 loss: 0.02705861\n",
            "iteration:  81 loss: 0.07195942\n",
            "iteration:  82 loss: 0.16056900\n",
            "iteration:  83 loss: 0.28974500\n",
            "iteration:  84 loss: 0.04019007\n",
            "iteration:  85 loss: 0.04582593\n",
            "iteration:  86 loss: 0.07936127\n",
            "iteration:  87 loss: 0.01711003\n",
            "iteration:  88 loss: 0.23417710\n",
            "iteration:  89 loss: 0.07247743\n",
            "iteration:  90 loss: 0.15414274\n",
            "iteration:  91 loss: 0.05738639\n",
            "iteration:  92 loss: 0.11198708\n",
            "iteration:  93 loss: 0.02120117\n",
            "iteration:  94 loss: 0.19217058\n",
            "iteration:  95 loss: 0.08317677\n",
            "iteration:  96 loss: 0.17934716\n",
            "iteration:  97 loss: 0.16448489\n",
            "iteration:  98 loss: 0.16616166\n",
            "iteration:  99 loss: 0.03803737\n",
            "iteration: 100 loss: 0.23632459\n",
            "iteration: 101 loss: 0.19345716\n",
            "iteration: 102 loss: 0.39653265\n",
            "iteration: 103 loss: 0.06797289\n",
            "iteration: 104 loss: 0.48638955\n",
            "iteration: 105 loss: 0.06533238\n",
            "iteration: 106 loss: 0.17924510\n",
            "iteration: 107 loss: 0.05553593\n",
            "iteration: 108 loss: 0.01730774\n",
            "iteration: 109 loss: 0.08577834\n",
            "iteration: 110 loss: 0.02419917\n",
            "iteration: 111 loss: 0.10261274\n",
            "iteration: 112 loss: 0.06000391\n",
            "iteration: 113 loss: 0.16016871\n",
            "iteration: 114 loss: 0.09345853\n",
            "iteration: 115 loss: 0.03248935\n",
            "iteration: 116 loss: 0.20129350\n",
            "iteration: 117 loss: 0.04749094\n",
            "iteration: 118 loss: 0.08472044\n",
            "iteration: 119 loss: 0.06331718\n",
            "iteration: 120 loss: 0.09057841\n",
            "iteration: 121 loss: 0.29389790\n",
            "iteration: 122 loss: 0.08122408\n",
            "iteration: 123 loss: 0.01789922\n",
            "iteration: 124 loss: 0.16514497\n",
            "iteration: 125 loss: 0.07050998\n",
            "iteration: 126 loss: 0.05752654\n",
            "iteration: 127 loss: 0.09092247\n",
            "iteration: 128 loss: 0.01317247\n",
            "iteration: 129 loss: 0.01131625\n",
            "iteration: 130 loss: 0.09149246\n",
            "iteration: 131 loss: 0.08922206\n",
            "iteration: 132 loss: 0.12356079\n",
            "iteration: 133 loss: 0.12423780\n",
            "iteration: 134 loss: 0.06435718\n",
            "iteration: 135 loss: 0.07499190\n",
            "iteration: 136 loss: 0.03855275\n",
            "iteration: 137 loss: 0.16224155\n",
            "iteration: 138 loss: 0.10221729\n",
            "iteration: 139 loss: 0.10200043\n",
            "iteration: 140 loss: 0.08625096\n",
            "iteration: 141 loss: 0.04236146\n",
            "iteration: 142 loss: 0.11669009\n",
            "iteration: 143 loss: 0.04529402\n",
            "iteration: 144 loss: 0.11668560\n",
            "iteration: 145 loss: 0.36774319\n",
            "iteration: 146 loss: 0.13816018\n",
            "iteration: 147 loss: 0.08464012\n",
            "iteration: 148 loss: 0.08054098\n",
            "iteration: 149 loss: 0.07554971\n",
            "iteration: 150 loss: 0.03242407\n",
            "iteration: 151 loss: 0.04957893\n",
            "iteration: 152 loss: 0.14671355\n",
            "iteration: 153 loss: 0.68699241\n",
            "iteration: 154 loss: 0.07482282\n",
            "iteration: 155 loss: 0.12866536\n",
            "iteration: 156 loss: 0.15022126\n",
            "iteration: 157 loss: 0.19505107\n",
            "iteration: 158 loss: 0.06598515\n",
            "iteration: 159 loss: 0.04792315\n",
            "iteration: 160 loss: 0.20170912\n",
            "iteration: 161 loss: 0.19012953\n",
            "iteration: 162 loss: 0.08548088\n",
            "iteration: 163 loss: 0.05542268\n",
            "iteration: 164 loss: 0.03947590\n",
            "iteration: 165 loss: 0.06750177\n",
            "iteration: 166 loss: 0.13503942\n",
            "iteration: 167 loss: 0.05858022\n",
            "iteration: 168 loss: 0.31177106\n",
            "iteration: 169 loss: 0.14473704\n",
            "iteration: 170 loss: 0.60984075\n",
            "iteration: 171 loss: 0.03261475\n",
            "iteration: 172 loss: 0.23674478\n",
            "iteration: 173 loss: 0.11383227\n",
            "iteration: 174 loss: 0.10445779\n",
            "iteration: 175 loss: 0.27725270\n",
            "iteration: 176 loss: 0.07384287\n",
            "iteration: 177 loss: 0.11886939\n",
            "iteration: 178 loss: 0.05538609\n",
            "iteration: 179 loss: 0.06026058\n",
            "iteration: 180 loss: 0.04421489\n",
            "iteration: 181 loss: 0.19370137\n",
            "iteration: 182 loss: 0.06397664\n",
            "iteration: 183 loss: 0.06511402\n",
            "iteration: 184 loss: 0.22775635\n",
            "iteration: 185 loss: 0.15758464\n",
            "iteration: 186 loss: 0.04191075\n",
            "iteration: 187 loss: 0.33076647\n",
            "iteration: 188 loss: 0.09176447\n",
            "iteration: 189 loss: 0.12106689\n",
            "iteration: 190 loss: 0.04712409\n",
            "iteration: 191 loss: 0.18253475\n",
            "iteration: 192 loss: 0.02130670\n",
            "iteration: 193 loss: 0.02538040\n",
            "iteration: 194 loss: 0.10234873\n",
            "iteration: 195 loss: 0.13894011\n",
            "iteration: 196 loss: 0.08162162\n",
            "iteration: 197 loss: 0.04508917\n",
            "iteration: 198 loss: 0.10274836\n",
            "iteration: 199 loss: 0.09012100\n",
            "epoch:  96 mean loss training: 0.13024771\n",
            "epoch:  96 mean loss validation: 0.34919682\n",
            "iteration:   0 loss: 0.18366486\n",
            "iteration:   1 loss: 0.04890075\n",
            "iteration:   2 loss: 0.03624538\n",
            "iteration:   3 loss: 0.04781317\n",
            "iteration:   4 loss: 0.02910728\n",
            "iteration:   5 loss: 0.06846499\n",
            "iteration:   6 loss: 0.01988113\n",
            "iteration:   7 loss: 0.05525313\n",
            "iteration:   8 loss: 0.50520533\n",
            "iteration:   9 loss: 0.10958382\n",
            "iteration:  10 loss: 0.04164977\n",
            "iteration:  11 loss: 0.18090858\n",
            "iteration:  12 loss: 0.01674367\n",
            "iteration:  13 loss: 0.07167581\n",
            "iteration:  14 loss: 0.03912861\n",
            "iteration:  15 loss: 0.28492203\n",
            "iteration:  16 loss: 0.03823754\n",
            "iteration:  17 loss: 0.04697278\n",
            "iteration:  18 loss: 0.54530811\n",
            "iteration:  19 loss: 0.21209991\n",
            "iteration:  20 loss: 0.02041869\n",
            "iteration:  21 loss: 0.03125526\n",
            "iteration:  22 loss: 0.01811180\n",
            "iteration:  23 loss: 0.05068059\n",
            "iteration:  24 loss: 0.75091100\n",
            "iteration:  25 loss: 0.03035734\n",
            "iteration:  26 loss: 0.27946588\n",
            "iteration:  27 loss: 0.03740443\n",
            "iteration:  28 loss: 0.44869706\n",
            "iteration:  29 loss: 0.02964569\n",
            "iteration:  30 loss: 0.10379845\n",
            "iteration:  31 loss: 0.21761227\n",
            "iteration:  32 loss: 0.07772936\n",
            "iteration:  33 loss: 0.11961173\n",
            "iteration:  34 loss: 0.10941423\n",
            "iteration:  35 loss: 0.08747105\n",
            "iteration:  36 loss: 0.09778894\n",
            "iteration:  37 loss: 0.06139637\n",
            "iteration:  38 loss: 0.23740263\n",
            "iteration:  39 loss: 0.20535743\n",
            "iteration:  40 loss: 0.02986211\n",
            "iteration:  41 loss: 0.20086569\n",
            "iteration:  42 loss: 0.02971337\n",
            "iteration:  43 loss: 0.87911683\n",
            "iteration:  44 loss: 0.12251559\n",
            "iteration:  45 loss: 0.00928727\n",
            "iteration:  46 loss: 0.10473600\n",
            "iteration:  47 loss: 0.04505377\n",
            "iteration:  48 loss: 0.27994883\n",
            "iteration:  49 loss: 0.13574736\n",
            "iteration:  50 loss: 0.11549415\n",
            "iteration:  51 loss: 0.06904251\n",
            "iteration:  52 loss: 0.07811850\n",
            "iteration:  53 loss: 0.07534800\n",
            "iteration:  54 loss: 0.03373518\n",
            "iteration:  55 loss: 0.06191963\n",
            "iteration:  56 loss: 0.07606355\n",
            "iteration:  57 loss: 0.11598600\n",
            "iteration:  58 loss: 0.02445467\n",
            "iteration:  59 loss: 0.17929722\n",
            "iteration:  60 loss: 0.01186081\n",
            "iteration:  61 loss: 0.13773212\n",
            "iteration:  62 loss: 0.05289435\n",
            "iteration:  63 loss: 0.02147964\n",
            "iteration:  64 loss: 0.05692009\n",
            "iteration:  65 loss: 0.07821645\n",
            "iteration:  66 loss: 0.02299035\n",
            "iteration:  67 loss: 0.23799057\n",
            "iteration:  68 loss: 0.07967532\n",
            "iteration:  69 loss: 0.28372595\n",
            "iteration:  70 loss: 0.16018969\n",
            "iteration:  71 loss: 0.15882310\n",
            "iteration:  72 loss: 0.09156752\n",
            "iteration:  73 loss: 0.02757673\n",
            "iteration:  74 loss: 0.08805574\n",
            "iteration:  75 loss: 0.06914395\n",
            "iteration:  76 loss: 0.22931387\n",
            "iteration:  77 loss: 0.04350012\n",
            "iteration:  78 loss: 0.02445379\n",
            "iteration:  79 loss: 0.17072675\n",
            "iteration:  80 loss: 0.04986792\n",
            "iteration:  81 loss: 0.25447205\n",
            "iteration:  82 loss: 0.08358289\n",
            "iteration:  83 loss: 0.04010710\n",
            "iteration:  84 loss: 0.06449811\n",
            "iteration:  85 loss: 0.06241041\n",
            "iteration:  86 loss: 0.04925714\n",
            "iteration:  87 loss: 0.02109448\n",
            "iteration:  88 loss: 0.06473344\n",
            "iteration:  89 loss: 0.01654370\n",
            "iteration:  90 loss: 0.14319685\n",
            "iteration:  91 loss: 0.03335928\n",
            "iteration:  92 loss: 0.15859093\n",
            "iteration:  93 loss: 0.27295712\n",
            "iteration:  94 loss: 0.32073185\n",
            "iteration:  95 loss: 0.05345461\n",
            "iteration:  96 loss: 0.09954324\n",
            "iteration:  97 loss: 0.13009711\n",
            "iteration:  98 loss: 0.08013346\n",
            "iteration:  99 loss: 0.25250614\n",
            "iteration: 100 loss: 0.06941067\n",
            "iteration: 101 loss: 0.12593170\n",
            "iteration: 102 loss: 0.10104389\n",
            "iteration: 103 loss: 0.03012742\n",
            "iteration: 104 loss: 0.19013059\n",
            "iteration: 105 loss: 0.08209369\n",
            "iteration: 106 loss: 0.49239528\n",
            "iteration: 107 loss: 0.12749243\n",
            "iteration: 108 loss: 0.42595226\n",
            "iteration: 109 loss: 0.15256824\n",
            "iteration: 110 loss: 0.22979487\n",
            "iteration: 111 loss: 0.39695331\n",
            "iteration: 112 loss: 0.42231369\n",
            "iteration: 113 loss: 0.31000814\n",
            "iteration: 114 loss: 0.03940675\n",
            "iteration: 115 loss: 0.05704873\n",
            "iteration: 116 loss: 0.47401714\n",
            "iteration: 117 loss: 0.07930834\n",
            "iteration: 118 loss: 0.23732837\n",
            "iteration: 119 loss: 0.06036739\n",
            "iteration: 120 loss: 0.22263502\n",
            "iteration: 121 loss: 0.07889682\n",
            "iteration: 122 loss: 0.20781438\n",
            "iteration: 123 loss: 0.42323017\n",
            "iteration: 124 loss: 0.10128956\n",
            "iteration: 125 loss: 0.13284625\n",
            "iteration: 126 loss: 0.60652500\n",
            "iteration: 127 loss: 0.16520955\n",
            "iteration: 128 loss: 0.01604961\n",
            "iteration: 129 loss: 0.06079885\n",
            "iteration: 130 loss: 0.11753273\n",
            "iteration: 131 loss: 0.05531020\n",
            "iteration: 132 loss: 0.20287588\n",
            "iteration: 133 loss: 0.06317177\n",
            "iteration: 134 loss: 0.03571303\n",
            "iteration: 135 loss: 0.53249031\n",
            "iteration: 136 loss: 0.03403638\n",
            "iteration: 137 loss: 0.06832320\n",
            "iteration: 138 loss: 0.13369024\n",
            "iteration: 139 loss: 0.19120067\n",
            "iteration: 140 loss: 0.05664672\n",
            "iteration: 141 loss: 0.38957641\n",
            "iteration: 142 loss: 0.13434021\n",
            "iteration: 143 loss: 0.08514564\n",
            "iteration: 144 loss: 0.21436909\n",
            "iteration: 145 loss: 0.07749038\n",
            "iteration: 146 loss: 0.19129111\n",
            "iteration: 147 loss: 0.26517001\n",
            "iteration: 148 loss: 0.07737682\n",
            "iteration: 149 loss: 0.05975154\n",
            "iteration: 150 loss: 0.12810604\n",
            "iteration: 151 loss: 0.09793478\n",
            "iteration: 152 loss: 0.15050226\n",
            "iteration: 153 loss: 0.18102767\n",
            "iteration: 154 loss: 0.18302001\n",
            "iteration: 155 loss: 0.05252482\n",
            "iteration: 156 loss: 0.21658725\n",
            "iteration: 157 loss: 0.09194452\n",
            "iteration: 158 loss: 0.05241681\n",
            "iteration: 159 loss: 0.24991490\n",
            "iteration: 160 loss: 0.05556308\n",
            "iteration: 161 loss: 0.15808514\n",
            "iteration: 162 loss: 0.08954549\n",
            "iteration: 163 loss: 0.01830227\n",
            "iteration: 164 loss: 0.16523309\n",
            "iteration: 165 loss: 0.07196315\n",
            "iteration: 166 loss: 0.06324445\n",
            "iteration: 167 loss: 0.10900944\n",
            "iteration: 168 loss: 0.03647966\n",
            "iteration: 169 loss: 0.02316117\n",
            "iteration: 170 loss: 0.06203314\n",
            "iteration: 171 loss: 0.04782011\n",
            "iteration: 172 loss: 0.06958330\n",
            "iteration: 173 loss: 0.09506259\n",
            "iteration: 174 loss: 0.27207413\n",
            "iteration: 175 loss: 0.02320550\n",
            "iteration: 176 loss: 0.05843013\n",
            "iteration: 177 loss: 0.36860427\n",
            "iteration: 178 loss: 0.10651053\n",
            "iteration: 179 loss: 0.05621303\n",
            "iteration: 180 loss: 0.18820137\n",
            "iteration: 181 loss: 0.03738956\n",
            "iteration: 182 loss: 0.01844099\n",
            "iteration: 183 loss: 0.18127821\n",
            "iteration: 184 loss: 0.01777602\n",
            "iteration: 185 loss: 0.14429714\n",
            "iteration: 186 loss: 0.03112090\n",
            "iteration: 187 loss: 0.12698728\n",
            "iteration: 188 loss: 0.06615768\n",
            "iteration: 189 loss: 0.09092708\n",
            "iteration: 190 loss: 0.09115966\n",
            "iteration: 191 loss: 0.08660908\n",
            "iteration: 192 loss: 0.14410366\n",
            "iteration: 193 loss: 0.02056928\n",
            "iteration: 194 loss: 0.07149915\n",
            "iteration: 195 loss: 0.02701509\n",
            "iteration: 196 loss: 0.06636760\n",
            "iteration: 197 loss: 0.83681911\n",
            "iteration: 198 loss: 0.13888384\n",
            "iteration: 199 loss: 0.13207462\n",
            "epoch:  97 mean loss training: 0.13768423\n",
            "epoch:  97 mean loss validation: 0.36977673\n",
            "iteration:   0 loss: 0.07495350\n",
            "iteration:   1 loss: 0.25511628\n",
            "iteration:   2 loss: 0.02279644\n",
            "iteration:   3 loss: 0.03678309\n",
            "iteration:   4 loss: 0.06363855\n",
            "iteration:   5 loss: 0.07703437\n",
            "iteration:   6 loss: 0.04268035\n",
            "iteration:   7 loss: 0.06911783\n",
            "iteration:   8 loss: 0.06601412\n",
            "iteration:   9 loss: 0.32861730\n",
            "iteration:  10 loss: 0.13533384\n",
            "iteration:  11 loss: 0.07742701\n",
            "iteration:  12 loss: 0.02919986\n",
            "iteration:  13 loss: 0.04998886\n",
            "iteration:  14 loss: 0.02592352\n",
            "iteration:  15 loss: 0.11666559\n",
            "iteration:  16 loss: 0.15725467\n",
            "iteration:  17 loss: 0.04498853\n",
            "iteration:  18 loss: 0.05069711\n",
            "iteration:  19 loss: 0.01441239\n",
            "iteration:  20 loss: 0.07333590\n",
            "iteration:  21 loss: 0.11742469\n",
            "iteration:  22 loss: 0.04782823\n",
            "iteration:  23 loss: 0.10154151\n",
            "iteration:  24 loss: 0.11720678\n",
            "iteration:  25 loss: 0.05892501\n",
            "iteration:  26 loss: 0.08907020\n",
            "iteration:  27 loss: 0.07412168\n",
            "iteration:  28 loss: 0.40534610\n",
            "iteration:  29 loss: 0.06890962\n",
            "iteration:  30 loss: 0.20297191\n",
            "iteration:  31 loss: 0.07014207\n",
            "iteration:  32 loss: 0.13339740\n",
            "iteration:  33 loss: 0.10710601\n",
            "iteration:  34 loss: 0.09555258\n",
            "iteration:  35 loss: 0.04608392\n",
            "iteration:  36 loss: 0.20582779\n",
            "iteration:  37 loss: 0.06125924\n",
            "iteration:  38 loss: 0.13715406\n",
            "iteration:  39 loss: 0.34610003\n",
            "iteration:  40 loss: 0.18241689\n",
            "iteration:  41 loss: 0.16682498\n",
            "iteration:  42 loss: 0.38113052\n",
            "iteration:  43 loss: 0.01643403\n",
            "iteration:  44 loss: 0.01294906\n",
            "iteration:  45 loss: 0.01014252\n",
            "iteration:  46 loss: 0.07371729\n",
            "iteration:  47 loss: 0.04823194\n",
            "iteration:  48 loss: 0.01988430\n",
            "iteration:  49 loss: 0.22484656\n",
            "iteration:  50 loss: 0.09696069\n",
            "iteration:  51 loss: 0.17598914\n",
            "iteration:  52 loss: 0.04732800\n",
            "iteration:  53 loss: 0.07787825\n",
            "iteration:  54 loss: 0.11779248\n",
            "iteration:  55 loss: 0.13895558\n",
            "iteration:  56 loss: 0.20582265\n",
            "iteration:  57 loss: 0.06389368\n",
            "iteration:  58 loss: 0.10643451\n",
            "iteration:  59 loss: 0.32105902\n",
            "iteration:  60 loss: 0.05131359\n",
            "iteration:  61 loss: 0.02752699\n",
            "iteration:  62 loss: 0.45345390\n",
            "iteration:  63 loss: 0.13807397\n",
            "iteration:  64 loss: 0.11968386\n",
            "iteration:  65 loss: 0.17778996\n",
            "iteration:  66 loss: 0.08043325\n",
            "iteration:  67 loss: 0.06845292\n",
            "iteration:  68 loss: 0.09136166\n",
            "iteration:  69 loss: 0.11979974\n",
            "iteration:  70 loss: 0.07088601\n",
            "iteration:  71 loss: 0.14547728\n",
            "iteration:  72 loss: 0.08046971\n",
            "iteration:  73 loss: 0.01727719\n",
            "iteration:  74 loss: 0.04898497\n",
            "iteration:  75 loss: 0.07697242\n",
            "iteration:  76 loss: 0.02594328\n",
            "iteration:  77 loss: 0.06112549\n",
            "iteration:  78 loss: 0.04728987\n",
            "iteration:  79 loss: 0.04216429\n",
            "iteration:  80 loss: 0.18836172\n",
            "iteration:  81 loss: 0.05338898\n",
            "iteration:  82 loss: 0.13719967\n",
            "iteration:  83 loss: 0.25604245\n",
            "iteration:  84 loss: 0.04363021\n",
            "iteration:  85 loss: 0.07201593\n",
            "iteration:  86 loss: 0.09937713\n",
            "iteration:  87 loss: 0.09737444\n",
            "iteration:  88 loss: 0.21456592\n",
            "iteration:  89 loss: 0.11325405\n",
            "iteration:  90 loss: 0.06459309\n",
            "iteration:  91 loss: 0.28878236\n",
            "iteration:  92 loss: 0.03816572\n",
            "iteration:  93 loss: 0.36935222\n",
            "iteration:  94 loss: 0.08057030\n",
            "iteration:  95 loss: 0.13716362\n",
            "iteration:  96 loss: 0.14213848\n",
            "iteration:  97 loss: 0.01222406\n",
            "iteration:  98 loss: 0.16751076\n",
            "iteration:  99 loss: 0.05732402\n",
            "iteration: 100 loss: 0.27776703\n",
            "iteration: 101 loss: 0.01757486\n",
            "iteration: 102 loss: 0.04505616\n",
            "iteration: 103 loss: 0.11966627\n",
            "iteration: 104 loss: 0.09850192\n",
            "iteration: 105 loss: 0.04812474\n",
            "iteration: 106 loss: 0.17718968\n",
            "iteration: 107 loss: 0.15294656\n",
            "iteration: 108 loss: 0.02933324\n",
            "iteration: 109 loss: 0.20818904\n",
            "iteration: 110 loss: 0.26423758\n",
            "iteration: 111 loss: 0.06046001\n",
            "iteration: 112 loss: 0.08594571\n",
            "iteration: 113 loss: 0.04741067\n",
            "iteration: 114 loss: 0.26572299\n",
            "iteration: 115 loss: 0.02942406\n",
            "iteration: 116 loss: 0.09619515\n",
            "iteration: 117 loss: 0.06464703\n",
            "iteration: 118 loss: 0.12154257\n",
            "iteration: 119 loss: 0.31231025\n",
            "iteration: 120 loss: 0.21866015\n",
            "iteration: 121 loss: 0.08555249\n",
            "iteration: 122 loss: 0.22100188\n",
            "iteration: 123 loss: 0.06281596\n",
            "iteration: 124 loss: 0.03572398\n",
            "iteration: 125 loss: 0.10512788\n",
            "iteration: 126 loss: 0.08564000\n",
            "iteration: 127 loss: 0.03645009\n",
            "iteration: 128 loss: 0.02388566\n",
            "iteration: 129 loss: 0.38461226\n",
            "iteration: 130 loss: 0.04932728\n",
            "iteration: 131 loss: 0.11803201\n",
            "iteration: 132 loss: 0.31990084\n",
            "iteration: 133 loss: 0.05481907\n",
            "iteration: 134 loss: 0.17246507\n",
            "iteration: 135 loss: 0.25498757\n",
            "iteration: 136 loss: 0.16226029\n",
            "iteration: 137 loss: 0.05340704\n",
            "iteration: 138 loss: 0.06040033\n",
            "iteration: 139 loss: 0.07161409\n",
            "iteration: 140 loss: 0.04337542\n",
            "iteration: 141 loss: 0.10058976\n",
            "iteration: 142 loss: 0.07912764\n",
            "iteration: 143 loss: 0.02447346\n",
            "iteration: 144 loss: 0.28775281\n",
            "iteration: 145 loss: 0.03416763\n",
            "iteration: 146 loss: 0.10669824\n",
            "iteration: 147 loss: 0.01173008\n",
            "iteration: 148 loss: 0.12530418\n",
            "iteration: 149 loss: 1.26136243\n",
            "iteration: 150 loss: 0.06199677\n",
            "iteration: 151 loss: 0.11051853\n",
            "iteration: 152 loss: 0.09965087\n",
            "iteration: 153 loss: 0.08413576\n",
            "iteration: 154 loss: 0.02469349\n",
            "iteration: 155 loss: 0.19210003\n",
            "iteration: 156 loss: 0.03880518\n",
            "iteration: 157 loss: 0.10569692\n",
            "iteration: 158 loss: 0.13867725\n",
            "iteration: 159 loss: 0.14585459\n",
            "iteration: 160 loss: 0.31501710\n",
            "iteration: 161 loss: 0.05153539\n",
            "iteration: 162 loss: 0.04702121\n",
            "iteration: 163 loss: 0.04794974\n",
            "iteration: 164 loss: 0.04507292\n",
            "iteration: 165 loss: 0.05191959\n",
            "iteration: 166 loss: 0.33295456\n",
            "iteration: 167 loss: 0.26772267\n",
            "iteration: 168 loss: 0.30439290\n",
            "iteration: 169 loss: 0.07375701\n",
            "iteration: 170 loss: 0.32324040\n",
            "iteration: 171 loss: 0.01661945\n",
            "iteration: 172 loss: 0.08424576\n",
            "iteration: 173 loss: 0.37482098\n",
            "iteration: 174 loss: 0.09495556\n",
            "iteration: 175 loss: 0.18427914\n",
            "iteration: 176 loss: 0.10546979\n",
            "iteration: 177 loss: 0.10592607\n",
            "iteration: 178 loss: 0.05740553\n",
            "iteration: 179 loss: 0.05334978\n",
            "iteration: 180 loss: 0.04207630\n",
            "iteration: 181 loss: 0.79941642\n",
            "iteration: 182 loss: 0.09851138\n",
            "iteration: 183 loss: 0.10339327\n",
            "iteration: 184 loss: 0.56720859\n",
            "iteration: 185 loss: 0.09543785\n",
            "iteration: 186 loss: 0.07202528\n",
            "iteration: 187 loss: 0.05916819\n",
            "iteration: 188 loss: 0.14356673\n",
            "iteration: 189 loss: 0.20013280\n",
            "iteration: 190 loss: 0.33224180\n",
            "iteration: 191 loss: 0.05627164\n",
            "iteration: 192 loss: 0.05859537\n",
            "iteration: 193 loss: 0.05664914\n",
            "iteration: 194 loss: 0.11855952\n",
            "iteration: 195 loss: 0.31565592\n",
            "iteration: 196 loss: 0.01831150\n",
            "iteration: 197 loss: 0.03885483\n",
            "iteration: 198 loss: 0.17878148\n",
            "iteration: 199 loss: 0.14609990\n",
            "epoch:  98 mean loss training: 0.12989205\n",
            "epoch:  98 mean loss validation: 0.40047884\n",
            "iteration:   0 loss: 0.14494769\n",
            "iteration:   1 loss: 0.20279449\n",
            "iteration:   2 loss: 0.32080585\n",
            "iteration:   3 loss: 0.07567506\n",
            "iteration:   4 loss: 0.15674694\n",
            "iteration:   5 loss: 0.04872847\n",
            "iteration:   6 loss: 0.04046233\n",
            "iteration:   7 loss: 0.08423869\n",
            "iteration:   8 loss: 0.04806318\n",
            "iteration:   9 loss: 0.09130395\n",
            "iteration:  10 loss: 0.06052953\n",
            "iteration:  11 loss: 0.03922519\n",
            "iteration:  12 loss: 0.02317860\n",
            "iteration:  13 loss: 0.18451032\n",
            "iteration:  14 loss: 0.10925265\n",
            "iteration:  15 loss: 0.12453908\n",
            "iteration:  16 loss: 0.31093410\n",
            "iteration:  17 loss: 0.21082076\n",
            "iteration:  18 loss: 0.03773612\n",
            "iteration:  19 loss: 0.30471760\n",
            "iteration:  20 loss: 0.01322174\n",
            "iteration:  21 loss: 0.09470874\n",
            "iteration:  22 loss: 0.06898924\n",
            "iteration:  23 loss: 0.19780771\n",
            "iteration:  24 loss: 0.02452534\n",
            "iteration:  25 loss: 0.12384146\n",
            "iteration:  26 loss: 0.10124315\n",
            "iteration:  27 loss: 0.13295648\n",
            "iteration:  28 loss: 0.05716022\n",
            "iteration:  29 loss: 0.45662066\n",
            "iteration:  30 loss: 0.02102114\n",
            "iteration:  31 loss: 0.07694240\n",
            "iteration:  32 loss: 0.05002035\n",
            "iteration:  33 loss: 0.03782585\n",
            "iteration:  34 loss: 0.05722315\n",
            "iteration:  35 loss: 0.02911811\n",
            "iteration:  36 loss: 0.01161814\n",
            "iteration:  37 loss: 0.07124986\n",
            "iteration:  38 loss: 0.03681919\n",
            "iteration:  39 loss: 0.11283243\n",
            "iteration:  40 loss: 0.06924247\n",
            "iteration:  41 loss: 0.10837063\n",
            "iteration:  42 loss: 0.12063000\n",
            "iteration:  43 loss: 0.22570141\n",
            "iteration:  44 loss: 0.07077605\n",
            "iteration:  45 loss: 0.34808969\n",
            "iteration:  46 loss: 0.04861081\n",
            "iteration:  47 loss: 0.03049505\n",
            "iteration:  48 loss: 0.03649009\n",
            "iteration:  49 loss: 0.05421923\n",
            "iteration:  50 loss: 0.05648968\n",
            "iteration:  51 loss: 0.12655744\n",
            "iteration:  52 loss: 0.07487616\n",
            "iteration:  53 loss: 0.06489111\n",
            "iteration:  54 loss: 0.03652197\n",
            "iteration:  55 loss: 0.42799580\n",
            "iteration:  56 loss: 0.06330501\n",
            "iteration:  57 loss: 0.15936145\n",
            "iteration:  58 loss: 0.07954808\n",
            "iteration:  59 loss: 0.17240822\n",
            "iteration:  60 loss: 0.31048769\n",
            "iteration:  61 loss: 0.11228087\n",
            "iteration:  62 loss: 0.03336544\n",
            "iteration:  63 loss: 0.08316688\n",
            "iteration:  64 loss: 0.16237819\n",
            "iteration:  65 loss: 0.08006197\n",
            "iteration:  66 loss: 0.43779841\n",
            "iteration:  67 loss: 0.05104886\n",
            "iteration:  68 loss: 0.08434047\n",
            "iteration:  69 loss: 0.14884745\n",
            "iteration:  70 loss: 0.11159734\n",
            "iteration:  71 loss: 0.09405815\n",
            "iteration:  72 loss: 0.14171602\n",
            "iteration:  73 loss: 0.06761003\n",
            "iteration:  74 loss: 0.08822191\n",
            "iteration:  75 loss: 0.22183865\n",
            "iteration:  76 loss: 0.17331822\n",
            "iteration:  77 loss: 0.02942926\n",
            "iteration:  78 loss: 0.06573717\n",
            "iteration:  79 loss: 0.10788909\n",
            "iteration:  80 loss: 0.07603938\n",
            "iteration:  81 loss: 0.88837087\n",
            "iteration:  82 loss: 0.04921753\n",
            "iteration:  83 loss: 0.13244727\n",
            "iteration:  84 loss: 0.06019654\n",
            "iteration:  85 loss: 0.22424278\n",
            "iteration:  86 loss: 0.15056330\n",
            "iteration:  87 loss: 0.16932845\n",
            "iteration:  88 loss: 0.02865666\n",
            "iteration:  89 loss: 0.04992703\n",
            "iteration:  90 loss: 0.34366453\n",
            "iteration:  91 loss: 0.08118429\n",
            "iteration:  92 loss: 0.11073080\n",
            "iteration:  93 loss: 0.21930741\n",
            "iteration:  94 loss: 0.06477812\n",
            "iteration:  95 loss: 0.15371518\n",
            "iteration:  96 loss: 0.20975840\n",
            "iteration:  97 loss: 0.46511659\n",
            "iteration:  98 loss: 0.03232789\n",
            "iteration:  99 loss: 0.43533990\n",
            "iteration: 100 loss: 0.13311057\n",
            "iteration: 101 loss: 0.06531230\n",
            "iteration: 102 loss: 0.12214725\n",
            "iteration: 103 loss: 0.17549697\n",
            "iteration: 104 loss: 0.05453547\n",
            "iteration: 105 loss: 0.19893984\n",
            "iteration: 106 loss: 0.33874235\n",
            "iteration: 107 loss: 0.09824280\n",
            "iteration: 108 loss: 0.05837511\n",
            "iteration: 109 loss: 0.05404350\n",
            "iteration: 110 loss: 0.12555480\n",
            "iteration: 111 loss: 0.25785747\n",
            "iteration: 112 loss: 0.08568558\n",
            "iteration: 113 loss: 0.05005896\n",
            "iteration: 114 loss: 0.02988796\n",
            "iteration: 115 loss: 0.13102639\n",
            "iteration: 116 loss: 0.15606233\n",
            "iteration: 117 loss: 0.09711409\n",
            "iteration: 118 loss: 0.04476079\n",
            "iteration: 119 loss: 0.01979947\n",
            "iteration: 120 loss: 0.24428886\n",
            "iteration: 121 loss: 0.03806612\n",
            "iteration: 122 loss: 0.08903545\n",
            "iteration: 123 loss: 0.09733576\n",
            "iteration: 124 loss: 0.25240618\n",
            "iteration: 125 loss: 0.08930921\n",
            "iteration: 126 loss: 0.04232343\n",
            "iteration: 127 loss: 0.20272794\n",
            "iteration: 128 loss: 0.06736112\n",
            "iteration: 129 loss: 0.34432834\n",
            "iteration: 130 loss: 0.07221599\n",
            "iteration: 131 loss: 0.01858656\n",
            "iteration: 132 loss: 0.06572045\n",
            "iteration: 133 loss: 0.07510984\n",
            "iteration: 134 loss: 0.27708685\n",
            "iteration: 135 loss: 0.07294624\n",
            "iteration: 136 loss: 0.09168708\n",
            "iteration: 137 loss: 0.06670000\n",
            "iteration: 138 loss: 0.06723442\n",
            "iteration: 139 loss: 0.41406032\n",
            "iteration: 140 loss: 0.04918320\n",
            "iteration: 141 loss: 0.02560052\n",
            "iteration: 142 loss: 0.15845159\n",
            "iteration: 143 loss: 0.12229154\n",
            "iteration: 144 loss: 0.02897547\n",
            "iteration: 145 loss: 0.16660441\n",
            "iteration: 146 loss: 0.17439134\n",
            "iteration: 147 loss: 0.05671701\n",
            "iteration: 148 loss: 0.04227593\n",
            "iteration: 149 loss: 0.10898884\n",
            "iteration: 150 loss: 0.06902453\n",
            "iteration: 151 loss: 0.28722525\n",
            "iteration: 152 loss: 0.23400465\n",
            "iteration: 153 loss: 0.05450155\n",
            "iteration: 154 loss: 0.13149539\n",
            "iteration: 155 loss: 0.11366401\n",
            "iteration: 156 loss: 0.07247002\n",
            "iteration: 157 loss: 0.01247350\n",
            "iteration: 158 loss: 0.16623431\n",
            "iteration: 159 loss: 0.15762351\n",
            "iteration: 160 loss: 0.01298232\n",
            "iteration: 161 loss: 0.05171362\n",
            "iteration: 162 loss: 0.02185762\n",
            "iteration: 163 loss: 0.02772163\n",
            "iteration: 164 loss: 0.53075552\n",
            "iteration: 165 loss: 0.19653788\n",
            "iteration: 166 loss: 0.12195300\n",
            "iteration: 167 loss: 0.08750199\n",
            "iteration: 168 loss: 0.44250059\n",
            "iteration: 169 loss: 0.06452547\n",
            "iteration: 170 loss: 0.03705309\n",
            "iteration: 171 loss: 0.07264468\n",
            "iteration: 172 loss: 0.14028220\n",
            "iteration: 173 loss: 0.08661799\n",
            "iteration: 174 loss: 0.10670405\n",
            "iteration: 175 loss: 0.02290260\n",
            "iteration: 176 loss: 0.03072799\n",
            "iteration: 177 loss: 0.02334296\n",
            "iteration: 178 loss: 0.07968874\n",
            "iteration: 179 loss: 0.18138942\n",
            "iteration: 180 loss: 0.09331511\n",
            "iteration: 181 loss: 0.34817877\n",
            "iteration: 182 loss: 0.02324049\n",
            "iteration: 183 loss: 0.31907973\n",
            "iteration: 184 loss: 0.08162491\n",
            "iteration: 185 loss: 0.51328498\n",
            "iteration: 186 loss: 0.30369648\n",
            "iteration: 187 loss: 0.18056254\n",
            "iteration: 188 loss: 0.08324853\n",
            "iteration: 189 loss: 0.04382848\n",
            "iteration: 190 loss: 0.18698490\n",
            "iteration: 191 loss: 0.31115705\n",
            "iteration: 192 loss: 0.11277870\n",
            "iteration: 193 loss: 0.08212607\n",
            "iteration: 194 loss: 0.51394093\n",
            "iteration: 195 loss: 0.06806698\n",
            "iteration: 196 loss: 0.06695960\n",
            "iteration: 197 loss: 0.03308316\n",
            "iteration: 198 loss: 0.00604752\n",
            "iteration: 199 loss: 0.09457694\n",
            "epoch:  99 mean loss training: 0.13171153\n",
            "epoch:  99 mean loss validation: 0.39487299\n",
            "iteration:   0 loss: 0.06706701\n",
            "iteration:   1 loss: 0.27991825\n",
            "iteration:   2 loss: 0.28046352\n",
            "iteration:   3 loss: 0.13174805\n",
            "iteration:   4 loss: 0.16575654\n",
            "iteration:   5 loss: 0.20936100\n",
            "iteration:   6 loss: 0.11294992\n",
            "iteration:   7 loss: 0.14072180\n",
            "iteration:   8 loss: 0.05780903\n",
            "iteration:   9 loss: 0.18253733\n",
            "iteration:  10 loss: 0.17718044\n",
            "iteration:  11 loss: 0.11990470\n",
            "iteration:  12 loss: 0.14184496\n",
            "iteration:  13 loss: 0.01777100\n",
            "iteration:  14 loss: 0.05780637\n",
            "iteration:  15 loss: 0.03485409\n",
            "iteration:  16 loss: 0.15310265\n",
            "iteration:  17 loss: 0.06020970\n",
            "iteration:  18 loss: 0.04002317\n",
            "iteration:  19 loss: 0.03054800\n",
            "iteration:  20 loss: 0.21691129\n",
            "iteration:  21 loss: 0.24254079\n",
            "iteration:  22 loss: 0.13368142\n",
            "iteration:  23 loss: 0.08278754\n",
            "iteration:  24 loss: 0.02077823\n",
            "iteration:  25 loss: 0.06230596\n",
            "iteration:  26 loss: 0.50227946\n",
            "iteration:  27 loss: 0.08885981\n",
            "iteration:  28 loss: 0.01858177\n",
            "iteration:  29 loss: 0.06417313\n",
            "iteration:  30 loss: 0.04203137\n",
            "iteration:  31 loss: 0.05617411\n",
            "iteration:  32 loss: 0.05198040\n",
            "iteration:  33 loss: 0.22032182\n",
            "iteration:  34 loss: 0.05161523\n",
            "iteration:  35 loss: 0.02857480\n",
            "iteration:  36 loss: 0.06840620\n",
            "iteration:  37 loss: 0.07165850\n",
            "iteration:  38 loss: 0.66406703\n",
            "iteration:  39 loss: 0.17791960\n",
            "iteration:  40 loss: 0.04663407\n",
            "iteration:  41 loss: 0.06084449\n",
            "iteration:  42 loss: 0.17985804\n",
            "iteration:  43 loss: 0.36457807\n",
            "iteration:  44 loss: 0.03827794\n",
            "iteration:  45 loss: 0.11172242\n",
            "iteration:  46 loss: 0.08871736\n",
            "iteration:  47 loss: 0.06088833\n",
            "iteration:  48 loss: 0.16568989\n",
            "iteration:  49 loss: 0.03122256\n",
            "iteration:  50 loss: 0.19319798\n",
            "iteration:  51 loss: 0.14691781\n",
            "iteration:  52 loss: 0.07714776\n",
            "iteration:  53 loss: 0.08566622\n",
            "iteration:  54 loss: 0.17535640\n",
            "iteration:  55 loss: 0.16307884\n",
            "iteration:  56 loss: 0.10305121\n",
            "iteration:  57 loss: 0.01802683\n",
            "iteration:  58 loss: 0.06356994\n",
            "iteration:  59 loss: 0.13814957\n",
            "iteration:  60 loss: 0.08999185\n",
            "iteration:  61 loss: 0.54661977\n",
            "iteration:  62 loss: 0.15409154\n",
            "iteration:  63 loss: 0.72911090\n",
            "iteration:  64 loss: 0.13321055\n",
            "iteration:  65 loss: 0.09070642\n",
            "iteration:  66 loss: 0.14776111\n",
            "iteration:  67 loss: 0.14460479\n",
            "iteration:  68 loss: 0.14727043\n",
            "iteration:  69 loss: 0.05093994\n",
            "iteration:  70 loss: 0.02534815\n",
            "iteration:  71 loss: 0.19797763\n",
            "iteration:  72 loss: 0.11638421\n",
            "iteration:  73 loss: 0.01110533\n",
            "iteration:  74 loss: 0.17737840\n",
            "iteration:  75 loss: 0.08603687\n",
            "iteration:  76 loss: 0.31308669\n",
            "iteration:  77 loss: 0.23980416\n",
            "iteration:  78 loss: 0.24137639\n",
            "iteration:  79 loss: 0.16150229\n",
            "iteration:  80 loss: 0.09221630\n",
            "iteration:  81 loss: 0.02919644\n",
            "iteration:  82 loss: 0.21662414\n",
            "iteration:  83 loss: 0.07405489\n",
            "iteration:  84 loss: 0.02569422\n",
            "iteration:  85 loss: 0.13281789\n",
            "iteration:  86 loss: 0.06349535\n",
            "iteration:  87 loss: 0.03384042\n",
            "iteration:  88 loss: 0.02545987\n",
            "iteration:  89 loss: 0.13362405\n",
            "iteration:  90 loss: 0.09019405\n",
            "iteration:  91 loss: 0.58921158\n",
            "iteration:  92 loss: 0.00548508\n",
            "iteration:  93 loss: 0.02871967\n",
            "iteration:  94 loss: 0.08582775\n",
            "iteration:  95 loss: 0.10110847\n",
            "iteration:  96 loss: 0.24005023\n",
            "iteration:  97 loss: 0.08434029\n",
            "iteration:  98 loss: 0.10602201\n",
            "iteration:  99 loss: 0.12425280\n",
            "iteration: 100 loss: 0.47271937\n",
            "iteration: 101 loss: 0.14355521\n",
            "iteration: 102 loss: 0.06181695\n",
            "iteration: 103 loss: 0.63682777\n",
            "iteration: 104 loss: 0.11827123\n",
            "iteration: 105 loss: 0.13050243\n",
            "iteration: 106 loss: 0.27110067\n",
            "iteration: 107 loss: 0.10263306\n",
            "iteration: 108 loss: 0.04862237\n",
            "iteration: 109 loss: 0.07645864\n",
            "iteration: 110 loss: 0.02419221\n",
            "iteration: 111 loss: 0.14782554\n",
            "iteration: 112 loss: 0.04628663\n",
            "iteration: 113 loss: 0.04389541\n",
            "iteration: 114 loss: 0.33123684\n",
            "iteration: 115 loss: 0.04568104\n",
            "iteration: 116 loss: 0.07145353\n",
            "iteration: 117 loss: 0.03259524\n",
            "iteration: 118 loss: 0.10581954\n",
            "iteration: 119 loss: 0.02609373\n",
            "iteration: 120 loss: 0.15800424\n",
            "iteration: 121 loss: 0.12043965\n",
            "iteration: 122 loss: 0.02778840\n",
            "iteration: 123 loss: 0.60588211\n",
            "iteration: 124 loss: 0.03957296\n",
            "iteration: 125 loss: 0.09286938\n",
            "iteration: 126 loss: 0.06260367\n",
            "iteration: 127 loss: 0.08900896\n",
            "iteration: 128 loss: 0.15590790\n",
            "iteration: 129 loss: 0.02989830\n",
            "iteration: 130 loss: 0.36038265\n",
            "iteration: 131 loss: 0.04815619\n",
            "iteration: 132 loss: 0.12018408\n",
            "iteration: 133 loss: 0.04279431\n",
            "iteration: 134 loss: 0.06596942\n",
            "iteration: 135 loss: 0.06258133\n",
            "iteration: 136 loss: 0.05910675\n",
            "iteration: 137 loss: 0.05927017\n",
            "iteration: 138 loss: 0.03480828\n",
            "iteration: 139 loss: 0.21077666\n",
            "iteration: 140 loss: 0.03345481\n",
            "iteration: 141 loss: 0.28931463\n",
            "iteration: 142 loss: 0.06915133\n",
            "iteration: 143 loss: 0.03027524\n",
            "iteration: 144 loss: 0.04659076\n",
            "iteration: 145 loss: 0.05976645\n",
            "iteration: 146 loss: 0.00921083\n",
            "iteration: 147 loss: 0.09952737\n",
            "iteration: 148 loss: 0.24310607\n",
            "iteration: 149 loss: 0.03624261\n",
            "iteration: 150 loss: 0.10306253\n",
            "iteration: 151 loss: 0.08782431\n",
            "iteration: 152 loss: 0.19936496\n",
            "iteration: 153 loss: 0.10227326\n",
            "iteration: 154 loss: 0.29679710\n",
            "iteration: 155 loss: 0.17259082\n",
            "iteration: 156 loss: 0.01159604\n",
            "iteration: 157 loss: 0.03208164\n",
            "iteration: 158 loss: 0.01308862\n",
            "iteration: 159 loss: 0.44361788\n",
            "iteration: 160 loss: 0.07947270\n",
            "iteration: 161 loss: 0.18126655\n",
            "iteration: 162 loss: 0.05752038\n",
            "iteration: 163 loss: 0.01090389\n",
            "iteration: 164 loss: 0.14754640\n",
            "iteration: 165 loss: 0.06015152\n",
            "iteration: 166 loss: 0.28993663\n",
            "iteration: 167 loss: 0.09027534\n",
            "iteration: 168 loss: 0.07290224\n",
            "iteration: 169 loss: 0.17411149\n",
            "iteration: 170 loss: 0.08349892\n",
            "iteration: 171 loss: 0.15107276\n",
            "iteration: 172 loss: 0.05263904\n",
            "iteration: 173 loss: 0.17000045\n",
            "iteration: 174 loss: 0.34048256\n",
            "iteration: 175 loss: 0.08873222\n",
            "iteration: 176 loss: 0.18599035\n",
            "iteration: 177 loss: 0.07558241\n",
            "iteration: 178 loss: 0.15996154\n",
            "iteration: 179 loss: 0.05851184\n",
            "iteration: 180 loss: 0.15083690\n",
            "iteration: 181 loss: 0.12156888\n",
            "iteration: 182 loss: 0.10450992\n",
            "iteration: 183 loss: 0.07418855\n",
            "iteration: 184 loss: 0.03476943\n",
            "iteration: 185 loss: 0.02702406\n",
            "iteration: 186 loss: 0.07821386\n",
            "iteration: 187 loss: 0.01329463\n",
            "iteration: 188 loss: 0.06965262\n",
            "iteration: 189 loss: 0.01847064\n",
            "iteration: 190 loss: 0.13325237\n",
            "iteration: 191 loss: 0.05369930\n",
            "iteration: 192 loss: 0.09473649\n",
            "iteration: 193 loss: 0.16343024\n",
            "iteration: 194 loss: 0.12694836\n",
            "iteration: 195 loss: 0.03824329\n",
            "iteration: 196 loss: 0.04832888\n",
            "iteration: 197 loss: 0.11071546\n",
            "iteration: 198 loss: 0.27266404\n",
            "iteration: 199 loss: 0.05645705\n",
            "epoch: 100 mean loss training: 0.12905230\n",
            "epoch: 100 mean loss validation: 0.39147219\n"
          ]
        }
      ],
      "source": [
        "#Training the data\n",
        "epochs = 100 #The number of times the dataset will be used to train the model\n",
        "batch_size = 10 #Besaran kumpulan atau pecahan data dari dataset\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_valid = []\n",
        "\n",
        "best_loss_valid = np.inf\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    aggregated_losses_train = []\n",
        "    aggregated_losses_valid = []\n",
        "    i += 1\n",
        "#added random permutation for shuffle data training\n",
        "    idxs_train = np.random.permutation(train_records)\n",
        "    for j in range((train_records//batch_size)+1):\n",
        "        start_train = j*batch_size\n",
        "        end_train = start_train+batch_size\n",
        "        \n",
        "        idxs_batch_train = idxs_train[start_train:end_train] #for shuffle training dataset\n",
        "\n",
        "        #input is replaced with categorical_train_data and numerical_train_data    \n",
        "        train, weights1, weights2, train_embed = model(numerical_train_data[idxs_batch_train], categorical_train_data[idxs_batch_train])\n",
        "                \n",
        "        train_loss = loss_function(train_embed, train_outputs[idxs_batch_train])\n",
        "        aggregated_losses_train.append(train_loss)\n",
        "\n",
        "        print(f'iteration: {j:3} loss: {train_loss.item():10.8f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        mean_loss_train = torch.mean(torch.stack(aggregated_losses_train))\n",
        "        \n",
        "    print(f'epoch: {i:3} mean loss training: {mean_loss_train.item():10.8f}')\n",
        "    mean_losses_train.append(mean_loss_train)\n",
        "#==============================================================================================\n",
        "#validation\n",
        "#==============================================================================================    \n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for k in range((valid_records//batch_size)+1):\n",
        "            start_valid = k*batch_size\n",
        "            end_valid = start_valid+batch_size\n",
        "\n",
        "            #input is replaced with categorical_valid_data and numerical_valid_data\n",
        "            valid, weights1, weights2, valid_embed = model(numerical_valid_data[start_valid:end_valid], categorical_valid_data[start_valid:end_valid])\n",
        " \n",
        "            #loss name is differentiated between train and validation, outputs is changed to valid_outputs\n",
        "            valid_loss = loss_function(valid_embed, valid_outputs[start_valid:end_valid])\n",
        "            aggregated_losses_valid.append(valid_loss)\n",
        "    \n",
        "    mean_loss_valid = torch.mean(torch.stack(aggregated_losses_valid))\n",
        "\n",
        "    print(f'epoch: {i:3} mean loss validation: {mean_loss_valid:.8f}')\n",
        "    \n",
        "#======================================================================\n",
        "#The model is saved when the loss is lowest not at the end of the epoch\n",
        "#======================================================================\n",
        "    if mean_loss_valid.cpu().numpy()[()] < best_loss_valid:\n",
        "        best_loss_valid = mean_loss_valid\n",
        "        torch.save(model.state_dict(),\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/SoftMax.pth\".format(churn_percentage))\n",
        "        best_epoch = i        \n",
        "    \n",
        "    mean_losses_valid.append(mean_loss_valid)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GQBZ0B-ktnI",
        "outputId": "dbdb47ee-8898-4c02-bd8e-b33049f2a438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Load training model\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/SoftMax.pth\".format(churn_percentage)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1GBMJOYktnJ",
        "outputId": "f7b7f178-f207-4d81-e480-6d5f9e976bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.32636237\n"
          ]
        }
      ],
      "source": [
        "#Creating predictions\n",
        "with torch.no_grad():\n",
        "    valid, weights1, weights2, valid_embed = model(numerical_valid_data, categorical_valid_data)\n",
        "    valid_loss = loss_function(valid_embed, valid_outputs)\n",
        "    total_valid_loss = valid_loss\n",
        "print(f'Loss: {total_valid_loss:.8f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGaN4JovktnJ",
        "outputId": "d8e0b265-84c5-4b30-d3cd-ad1d9df4dad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[539  25]\n",
            " [ 24  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       564\n",
            "           1       0.76      0.76      0.76       102\n",
            "\n",
            "    accuracy                           0.93       666\n",
            "   macro avg       0.86      0.86      0.86       666\n",
            "weighted avg       0.93      0.93      0.93       666\n",
            "\n",
            "Accuracy:  0.9264264264264265\n",
            "F1 Score:  0.8587486744432662\n"
          ]
        }
      ],
      "source": [
        "#=============================================================================================\n",
        "#the main result seen is the F1 Score, because\n",
        "#the misleading accuracy metric is used for imbalance data\n",
        "#=============================================================================================\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "valid_val = np.argmax(valid_embed, axis=1)\n",
        "print(confusion_matrix(valid_outputs, valid_val))\n",
        "print(classification_report(valid_outputs, valid_val))\n",
        "print(\"Accuracy: \", accuracy_score(valid_outputs, valid_val))\n",
        "print(\"F1 Score: \", f1_score(valid_outputs, valid_val, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxioLe5IktnK"
      },
      "outputs": [],
      "source": [
        "#function for smoothing the loss plot by using exponential moving average\n",
        "#https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar\n",
        "def smooth(scalars, weight):\n",
        "    last = scalars[0]  # First value in the plot (first timestep)\n",
        "    smoothed = list()\n",
        "    for point in scalars:\n",
        "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
        "        smoothed.append(smoothed_val)                        # Save it\n",
        "        last = smoothed_val                                  # Anchor the last smoothed value\n",
        "\n",
        "    return smoothed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "me_losses_train = []\n",
        "for l in mean_losses_train:\n",
        "  me_losses_train.append(l.detach().numpy())\n",
        "\n",
        "print (me_losses_train)\n",
        "print (mean_losses_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu2io9mR6Oyn",
        "outputId": "9026c2f8-2a70-420e-eac1-51ad53b58ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(0.5190724, dtype=float32), array(0.37627235, dtype=float32), array(0.33681953, dtype=float32), array(0.308613, dtype=float32), array(0.28680784, dtype=float32), array(0.27964643, dtype=float32), array(0.2613758, dtype=float32), array(0.24996875, dtype=float32), array(0.2291999, dtype=float32), array(0.22413215, dtype=float32), array(0.2085532, dtype=float32), array(0.22050571, dtype=float32), array(0.19952196, dtype=float32), array(0.20341179, dtype=float32), array(0.18515445, dtype=float32), array(0.17772266, dtype=float32), array(0.18537876, dtype=float32), array(0.18123122, dtype=float32), array(0.16650434, dtype=float32), array(0.17941003, dtype=float32), array(0.16708155, dtype=float32), array(0.14504996, dtype=float32), array(0.16109324, dtype=float32), array(0.16166736, dtype=float32), array(0.16290417, dtype=float32), array(0.15307324, dtype=float32), array(0.1496938, dtype=float32), array(0.1480154, dtype=float32), array(0.14841573, dtype=float32), array(0.1537029, dtype=float32), array(0.15883487, dtype=float32), array(0.13309556, dtype=float32), array(0.1328233, dtype=float32), array(0.14256462, dtype=float32), array(0.1415374, dtype=float32), array(0.14106633, dtype=float32), array(0.14649335, dtype=float32), array(0.13055284, dtype=float32), array(0.13872223, dtype=float32), array(0.13010056, dtype=float32), array(0.13097708, dtype=float32), array(0.13470627, dtype=float32), array(0.12890552, dtype=float32), array(0.13799846, dtype=float32), array(0.14609486, dtype=float32), array(0.1294938, dtype=float32), array(0.13871332, dtype=float32), array(0.11769392, dtype=float32), array(0.14602955, dtype=float32), array(0.1409466, dtype=float32), array(0.14114074, dtype=float32), array(0.14329687, dtype=float32), array(0.1272227, dtype=float32), array(0.13490275, dtype=float32), array(0.14078237, dtype=float32), array(0.11820878, dtype=float32), array(0.13938461, dtype=float32), array(0.1283956, dtype=float32), array(0.13792321, dtype=float32), array(0.14300591, dtype=float32), array(0.14338316, dtype=float32), array(0.14047137, dtype=float32), array(0.12214663, dtype=float32), array(0.14278501, dtype=float32), array(0.13646966, dtype=float32), array(0.13263077, dtype=float32), array(0.13384971, dtype=float32), array(0.14304638, dtype=float32), array(0.13586959, dtype=float32), array(0.1262713, dtype=float32), array(0.13514967, dtype=float32), array(0.13471742, dtype=float32), array(0.12547033, dtype=float32), array(0.14430323, dtype=float32), array(0.13339783, dtype=float32), array(0.13099982, dtype=float32), array(0.1414273, dtype=float32), array(0.13749625, dtype=float32), array(0.12544122, dtype=float32), array(0.14066458, dtype=float32), array(0.13371955, dtype=float32), array(0.13231294, dtype=float32), array(0.12745932, dtype=float32), array(0.13269953, dtype=float32), array(0.13719164, dtype=float32), array(0.13195412, dtype=float32), array(0.14802359, dtype=float32), array(0.13152935, dtype=float32), array(0.1442928, dtype=float32), array(0.14220981, dtype=float32), array(0.1366626, dtype=float32), array(0.1272495, dtype=float32), array(0.13968098, dtype=float32), array(0.1468809, dtype=float32), array(0.1209875, dtype=float32), array(0.13024771, dtype=float32), array(0.13768423, dtype=float32), array(0.12989205, dtype=float32), array(0.13171153, dtype=float32), array(0.1290523, dtype=float32)]\n",
            "[tensor(0.3628), tensor(0.3347), tensor(0.3271), tensor(0.3180), tensor(0.3092), tensor(0.3389), tensor(0.3162), tensor(0.3425), tensor(0.3512), tensor(0.3225), tensor(0.3536), tensor(0.3218), tensor(0.3478), tensor(0.3619), tensor(0.4014), tensor(0.3574), tensor(0.3633), tensor(0.3453), tensor(0.3803), tensor(0.4021), tensor(0.3723), tensor(0.3424), tensor(0.3434), tensor(0.3680), tensor(0.3694), tensor(0.3579), tensor(0.3431), tensor(0.3477), tensor(0.3741), tensor(0.3495), tensor(0.3650), tensor(0.3596), tensor(0.3600), tensor(0.3724), tensor(0.3761), tensor(0.3812), tensor(0.3517), tensor(0.3616), tensor(0.3930), tensor(0.3660), tensor(0.3659), tensor(0.3748), tensor(0.3749), tensor(0.3591), tensor(0.3921), tensor(0.3618), tensor(0.3806), tensor(0.3943), tensor(0.3567), tensor(0.3822), tensor(0.3668), tensor(0.4061), tensor(0.4037), tensor(0.3686), tensor(0.3683), tensor(0.3539), tensor(0.3674), tensor(0.3774), tensor(0.3664), tensor(0.3995), tensor(0.3663), tensor(0.3619), tensor(0.3876), tensor(0.3681), tensor(0.3586), tensor(0.3723), tensor(0.3620), tensor(0.4047), tensor(0.3721), tensor(0.3710), tensor(0.3851), tensor(0.3532), tensor(0.4197), tensor(0.3666), tensor(0.3500), tensor(0.3701), tensor(0.3739), tensor(0.3837), tensor(0.3909), tensor(0.3575), tensor(0.3768), tensor(0.3665), tensor(0.3707), tensor(0.3533), tensor(0.4009), tensor(0.3698), tensor(0.3625), tensor(0.3757), tensor(0.3664), tensor(0.3789), tensor(0.3605), tensor(0.3633), tensor(0.3859), tensor(0.3726), tensor(0.3680), tensor(0.3492), tensor(0.3698), tensor(0.4005), tensor(0.3949), tensor(0.3915)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4WPhU9UktnK",
        "outputId": "9e3d357e-7f98-4052-b19f-a7f0610a8701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "epochs_to_show = 100\n",
        "smoothing = 0.5\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(me_losses_train[0:epochs_to_show],smoothing), label = 'train')\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(mean_losses_valid[0:epochs_to_show],smoothing), label = 'validation')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('loss graph')\n",
        "plt.show()\n",
        "print(best_epoch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Softmax_1st_Run_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}