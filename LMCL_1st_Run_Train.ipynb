{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DinneRatj/Vector-Embedding-Model/blob/main/LMCL_1st_Run_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meTBh7oChFPJ",
        "outputId": "9bdc092e-fde3-498e-92ec-08506e97205f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f48f9e5b930>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Imporing libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Added so that the random numbers are always the same when the program is run, so the results are always the same\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting to Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k-EbN3UGtd3",
        "outputId": "2258342d-01b2-4d32-f255-1024cacf6f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "fp0MZCw2hFPM",
        "outputId": "33502c82-c01f-4339-e88c-5585da747189"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     state  account length  area code phone number international plan  \\\n",
              "0       KS             128        415     382-4657                 no   \n",
              "1       OH             107        415     371-7191                 no   \n",
              "2       NJ             137        415     358-1921                 no   \n",
              "3       OH              84        408     375-9999                yes   \n",
              "4       OK              75        415     330-6626                yes   \n",
              "...    ...             ...        ...          ...                ...   \n",
              "3328    AZ             192        415     414-4276                 no   \n",
              "3329    WV              68        415     370-3271                 no   \n",
              "3330    RI              28        510     328-8230                 no   \n",
              "3331    CT             184        510     364-6381                yes   \n",
              "3332    TN              74        415     400-4344                 no   \n",
              "\n",
              "     voice mail plan  number vmail messages  total day minutes  \\\n",
              "0                yes                     25              265.1   \n",
              "1                yes                     26              161.6   \n",
              "2                 no                      0              243.4   \n",
              "3                 no                      0              299.4   \n",
              "4                 no                      0              166.7   \n",
              "...              ...                    ...                ...   \n",
              "3328             yes                     36              156.2   \n",
              "3329              no                      0              231.1   \n",
              "3330              no                      0              180.8   \n",
              "3331              no                      0              213.8   \n",
              "3332             yes                     25              234.4   \n",
              "\n",
              "      total day calls  total day charge  total eve minutes  total eve calls  \\\n",
              "0                 110             45.07              197.4               99   \n",
              "1                 123             27.47              195.5              103   \n",
              "2                 114             41.38              121.2              110   \n",
              "3                  71             50.90               61.9               88   \n",
              "4                 113             28.34              148.3              122   \n",
              "...               ...               ...                ...              ...   \n",
              "3328               77             26.55              215.5              126   \n",
              "3329               57             39.29              153.4               55   \n",
              "3330              109             30.74              288.8               58   \n",
              "3331              105             36.35              159.6               84   \n",
              "3332              113             39.85              265.9               82   \n",
              "\n",
              "      total eve charge  total night minutes  total night calls  \\\n",
              "0                16.78                244.7                 91   \n",
              "1                16.62                254.4                103   \n",
              "2                10.30                162.6                104   \n",
              "3                 5.26                196.9                 89   \n",
              "4                12.61                186.9                121   \n",
              "...                ...                  ...                ...   \n",
              "3328             18.32                279.1                 83   \n",
              "3329             13.04                191.3                123   \n",
              "3330             24.55                191.9                 91   \n",
              "3331             13.57                139.2                137   \n",
              "3332             22.60                241.4                 77   \n",
              "\n",
              "      total night charge  total intl minutes  total intl calls  \\\n",
              "0                  11.01                10.0                 3   \n",
              "1                  11.45                13.7                 3   \n",
              "2                   7.32                12.2                 5   \n",
              "3                   8.86                 6.6                 7   \n",
              "4                   8.41                10.1                 3   \n",
              "...                  ...                 ...               ...   \n",
              "3328               12.56                 9.9                 6   \n",
              "3329                8.61                 9.6                 4   \n",
              "3330                8.64                14.1                 6   \n",
              "3331                6.26                 5.0                10   \n",
              "3332               10.86                13.7                 4   \n",
              "\n",
              "      total intl charge  customer service calls  churn  \n",
              "0                  2.70                       1  False  \n",
              "1                  3.70                       1  False  \n",
              "2                  3.29                       0  False  \n",
              "3                  1.78                       2  False  \n",
              "4                  2.73                       3  False  \n",
              "...                 ...                     ...    ...  \n",
              "3328               2.67                       2  False  \n",
              "3329               2.59                       3  False  \n",
              "3330               3.81                       2  False  \n",
              "3331               1.35                       2  False  \n",
              "3332               3.70                       0  False  \n",
              "\n",
              "[3333 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e662704-5b64-41ef-8576-69f233dacd3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>account length</th>\n",
              "      <th>area code</th>\n",
              "      <th>phone number</th>\n",
              "      <th>international plan</th>\n",
              "      <th>voice mail plan</th>\n",
              "      <th>number vmail messages</th>\n",
              "      <th>total day minutes</th>\n",
              "      <th>total day calls</th>\n",
              "      <th>total day charge</th>\n",
              "      <th>total eve minutes</th>\n",
              "      <th>total eve calls</th>\n",
              "      <th>total eve charge</th>\n",
              "      <th>total night minutes</th>\n",
              "      <th>total night calls</th>\n",
              "      <th>total night charge</th>\n",
              "      <th>total intl minutes</th>\n",
              "      <th>total intl calls</th>\n",
              "      <th>total intl charge</th>\n",
              "      <th>customer service calls</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KS</td>\n",
              "      <td>128</td>\n",
              "      <td>415</td>\n",
              "      <td>382-4657</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>265.1</td>\n",
              "      <td>110</td>\n",
              "      <td>45.07</td>\n",
              "      <td>197.4</td>\n",
              "      <td>99</td>\n",
              "      <td>16.78</td>\n",
              "      <td>244.7</td>\n",
              "      <td>91</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OH</td>\n",
              "      <td>107</td>\n",
              "      <td>415</td>\n",
              "      <td>371-7191</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>26</td>\n",
              "      <td>161.6</td>\n",
              "      <td>123</td>\n",
              "      <td>27.47</td>\n",
              "      <td>195.5</td>\n",
              "      <td>103</td>\n",
              "      <td>16.62</td>\n",
              "      <td>254.4</td>\n",
              "      <td>103</td>\n",
              "      <td>11.45</td>\n",
              "      <td>13.7</td>\n",
              "      <td>3</td>\n",
              "      <td>3.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NJ</td>\n",
              "      <td>137</td>\n",
              "      <td>415</td>\n",
              "      <td>358-1921</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>243.4</td>\n",
              "      <td>114</td>\n",
              "      <td>41.38</td>\n",
              "      <td>121.2</td>\n",
              "      <td>110</td>\n",
              "      <td>10.30</td>\n",
              "      <td>162.6</td>\n",
              "      <td>104</td>\n",
              "      <td>7.32</td>\n",
              "      <td>12.2</td>\n",
              "      <td>5</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OH</td>\n",
              "      <td>84</td>\n",
              "      <td>408</td>\n",
              "      <td>375-9999</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>299.4</td>\n",
              "      <td>71</td>\n",
              "      <td>50.90</td>\n",
              "      <td>61.9</td>\n",
              "      <td>88</td>\n",
              "      <td>5.26</td>\n",
              "      <td>196.9</td>\n",
              "      <td>89</td>\n",
              "      <td>8.86</td>\n",
              "      <td>6.6</td>\n",
              "      <td>7</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OK</td>\n",
              "      <td>75</td>\n",
              "      <td>415</td>\n",
              "      <td>330-6626</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>166.7</td>\n",
              "      <td>113</td>\n",
              "      <td>28.34</td>\n",
              "      <td>148.3</td>\n",
              "      <td>122</td>\n",
              "      <td>12.61</td>\n",
              "      <td>186.9</td>\n",
              "      <td>121</td>\n",
              "      <td>8.41</td>\n",
              "      <td>10.1</td>\n",
              "      <td>3</td>\n",
              "      <td>2.73</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>AZ</td>\n",
              "      <td>192</td>\n",
              "      <td>415</td>\n",
              "      <td>414-4276</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>36</td>\n",
              "      <td>156.2</td>\n",
              "      <td>77</td>\n",
              "      <td>26.55</td>\n",
              "      <td>215.5</td>\n",
              "      <td>126</td>\n",
              "      <td>18.32</td>\n",
              "      <td>279.1</td>\n",
              "      <td>83</td>\n",
              "      <td>12.56</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "      <td>2.67</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3329</th>\n",
              "      <td>WV</td>\n",
              "      <td>68</td>\n",
              "      <td>415</td>\n",
              "      <td>370-3271</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>231.1</td>\n",
              "      <td>57</td>\n",
              "      <td>39.29</td>\n",
              "      <td>153.4</td>\n",
              "      <td>55</td>\n",
              "      <td>13.04</td>\n",
              "      <td>191.3</td>\n",
              "      <td>123</td>\n",
              "      <td>8.61</td>\n",
              "      <td>9.6</td>\n",
              "      <td>4</td>\n",
              "      <td>2.59</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>RI</td>\n",
              "      <td>28</td>\n",
              "      <td>510</td>\n",
              "      <td>328-8230</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>180.8</td>\n",
              "      <td>109</td>\n",
              "      <td>30.74</td>\n",
              "      <td>288.8</td>\n",
              "      <td>58</td>\n",
              "      <td>24.55</td>\n",
              "      <td>191.9</td>\n",
              "      <td>91</td>\n",
              "      <td>8.64</td>\n",
              "      <td>14.1</td>\n",
              "      <td>6</td>\n",
              "      <td>3.81</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3331</th>\n",
              "      <td>CT</td>\n",
              "      <td>184</td>\n",
              "      <td>510</td>\n",
              "      <td>364-6381</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>213.8</td>\n",
              "      <td>105</td>\n",
              "      <td>36.35</td>\n",
              "      <td>159.6</td>\n",
              "      <td>84</td>\n",
              "      <td>13.57</td>\n",
              "      <td>139.2</td>\n",
              "      <td>137</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3332</th>\n",
              "      <td>TN</td>\n",
              "      <td>74</td>\n",
              "      <td>415</td>\n",
              "      <td>400-4344</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>234.4</td>\n",
              "      <td>113</td>\n",
              "      <td>39.85</td>\n",
              "      <td>265.9</td>\n",
              "      <td>82</td>\n",
              "      <td>22.60</td>\n",
              "      <td>241.4</td>\n",
              "      <td>77</td>\n",
              "      <td>10.86</td>\n",
              "      <td>13.7</td>\n",
              "      <td>4</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3333 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e662704-5b64-41ef-8576-69f233dacd3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e662704-5b64-41ef-8576-69f233dacd3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e662704-5b64-41ef-8576-69f233dacd3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Reading data\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/bigml_59c28831336c6604c800002a.csv\")\n",
        "pd.options.display.max_columns = None\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuZRNrsYhFPN"
      },
      "outputs": [],
      "source": [
        "#Defining columns\n",
        "numerical_columns = ['number vmail messages', 'total day minutes', 'total day calls',\n",
        "                     'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes',\n",
        "                     'total night calls', 'total night charge', 'total intl minutes', 'total intl calls',\n",
        "                     'total intl charge', 'customer service calls']\n",
        "categorical_columns = ['state', 'international plan', 'voice mail plan','area code']\n",
        "outputs = ['churn']\n",
        "\n",
        "#Input >14 Numerical coloums and 4 Categorical coloums\n",
        "#Output > 1 Categorical coloum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "churn_data = dataset[dataset['churn'] == 'True']\n",
        "notchurn_data = dataset[dataset['churn'] == 'False']"
      ],
      "metadata": {
        "id": "SyY-utDEHaey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34Hmwl3ChFPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76427eea-0121-4250-a7d4-f51ee6ff6bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2349,  1.5668,  0.4766,  ..., -0.6012, -0.0857, -0.4279],\n",
            "        [ 1.3079, -0.3337,  1.1245,  ..., -0.6012,  1.2412, -0.4279],\n",
            "        [-0.5918,  1.1683,  0.6760,  ...,  0.2115,  0.6972, -1.1882],\n",
            "        ...,\n",
            "        [-0.5918,  0.0188,  0.4268,  ...,  0.6179,  1.3871,  0.3324],\n",
            "        [-0.5918,  0.6248,  0.2275,  ...,  2.2434, -1.8770,  0.3324],\n",
            "        [ 1.2349,  1.0030,  0.6261,  ..., -0.1948,  1.2412, -1.1882]])\n",
            "torch.float32\n",
            "torch.Size([3333, 14])\n",
            "___________________________________________________________________________\n",
            "tensor([[16,  0,  1,  1],\n",
            "        [35,  0,  1,  1],\n",
            "        [31,  0,  0,  1],\n",
            "        ...,\n",
            "        [39,  0,  0,  2],\n",
            "        [ 6,  1,  0,  2],\n",
            "        [42,  0,  1,  1]])\n",
            "torch.int64\n",
            "torch.Size([3333, 4])\n",
            "___________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Processing columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Numerical\n",
        "#Convert our numerical columns to tensors\n",
        "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
        "\n",
        "#Fixed how to use scaler\n",
        "numerical_data = scaler.fit_transform(numerical_data)\n",
        "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
        "\n",
        "#Categorical\n",
        "#Convert the types for categorical columns to category\n",
        "for category in categorical_columns:\n",
        "    dataset[category] = dataset[category].astype('category')\n",
        "\n",
        "#Convert data in the four categorical columns into numpy arrays and then stack all the columns horizontally \n",
        "st = dataset['state'].cat.codes.values\n",
        "ip = dataset['international plan'].cat.codes.values\n",
        "vm = dataset['voice mail plan'].cat.codes.values\n",
        "ac = dataset['area code'].cat.codes.values\n",
        "\n",
        "categorical_data = np.stack([st, ip, vm, ac], 1)\n",
        "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
        "\n",
        "#Outputs\n",
        "#Convert the output numpy array into a tensor object\n",
        "dataset[outputs] = dataset[outputs].astype(int)\n",
        "outputs = torch.tensor(dataset[outputs].values).flatten()\n",
        "outputs = outputs.long()\n",
        "\n",
        "#Print Outputs\n",
        "print(numerical_data)\n",
        "print(numerical_data.dtype)\n",
        "print(numerical_data.shape)\n",
        "print('_' * 75)\n",
        "\n",
        "print(categorical_data)\n",
        "print(categorical_data.dtype)\n",
        "print(categorical_data.shape)\n",
        "print('_' * 75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDn5akU9hFPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c8b42c-a969-430c-b72b-6ebe3d5e9fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999\n",
            "666\n",
            "666\n"
          ]
        }
      ],
      "source": [
        "#Dividing dataset into Training, Valid and Test\n",
        "total_records = 3333\n",
        "\n",
        "train_records = int(total_records * .6)\n",
        "valid_records = int(total_records * .2)\n",
        "test_records = int(total_records * .2)\n",
        "\n",
        "numerical_train_data = numerical_data[:train_records]\n",
        "numerical_valid_data = numerical_data[train_records:train_records+valid_records]\n",
        "numerical_test_data = numerical_data[train_records+valid_records:total_records]\n",
        "\n",
        "categorical_train_data = categorical_data[:train_records]\n",
        "categorical_valid_data = categorical_data[train_records:train_records+valid_records]\n",
        "categorical_test_data = categorical_data[train_records+valid_records:total_records]\n",
        "\n",
        "train_outputs = outputs[:train_records]\n",
        "valid_outputs = outputs[train_records:train_records+valid_records]\n",
        "test_outputs = outputs[train_records+valid_records:total_records]\n",
        "\n",
        "#Print divide dataset\n",
        "print(train_records)\n",
        "print(valid_records)\n",
        "print(test_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyVTP22PhFPQ"
      },
      "outputs": [],
      "source": [
        "#Define a class named Model, which will be used to train the model\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "#Creating the Neural Network\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(14, 100) #Numerical\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm1d(100)\n",
        "\n",
        "# =============================================================================================\n",
        "#weights 1 and 2 don't exist in this model because they already exist\n",
        "#as the self.centers attribute in the LMC_Loss model in model_utils.py.\n",
        "# =============================================================================================\n",
        "        \n",
        "        #Categorical\n",
        "        self.layer1_1 = nn.Embedding(51, 5) #51 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_1 = nn.BatchNorm1d(5)\n",
        "        self.layer1_2 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_2 = nn.BatchNorm1d(5)\n",
        "        self.layer1_3 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_3 = nn.BatchNorm1d(5)\n",
        "        self.layer1_4 = nn.Embedding(3, 5) #3 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_4 = nn.BatchNorm1d(5)\n",
        "        \n",
        "        self.layer2 = nn.Linear(120, 120)\n",
        "        self.bn2 = nn.BatchNorm1d(120)\n",
        "        \n",
        "    def forward(self, x_numerical, x_categorical):\n",
        "        x1 = self.layer1(x_numerical)\n",
        "        x1 = self.relu(x1)\n",
        "        x1 = self.bn1(x1)\n",
        "               \n",
        "        #Decoder\n",
        "        x1_embedding = self.layer1_1(x_categorical[:,0])\n",
        "        x1_embedding = self.relu(x1_embedding)\n",
        "        x1_embedding = self.bn1_1(x1_embedding)\n",
        "        \n",
        "        x2_embedding = self.layer1_2(x_categorical[:,1])\n",
        "        x2_embedding = self.relu(x2_embedding)\n",
        "        x2_embedding = self.bn1_2(x2_embedding)\n",
        "        \n",
        "        x3_embedding = self.layer1_3(x_categorical[:,2])\n",
        "        x3_embedding = self.relu(x3_embedding)\n",
        "        x3_embedding = self.bn1_3(x3_embedding)\n",
        "        \n",
        "        x4_embedding = self.layer1_4(x_categorical[:,3])\n",
        "        x4_embedding = self.relu(x4_embedding)\n",
        "        x4_embedding = self.bn1_4(x4_embedding)\n",
        "        \n",
        "        x_embedding = torch.cat([x1_embedding,x2_embedding,x3_embedding,x4_embedding], 1)\n",
        "                \n",
        "        x1 = torch.cat([x1, x_embedding], 1)\n",
        "        \n",
        "        #Decoder\n",
        "        x2 = self.layer2(x1)        \n",
        "        emb = self.relu(x2)\n",
        "        x2 = self.bn2(emb)\n",
        "        \n",
        "        return emb, x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbROfHsjhFPR"
      },
      "outputs": [],
      "source": [
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/model_lmcl.py /content"
      ],
      "metadata": {
        "id": "2BQd7UZrW5lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG381vELhFPS"
      },
      "outputs": [],
      "source": [
        "# =============================================================================================\n",
        "#loss function is replaced by using Large Margin Cosine (LMC) Loss implementation\n",
        "#in the following YirongMao repo: https://github.com/YirongMao/softmax_variants\n",
        "#\n",
        "#LMC Loss is a loss function that in theory will be implemented from the start.\n",
        "#Paper LMC Loss: https://arxiv.org/abs/1801.09414\n",
        "#\n",
        "#Cosine Embedding Loss which was used was not actually the same mathematical as LMC Loss,\n",
        "#therefore the implementation was replaced with the implementation of YirongMao \n",
        "# =============================================================================================\n",
        "# =============================================================================================\n",
        "#Added a new loss with the LMCLoss library from YirongMao. Cross entropy loss persists,\n",
        "#because YirongMao's LMCLoss implementation still utilizes Pytorch's original Cross Entropy Loss.\n",
        "#Check the implementation method at: https://github.com/YirongMao/softmax_variants/blob/master/train_mnist_LMCL.py\n",
        "#\n",
        "#margin (parameter m) uses a value of 0.35,\n",
        "#according to the results of research from the original LMCLoss paper which found that the optimal value was 0.35 or 4.\n",
        "# =============================================================================================\n",
        "import model_lmcl\n",
        "\n",
        "lmcl_loss = model_lmcl.LMCL_loss(num_classes=2, feat_dim=120, m=0.35)\n",
        "optimizer_lmcl = torch.optim.Adam(lmcl_loss.parameters(), lr=0.001)\n",
        "# =============================================================================================\n",
        "#For cross entropy loss, weight parameter was added,\n",
        "#because it turned out to be imbalance data (not churn data is much more than churn, with a ratio of around 85:15).\n",
        "#This imbalance data causes the prediction results for the churn class to be not good.\n",
        "#The solution is to use the weight parameter in the cross entropy loss,\n",
        "#so the penalty for being wrong in the churn class is much greater than for being wrong in the not churn class.\n",
        "#Penalty is increased by the inverse ratio of the ratio of the amount of data (15:85)\n",
        "#===============================================================================================\n",
        "#Defining churn:loyal weight ratio. churn_percentage=0.7 means churn:loyal weight ratio of 7:3.\n",
        "#===============================================================================================\n",
        "churn_percentage = 0.7\n",
        "\n",
        "#Defining loss function\n",
        "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1-churn_percentage, churn_percentage]))\n",
        "\n",
        "#Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Added learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "i9_GI4s7hFPS",
        "outputId": "0932d415-ae1c-4f21-a253-1c849c91dbe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration:  50 loss: 0.04543713\n",
            "iteration:  51 loss: 1.06574643\n",
            "iteration:  52 loss: 0.12058458\n",
            "iteration:  53 loss: 0.82898432\n",
            "iteration:  54 loss: 0.10407743\n",
            "iteration:  55 loss: 0.13535579\n",
            "iteration:  56 loss: 0.61765909\n",
            "iteration:  57 loss: 1.14344633\n",
            "iteration:  58 loss: 0.05880702\n",
            "iteration:  59 loss: 0.25931439\n",
            "iteration:  60 loss: 0.49173939\n",
            "iteration:  61 loss: 0.14766651\n",
            "iteration:  62 loss: 0.24228287\n",
            "iteration:  63 loss: 1.48857081\n",
            "iteration:  64 loss: 0.23354550\n",
            "iteration:  65 loss: 0.69478321\n",
            "iteration:  66 loss: 0.14913011\n",
            "iteration:  67 loss: 1.01760137\n",
            "iteration:  68 loss: 0.22796923\n",
            "iteration:  69 loss: 1.43806672\n",
            "iteration:  70 loss: 1.44672883\n",
            "iteration:  71 loss: 0.44512179\n",
            "iteration:  72 loss: 1.37998033\n",
            "iteration:  73 loss: 1.02325737\n",
            "iteration:  74 loss: 0.14885002\n",
            "iteration:  75 loss: 2.78266573\n",
            "iteration:  76 loss: 0.90702122\n",
            "iteration:  77 loss: 0.47569045\n",
            "iteration:  78 loss: 0.09320880\n",
            "iteration:  79 loss: 0.54189950\n",
            "iteration:  80 loss: 0.41932023\n",
            "iteration:  81 loss: 0.07757411\n",
            "iteration:  82 loss: 0.71644241\n",
            "iteration:  83 loss: 0.17587984\n",
            "iteration:  84 loss: 0.34775230\n",
            "iteration:  85 loss: 0.38015309\n",
            "iteration:  86 loss: 0.62355423\n",
            "iteration:  87 loss: 1.12137330\n",
            "iteration:  88 loss: 0.17765245\n",
            "iteration:  89 loss: 0.50054425\n",
            "iteration:  90 loss: 0.67778301\n",
            "iteration:  91 loss: 0.39927793\n",
            "iteration:  92 loss: 0.45261785\n",
            "iteration:  93 loss: 0.06286371\n",
            "iteration:  94 loss: 0.40730360\n",
            "iteration:  95 loss: 1.46320772\n",
            "iteration:  96 loss: 2.85141492\n",
            "iteration:  97 loss: 0.42134207\n",
            "iteration:  98 loss: 0.37740573\n",
            "iteration:  99 loss: 0.12690416\n",
            "iteration: 100 loss: 1.10213268\n",
            "iteration: 101 loss: 0.19581904\n",
            "iteration: 102 loss: 0.65095311\n",
            "iteration: 103 loss: 0.02714406\n",
            "iteration: 104 loss: 0.66141373\n",
            "iteration: 105 loss: 0.07232233\n",
            "iteration: 106 loss: 0.11052272\n",
            "iteration: 107 loss: 0.55365318\n",
            "iteration: 108 loss: 0.05882796\n",
            "iteration: 109 loss: 0.11905354\n",
            "iteration: 110 loss: 0.27471590\n",
            "iteration: 111 loss: 0.49099809\n",
            "iteration: 112 loss: 0.37592894\n",
            "iteration: 113 loss: 0.31082863\n",
            "iteration: 114 loss: 0.33144850\n",
            "iteration: 115 loss: 0.68623078\n",
            "iteration: 116 loss: 0.12792817\n",
            "iteration: 117 loss: 0.39767307\n",
            "iteration: 118 loss: 0.79131120\n",
            "iteration: 119 loss: 0.54248756\n",
            "iteration: 120 loss: 0.08823819\n",
            "iteration: 121 loss: 1.08556473\n",
            "iteration: 122 loss: 0.56044221\n",
            "iteration: 123 loss: 0.26555926\n",
            "iteration: 124 loss: 0.31730282\n",
            "iteration: 125 loss: 0.77306110\n",
            "iteration: 126 loss: 0.35496575\n",
            "iteration: 127 loss: 0.17470729\n",
            "iteration: 128 loss: 0.17382494\n",
            "iteration: 129 loss: 0.15481961\n",
            "iteration: 130 loss: 0.27764487\n",
            "iteration: 131 loss: 0.23235714\n",
            "iteration: 132 loss: 0.40413201\n",
            "iteration: 133 loss: 0.07350510\n",
            "iteration: 134 loss: 1.13634109\n",
            "iteration: 135 loss: 0.35295969\n",
            "iteration: 136 loss: 0.06890623\n",
            "iteration: 137 loss: 0.24542929\n",
            "iteration: 138 loss: 0.24375691\n",
            "iteration: 139 loss: 0.25341821\n",
            "iteration: 140 loss: 0.08972938\n",
            "iteration: 141 loss: 0.14705452\n",
            "iteration: 142 loss: 0.48359597\n",
            "iteration: 143 loss: 0.42062551\n",
            "iteration: 144 loss: 0.27806377\n",
            "iteration: 145 loss: 0.52812725\n",
            "iteration: 146 loss: 0.14056082\n",
            "iteration: 147 loss: 1.34988093\n",
            "iteration: 148 loss: 0.43004432\n",
            "iteration: 149 loss: 0.98194933\n",
            "iteration: 150 loss: 0.23450135\n",
            "iteration: 151 loss: 2.40800071\n",
            "iteration: 152 loss: 1.81385553\n",
            "iteration: 153 loss: 0.09549507\n",
            "iteration: 154 loss: 0.95776790\n",
            "iteration: 155 loss: 2.39849257\n",
            "iteration: 156 loss: 0.37557858\n",
            "iteration: 157 loss: 0.53270656\n",
            "iteration: 158 loss: 0.04377031\n",
            "iteration: 159 loss: 0.08557948\n",
            "iteration: 160 loss: 0.15039377\n",
            "iteration: 161 loss: 0.10715363\n",
            "iteration: 162 loss: 0.12756354\n",
            "iteration: 163 loss: 0.61017323\n",
            "iteration: 164 loss: 0.08444636\n",
            "iteration: 165 loss: 0.52545041\n",
            "iteration: 166 loss: 0.14610167\n",
            "iteration: 167 loss: 0.55065072\n",
            "iteration: 168 loss: 0.43460482\n",
            "iteration: 169 loss: 0.64204061\n",
            "iteration: 170 loss: 0.52038658\n",
            "iteration: 171 loss: 1.84521520\n",
            "iteration: 172 loss: 1.59947634\n",
            "iteration: 173 loss: 0.12672922\n",
            "iteration: 174 loss: 0.37789375\n",
            "iteration: 175 loss: 0.05241085\n",
            "iteration: 176 loss: 0.58817798\n",
            "iteration: 177 loss: 1.20274162\n",
            "iteration: 178 loss: 0.25633377\n",
            "iteration: 179 loss: 1.02083170\n",
            "iteration: 180 loss: 0.51383001\n",
            "iteration: 181 loss: 0.53281021\n",
            "iteration: 182 loss: 0.38178363\n",
            "iteration: 183 loss: 0.48800519\n",
            "iteration: 184 loss: 0.33680466\n",
            "iteration: 185 loss: 0.87551427\n",
            "iteration: 186 loss: 0.47666466\n",
            "iteration: 187 loss: 0.24055704\n",
            "iteration: 188 loss: 0.25495109\n",
            "iteration: 189 loss: 1.18010747\n",
            "iteration: 190 loss: 0.48328978\n",
            "iteration: 191 loss: 0.88768458\n",
            "iteration: 192 loss: 0.11848044\n",
            "iteration: 193 loss: 0.42240712\n",
            "iteration: 194 loss: 0.23709576\n",
            "iteration: 195 loss: 1.42511642\n",
            "iteration: 196 loss: 0.27350178\n",
            "iteration: 197 loss: 0.42805848\n",
            "iteration: 198 loss: 0.02841588\n",
            "iteration: 199 loss: 0.86002463\n",
            "epoch:  76 mean loss training: 0.58810502\n",
            "epoch:  76 mean loss validation: 0.84777570\n",
            "iteration:   0 loss: 0.82299352\n",
            "iteration:   1 loss: 0.23163478\n",
            "iteration:   2 loss: 0.44580442\n",
            "iteration:   3 loss: 0.42993355\n",
            "iteration:   4 loss: 1.89145184\n",
            "iteration:   5 loss: 0.11825275\n",
            "iteration:   6 loss: 0.32759467\n",
            "iteration:   7 loss: 0.98012400\n",
            "iteration:   8 loss: 0.59286278\n",
            "iteration:   9 loss: 0.73810732\n",
            "iteration:  10 loss: 0.96232063\n",
            "iteration:  11 loss: 0.10689356\n",
            "iteration:  12 loss: 0.20520151\n",
            "iteration:  13 loss: 0.44623566\n",
            "iteration:  14 loss: 0.42794961\n",
            "iteration:  15 loss: 0.34592497\n",
            "iteration:  16 loss: 0.29392985\n",
            "iteration:  17 loss: 0.25817272\n",
            "iteration:  18 loss: 0.50364274\n",
            "iteration:  19 loss: 0.20679811\n",
            "iteration:  20 loss: 0.16650908\n",
            "iteration:  21 loss: 0.26322815\n",
            "iteration:  22 loss: 0.83616507\n",
            "iteration:  23 loss: 0.83886117\n",
            "iteration:  24 loss: 0.15495004\n",
            "iteration:  25 loss: 0.22986048\n",
            "iteration:  26 loss: 0.04360548\n",
            "iteration:  27 loss: 0.40988734\n",
            "iteration:  28 loss: 0.48268861\n",
            "iteration:  29 loss: 0.16543445\n",
            "iteration:  30 loss: 0.25466633\n",
            "iteration:  31 loss: 0.27596241\n",
            "iteration:  32 loss: 0.41607159\n",
            "iteration:  33 loss: 0.35849041\n",
            "iteration:  34 loss: 0.05791418\n",
            "iteration:  35 loss: 0.51739120\n",
            "iteration:  36 loss: 0.11843117\n",
            "iteration:  37 loss: 0.45635208\n",
            "iteration:  38 loss: 0.51985377\n",
            "iteration:  39 loss: 0.56183755\n",
            "iteration:  40 loss: 0.33045375\n",
            "iteration:  41 loss: 0.30763900\n",
            "iteration:  42 loss: 0.71716046\n",
            "iteration:  43 loss: 0.30170295\n",
            "iteration:  44 loss: 0.06174608\n",
            "iteration:  45 loss: 0.20553656\n",
            "iteration:  46 loss: 1.59425867\n",
            "iteration:  47 loss: 0.06663816\n",
            "iteration:  48 loss: 0.30501840\n",
            "iteration:  49 loss: 0.36593050\n",
            "iteration:  50 loss: 0.07826253\n",
            "iteration:  51 loss: 0.90007883\n",
            "iteration:  52 loss: 0.53117174\n",
            "iteration:  53 loss: 0.30581713\n",
            "iteration:  54 loss: 0.45035905\n",
            "iteration:  55 loss: 0.11173736\n",
            "iteration:  56 loss: 0.54942787\n",
            "iteration:  57 loss: 0.92032892\n",
            "iteration:  58 loss: 0.56082964\n",
            "iteration:  59 loss: 0.28686449\n",
            "iteration:  60 loss: 0.07595406\n",
            "iteration:  61 loss: 0.61529112\n",
            "iteration:  62 loss: 0.68349701\n",
            "iteration:  63 loss: 0.69973916\n",
            "iteration:  64 loss: 0.03981858\n",
            "iteration:  65 loss: 1.52534842\n",
            "iteration:  66 loss: 0.72504091\n",
            "iteration:  67 loss: 0.24826644\n",
            "iteration:  68 loss: 0.08091243\n",
            "iteration:  69 loss: 0.38116211\n",
            "iteration:  70 loss: 0.25264612\n",
            "iteration:  71 loss: 1.60353065\n",
            "iteration:  72 loss: 0.64504671\n",
            "iteration:  73 loss: 0.16094306\n",
            "iteration:  74 loss: 0.75635082\n",
            "iteration:  75 loss: 1.83866799\n",
            "iteration:  76 loss: 0.36736280\n",
            "iteration:  77 loss: 0.53280181\n",
            "iteration:  78 loss: 0.93953156\n",
            "iteration:  79 loss: 0.04369284\n",
            "iteration:  80 loss: 0.24234973\n",
            "iteration:  81 loss: 0.10161785\n",
            "iteration:  82 loss: 1.02276719\n",
            "iteration:  83 loss: 1.13289034\n",
            "iteration:  84 loss: 0.18063211\n",
            "iteration:  85 loss: 0.23571390\n",
            "iteration:  86 loss: 0.49868655\n",
            "iteration:  87 loss: 1.20742309\n",
            "iteration:  88 loss: 2.44960332\n",
            "iteration:  89 loss: 0.15653798\n",
            "iteration:  90 loss: 0.07930044\n",
            "iteration:  91 loss: 0.95820409\n",
            "iteration:  92 loss: 0.71138608\n",
            "iteration:  93 loss: 0.47012219\n",
            "iteration:  94 loss: 1.09757197\n",
            "iteration:  95 loss: 0.31690410\n",
            "iteration:  96 loss: 0.40697429\n",
            "iteration:  97 loss: 2.42374372\n",
            "iteration:  98 loss: 0.54999787\n",
            "iteration:  99 loss: 0.22143465\n",
            "iteration: 100 loss: 0.20450497\n",
            "iteration: 101 loss: 0.24640223\n",
            "iteration: 102 loss: 0.05623043\n",
            "iteration: 103 loss: 0.21452703\n",
            "iteration: 104 loss: 1.65732992\n",
            "iteration: 105 loss: 0.97117454\n",
            "iteration: 106 loss: 0.18851900\n",
            "iteration: 107 loss: 0.37122074\n",
            "iteration: 108 loss: 0.04831862\n",
            "iteration: 109 loss: 0.54039484\n",
            "iteration: 110 loss: 2.93453264\n",
            "iteration: 111 loss: 1.15275431\n",
            "iteration: 112 loss: 0.36744636\n",
            "iteration: 113 loss: 0.04295795\n",
            "iteration: 114 loss: 0.38575438\n",
            "iteration: 115 loss: 2.89116836\n",
            "iteration: 116 loss: 0.87134689\n",
            "iteration: 117 loss: 0.19307634\n",
            "iteration: 118 loss: 0.41453657\n",
            "iteration: 119 loss: 0.62859124\n",
            "iteration: 120 loss: 0.06187189\n",
            "iteration: 121 loss: 0.05200129\n",
            "iteration: 122 loss: 1.22565377\n",
            "iteration: 123 loss: 3.33701897\n",
            "iteration: 124 loss: 0.13390358\n",
            "iteration: 125 loss: 0.71562016\n",
            "iteration: 126 loss: 0.43748814\n",
            "iteration: 127 loss: 0.71104509\n",
            "iteration: 128 loss: 0.77580315\n",
            "iteration: 129 loss: 1.47426677\n",
            "iteration: 130 loss: 0.06607020\n",
            "iteration: 131 loss: 0.66391897\n",
            "iteration: 132 loss: 0.07045385\n",
            "iteration: 133 loss: 0.06756204\n",
            "iteration: 134 loss: 0.29779938\n",
            "iteration: 135 loss: 0.53696185\n",
            "iteration: 136 loss: 1.85442984\n",
            "iteration: 137 loss: 0.87852997\n",
            "iteration: 138 loss: 0.43881658\n",
            "iteration: 139 loss: 0.41628116\n",
            "iteration: 140 loss: 1.57858622\n",
            "iteration: 141 loss: 0.38178515\n",
            "iteration: 142 loss: 0.47315887\n",
            "iteration: 143 loss: 0.50359225\n",
            "iteration: 144 loss: 0.71412474\n",
            "iteration: 145 loss: 0.62731838\n",
            "iteration: 146 loss: 1.34821463\n",
            "iteration: 147 loss: 1.00913048\n",
            "iteration: 148 loss: 0.04721612\n",
            "iteration: 149 loss: 0.43165335\n",
            "iteration: 150 loss: 0.45825851\n",
            "iteration: 151 loss: 0.09261725\n",
            "iteration: 152 loss: 1.77943981\n",
            "iteration: 153 loss: 0.25154409\n",
            "iteration: 154 loss: 0.03328396\n",
            "iteration: 155 loss: 0.14515163\n",
            "iteration: 156 loss: 0.34877768\n",
            "iteration: 157 loss: 0.79202205\n",
            "iteration: 158 loss: 2.70827794\n",
            "iteration: 159 loss: 0.44611549\n",
            "iteration: 160 loss: 0.33046743\n",
            "iteration: 161 loss: 0.90478164\n",
            "iteration: 162 loss: 0.39546815\n",
            "iteration: 163 loss: 0.23437031\n",
            "iteration: 164 loss: 0.22630540\n",
            "iteration: 165 loss: 0.17097975\n",
            "iteration: 166 loss: 0.84926391\n",
            "iteration: 167 loss: 0.18601792\n",
            "iteration: 168 loss: 1.39108062\n",
            "iteration: 169 loss: 0.60569060\n",
            "iteration: 170 loss: 0.53400177\n",
            "iteration: 171 loss: 1.23030877\n",
            "iteration: 172 loss: 2.02219200\n",
            "iteration: 173 loss: 0.48731807\n",
            "iteration: 174 loss: 0.77066803\n",
            "iteration: 175 loss: 0.35122788\n",
            "iteration: 176 loss: 0.05267996\n",
            "iteration: 177 loss: 0.77086860\n",
            "iteration: 178 loss: 0.37436134\n",
            "iteration: 179 loss: 1.03866994\n",
            "iteration: 180 loss: 1.38158154\n",
            "iteration: 181 loss: 1.17564833\n",
            "iteration: 182 loss: 0.61525106\n",
            "iteration: 183 loss: 0.10401340\n",
            "iteration: 184 loss: 0.12762117\n",
            "iteration: 185 loss: 0.82063681\n",
            "iteration: 186 loss: 0.09369989\n",
            "iteration: 187 loss: 0.11310087\n",
            "iteration: 188 loss: 1.23779976\n",
            "iteration: 189 loss: 0.74718630\n",
            "iteration: 190 loss: 0.38558167\n",
            "iteration: 191 loss: 0.42797205\n",
            "iteration: 192 loss: 0.04772262\n",
            "iteration: 193 loss: 0.19392875\n",
            "iteration: 194 loss: 0.15744822\n",
            "iteration: 195 loss: 0.37164661\n",
            "iteration: 196 loss: 0.72712016\n",
            "iteration: 197 loss: 0.02440082\n",
            "iteration: 198 loss: 0.07395350\n",
            "iteration: 199 loss: 0.34189007\n",
            "epoch:  77 mean loss training: 0.58977950\n",
            "epoch:  77 mean loss validation: 0.86146206\n",
            "iteration:   0 loss: 0.40443766\n",
            "iteration:   1 loss: 0.39094794\n",
            "iteration:   2 loss: 0.16686215\n",
            "iteration:   3 loss: 1.79992902\n",
            "iteration:   4 loss: 0.49074885\n",
            "iteration:   5 loss: 0.10108609\n",
            "iteration:   6 loss: 2.07067275\n",
            "iteration:   7 loss: 0.31418872\n",
            "iteration:   8 loss: 0.51942104\n",
            "iteration:   9 loss: 0.03264495\n",
            "iteration:  10 loss: 0.73622680\n",
            "iteration:  11 loss: 0.25940201\n",
            "iteration:  12 loss: 0.20058715\n",
            "iteration:  13 loss: 0.36619273\n",
            "iteration:  14 loss: 0.19071521\n",
            "iteration:  15 loss: 1.68692303\n",
            "iteration:  16 loss: 1.21568954\n",
            "iteration:  17 loss: 0.27705249\n",
            "iteration:  18 loss: 0.55431503\n",
            "iteration:  19 loss: 0.21106254\n",
            "iteration:  20 loss: 0.12666939\n",
            "iteration:  21 loss: 0.16436246\n",
            "iteration:  22 loss: 0.36868218\n",
            "iteration:  23 loss: 0.27010223\n",
            "iteration:  24 loss: 0.52757990\n",
            "iteration:  25 loss: 0.06923901\n",
            "iteration:  26 loss: 0.57298410\n",
            "iteration:  27 loss: 0.16086325\n",
            "iteration:  28 loss: 0.12124296\n",
            "iteration:  29 loss: 0.31073135\n",
            "iteration:  30 loss: 0.21026206\n",
            "iteration:  31 loss: 0.77447414\n",
            "iteration:  32 loss: 0.51158524\n",
            "iteration:  33 loss: 0.14987220\n",
            "iteration:  34 loss: 0.32441500\n",
            "iteration:  35 loss: 2.50362587\n",
            "iteration:  36 loss: 0.11718335\n",
            "iteration:  37 loss: 0.53709865\n",
            "iteration:  38 loss: 2.07587218\n",
            "iteration:  39 loss: 0.20402777\n",
            "iteration:  40 loss: 0.12848388\n",
            "iteration:  41 loss: 0.76081425\n",
            "iteration:  42 loss: 0.67016029\n",
            "iteration:  43 loss: 0.15262432\n",
            "iteration:  44 loss: 1.54130399\n",
            "iteration:  45 loss: 0.46809280\n",
            "iteration:  46 loss: 0.10076676\n",
            "iteration:  47 loss: 0.24218738\n",
            "iteration:  48 loss: 0.21145847\n",
            "iteration:  49 loss: 0.16697741\n",
            "iteration:  50 loss: 0.12814261\n",
            "iteration:  51 loss: 0.07289971\n",
            "iteration:  52 loss: 0.41587275\n",
            "iteration:  53 loss: 1.27967703\n",
            "iteration:  54 loss: 0.12474282\n",
            "iteration:  55 loss: 1.40936303\n",
            "iteration:  56 loss: 1.68194926\n",
            "iteration:  57 loss: 0.04753990\n",
            "iteration:  58 loss: 0.33286625\n",
            "iteration:  59 loss: 1.30494702\n",
            "iteration:  60 loss: 0.55698699\n",
            "iteration:  61 loss: 0.12915169\n",
            "iteration:  62 loss: 0.90248448\n",
            "iteration:  63 loss: 0.47291830\n",
            "iteration:  64 loss: 2.54525208\n",
            "iteration:  65 loss: 0.22368522\n",
            "iteration:  66 loss: 2.45055199\n",
            "iteration:  67 loss: 1.23804021\n",
            "iteration:  68 loss: 0.94243991\n",
            "iteration:  69 loss: 0.24825948\n",
            "iteration:  70 loss: 0.34070024\n",
            "iteration:  71 loss: 0.66919100\n",
            "iteration:  72 loss: 1.62970638\n",
            "iteration:  73 loss: 0.24021764\n",
            "iteration:  74 loss: 0.91272408\n",
            "iteration:  75 loss: 0.25139919\n",
            "iteration:  76 loss: 0.02139969\n",
            "iteration:  77 loss: 0.86015737\n",
            "iteration:  78 loss: 0.43467993\n",
            "iteration:  79 loss: 1.59042728\n",
            "iteration:  80 loss: 1.32110012\n",
            "iteration:  81 loss: 0.06510828\n",
            "iteration:  82 loss: 0.21902181\n",
            "iteration:  83 loss: 0.04165559\n",
            "iteration:  84 loss: 0.14614247\n",
            "iteration:  85 loss: 0.74086314\n",
            "iteration:  86 loss: 0.11535838\n",
            "iteration:  87 loss: 0.03659189\n",
            "iteration:  88 loss: 0.40264848\n",
            "iteration:  89 loss: 0.34369656\n",
            "iteration:  90 loss: 0.85596752\n",
            "iteration:  91 loss: 0.30061164\n",
            "iteration:  92 loss: 0.09109227\n",
            "iteration:  93 loss: 0.22628441\n",
            "iteration:  94 loss: 0.25981310\n",
            "iteration:  95 loss: 0.42208201\n",
            "iteration:  96 loss: 0.50863343\n",
            "iteration:  97 loss: 0.04944834\n",
            "iteration:  98 loss: 0.41346470\n",
            "iteration:  99 loss: 0.10547068\n",
            "iteration: 100 loss: 0.35852644\n",
            "iteration: 101 loss: 0.94413781\n",
            "iteration: 102 loss: 2.61881566\n",
            "iteration: 103 loss: 1.08626115\n",
            "iteration: 104 loss: 0.93539685\n",
            "iteration: 105 loss: 0.03372880\n",
            "iteration: 106 loss: 1.89259505\n",
            "iteration: 107 loss: 0.07864378\n",
            "iteration: 108 loss: 1.09695673\n",
            "iteration: 109 loss: 0.16132496\n",
            "iteration: 110 loss: 2.42182732\n",
            "iteration: 111 loss: 0.22859958\n",
            "iteration: 112 loss: 0.42886984\n",
            "iteration: 113 loss: 0.43726289\n",
            "iteration: 114 loss: 0.69563842\n",
            "iteration: 115 loss: 0.53286922\n",
            "iteration: 116 loss: 0.78083760\n",
            "iteration: 117 loss: 0.99929917\n",
            "iteration: 118 loss: 0.45047083\n",
            "iteration: 119 loss: 0.74750030\n",
            "iteration: 120 loss: 0.33769795\n",
            "iteration: 121 loss: 0.15624708\n",
            "iteration: 122 loss: 0.08442850\n",
            "iteration: 123 loss: 0.66810250\n",
            "iteration: 124 loss: 0.35515738\n",
            "iteration: 125 loss: 0.81316096\n",
            "iteration: 126 loss: 0.17909680\n",
            "iteration: 127 loss: 0.39613390\n",
            "iteration: 128 loss: 0.54270053\n",
            "iteration: 129 loss: 0.15147521\n",
            "iteration: 130 loss: 0.04351931\n",
            "iteration: 131 loss: 0.43241763\n",
            "iteration: 132 loss: 0.55881047\n",
            "iteration: 133 loss: 0.53328156\n",
            "iteration: 134 loss: 0.99377036\n",
            "iteration: 135 loss: 1.24148202\n",
            "iteration: 136 loss: 0.37924236\n",
            "iteration: 137 loss: 0.58850032\n",
            "iteration: 138 loss: 1.55821764\n",
            "iteration: 139 loss: 0.19094504\n",
            "iteration: 140 loss: 0.12658411\n",
            "iteration: 141 loss: 0.47185495\n",
            "iteration: 142 loss: 2.43228936\n",
            "iteration: 143 loss: 0.40915737\n",
            "iteration: 144 loss: 0.33198008\n",
            "iteration: 145 loss: 0.16336668\n",
            "iteration: 146 loss: 0.34731406\n",
            "iteration: 147 loss: 0.28337246\n",
            "iteration: 148 loss: 0.49848086\n",
            "iteration: 149 loss: 0.69656080\n",
            "iteration: 150 loss: 0.22994964\n",
            "iteration: 151 loss: 0.49643210\n",
            "iteration: 152 loss: 0.53813291\n",
            "iteration: 153 loss: 0.04473298\n",
            "iteration: 154 loss: 3.62068200\n",
            "iteration: 155 loss: 0.25710362\n",
            "iteration: 156 loss: 0.27354053\n",
            "iteration: 157 loss: 1.13030314\n",
            "iteration: 158 loss: 0.21669771\n",
            "iteration: 159 loss: 1.41188717\n",
            "iteration: 160 loss: 0.21968967\n",
            "iteration: 161 loss: 0.45061815\n",
            "iteration: 162 loss: 1.34217751\n",
            "iteration: 163 loss: 0.32146791\n",
            "iteration: 164 loss: 0.21767060\n",
            "iteration: 165 loss: 0.11064882\n",
            "iteration: 166 loss: 0.64151943\n",
            "iteration: 167 loss: 1.41398823\n",
            "iteration: 168 loss: 1.05189216\n",
            "iteration: 169 loss: 0.79754710\n",
            "iteration: 170 loss: 0.23903115\n",
            "iteration: 171 loss: 0.56891280\n",
            "iteration: 172 loss: 1.59179628\n",
            "iteration: 173 loss: 0.49445555\n",
            "iteration: 174 loss: 0.80133867\n",
            "iteration: 175 loss: 0.40568218\n",
            "iteration: 176 loss: 0.15388928\n",
            "iteration: 177 loss: 1.38316369\n",
            "iteration: 178 loss: 0.29413527\n",
            "iteration: 179 loss: 1.83628428\n",
            "iteration: 180 loss: 0.23164509\n",
            "iteration: 181 loss: 0.35207516\n",
            "iteration: 182 loss: 0.21282315\n",
            "iteration: 183 loss: 0.21666561\n",
            "iteration: 184 loss: 0.42878476\n",
            "iteration: 185 loss: 0.10147269\n",
            "iteration: 186 loss: 0.21345621\n",
            "iteration: 187 loss: 1.69279218\n",
            "iteration: 188 loss: 0.19043721\n",
            "iteration: 189 loss: 0.52757257\n",
            "iteration: 190 loss: 0.57615888\n",
            "iteration: 191 loss: 0.04575895\n",
            "iteration: 192 loss: 0.47412255\n",
            "iteration: 193 loss: 0.36218026\n",
            "iteration: 194 loss: 0.16493401\n",
            "iteration: 195 loss: 0.52631944\n",
            "iteration: 196 loss: 0.06649891\n",
            "iteration: 197 loss: 0.37490997\n",
            "iteration: 198 loss: 0.61608160\n",
            "iteration: 199 loss: 0.07225663\n",
            "epoch:  78 mean loss training: 0.59776759\n",
            "epoch:  78 mean loss validation: 0.84811431\n",
            "iteration:   0 loss: 0.13678643\n",
            "iteration:   1 loss: 0.30547750\n",
            "iteration:   2 loss: 0.41753381\n",
            "iteration:   3 loss: 0.19538444\n",
            "iteration:   4 loss: 0.34725857\n",
            "iteration:   5 loss: 0.47764605\n",
            "iteration:   6 loss: 0.06699909\n",
            "iteration:   7 loss: 0.18880151\n",
            "iteration:   8 loss: 0.14140862\n",
            "iteration:   9 loss: 0.24755542\n",
            "iteration:  10 loss: 0.39949292\n",
            "iteration:  11 loss: 1.16515350\n",
            "iteration:  12 loss: 0.09763028\n",
            "iteration:  13 loss: 0.76699358\n",
            "iteration:  14 loss: 0.05425546\n",
            "iteration:  15 loss: 0.50819707\n",
            "iteration:  16 loss: 0.65587592\n",
            "iteration:  17 loss: 0.23696333\n",
            "iteration:  18 loss: 0.22259261\n",
            "iteration:  19 loss: 0.39402726\n",
            "iteration:  20 loss: 0.13042112\n",
            "iteration:  21 loss: 0.27845660\n",
            "iteration:  22 loss: 0.31832150\n",
            "iteration:  23 loss: 3.05154729\n",
            "iteration:  24 loss: 1.00225377\n",
            "iteration:  25 loss: 0.62982464\n",
            "iteration:  26 loss: 0.15200685\n",
            "iteration:  27 loss: 0.46994233\n",
            "iteration:  28 loss: 0.55145937\n",
            "iteration:  29 loss: 0.32289359\n",
            "iteration:  30 loss: 0.13014203\n",
            "iteration:  31 loss: 0.74092966\n",
            "iteration:  32 loss: 0.51693773\n",
            "iteration:  33 loss: 0.89703107\n",
            "iteration:  34 loss: 0.10365742\n",
            "iteration:  35 loss: 0.30645573\n",
            "iteration:  36 loss: 0.10970302\n",
            "iteration:  37 loss: 0.05471398\n",
            "iteration:  38 loss: 0.19614342\n",
            "iteration:  39 loss: 1.41358280\n",
            "iteration:  40 loss: 0.86296612\n",
            "iteration:  41 loss: 0.49343708\n",
            "iteration:  42 loss: 0.32734615\n",
            "iteration:  43 loss: 0.04934774\n",
            "iteration:  44 loss: 0.23375247\n",
            "iteration:  45 loss: 0.57454997\n",
            "iteration:  46 loss: 1.32256305\n",
            "iteration:  47 loss: 1.14929330\n",
            "iteration:  48 loss: 0.69428688\n",
            "iteration:  49 loss: 0.04685517\n",
            "iteration:  50 loss: 2.16427231\n",
            "iteration:  51 loss: 1.26109219\n",
            "iteration:  52 loss: 0.94573390\n",
            "iteration:  53 loss: 0.11085511\n",
            "iteration:  54 loss: 0.05936478\n",
            "iteration:  55 loss: 1.62357426\n",
            "iteration:  56 loss: 0.41989490\n",
            "iteration:  57 loss: 1.17900860\n",
            "iteration:  58 loss: 0.19499190\n",
            "iteration:  59 loss: 3.73920727\n",
            "iteration:  60 loss: 0.60887808\n",
            "iteration:  61 loss: 0.29793739\n",
            "iteration:  62 loss: 0.92839009\n",
            "iteration:  63 loss: 0.74042010\n",
            "iteration:  64 loss: 0.15628034\n",
            "iteration:  65 loss: 1.42412317\n",
            "iteration:  66 loss: 1.37832665\n",
            "iteration:  67 loss: 0.83430421\n",
            "iteration:  68 loss: 1.11404288\n",
            "iteration:  69 loss: 0.40717310\n",
            "iteration:  70 loss: 0.51952267\n",
            "iteration:  71 loss: 0.46090919\n",
            "iteration:  72 loss: 0.28340128\n",
            "iteration:  73 loss: 1.57092464\n",
            "iteration:  74 loss: 0.12973043\n",
            "iteration:  75 loss: 0.31420094\n",
            "iteration:  76 loss: 0.66238183\n",
            "iteration:  77 loss: 0.13986543\n",
            "iteration:  78 loss: 1.53544247\n",
            "iteration:  79 loss: 1.30297124\n",
            "iteration:  80 loss: 0.33116689\n",
            "iteration:  81 loss: 0.36339337\n",
            "iteration:  82 loss: 0.05794087\n",
            "iteration:  83 loss: 0.34367633\n",
            "iteration:  84 loss: 0.76478225\n",
            "iteration:  85 loss: 0.08719504\n",
            "iteration:  86 loss: 0.37675458\n",
            "iteration:  87 loss: 0.59053773\n",
            "iteration:  88 loss: 0.06287230\n",
            "iteration:  89 loss: 0.24533969\n",
            "iteration:  90 loss: 0.86383563\n",
            "iteration:  91 loss: 0.26510167\n",
            "iteration:  92 loss: 1.29753494\n",
            "iteration:  93 loss: 0.72549057\n",
            "iteration:  94 loss: 0.11655814\n",
            "iteration:  95 loss: 0.15714821\n",
            "iteration:  96 loss: 0.14938684\n",
            "iteration:  97 loss: 0.21627787\n",
            "iteration:  98 loss: 0.16911674\n",
            "iteration:  99 loss: 2.07552290\n",
            "iteration: 100 loss: 0.49522072\n",
            "iteration: 101 loss: 0.40344116\n",
            "iteration: 102 loss: 0.15080783\n",
            "iteration: 103 loss: 0.19089003\n",
            "iteration: 104 loss: 1.37595892\n",
            "iteration: 105 loss: 1.10190034\n",
            "iteration: 106 loss: 0.16292451\n",
            "iteration: 107 loss: 0.20778663\n",
            "iteration: 108 loss: 0.35763168\n",
            "iteration: 109 loss: 0.50448477\n",
            "iteration: 110 loss: 0.18124889\n",
            "iteration: 111 loss: 0.01310434\n",
            "iteration: 112 loss: 0.48392069\n",
            "iteration: 113 loss: 0.06157090\n",
            "iteration: 114 loss: 1.52934802\n",
            "iteration: 115 loss: 0.14263451\n",
            "iteration: 116 loss: 0.54233772\n",
            "iteration: 117 loss: 0.71993530\n",
            "iteration: 118 loss: 0.10225236\n",
            "iteration: 119 loss: 0.18489891\n",
            "iteration: 120 loss: 0.29222900\n",
            "iteration: 121 loss: 1.24388218\n",
            "iteration: 122 loss: 0.20949604\n",
            "iteration: 123 loss: 0.28180093\n",
            "iteration: 124 loss: 0.08419596\n",
            "iteration: 125 loss: 0.11652980\n",
            "iteration: 126 loss: 1.47103953\n",
            "iteration: 127 loss: 0.87912023\n",
            "iteration: 128 loss: 0.22263424\n",
            "iteration: 129 loss: 0.67452842\n",
            "iteration: 130 loss: 0.21853995\n",
            "iteration: 131 loss: 0.15800354\n",
            "iteration: 132 loss: 0.95695150\n",
            "iteration: 133 loss: 0.13983659\n",
            "iteration: 134 loss: 0.34702873\n",
            "iteration: 135 loss: 0.71751833\n",
            "iteration: 136 loss: 1.21758616\n",
            "iteration: 137 loss: 2.45370293\n",
            "iteration: 138 loss: 0.22926630\n",
            "iteration: 139 loss: 0.59677029\n",
            "iteration: 140 loss: 0.17476347\n",
            "iteration: 141 loss: 0.18663256\n",
            "iteration: 142 loss: 0.37483293\n",
            "iteration: 143 loss: 1.15908110\n",
            "iteration: 144 loss: 1.57246828\n",
            "iteration: 145 loss: 0.16185856\n",
            "iteration: 146 loss: 2.59591460\n",
            "iteration: 147 loss: 0.07061078\n",
            "iteration: 148 loss: 0.66584408\n",
            "iteration: 149 loss: 0.27539292\n",
            "iteration: 150 loss: 0.90027618\n",
            "iteration: 151 loss: 0.24662550\n",
            "iteration: 152 loss: 0.20966786\n",
            "iteration: 153 loss: 2.15405393\n",
            "iteration: 154 loss: 0.16268994\n",
            "iteration: 155 loss: 0.44235268\n",
            "iteration: 156 loss: 0.70324934\n",
            "iteration: 157 loss: 0.07327282\n",
            "iteration: 158 loss: 0.31834975\n",
            "iteration: 159 loss: 0.43225282\n",
            "iteration: 160 loss: 0.10461751\n",
            "iteration: 161 loss: 1.18349135\n",
            "iteration: 162 loss: 0.43256295\n",
            "iteration: 163 loss: 0.05985999\n",
            "iteration: 164 loss: 3.36432767\n",
            "iteration: 165 loss: 0.43507674\n",
            "iteration: 166 loss: 0.21912360\n",
            "iteration: 167 loss: 0.73722708\n",
            "iteration: 168 loss: 0.33677053\n",
            "iteration: 169 loss: 0.23556514\n",
            "iteration: 170 loss: 0.14607976\n",
            "iteration: 171 loss: 0.40167087\n",
            "iteration: 172 loss: 0.61017758\n",
            "iteration: 173 loss: 0.30055061\n",
            "iteration: 174 loss: 1.86386800\n",
            "iteration: 175 loss: 0.05358806\n",
            "iteration: 176 loss: 0.26943839\n",
            "iteration: 177 loss: 0.39676535\n",
            "iteration: 178 loss: 0.38939419\n",
            "iteration: 179 loss: 0.31211677\n",
            "iteration: 180 loss: 1.71357393\n",
            "iteration: 181 loss: 1.79485214\n",
            "iteration: 182 loss: 0.32239568\n",
            "iteration: 183 loss: 0.67376566\n",
            "iteration: 184 loss: 0.36009672\n",
            "iteration: 185 loss: 0.83008176\n",
            "iteration: 186 loss: 0.19659096\n",
            "iteration: 187 loss: 0.22100843\n",
            "iteration: 188 loss: 0.26080927\n",
            "iteration: 189 loss: 0.67344248\n",
            "iteration: 190 loss: 0.27840683\n",
            "iteration: 191 loss: 0.76476753\n",
            "iteration: 192 loss: 0.16443275\n",
            "iteration: 193 loss: 0.05815225\n",
            "iteration: 194 loss: 0.11423832\n",
            "iteration: 195 loss: 0.13068520\n",
            "iteration: 196 loss: 0.43274024\n",
            "iteration: 197 loss: 1.02495694\n",
            "iteration: 198 loss: 0.31806919\n",
            "iteration: 199 loss: 0.39743602\n",
            "epoch:  79 mean loss training: 0.58404815\n",
            "epoch:  79 mean loss validation: 0.86638641\n",
            "iteration:   0 loss: 0.80234671\n",
            "iteration:   1 loss: 0.50754619\n",
            "iteration:   2 loss: 0.88860685\n",
            "iteration:   3 loss: 0.69547027\n",
            "iteration:   4 loss: 0.07735713\n",
            "iteration:   5 loss: 1.26981473\n",
            "iteration:   6 loss: 0.57473922\n",
            "iteration:   7 loss: 0.63980114\n",
            "iteration:   8 loss: 0.25988436\n",
            "iteration:   9 loss: 1.22071946\n",
            "iteration:  10 loss: 0.41263106\n",
            "iteration:  11 loss: 0.24428314\n",
            "iteration:  12 loss: 0.12271605\n",
            "iteration:  13 loss: 0.11231100\n",
            "iteration:  14 loss: 1.75266159\n",
            "iteration:  15 loss: 0.12421004\n",
            "iteration:  16 loss: 1.39867890\n",
            "iteration:  17 loss: 0.18825741\n",
            "iteration:  18 loss: 0.28775927\n",
            "iteration:  19 loss: 0.19316697\n",
            "iteration:  20 loss: 0.18540403\n",
            "iteration:  21 loss: 0.17526802\n",
            "iteration:  22 loss: 0.72361386\n",
            "iteration:  23 loss: 0.04733457\n",
            "iteration:  24 loss: 0.31327176\n",
            "iteration:  25 loss: 0.29759997\n",
            "iteration:  26 loss: 0.89071828\n",
            "iteration:  27 loss: 0.04759659\n",
            "iteration:  28 loss: 0.66668350\n",
            "iteration:  29 loss: 0.63719171\n",
            "iteration:  30 loss: 0.98945320\n",
            "iteration:  31 loss: 0.66722614\n",
            "iteration:  32 loss: 0.39869168\n",
            "iteration:  33 loss: 0.30626005\n",
            "iteration:  34 loss: 0.29101041\n",
            "iteration:  35 loss: 0.41708168\n",
            "iteration:  36 loss: 0.51069194\n",
            "iteration:  37 loss: 0.39380580\n",
            "iteration:  38 loss: 0.32638821\n",
            "iteration:  39 loss: 0.58376706\n",
            "iteration:  40 loss: 0.34788984\n",
            "iteration:  41 loss: 0.09102874\n",
            "iteration:  42 loss: 2.19129634\n",
            "iteration:  43 loss: 0.39245728\n",
            "iteration:  44 loss: 1.88209283\n",
            "iteration:  45 loss: 0.30928412\n",
            "iteration:  46 loss: 0.13495767\n",
            "iteration:  47 loss: 0.82220751\n",
            "iteration:  48 loss: 1.33601320\n",
            "iteration:  49 loss: 0.10584922\n",
            "iteration:  50 loss: 0.26069936\n",
            "iteration:  51 loss: 0.38382244\n",
            "iteration:  52 loss: 0.20304032\n",
            "iteration:  53 loss: 0.13972807\n",
            "iteration:  54 loss: 0.52689385\n",
            "iteration:  55 loss: 0.15653314\n",
            "iteration:  56 loss: 0.13969499\n",
            "iteration:  57 loss: 1.01829195\n",
            "iteration:  58 loss: 0.72088253\n",
            "iteration:  59 loss: 2.39840007\n",
            "iteration:  60 loss: 1.21961379\n",
            "iteration:  61 loss: 0.33173174\n",
            "iteration:  62 loss: 0.53887504\n",
            "iteration:  63 loss: 0.02920510\n",
            "iteration:  64 loss: 1.08538210\n",
            "iteration:  65 loss: 0.99809742\n",
            "iteration:  66 loss: 0.16932829\n",
            "iteration:  67 loss: 0.45950368\n",
            "iteration:  68 loss: 0.52555239\n",
            "iteration:  69 loss: 0.41216534\n",
            "iteration:  70 loss: 0.32530048\n",
            "iteration:  71 loss: 0.22440399\n",
            "iteration:  72 loss: 0.57539833\n",
            "iteration:  73 loss: 1.66923690\n",
            "iteration:  74 loss: 0.14474237\n",
            "iteration:  75 loss: 0.07244863\n",
            "iteration:  76 loss: 0.45518455\n",
            "iteration:  77 loss: 0.17968906\n",
            "iteration:  78 loss: 1.12568498\n",
            "iteration:  79 loss: 0.34673852\n",
            "iteration:  80 loss: 0.18331692\n",
            "iteration:  81 loss: 0.01870591\n",
            "iteration:  82 loss: 0.09909364\n",
            "iteration:  83 loss: 0.05518183\n",
            "iteration:  84 loss: 0.74063361\n",
            "iteration:  85 loss: 0.14482814\n",
            "iteration:  86 loss: 0.52295983\n",
            "iteration:  87 loss: 0.07016912\n",
            "iteration:  88 loss: 0.15034932\n",
            "iteration:  89 loss: 0.18021734\n",
            "iteration:  90 loss: 0.11065980\n",
            "iteration:  91 loss: 0.55585259\n",
            "iteration:  92 loss: 0.20194218\n",
            "iteration:  93 loss: 0.16533221\n",
            "iteration:  94 loss: 0.38065222\n",
            "iteration:  95 loss: 1.76986003\n",
            "iteration:  96 loss: 1.27450216\n",
            "iteration:  97 loss: 0.34958348\n",
            "iteration:  98 loss: 0.09477211\n",
            "iteration:  99 loss: 0.61187613\n",
            "iteration: 100 loss: 0.58594972\n",
            "iteration: 101 loss: 1.44405186\n",
            "iteration: 102 loss: 0.11846504\n",
            "iteration: 103 loss: 0.18478890\n",
            "iteration: 104 loss: 0.15144484\n",
            "iteration: 105 loss: 0.05897201\n",
            "iteration: 106 loss: 0.39845830\n",
            "iteration: 107 loss: 3.34258676\n",
            "iteration: 108 loss: 0.49717581\n",
            "iteration: 109 loss: 0.05098759\n",
            "iteration: 110 loss: 1.08911705\n",
            "iteration: 111 loss: 0.25726885\n",
            "iteration: 112 loss: 0.21115321\n",
            "iteration: 113 loss: 0.75033045\n",
            "iteration: 114 loss: 0.23882566\n",
            "iteration: 115 loss: 0.27229226\n",
            "iteration: 116 loss: 0.09826183\n",
            "iteration: 117 loss: 0.12908629\n",
            "iteration: 118 loss: 0.74986947\n",
            "iteration: 119 loss: 0.66651374\n",
            "iteration: 120 loss: 0.25177136\n",
            "iteration: 121 loss: 0.56862921\n",
            "iteration: 122 loss: 1.47332633\n",
            "iteration: 123 loss: 1.69629264\n",
            "iteration: 124 loss: 0.13344899\n",
            "iteration: 125 loss: 0.26548699\n",
            "iteration: 126 loss: 0.35674232\n",
            "iteration: 127 loss: 0.48202625\n",
            "iteration: 128 loss: 0.41854364\n",
            "iteration: 129 loss: 0.20636609\n",
            "iteration: 130 loss: 0.67768675\n",
            "iteration: 131 loss: 0.15263875\n",
            "iteration: 132 loss: 0.38778168\n",
            "iteration: 133 loss: 0.77570713\n",
            "iteration: 134 loss: 1.23513556\n",
            "iteration: 135 loss: 1.94154179\n",
            "iteration: 136 loss: 0.22037165\n",
            "iteration: 137 loss: 0.24528787\n",
            "iteration: 138 loss: 0.19817266\n",
            "iteration: 139 loss: 0.30913803\n",
            "iteration: 140 loss: 1.02788413\n",
            "iteration: 141 loss: 1.64709055\n",
            "iteration: 142 loss: 1.49057186\n",
            "iteration: 143 loss: 0.91829002\n",
            "iteration: 144 loss: 1.22171307\n",
            "iteration: 145 loss: 0.17144506\n",
            "iteration: 146 loss: 0.49314460\n",
            "iteration: 147 loss: 0.28273547\n",
            "iteration: 148 loss: 0.57739413\n",
            "iteration: 149 loss: 0.25525057\n",
            "iteration: 150 loss: 0.39856350\n",
            "iteration: 151 loss: 0.92175376\n",
            "iteration: 152 loss: 0.30601716\n",
            "iteration: 153 loss: 0.10727044\n",
            "iteration: 154 loss: 0.08755147\n",
            "iteration: 155 loss: 0.55345070\n",
            "iteration: 156 loss: 0.09034785\n",
            "iteration: 157 loss: 0.12783368\n",
            "iteration: 158 loss: 0.11875610\n",
            "iteration: 159 loss: 0.25844520\n",
            "iteration: 160 loss: 0.26546973\n",
            "iteration: 161 loss: 0.13565101\n",
            "iteration: 162 loss: 1.73751104\n",
            "iteration: 163 loss: 1.05483222\n",
            "iteration: 164 loss: 0.60934108\n",
            "iteration: 165 loss: 0.44807482\n",
            "iteration: 166 loss: 0.12648964\n",
            "iteration: 167 loss: 0.58012891\n",
            "iteration: 168 loss: 0.13921115\n",
            "iteration: 169 loss: 0.45326418\n",
            "iteration: 170 loss: 0.24894907\n",
            "iteration: 171 loss: 1.52722824\n",
            "iteration: 172 loss: 1.55272365\n",
            "iteration: 173 loss: 0.22820219\n",
            "iteration: 174 loss: 0.07851180\n",
            "iteration: 175 loss: 0.53399587\n",
            "iteration: 176 loss: 0.38103494\n",
            "iteration: 177 loss: 0.06270434\n",
            "iteration: 178 loss: 0.76424670\n",
            "iteration: 179 loss: 0.38171974\n",
            "iteration: 180 loss: 0.15298277\n",
            "iteration: 181 loss: 1.79559457\n",
            "iteration: 182 loss: 0.60547125\n",
            "iteration: 183 loss: 0.05274601\n",
            "iteration: 184 loss: 2.16236401\n",
            "iteration: 185 loss: 0.29978380\n",
            "iteration: 186 loss: 0.11501263\n",
            "iteration: 187 loss: 0.23767264\n",
            "iteration: 188 loss: 0.68199503\n",
            "iteration: 189 loss: 0.26034114\n",
            "iteration: 190 loss: 1.54829895\n",
            "iteration: 191 loss: 0.05128451\n",
            "iteration: 192 loss: 0.32901010\n",
            "iteration: 193 loss: 1.17285407\n",
            "iteration: 194 loss: 0.18828395\n",
            "iteration: 195 loss: 1.50314391\n",
            "iteration: 196 loss: 0.39524344\n",
            "iteration: 197 loss: 0.59621948\n",
            "iteration: 198 loss: 0.15385064\n",
            "iteration: 199 loss: 0.61232013\n",
            "epoch:  80 mean loss training: 0.55671406\n",
            "epoch:  80 mean loss validation: 0.83880937\n",
            "iteration:   0 loss: 1.12971961\n",
            "iteration:   1 loss: 0.48912236\n",
            "iteration:   2 loss: 0.35810795\n",
            "iteration:   3 loss: 0.11713655\n",
            "iteration:   4 loss: 0.42800954\n",
            "iteration:   5 loss: 1.99736631\n",
            "iteration:   6 loss: 0.37422004\n",
            "iteration:   7 loss: 0.08631757\n",
            "iteration:   8 loss: 0.21465430\n",
            "iteration:   9 loss: 0.44190183\n",
            "iteration:  10 loss: 0.26294822\n",
            "iteration:  11 loss: 0.16125792\n",
            "iteration:  12 loss: 0.10918454\n",
            "iteration:  13 loss: 0.84779721\n",
            "iteration:  14 loss: 0.47964537\n",
            "iteration:  15 loss: 0.65358621\n",
            "iteration:  16 loss: 2.25303435\n",
            "iteration:  17 loss: 0.28465888\n",
            "iteration:  18 loss: 0.09272800\n",
            "iteration:  19 loss: 1.35491252\n",
            "iteration:  20 loss: 0.32447639\n",
            "iteration:  21 loss: 0.57572758\n",
            "iteration:  22 loss: 3.61624813\n",
            "iteration:  23 loss: 0.08188716\n",
            "iteration:  24 loss: 1.30151069\n",
            "iteration:  25 loss: 1.09103739\n",
            "iteration:  26 loss: 0.12062050\n",
            "iteration:  27 loss: 0.15417144\n",
            "iteration:  28 loss: 0.63960159\n",
            "iteration:  29 loss: 0.14840762\n",
            "iteration:  30 loss: 2.18601680\n",
            "iteration:  31 loss: 1.02791488\n",
            "iteration:  32 loss: 0.63373029\n",
            "iteration:  33 loss: 2.38806200\n",
            "iteration:  34 loss: 0.45669141\n",
            "iteration:  35 loss: 0.22944129\n",
            "iteration:  36 loss: 0.96906722\n",
            "iteration:  37 loss: 0.35755855\n",
            "iteration:  38 loss: 0.32058913\n",
            "iteration:  39 loss: 0.45402074\n",
            "iteration:  40 loss: 0.76268870\n",
            "iteration:  41 loss: 0.08866466\n",
            "iteration:  42 loss: 0.67675376\n",
            "iteration:  43 loss: 0.12807469\n",
            "iteration:  44 loss: 1.34266841\n",
            "iteration:  45 loss: 1.25390911\n",
            "iteration:  46 loss: 1.01987207\n",
            "iteration:  47 loss: 0.48776945\n",
            "iteration:  48 loss: 0.80200529\n",
            "iteration:  49 loss: 0.13420798\n",
            "iteration:  50 loss: 0.30837929\n",
            "iteration:  51 loss: 0.12777865\n",
            "iteration:  52 loss: 0.42962387\n",
            "iteration:  53 loss: 2.52178526\n",
            "iteration:  54 loss: 0.22614856\n",
            "iteration:  55 loss: 0.59182113\n",
            "iteration:  56 loss: 0.81506407\n",
            "iteration:  57 loss: 0.29725289\n",
            "iteration:  58 loss: 1.39848971\n",
            "iteration:  59 loss: 0.10586132\n",
            "iteration:  60 loss: 0.48462060\n",
            "iteration:  61 loss: 0.12869394\n",
            "iteration:  62 loss: 1.09660780\n",
            "iteration:  63 loss: 0.37177953\n",
            "iteration:  64 loss: 0.11173317\n",
            "iteration:  65 loss: 0.13732709\n",
            "iteration:  66 loss: 0.38479832\n",
            "iteration:  67 loss: 0.21261285\n",
            "iteration:  68 loss: 0.50184339\n",
            "iteration:  69 loss: 0.68480515\n",
            "iteration:  70 loss: 0.20299448\n",
            "iteration:  71 loss: 1.45224893\n",
            "iteration:  72 loss: 1.64004028\n",
            "iteration:  73 loss: 1.40123665\n",
            "iteration:  74 loss: 0.15493169\n",
            "iteration:  75 loss: 0.46656138\n",
            "iteration:  76 loss: 0.13262427\n",
            "iteration:  77 loss: 1.96952307\n",
            "iteration:  78 loss: 0.07420502\n",
            "iteration:  79 loss: 0.47151873\n",
            "iteration:  80 loss: 0.85520089\n",
            "iteration:  81 loss: 0.39841762\n",
            "iteration:  82 loss: 1.35189259\n",
            "iteration:  83 loss: 0.59033400\n",
            "iteration:  84 loss: 0.52884871\n",
            "iteration:  85 loss: 0.23955743\n",
            "iteration:  86 loss: 0.50205010\n",
            "iteration:  87 loss: 0.27191970\n",
            "iteration:  88 loss: 0.49428415\n",
            "iteration:  89 loss: 0.15760100\n",
            "iteration:  90 loss: 0.06366019\n",
            "iteration:  91 loss: 0.57721722\n",
            "iteration:  92 loss: 0.29725558\n",
            "iteration:  93 loss: 0.10000064\n",
            "iteration:  94 loss: 2.19329882\n",
            "iteration:  95 loss: 0.57965696\n",
            "iteration:  96 loss: 0.09798635\n",
            "iteration:  97 loss: 0.49820629\n",
            "iteration:  98 loss: 0.15112746\n",
            "iteration:  99 loss: 0.48573405\n",
            "iteration: 100 loss: 0.55529815\n",
            "iteration: 101 loss: 0.70821744\n",
            "iteration: 102 loss: 0.26720393\n",
            "iteration: 103 loss: 0.17961089\n",
            "iteration: 104 loss: 1.41037107\n",
            "iteration: 105 loss: 0.63710189\n",
            "iteration: 106 loss: 0.68118364\n",
            "iteration: 107 loss: 0.60318506\n",
            "iteration: 108 loss: 1.11914957\n",
            "iteration: 109 loss: 0.56241506\n",
            "iteration: 110 loss: 0.59954470\n",
            "iteration: 111 loss: 0.27368909\n",
            "iteration: 112 loss: 0.12872578\n",
            "iteration: 113 loss: 0.11670518\n",
            "iteration: 114 loss: 0.12705591\n",
            "iteration: 115 loss: 0.08835385\n",
            "iteration: 116 loss: 0.85149646\n",
            "iteration: 117 loss: 1.20399582\n",
            "iteration: 118 loss: 1.32871318\n",
            "iteration: 119 loss: 0.11865930\n",
            "iteration: 120 loss: 0.11645146\n",
            "iteration: 121 loss: 0.23137131\n",
            "iteration: 122 loss: 1.15298581\n",
            "iteration: 123 loss: 1.52523708\n",
            "iteration: 124 loss: 0.25881121\n",
            "iteration: 125 loss: 0.78369230\n",
            "iteration: 126 loss: 0.07647730\n",
            "iteration: 127 loss: 2.52468014\n",
            "iteration: 128 loss: 0.46451440\n",
            "iteration: 129 loss: 0.27636442\n",
            "iteration: 130 loss: 1.10028946\n",
            "iteration: 131 loss: 0.04176767\n",
            "iteration: 132 loss: 0.02864685\n",
            "iteration: 133 loss: 0.36414802\n",
            "iteration: 134 loss: 0.44950917\n",
            "iteration: 135 loss: 0.92702627\n",
            "iteration: 136 loss: 0.10084923\n",
            "iteration: 137 loss: 0.40958536\n",
            "iteration: 138 loss: 0.38907397\n",
            "iteration: 139 loss: 1.17770731\n",
            "iteration: 140 loss: 0.57321429\n",
            "iteration: 141 loss: 0.20378351\n",
            "iteration: 142 loss: 1.89516735\n",
            "iteration: 143 loss: 0.09856591\n",
            "iteration: 144 loss: 0.09520080\n",
            "iteration: 145 loss: 0.12020890\n",
            "iteration: 146 loss: 0.27251565\n",
            "iteration: 147 loss: 0.36729074\n",
            "iteration: 148 loss: 0.13855889\n",
            "iteration: 149 loss: 0.52217263\n",
            "iteration: 150 loss: 0.25592071\n",
            "iteration: 151 loss: 0.44262090\n",
            "iteration: 152 loss: 1.14597404\n",
            "iteration: 153 loss: 0.59770125\n",
            "iteration: 154 loss: 0.36967865\n",
            "iteration: 155 loss: 0.44978952\n",
            "iteration: 156 loss: 0.48851582\n",
            "iteration: 157 loss: 0.44153148\n",
            "iteration: 158 loss: 1.78853452\n",
            "iteration: 159 loss: 0.56363738\n",
            "iteration: 160 loss: 0.42909431\n",
            "iteration: 161 loss: 0.04310929\n",
            "iteration: 162 loss: 0.30260107\n",
            "iteration: 163 loss: 2.19614649\n",
            "iteration: 164 loss: 0.80667883\n",
            "iteration: 165 loss: 0.36078447\n",
            "iteration: 166 loss: 2.04494023\n",
            "iteration: 167 loss: 0.42861202\n",
            "iteration: 168 loss: 0.40391058\n",
            "iteration: 169 loss: 0.29264215\n",
            "iteration: 170 loss: 0.29471919\n",
            "iteration: 171 loss: 1.02946186\n",
            "iteration: 172 loss: 0.03610537\n",
            "iteration: 173 loss: 0.59773421\n",
            "iteration: 174 loss: 0.80508775\n",
            "iteration: 175 loss: 0.60853076\n",
            "iteration: 176 loss: 0.05850349\n",
            "iteration: 177 loss: 1.24576831\n",
            "iteration: 178 loss: 0.06813085\n",
            "iteration: 179 loss: 0.25028214\n",
            "iteration: 180 loss: 0.33932447\n",
            "iteration: 181 loss: 0.20464990\n",
            "iteration: 182 loss: 1.19955575\n",
            "iteration: 183 loss: 0.12016556\n",
            "iteration: 184 loss: 0.25647137\n",
            "iteration: 185 loss: 1.75552142\n",
            "iteration: 186 loss: 0.05282359\n",
            "iteration: 187 loss: 0.88865328\n",
            "iteration: 188 loss: 0.17757072\n",
            "iteration: 189 loss: 0.05115863\n",
            "iteration: 190 loss: 0.75029945\n",
            "iteration: 191 loss: 0.15583543\n",
            "iteration: 192 loss: 0.03718526\n",
            "iteration: 193 loss: 0.48307595\n",
            "iteration: 194 loss: 0.37442857\n",
            "iteration: 195 loss: 0.72773081\n",
            "iteration: 196 loss: 0.53210682\n",
            "iteration: 197 loss: 0.33861852\n",
            "iteration: 198 loss: 0.05944791\n",
            "iteration: 199 loss: 0.40814233\n",
            "epoch:  81 mean loss training: 0.60626465\n",
            "epoch:  81 mean loss validation: 0.86884123\n",
            "iteration:   0 loss: 0.35503739\n",
            "iteration:   1 loss: 0.16816664\n",
            "iteration:   2 loss: 1.51535070\n",
            "iteration:   3 loss: 0.96937281\n",
            "iteration:   4 loss: 0.18553975\n",
            "iteration:   5 loss: 0.24211574\n",
            "iteration:   6 loss: 2.76242447\n",
            "iteration:   7 loss: 0.24805543\n",
            "iteration:   8 loss: 1.02500784\n",
            "iteration:   9 loss: 0.17211989\n",
            "iteration:  10 loss: 0.27914777\n",
            "iteration:  11 loss: 0.06548650\n",
            "iteration:  12 loss: 0.35388535\n",
            "iteration:  13 loss: 0.49320197\n",
            "iteration:  14 loss: 0.16076487\n",
            "iteration:  15 loss: 0.38371843\n",
            "iteration:  16 loss: 0.03663287\n",
            "iteration:  17 loss: 0.30942380\n",
            "iteration:  18 loss: 0.14651154\n",
            "iteration:  19 loss: 0.01382632\n",
            "iteration:  20 loss: 0.07835234\n",
            "iteration:  21 loss: 0.26753417\n",
            "iteration:  22 loss: 0.06981219\n",
            "iteration:  23 loss: 0.22613946\n",
            "iteration:  24 loss: 0.99229121\n",
            "iteration:  25 loss: 0.91611278\n",
            "iteration:  26 loss: 0.59496468\n",
            "iteration:  27 loss: 0.10704963\n",
            "iteration:  28 loss: 0.75328326\n",
            "iteration:  29 loss: 0.03655219\n",
            "iteration:  30 loss: 0.08516260\n",
            "iteration:  31 loss: 2.34564781\n",
            "iteration:  32 loss: 0.23147932\n",
            "iteration:  33 loss: 0.11685672\n",
            "iteration:  34 loss: 3.26851845\n",
            "iteration:  35 loss: 0.33285445\n",
            "iteration:  36 loss: 1.09682608\n",
            "iteration:  37 loss: 0.48058784\n",
            "iteration:  38 loss: 0.44838366\n",
            "iteration:  39 loss: 0.77798975\n",
            "iteration:  40 loss: 0.06158102\n",
            "iteration:  41 loss: 0.97164613\n",
            "iteration:  42 loss: 0.04207732\n",
            "iteration:  43 loss: 0.44864190\n",
            "iteration:  44 loss: 0.83225250\n",
            "iteration:  45 loss: 0.33887175\n",
            "iteration:  46 loss: 1.16840267\n",
            "iteration:  47 loss: 0.27607232\n",
            "iteration:  48 loss: 0.40266207\n",
            "iteration:  49 loss: 1.69023097\n",
            "iteration:  50 loss: 0.27227318\n",
            "iteration:  51 loss: 2.74973130\n",
            "iteration:  52 loss: 0.45517227\n",
            "iteration:  53 loss: 0.17121419\n",
            "iteration:  54 loss: 0.12220281\n",
            "iteration:  55 loss: 1.66336763\n",
            "iteration:  56 loss: 1.75514996\n",
            "iteration:  57 loss: 0.08478127\n",
            "iteration:  58 loss: 0.56784165\n",
            "iteration:  59 loss: 1.41283929\n",
            "iteration:  60 loss: 1.91413903\n",
            "iteration:  61 loss: 0.19974183\n",
            "iteration:  62 loss: 0.51501435\n",
            "iteration:  63 loss: 1.70457470\n",
            "iteration:  64 loss: 1.62827182\n",
            "iteration:  65 loss: 0.08160396\n",
            "iteration:  66 loss: 0.31303492\n",
            "iteration:  67 loss: 1.32171965\n",
            "iteration:  68 loss: 0.47636095\n",
            "iteration:  69 loss: 0.05854462\n",
            "iteration:  70 loss: 1.17628479\n",
            "iteration:  71 loss: 0.02808948\n",
            "iteration:  72 loss: 0.50040895\n",
            "iteration:  73 loss: 0.38786924\n",
            "iteration:  74 loss: 1.72433710\n",
            "iteration:  75 loss: 0.82744783\n",
            "iteration:  76 loss: 1.68272245\n",
            "iteration:  77 loss: 0.46994230\n",
            "iteration:  78 loss: 0.23902507\n",
            "iteration:  79 loss: 0.10823848\n",
            "iteration:  80 loss: 1.70193970\n",
            "iteration:  81 loss: 0.27359816\n",
            "iteration:  82 loss: 1.39689422\n",
            "iteration:  83 loss: 0.34054804\n",
            "iteration:  84 loss: 0.15958877\n",
            "iteration:  85 loss: 0.16882902\n",
            "iteration:  86 loss: 0.16864374\n",
            "iteration:  87 loss: 1.50426030\n",
            "iteration:  88 loss: 1.79703200\n",
            "iteration:  89 loss: 1.34632874\n",
            "iteration:  90 loss: 0.36710513\n",
            "iteration:  91 loss: 0.04115339\n",
            "iteration:  92 loss: 0.83562154\n",
            "iteration:  93 loss: 0.76369578\n",
            "iteration:  94 loss: 0.28616008\n",
            "iteration:  95 loss: 0.10685003\n",
            "iteration:  96 loss: 0.22773392\n",
            "iteration:  97 loss: 0.07885823\n",
            "iteration:  98 loss: 0.46506420\n",
            "iteration:  99 loss: 0.21444045\n",
            "iteration: 100 loss: 2.13629484\n",
            "iteration: 101 loss: 1.21919942\n",
            "iteration: 102 loss: 0.33755067\n",
            "iteration: 103 loss: 0.48846737\n",
            "iteration: 104 loss: 0.35362786\n",
            "iteration: 105 loss: 0.29405743\n",
            "iteration: 106 loss: 0.19670135\n",
            "iteration: 107 loss: 0.59542120\n",
            "iteration: 108 loss: 1.40608096\n",
            "iteration: 109 loss: 0.90401173\n",
            "iteration: 110 loss: 0.05801274\n",
            "iteration: 111 loss: 0.38096538\n",
            "iteration: 112 loss: 0.11575188\n",
            "iteration: 113 loss: 0.20900698\n",
            "iteration: 114 loss: 0.08889349\n",
            "iteration: 115 loss: 0.80825168\n",
            "iteration: 116 loss: 0.68608230\n",
            "iteration: 117 loss: 0.44168562\n",
            "iteration: 118 loss: 0.26056758\n",
            "iteration: 119 loss: 0.32270387\n",
            "iteration: 120 loss: 0.43147379\n",
            "iteration: 121 loss: 0.81319749\n",
            "iteration: 122 loss: 0.03688454\n",
            "iteration: 123 loss: 0.14156520\n",
            "iteration: 124 loss: 0.99156988\n",
            "iteration: 125 loss: 1.88174534\n",
            "iteration: 126 loss: 0.42941707\n",
            "iteration: 127 loss: 0.89464009\n",
            "iteration: 128 loss: 0.19222006\n",
            "iteration: 129 loss: 1.51118016\n",
            "iteration: 130 loss: 0.10938765\n",
            "iteration: 131 loss: 0.42207927\n",
            "iteration: 132 loss: 0.11028362\n",
            "iteration: 133 loss: 0.49088752\n",
            "iteration: 134 loss: 0.25947204\n",
            "iteration: 135 loss: 1.37244034\n",
            "iteration: 136 loss: 0.52110684\n",
            "iteration: 137 loss: 0.50132096\n",
            "iteration: 138 loss: 0.18375981\n",
            "iteration: 139 loss: 0.15779474\n",
            "iteration: 140 loss: 0.30985695\n",
            "iteration: 141 loss: 0.85253787\n",
            "iteration: 142 loss: 0.56549430\n",
            "iteration: 143 loss: 0.37701213\n",
            "iteration: 144 loss: 0.62862366\n",
            "iteration: 145 loss: 0.59552950\n",
            "iteration: 146 loss: 1.44102442\n",
            "iteration: 147 loss: 1.12872422\n",
            "iteration: 148 loss: 0.39646715\n",
            "iteration: 149 loss: 0.32240131\n",
            "iteration: 150 loss: 1.09849286\n",
            "iteration: 151 loss: 0.21765852\n",
            "iteration: 152 loss: 0.54107243\n",
            "iteration: 153 loss: 0.19736846\n",
            "iteration: 154 loss: 0.47503102\n",
            "iteration: 155 loss: 0.15792564\n",
            "iteration: 156 loss: 0.20192155\n",
            "iteration: 157 loss: 0.91215116\n",
            "iteration: 158 loss: 1.98213661\n",
            "iteration: 159 loss: 0.05576046\n",
            "iteration: 160 loss: 0.27019987\n",
            "iteration: 161 loss: 0.73644745\n",
            "iteration: 162 loss: 0.27566755\n",
            "iteration: 163 loss: 0.26609209\n",
            "iteration: 164 loss: 0.40819523\n",
            "iteration: 165 loss: 0.33579832\n",
            "iteration: 166 loss: 0.61973470\n",
            "iteration: 167 loss: 0.14569490\n",
            "iteration: 168 loss: 0.42470351\n",
            "iteration: 169 loss: 0.17729279\n",
            "iteration: 170 loss: 0.62762058\n",
            "iteration: 171 loss: 0.03759939\n",
            "iteration: 172 loss: 0.71838737\n",
            "iteration: 173 loss: 0.04847586\n",
            "iteration: 174 loss: 1.32637060\n",
            "iteration: 175 loss: 0.27895659\n",
            "iteration: 176 loss: 0.15007749\n",
            "iteration: 177 loss: 0.04885902\n",
            "iteration: 178 loss: 0.34813985\n",
            "iteration: 179 loss: 0.85463119\n",
            "iteration: 180 loss: 1.38887084\n",
            "iteration: 181 loss: 0.25376150\n",
            "iteration: 182 loss: 0.19555113\n",
            "iteration: 183 loss: 0.27644241\n",
            "iteration: 184 loss: 0.38502526\n",
            "iteration: 185 loss: 0.59644669\n",
            "iteration: 186 loss: 0.76117194\n",
            "iteration: 187 loss: 0.33663779\n",
            "iteration: 188 loss: 0.38621166\n",
            "iteration: 189 loss: 0.11249069\n",
            "iteration: 190 loss: 0.32508016\n",
            "iteration: 191 loss: 0.67213780\n",
            "iteration: 192 loss: 0.33206201\n",
            "iteration: 193 loss: 1.42354465\n",
            "iteration: 194 loss: 0.15485276\n",
            "iteration: 195 loss: 0.37813577\n",
            "iteration: 196 loss: 0.06470978\n",
            "iteration: 197 loss: 1.37844086\n",
            "iteration: 198 loss: 0.35515434\n",
            "iteration: 199 loss: 0.83880663\n",
            "epoch:  82 mean loss training: 0.60064179\n",
            "epoch:  82 mean loss validation: 0.82006145\n",
            "iteration:   0 loss: 0.71823716\n",
            "iteration:   1 loss: 0.47019768\n",
            "iteration:   2 loss: 0.05589798\n",
            "iteration:   3 loss: 0.21544372\n",
            "iteration:   4 loss: 2.13169599\n",
            "iteration:   5 loss: 0.57118618\n",
            "iteration:   6 loss: 0.45384574\n",
            "iteration:   7 loss: 0.68971944\n",
            "iteration:   8 loss: 1.06464505\n",
            "iteration:   9 loss: 0.53013623\n",
            "iteration:  10 loss: 0.18807328\n",
            "iteration:  11 loss: 2.15977049\n",
            "iteration:  12 loss: 0.89305723\n",
            "iteration:  13 loss: 1.64367700\n",
            "iteration:  14 loss: 0.28194648\n",
            "iteration:  15 loss: 0.24532723\n",
            "iteration:  16 loss: 0.57089603\n",
            "iteration:  17 loss: 0.36286283\n",
            "iteration:  18 loss: 0.45355901\n",
            "iteration:  19 loss: 0.17967355\n",
            "iteration:  20 loss: 0.22403073\n",
            "iteration:  21 loss: 0.27522147\n",
            "iteration:  22 loss: 0.22298159\n",
            "iteration:  23 loss: 0.54225141\n",
            "iteration:  24 loss: 0.58697057\n",
            "iteration:  25 loss: 1.98000252\n",
            "iteration:  26 loss: 0.45542958\n",
            "iteration:  27 loss: 0.04265929\n",
            "iteration:  28 loss: 0.31554562\n",
            "iteration:  29 loss: 0.17540380\n",
            "iteration:  30 loss: 0.18329179\n",
            "iteration:  31 loss: 0.38225296\n",
            "iteration:  32 loss: 0.61324668\n",
            "iteration:  33 loss: 0.14326780\n",
            "iteration:  34 loss: 0.08400599\n",
            "iteration:  35 loss: 1.02215791\n",
            "iteration:  36 loss: 0.03501641\n",
            "iteration:  37 loss: 1.39138591\n",
            "iteration:  38 loss: 0.19996656\n",
            "iteration:  39 loss: 2.21317935\n",
            "iteration:  40 loss: 0.50197840\n",
            "iteration:  41 loss: 0.24465676\n",
            "iteration:  42 loss: 0.24377611\n",
            "iteration:  43 loss: 1.24142790\n",
            "iteration:  44 loss: 1.29070807\n",
            "iteration:  45 loss: 0.68483704\n",
            "iteration:  46 loss: 0.59961015\n",
            "iteration:  47 loss: 0.73713821\n",
            "iteration:  48 loss: 0.27419886\n",
            "iteration:  49 loss: 0.97785228\n",
            "iteration:  50 loss: 0.12986545\n",
            "iteration:  51 loss: 1.19483662\n",
            "iteration:  52 loss: 0.90897316\n",
            "iteration:  53 loss: 1.80722308\n",
            "iteration:  54 loss: 0.29289472\n",
            "iteration:  55 loss: 0.53519356\n",
            "iteration:  56 loss: 0.12735932\n",
            "iteration:  57 loss: 0.35432011\n",
            "iteration:  58 loss: 0.24227597\n",
            "iteration:  59 loss: 0.55418712\n",
            "iteration:  60 loss: 0.32442772\n",
            "iteration:  61 loss: 0.49470884\n",
            "iteration:  62 loss: 0.11331820\n",
            "iteration:  63 loss: 0.09897732\n",
            "iteration:  64 loss: 0.28320244\n",
            "iteration:  65 loss: 1.13051248\n",
            "iteration:  66 loss: 0.91273046\n",
            "iteration:  67 loss: 1.01513362\n",
            "iteration:  68 loss: 0.69983202\n",
            "iteration:  69 loss: 0.03796577\n",
            "iteration:  70 loss: 0.41695035\n",
            "iteration:  71 loss: 0.06931807\n",
            "iteration:  72 loss: 0.19910571\n",
            "iteration:  73 loss: 0.10639674\n",
            "iteration:  74 loss: 0.16164708\n",
            "iteration:  75 loss: 0.06509128\n",
            "iteration:  76 loss: 1.65322435\n",
            "iteration:  77 loss: 0.61779535\n",
            "iteration:  78 loss: 0.41179809\n",
            "iteration:  79 loss: 0.31513152\n",
            "iteration:  80 loss: 0.31810129\n",
            "iteration:  81 loss: 0.56346744\n",
            "iteration:  82 loss: 2.71764398\n",
            "iteration:  83 loss: 0.23754068\n",
            "iteration:  84 loss: 0.35253400\n",
            "iteration:  85 loss: 0.46794698\n",
            "iteration:  86 loss: 0.71369749\n",
            "iteration:  87 loss: 0.05281067\n",
            "iteration:  88 loss: 0.54742873\n",
            "iteration:  89 loss: 0.07420793\n",
            "iteration:  90 loss: 0.40630347\n",
            "iteration:  91 loss: 0.07365353\n",
            "iteration:  92 loss: 0.26312944\n",
            "iteration:  93 loss: 1.17391491\n",
            "iteration:  94 loss: 0.84013957\n",
            "iteration:  95 loss: 0.02151877\n",
            "iteration:  96 loss: 0.35335997\n",
            "iteration:  97 loss: 0.41795501\n",
            "iteration:  98 loss: 1.91192567\n",
            "iteration:  99 loss: 0.31894723\n",
            "iteration: 100 loss: 2.70428920\n",
            "iteration: 101 loss: 0.54220253\n",
            "iteration: 102 loss: 0.12239396\n",
            "iteration: 103 loss: 0.43798664\n",
            "iteration: 104 loss: 0.08072185\n",
            "iteration: 105 loss: 0.40924397\n",
            "iteration: 106 loss: 2.29649282\n",
            "iteration: 107 loss: 0.60040897\n",
            "iteration: 108 loss: 1.98699844\n",
            "iteration: 109 loss: 2.00380039\n",
            "iteration: 110 loss: 0.41233957\n",
            "iteration: 111 loss: 0.62607598\n",
            "iteration: 112 loss: 0.51365966\n",
            "iteration: 113 loss: 0.11600909\n",
            "iteration: 114 loss: 0.21332130\n",
            "iteration: 115 loss: 0.30587867\n",
            "iteration: 116 loss: 0.11121617\n",
            "iteration: 117 loss: 1.67206085\n",
            "iteration: 118 loss: 0.19981520\n",
            "iteration: 119 loss: 0.17775624\n",
            "iteration: 120 loss: 0.38052422\n",
            "iteration: 121 loss: 0.75398582\n",
            "iteration: 122 loss: 0.64967275\n",
            "iteration: 123 loss: 0.86552787\n",
            "iteration: 124 loss: 0.34593585\n",
            "iteration: 125 loss: 0.27798930\n",
            "iteration: 126 loss: 0.12464931\n",
            "iteration: 127 loss: 0.63766110\n",
            "iteration: 128 loss: 0.19039896\n",
            "iteration: 129 loss: 0.11107630\n",
            "iteration: 130 loss: 1.95505810\n",
            "iteration: 131 loss: 0.08981986\n",
            "iteration: 132 loss: 1.00147128\n",
            "iteration: 133 loss: 0.29325321\n",
            "iteration: 134 loss: 2.03891945\n",
            "iteration: 135 loss: 0.98211306\n",
            "iteration: 136 loss: 0.49885675\n",
            "iteration: 137 loss: 0.41544911\n",
            "iteration: 138 loss: 0.15609862\n",
            "iteration: 139 loss: 1.17576337\n",
            "iteration: 140 loss: 1.91170979\n",
            "iteration: 141 loss: 0.31510216\n",
            "iteration: 142 loss: 1.90493870\n",
            "iteration: 143 loss: 0.14050743\n",
            "iteration: 144 loss: 0.46108755\n",
            "iteration: 145 loss: 1.42743504\n",
            "iteration: 146 loss: 0.09803361\n",
            "iteration: 147 loss: 0.50366676\n",
            "iteration: 148 loss: 0.44230428\n",
            "iteration: 149 loss: 0.25072712\n",
            "iteration: 150 loss: 0.61137676\n",
            "iteration: 151 loss: 0.53764665\n",
            "iteration: 152 loss: 1.05487835\n",
            "iteration: 153 loss: 2.12062860\n",
            "iteration: 154 loss: 0.46415579\n",
            "iteration: 155 loss: 0.93687671\n",
            "iteration: 156 loss: 1.72032738\n",
            "iteration: 157 loss: 0.41916841\n",
            "iteration: 158 loss: 1.32198906\n",
            "iteration: 159 loss: 0.83351254\n",
            "iteration: 160 loss: 0.14183563\n",
            "iteration: 161 loss: 0.69298530\n",
            "iteration: 162 loss: 0.66571808\n",
            "iteration: 163 loss: 0.66105855\n",
            "iteration: 164 loss: 0.13883361\n",
            "iteration: 165 loss: 1.47955763\n",
            "iteration: 166 loss: 0.76732820\n",
            "iteration: 167 loss: 0.25427383\n",
            "iteration: 168 loss: 0.63135672\n",
            "iteration: 169 loss: 0.64640743\n",
            "iteration: 170 loss: 0.07394744\n",
            "iteration: 171 loss: 0.30819193\n",
            "iteration: 172 loss: 0.31953925\n",
            "iteration: 173 loss: 0.13088849\n",
            "iteration: 174 loss: 0.49431753\n",
            "iteration: 175 loss: 0.40260959\n",
            "iteration: 176 loss: 0.51672012\n",
            "iteration: 177 loss: 0.05876838\n",
            "iteration: 178 loss: 1.43905592\n",
            "iteration: 179 loss: 0.46806058\n",
            "iteration: 180 loss: 2.08990765\n",
            "iteration: 181 loss: 0.19407095\n",
            "iteration: 182 loss: 2.21198750\n",
            "iteration: 183 loss: 0.08000143\n",
            "iteration: 184 loss: 0.43821833\n",
            "iteration: 185 loss: 0.14241405\n",
            "iteration: 186 loss: 0.55997187\n",
            "iteration: 187 loss: 0.39030293\n",
            "iteration: 188 loss: 0.56235760\n",
            "iteration: 189 loss: 0.07692252\n",
            "iteration: 190 loss: 0.24225445\n",
            "iteration: 191 loss: 0.43495187\n",
            "iteration: 192 loss: 0.43486828\n",
            "iteration: 193 loss: 0.21301074\n",
            "iteration: 194 loss: 0.24645433\n",
            "iteration: 195 loss: 1.41567945\n",
            "iteration: 196 loss: 0.26746494\n",
            "iteration: 197 loss: 0.70495421\n",
            "iteration: 198 loss: 0.05646236\n",
            "iteration: 199 loss: 0.03216799\n",
            "epoch:  83 mean loss training: 0.62951887\n",
            "epoch:  83 mean loss validation: 0.82016206\n",
            "iteration:   0 loss: 0.15557083\n",
            "iteration:   1 loss: 0.46399286\n",
            "iteration:   2 loss: 0.61268622\n",
            "iteration:   3 loss: 0.39134073\n",
            "iteration:   4 loss: 1.53399086\n",
            "iteration:   5 loss: 0.15754589\n",
            "iteration:   6 loss: 0.17858995\n",
            "iteration:   7 loss: 0.25632548\n",
            "iteration:   8 loss: 0.11665847\n",
            "iteration:   9 loss: 0.45120943\n",
            "iteration:  10 loss: 1.41215396\n",
            "iteration:  11 loss: 1.63196754\n",
            "iteration:  12 loss: 0.08987359\n",
            "iteration:  13 loss: 0.19420919\n",
            "iteration:  14 loss: 0.46835861\n",
            "iteration:  15 loss: 2.31663227\n",
            "iteration:  16 loss: 0.20491447\n",
            "iteration:  17 loss: 0.77524227\n",
            "iteration:  18 loss: 0.45685080\n",
            "iteration:  19 loss: 0.29059541\n",
            "iteration:  20 loss: 1.06243098\n",
            "iteration:  21 loss: 0.60758412\n",
            "iteration:  22 loss: 0.83688986\n",
            "iteration:  23 loss: 0.38265413\n",
            "iteration:  24 loss: 0.19847520\n",
            "iteration:  25 loss: 1.39071083\n",
            "iteration:  26 loss: 0.05549622\n",
            "iteration:  27 loss: 0.42038193\n",
            "iteration:  28 loss: 0.02886963\n",
            "iteration:  29 loss: 0.15418197\n",
            "iteration:  30 loss: 0.17314939\n",
            "iteration:  31 loss: 0.40066415\n",
            "iteration:  32 loss: 0.96208954\n",
            "iteration:  33 loss: 0.50988394\n",
            "iteration:  34 loss: 0.69905382\n",
            "iteration:  35 loss: 0.46782663\n",
            "iteration:  36 loss: 0.71885061\n",
            "iteration:  37 loss: 0.08752563\n",
            "iteration:  38 loss: 0.72810262\n",
            "iteration:  39 loss: 0.11624063\n",
            "iteration:  40 loss: 1.73994017\n",
            "iteration:  41 loss: 0.37280211\n",
            "iteration:  42 loss: 1.93909335\n",
            "iteration:  43 loss: 0.44594249\n",
            "iteration:  44 loss: 0.25620463\n",
            "iteration:  45 loss: 0.17210586\n",
            "iteration:  46 loss: 0.22108610\n",
            "iteration:  47 loss: 0.52117145\n",
            "iteration:  48 loss: 0.22283235\n",
            "iteration:  49 loss: 0.59654731\n",
            "iteration:  50 loss: 0.51326007\n",
            "iteration:  51 loss: 0.09575958\n",
            "iteration:  52 loss: 0.22319606\n",
            "iteration:  53 loss: 2.02427244\n",
            "iteration:  54 loss: 0.22363250\n",
            "iteration:  55 loss: 0.75441808\n",
            "iteration:  56 loss: 0.11414325\n",
            "iteration:  57 loss: 0.81578088\n",
            "iteration:  58 loss: 0.09480838\n",
            "iteration:  59 loss: 1.02433300\n",
            "iteration:  60 loss: 1.39380074\n",
            "iteration:  61 loss: 1.53123605\n",
            "iteration:  62 loss: 0.52612156\n",
            "iteration:  63 loss: 1.77920556\n",
            "iteration:  64 loss: 0.43635625\n",
            "iteration:  65 loss: 0.31051970\n",
            "iteration:  66 loss: 0.30177167\n",
            "iteration:  67 loss: 0.58300573\n",
            "iteration:  68 loss: 0.34865880\n",
            "iteration:  69 loss: 0.09341110\n",
            "iteration:  70 loss: 0.95273250\n",
            "iteration:  71 loss: 1.23139560\n",
            "iteration:  72 loss: 0.42676961\n",
            "iteration:  73 loss: 0.50921053\n",
            "iteration:  74 loss: 0.52919489\n",
            "iteration:  75 loss: 0.33523452\n",
            "iteration:  76 loss: 0.38432881\n",
            "iteration:  77 loss: 0.93376100\n",
            "iteration:  78 loss: 0.12552866\n",
            "iteration:  79 loss: 1.26811051\n",
            "iteration:  80 loss: 0.37979463\n",
            "iteration:  81 loss: 0.46731398\n",
            "iteration:  82 loss: 0.09054381\n",
            "iteration:  83 loss: 1.27597380\n",
            "iteration:  84 loss: 0.46353760\n",
            "iteration:  85 loss: 0.30922818\n",
            "iteration:  86 loss: 0.47131273\n",
            "iteration:  87 loss: 0.28220442\n",
            "iteration:  88 loss: 0.20385221\n",
            "iteration:  89 loss: 0.72716790\n",
            "iteration:  90 loss: 0.05704988\n",
            "iteration:  91 loss: 0.70645630\n",
            "iteration:  92 loss: 0.15322660\n",
            "iteration:  93 loss: 0.37612799\n",
            "iteration:  94 loss: 0.51364207\n",
            "iteration:  95 loss: 0.26821256\n",
            "iteration:  96 loss: 0.07052878\n",
            "iteration:  97 loss: 1.85667336\n",
            "iteration:  98 loss: 1.56910002\n",
            "iteration:  99 loss: 1.16622901\n",
            "iteration: 100 loss: 0.29260635\n",
            "iteration: 101 loss: 0.20500143\n",
            "iteration: 102 loss: 0.19494040\n",
            "iteration: 103 loss: 0.25053665\n",
            "iteration: 104 loss: 1.49398685\n",
            "iteration: 105 loss: 1.19191527\n",
            "iteration: 106 loss: 0.07144119\n",
            "iteration: 107 loss: 0.60838151\n",
            "iteration: 108 loss: 0.58871752\n",
            "iteration: 109 loss: 0.75979340\n",
            "iteration: 110 loss: 1.18787026\n",
            "iteration: 111 loss: 0.72928369\n",
            "iteration: 112 loss: 0.40315640\n",
            "iteration: 113 loss: 0.92985618\n",
            "iteration: 114 loss: 0.08481482\n",
            "iteration: 115 loss: 0.62246102\n",
            "iteration: 116 loss: 1.35083771\n",
            "iteration: 117 loss: 0.26708367\n",
            "iteration: 118 loss: 0.35200018\n",
            "iteration: 119 loss: 0.13838486\n",
            "iteration: 120 loss: 0.60070390\n",
            "iteration: 121 loss: 1.26705897\n",
            "iteration: 122 loss: 0.17513260\n",
            "iteration: 123 loss: 0.47216967\n",
            "iteration: 124 loss: 0.36905107\n",
            "iteration: 125 loss: 0.74649322\n",
            "iteration: 126 loss: 0.11423302\n",
            "iteration: 127 loss: 0.26046231\n",
            "iteration: 128 loss: 0.07144070\n",
            "iteration: 129 loss: 0.51769751\n",
            "iteration: 130 loss: 0.35180405\n",
            "iteration: 131 loss: 0.29918671\n",
            "iteration: 132 loss: 0.16246581\n",
            "iteration: 133 loss: 0.18177336\n",
            "iteration: 134 loss: 0.29265457\n",
            "iteration: 135 loss: 0.06139765\n",
            "iteration: 136 loss: 0.69912249\n",
            "iteration: 137 loss: 1.91170764\n",
            "iteration: 138 loss: 0.10038448\n",
            "iteration: 139 loss: 0.42917201\n",
            "iteration: 140 loss: 0.62336010\n",
            "iteration: 141 loss: 0.05640648\n",
            "iteration: 142 loss: 0.23551591\n",
            "iteration: 143 loss: 0.38621634\n",
            "iteration: 144 loss: 0.37398922\n",
            "iteration: 145 loss: 1.87461889\n",
            "iteration: 146 loss: 0.60942632\n",
            "iteration: 147 loss: 0.48123085\n",
            "iteration: 148 loss: 0.30675143\n",
            "iteration: 149 loss: 0.09062755\n",
            "iteration: 150 loss: 0.20365207\n",
            "iteration: 151 loss: 1.03704894\n",
            "iteration: 152 loss: 0.05325697\n",
            "iteration: 153 loss: 0.04506257\n",
            "iteration: 154 loss: 0.04853619\n",
            "iteration: 155 loss: 0.99532676\n",
            "iteration: 156 loss: 0.66622907\n",
            "iteration: 157 loss: 0.49678570\n",
            "iteration: 158 loss: 0.04486658\n",
            "iteration: 159 loss: 0.16692318\n",
            "iteration: 160 loss: 0.31905544\n",
            "iteration: 161 loss: 0.22858810\n",
            "iteration: 162 loss: 0.14460292\n",
            "iteration: 163 loss: 0.51859665\n",
            "iteration: 164 loss: 1.33709848\n",
            "iteration: 165 loss: 0.43770301\n",
            "iteration: 166 loss: 0.19152594\n",
            "iteration: 167 loss: 0.27739713\n",
            "iteration: 168 loss: 1.23382616\n",
            "iteration: 169 loss: 0.05394987\n",
            "iteration: 170 loss: 0.07539862\n",
            "iteration: 171 loss: 0.28580067\n",
            "iteration: 172 loss: 1.92835975\n",
            "iteration: 173 loss: 1.71212220\n",
            "iteration: 174 loss: 0.48217618\n",
            "iteration: 175 loss: 0.01829335\n",
            "iteration: 176 loss: 0.11645097\n",
            "iteration: 177 loss: 0.53700489\n",
            "iteration: 178 loss: 1.38223958\n",
            "iteration: 179 loss: 0.24684614\n",
            "iteration: 180 loss: 0.11975351\n",
            "iteration: 181 loss: 0.30809993\n",
            "iteration: 182 loss: 0.38590524\n",
            "iteration: 183 loss: 0.27044836\n",
            "iteration: 184 loss: 0.72621709\n",
            "iteration: 185 loss: 0.40796468\n",
            "iteration: 186 loss: 0.59799659\n",
            "iteration: 187 loss: 0.15972912\n",
            "iteration: 188 loss: 0.10910394\n",
            "iteration: 189 loss: 0.72882527\n",
            "iteration: 190 loss: 0.31504685\n",
            "iteration: 191 loss: 0.40285212\n",
            "iteration: 192 loss: 0.73776895\n",
            "iteration: 193 loss: 1.90701616\n",
            "iteration: 194 loss: 0.03161607\n",
            "iteration: 195 loss: 0.41773883\n",
            "iteration: 196 loss: 0.16574754\n",
            "iteration: 197 loss: 0.04348012\n",
            "iteration: 198 loss: 0.03199068\n",
            "iteration: 199 loss: 0.25091007\n",
            "epoch:  84 mean loss training: 0.54593951\n",
            "epoch:  84 mean loss validation: 0.88172191\n",
            "iteration:   0 loss: 0.14276513\n",
            "iteration:   1 loss: 1.54118693\n",
            "iteration:   2 loss: 0.36095801\n",
            "iteration:   3 loss: 0.34442282\n",
            "iteration:   4 loss: 0.12756258\n",
            "iteration:   5 loss: 0.06928457\n",
            "iteration:   6 loss: 0.41191110\n",
            "iteration:   7 loss: 1.85156751\n",
            "iteration:   8 loss: 0.46920341\n",
            "iteration:   9 loss: 0.43559280\n",
            "iteration:  10 loss: 0.55549484\n",
            "iteration:  11 loss: 0.78296089\n",
            "iteration:  12 loss: 0.51120222\n",
            "iteration:  13 loss: 1.05285597\n",
            "iteration:  14 loss: 0.13664721\n",
            "iteration:  15 loss: 0.23921598\n",
            "iteration:  16 loss: 0.13346355\n",
            "iteration:  17 loss: 0.42173946\n",
            "iteration:  18 loss: 2.02603173\n",
            "iteration:  19 loss: 0.30034825\n",
            "iteration:  20 loss: 0.36960068\n",
            "iteration:  21 loss: 1.66257405\n",
            "iteration:  22 loss: 0.56439471\n",
            "iteration:  23 loss: 1.63396108\n",
            "iteration:  24 loss: 0.94896573\n",
            "iteration:  25 loss: 0.44451839\n",
            "iteration:  26 loss: 0.41722098\n",
            "iteration:  27 loss: 0.17634214\n",
            "iteration:  28 loss: 0.56275529\n",
            "iteration:  29 loss: 0.05930841\n",
            "iteration:  30 loss: 0.08632663\n",
            "iteration:  31 loss: 1.33830273\n",
            "iteration:  32 loss: 2.88852739\n",
            "iteration:  33 loss: 0.18452330\n",
            "iteration:  34 loss: 0.19375800\n",
            "iteration:  35 loss: 1.27919161\n",
            "iteration:  36 loss: 0.14176251\n",
            "iteration:  37 loss: 0.59213531\n",
            "iteration:  38 loss: 0.42353886\n",
            "iteration:  39 loss: 1.66586292\n",
            "iteration:  40 loss: 0.64451134\n",
            "iteration:  41 loss: 0.32626039\n",
            "iteration:  42 loss: 0.02662011\n",
            "iteration:  43 loss: 0.24059416\n",
            "iteration:  44 loss: 1.10896289\n",
            "iteration:  45 loss: 0.15435417\n",
            "iteration:  46 loss: 0.16501945\n",
            "iteration:  47 loss: 1.61437619\n",
            "iteration:  48 loss: 0.66144180\n",
            "iteration:  49 loss: 1.08329844\n",
            "iteration:  50 loss: 0.37768292\n",
            "iteration:  51 loss: 0.68498027\n",
            "iteration:  52 loss: 1.83815026\n",
            "iteration:  53 loss: 0.41806534\n",
            "iteration:  54 loss: 0.86958164\n",
            "iteration:  55 loss: 1.63761914\n",
            "iteration:  56 loss: 0.19691260\n",
            "iteration:  57 loss: 0.70710450\n",
            "iteration:  58 loss: 0.30784923\n",
            "iteration:  59 loss: 1.03941762\n",
            "iteration:  60 loss: 0.18690957\n",
            "iteration:  61 loss: 0.08106687\n",
            "iteration:  62 loss: 0.39560425\n",
            "iteration:  63 loss: 0.40164793\n",
            "iteration:  64 loss: 0.05915425\n",
            "iteration:  65 loss: 0.05827565\n",
            "iteration:  66 loss: 0.26469991\n",
            "iteration:  67 loss: 0.24116492\n",
            "iteration:  68 loss: 0.44192579\n",
            "iteration:  69 loss: 0.15420578\n",
            "iteration:  70 loss: 0.89893723\n",
            "iteration:  71 loss: 0.03511857\n",
            "iteration:  72 loss: 0.14255744\n",
            "iteration:  73 loss: 0.35458258\n",
            "iteration:  74 loss: 0.01731829\n",
            "iteration:  75 loss: 0.37740988\n",
            "iteration:  76 loss: 0.25182801\n",
            "iteration:  77 loss: 0.63164979\n",
            "iteration:  78 loss: 0.84732598\n",
            "iteration:  79 loss: 0.09170532\n",
            "iteration:  80 loss: 0.07978961\n",
            "iteration:  81 loss: 0.15976445\n",
            "iteration:  82 loss: 1.33115602\n",
            "iteration:  83 loss: 0.16515245\n",
            "iteration:  84 loss: 0.24022295\n",
            "iteration:  85 loss: 0.95963138\n",
            "iteration:  86 loss: 0.09784940\n",
            "iteration:  87 loss: 0.55917722\n",
            "iteration:  88 loss: 0.12863277\n",
            "iteration:  89 loss: 0.76485908\n",
            "iteration:  90 loss: 0.07370546\n",
            "iteration:  91 loss: 1.52634430\n",
            "iteration:  92 loss: 0.29138333\n",
            "iteration:  93 loss: 0.05753365\n",
            "iteration:  94 loss: 0.42273766\n",
            "iteration:  95 loss: 0.53650540\n",
            "iteration:  96 loss: 0.29185927\n",
            "iteration:  97 loss: 0.63261867\n",
            "iteration:  98 loss: 0.45227993\n",
            "iteration:  99 loss: 0.10937995\n",
            "iteration: 100 loss: 0.12148612\n",
            "iteration: 101 loss: 0.33114022\n",
            "iteration: 102 loss: 0.27987120\n",
            "iteration: 103 loss: 0.05122590\n",
            "iteration: 104 loss: 0.31514549\n",
            "iteration: 105 loss: 1.48719490\n",
            "iteration: 106 loss: 0.91913348\n",
            "iteration: 107 loss: 0.08408982\n",
            "iteration: 108 loss: 2.02014208\n",
            "iteration: 109 loss: 0.03609764\n",
            "iteration: 110 loss: 0.58399647\n",
            "iteration: 111 loss: 0.97804648\n",
            "iteration: 112 loss: 0.29831073\n",
            "iteration: 113 loss: 1.73961210\n",
            "iteration: 114 loss: 0.51824015\n",
            "iteration: 115 loss: 0.86472201\n",
            "iteration: 116 loss: 1.04405391\n",
            "iteration: 117 loss: 0.19608553\n",
            "iteration: 118 loss: 0.99088752\n",
            "iteration: 119 loss: 0.74653125\n",
            "iteration: 120 loss: 1.34810448\n",
            "iteration: 121 loss: 0.73765051\n",
            "iteration: 122 loss: 0.66580021\n",
            "iteration: 123 loss: 0.08823251\n",
            "iteration: 124 loss: 1.10155678\n",
            "iteration: 125 loss: 0.61480314\n",
            "iteration: 126 loss: 0.26334542\n",
            "iteration: 127 loss: 0.35021561\n",
            "iteration: 128 loss: 0.57458609\n",
            "iteration: 129 loss: 0.13951091\n",
            "iteration: 130 loss: 0.19177076\n",
            "iteration: 131 loss: 0.12003127\n",
            "iteration: 132 loss: 0.96904224\n",
            "iteration: 133 loss: 0.46379420\n",
            "iteration: 134 loss: 0.17383230\n",
            "iteration: 135 loss: 0.91030645\n",
            "iteration: 136 loss: 0.39249471\n",
            "iteration: 137 loss: 0.35660797\n",
            "iteration: 138 loss: 0.10067499\n",
            "iteration: 139 loss: 0.29102272\n",
            "iteration: 140 loss: 1.41408253\n",
            "iteration: 141 loss: 0.46879852\n",
            "iteration: 142 loss: 1.27903605\n",
            "iteration: 143 loss: 0.22966343\n",
            "iteration: 144 loss: 0.34740490\n",
            "iteration: 145 loss: 0.70241457\n",
            "iteration: 146 loss: 0.54133081\n",
            "iteration: 147 loss: 0.02738058\n",
            "iteration: 148 loss: 0.08090689\n",
            "iteration: 149 loss: 0.11987008\n",
            "iteration: 150 loss: 0.75028664\n",
            "iteration: 151 loss: 1.67859495\n",
            "iteration: 152 loss: 0.43494731\n",
            "iteration: 153 loss: 0.53559679\n",
            "iteration: 154 loss: 0.62069148\n",
            "iteration: 155 loss: 0.40656894\n",
            "iteration: 156 loss: 0.21999897\n",
            "iteration: 157 loss: 0.39686275\n",
            "iteration: 158 loss: 0.41792920\n",
            "iteration: 159 loss: 0.17877020\n",
            "iteration: 160 loss: 0.36077538\n",
            "iteration: 161 loss: 0.13437146\n",
            "iteration: 162 loss: 1.37901151\n",
            "iteration: 163 loss: 0.08772592\n",
            "iteration: 164 loss: 0.19722915\n",
            "iteration: 165 loss: 1.41547322\n",
            "iteration: 166 loss: 2.47377491\n",
            "iteration: 167 loss: 0.53098834\n",
            "iteration: 168 loss: 0.58481950\n",
            "iteration: 169 loss: 0.30987886\n",
            "iteration: 170 loss: 2.28832865\n",
            "iteration: 171 loss: 0.06882407\n",
            "iteration: 172 loss: 0.11104207\n",
            "iteration: 173 loss: 1.38485253\n",
            "iteration: 174 loss: 0.19293989\n",
            "iteration: 175 loss: 1.34516263\n",
            "iteration: 176 loss: 0.62019706\n",
            "iteration: 177 loss: 0.68723154\n",
            "iteration: 178 loss: 0.41452983\n",
            "iteration: 179 loss: 0.28178242\n",
            "iteration: 180 loss: 0.52811676\n",
            "iteration: 181 loss: 0.08440742\n",
            "iteration: 182 loss: 1.64016998\n",
            "iteration: 183 loss: 0.18138465\n",
            "iteration: 184 loss: 0.54561549\n",
            "iteration: 185 loss: 2.04901624\n",
            "iteration: 186 loss: 0.80364537\n",
            "iteration: 187 loss: 0.95585120\n",
            "iteration: 188 loss: 0.10534413\n",
            "iteration: 189 loss: 0.07731809\n",
            "iteration: 190 loss: 0.24752322\n",
            "iteration: 191 loss: 0.35226229\n",
            "iteration: 192 loss: 0.03929338\n",
            "iteration: 193 loss: 0.14904408\n",
            "iteration: 194 loss: 1.07841527\n",
            "iteration: 195 loss: 0.25297168\n",
            "iteration: 196 loss: 0.58519185\n",
            "iteration: 197 loss: 0.49063089\n",
            "iteration: 198 loss: 0.55016345\n",
            "iteration: 199 loss: 0.47972518\n",
            "epoch:  85 mean loss training: 0.58420008\n",
            "epoch:  85 mean loss validation: 0.82255936\n",
            "iteration:   0 loss: 0.02521616\n",
            "iteration:   1 loss: 0.54005998\n",
            "iteration:   2 loss: 0.32761016\n",
            "iteration:   3 loss: 0.29251325\n",
            "iteration:   4 loss: 0.41128087\n",
            "iteration:   5 loss: 0.15936530\n",
            "iteration:   6 loss: 0.38355529\n",
            "iteration:   7 loss: 0.19914642\n",
            "iteration:   8 loss: 1.40594578\n",
            "iteration:   9 loss: 1.68968391\n",
            "iteration:  10 loss: 0.10443735\n",
            "iteration:  11 loss: 0.09796701\n",
            "iteration:  12 loss: 0.71352583\n",
            "iteration:  13 loss: 1.39326417\n",
            "iteration:  14 loss: 0.02617623\n",
            "iteration:  15 loss: 0.50318342\n",
            "iteration:  16 loss: 0.68958265\n",
            "iteration:  17 loss: 1.11444771\n",
            "iteration:  18 loss: 0.33634040\n",
            "iteration:  19 loss: 0.30995652\n",
            "iteration:  20 loss: 1.15079522\n",
            "iteration:  21 loss: 1.83477569\n",
            "iteration:  22 loss: 0.08026534\n",
            "iteration:  23 loss: 1.07840252\n",
            "iteration:  24 loss: 0.05451318\n",
            "iteration:  25 loss: 0.10647140\n",
            "iteration:  26 loss: 1.80861282\n",
            "iteration:  27 loss: 1.09134257\n",
            "iteration:  28 loss: 0.18204182\n",
            "iteration:  29 loss: 0.79786378\n",
            "iteration:  30 loss: 0.57013553\n",
            "iteration:  31 loss: 0.53544080\n",
            "iteration:  32 loss: 0.18304634\n",
            "iteration:  33 loss: 0.08874785\n",
            "iteration:  34 loss: 0.48621765\n",
            "iteration:  35 loss: 0.28469285\n",
            "iteration:  36 loss: 1.68036020\n",
            "iteration:  37 loss: 0.16984719\n",
            "iteration:  38 loss: 1.31426275\n",
            "iteration:  39 loss: 0.68231237\n",
            "iteration:  40 loss: 0.33738667\n",
            "iteration:  41 loss: 0.20724036\n",
            "iteration:  42 loss: 0.48287356\n",
            "iteration:  43 loss: 0.65373063\n",
            "iteration:  44 loss: 0.24967365\n",
            "iteration:  45 loss: 0.60627496\n",
            "iteration:  46 loss: 0.05564964\n",
            "iteration:  47 loss: 0.22102247\n",
            "iteration:  48 loss: 0.40102714\n",
            "iteration:  49 loss: 0.14931084\n",
            "iteration:  50 loss: 0.32474828\n",
            "iteration:  51 loss: 0.70780945\n",
            "iteration:  52 loss: 0.18181808\n",
            "iteration:  53 loss: 0.16725108\n",
            "iteration:  54 loss: 0.16500038\n",
            "iteration:  55 loss: 0.08929787\n",
            "iteration:  56 loss: 0.33228305\n",
            "iteration:  57 loss: 0.45805061\n",
            "iteration:  58 loss: 0.47217071\n",
            "iteration:  59 loss: 1.13832688\n",
            "iteration:  60 loss: 1.51107121\n",
            "iteration:  61 loss: 2.59746861\n",
            "iteration:  62 loss: 1.69017315\n",
            "iteration:  63 loss: 0.68193710\n",
            "iteration:  64 loss: 0.30696505\n",
            "iteration:  65 loss: 0.58935362\n",
            "iteration:  66 loss: 0.15928660\n",
            "iteration:  67 loss: 1.41814327\n",
            "iteration:  68 loss: 1.23311913\n",
            "iteration:  69 loss: 0.26898491\n",
            "iteration:  70 loss: 0.25136498\n",
            "iteration:  71 loss: 0.40545005\n",
            "iteration:  72 loss: 1.49060738\n",
            "iteration:  73 loss: 0.07238517\n",
            "iteration:  74 loss: 0.93903697\n",
            "iteration:  75 loss: 0.28423664\n",
            "iteration:  76 loss: 0.45346156\n",
            "iteration:  77 loss: 0.22782341\n",
            "iteration:  78 loss: 0.43063793\n",
            "iteration:  79 loss: 0.27726129\n",
            "iteration:  80 loss: 0.05300597\n",
            "iteration:  81 loss: 0.61492842\n",
            "iteration:  82 loss: 0.57087523\n",
            "iteration:  83 loss: 0.61889124\n",
            "iteration:  84 loss: 0.74504924\n",
            "iteration:  85 loss: 0.51227278\n",
            "iteration:  86 loss: 0.11503097\n",
            "iteration:  87 loss: 0.51220268\n",
            "iteration:  88 loss: 1.48556054\n",
            "iteration:  89 loss: 0.50094116\n",
            "iteration:  90 loss: 0.13235508\n",
            "iteration:  91 loss: 0.32234973\n",
            "iteration:  92 loss: 0.17504489\n",
            "iteration:  93 loss: 0.29352203\n",
            "iteration:  94 loss: 0.17950517\n",
            "iteration:  95 loss: 0.17770515\n",
            "iteration:  96 loss: 2.26499605\n",
            "iteration:  97 loss: 1.36031258\n",
            "iteration:  98 loss: 1.00124538\n",
            "iteration:  99 loss: 0.62815773\n",
            "iteration: 100 loss: 0.74840617\n",
            "iteration: 101 loss: 0.24058044\n",
            "iteration: 102 loss: 1.71544409\n",
            "iteration: 103 loss: 0.07695929\n",
            "iteration: 104 loss: 0.37156498\n",
            "iteration: 105 loss: 0.07829151\n",
            "iteration: 106 loss: 0.30917123\n",
            "iteration: 107 loss: 0.62223428\n",
            "iteration: 108 loss: 0.14664327\n",
            "iteration: 109 loss: 0.05868163\n",
            "iteration: 110 loss: 0.28403807\n",
            "iteration: 111 loss: 1.31365824\n",
            "iteration: 112 loss: 0.27928305\n",
            "iteration: 113 loss: 1.17134488\n",
            "iteration: 114 loss: 1.68826771\n",
            "iteration: 115 loss: 1.28017211\n",
            "iteration: 116 loss: 0.09644389\n",
            "iteration: 117 loss: 0.56608468\n",
            "iteration: 118 loss: 0.18227638\n",
            "iteration: 119 loss: 0.41209009\n",
            "iteration: 120 loss: 0.38605624\n",
            "iteration: 121 loss: 0.39414710\n",
            "iteration: 122 loss: 0.09478081\n",
            "iteration: 123 loss: 0.36339885\n",
            "iteration: 124 loss: 0.02872288\n",
            "iteration: 125 loss: 0.32301408\n",
            "iteration: 126 loss: 0.36144429\n",
            "iteration: 127 loss: 0.42215455\n",
            "iteration: 128 loss: 0.43523926\n",
            "iteration: 129 loss: 1.43298030\n",
            "iteration: 130 loss: 0.56958282\n",
            "iteration: 131 loss: 0.51480532\n",
            "iteration: 132 loss: 0.27389964\n",
            "iteration: 133 loss: 0.48040292\n",
            "iteration: 134 loss: 0.06452572\n",
            "iteration: 135 loss: 0.53611225\n",
            "iteration: 136 loss: 0.19328777\n",
            "iteration: 137 loss: 0.98134023\n",
            "iteration: 138 loss: 0.12214322\n",
            "iteration: 139 loss: 0.22913547\n",
            "iteration: 140 loss: 0.90308905\n",
            "iteration: 141 loss: 0.33242846\n",
            "iteration: 142 loss: 1.16552341\n",
            "iteration: 143 loss: 0.51883143\n",
            "iteration: 144 loss: 0.57089597\n",
            "iteration: 145 loss: 1.93539810\n",
            "iteration: 146 loss: 0.17930570\n",
            "iteration: 147 loss: 0.18459886\n",
            "iteration: 148 loss: 3.26407790\n",
            "iteration: 149 loss: 0.23640895\n",
            "iteration: 150 loss: 0.08916385\n",
            "iteration: 151 loss: 0.22856647\n",
            "iteration: 152 loss: 0.81956273\n",
            "iteration: 153 loss: 0.67150998\n",
            "iteration: 154 loss: 0.39554679\n",
            "iteration: 155 loss: 0.19535834\n",
            "iteration: 156 loss: 0.12715890\n",
            "iteration: 157 loss: 0.42031693\n",
            "iteration: 158 loss: 0.25479716\n",
            "iteration: 159 loss: 0.43732756\n",
            "iteration: 160 loss: 0.36146340\n",
            "iteration: 161 loss: 0.30340710\n",
            "iteration: 162 loss: 0.90687174\n",
            "iteration: 163 loss: 0.77132869\n",
            "iteration: 164 loss: 1.54179144\n",
            "iteration: 165 loss: 0.03865259\n",
            "iteration: 166 loss: 0.08852709\n",
            "iteration: 167 loss: 0.72111732\n",
            "iteration: 168 loss: 0.12668985\n",
            "iteration: 169 loss: 0.21003188\n",
            "iteration: 170 loss: 0.62271535\n",
            "iteration: 171 loss: 0.78437805\n",
            "iteration: 172 loss: 0.08686412\n",
            "iteration: 173 loss: 0.34520674\n",
            "iteration: 174 loss: 0.75773191\n",
            "iteration: 175 loss: 0.53108531\n",
            "iteration: 176 loss: 0.06383184\n",
            "iteration: 177 loss: 3.43473315\n",
            "iteration: 178 loss: 0.76983392\n",
            "iteration: 179 loss: 0.33725876\n",
            "iteration: 180 loss: 0.08082216\n",
            "iteration: 181 loss: 1.09777975\n",
            "iteration: 182 loss: 0.36659664\n",
            "iteration: 183 loss: 0.30864415\n",
            "iteration: 184 loss: 0.13217154\n",
            "iteration: 185 loss: 2.10108042\n",
            "iteration: 186 loss: 1.84911942\n",
            "iteration: 187 loss: 0.13027991\n",
            "iteration: 188 loss: 0.07613979\n",
            "iteration: 189 loss: 0.39940429\n",
            "iteration: 190 loss: 0.46119902\n",
            "iteration: 191 loss: 0.11355924\n",
            "iteration: 192 loss: 0.06387276\n",
            "iteration: 193 loss: 0.84044164\n",
            "iteration: 194 loss: 1.52398217\n",
            "iteration: 195 loss: 0.93297428\n",
            "iteration: 196 loss: 0.05851173\n",
            "iteration: 197 loss: 0.31993231\n",
            "iteration: 198 loss: 0.20180354\n",
            "iteration: 199 loss: 0.21536250\n",
            "epoch:  86 mean loss training: 0.58356476\n",
            "epoch:  86 mean loss validation: 0.88115042\n",
            "iteration:   0 loss: 2.13290596\n",
            "iteration:   1 loss: 0.10708563\n",
            "iteration:   2 loss: 0.34377083\n",
            "iteration:   3 loss: 0.35243136\n",
            "iteration:   4 loss: 0.18268354\n",
            "iteration:   5 loss: 0.13198334\n",
            "iteration:   6 loss: 0.17384239\n",
            "iteration:   7 loss: 1.47648585\n",
            "iteration:   8 loss: 0.37638336\n",
            "iteration:   9 loss: 0.97533149\n",
            "iteration:  10 loss: 0.76624799\n",
            "iteration:  11 loss: 1.14847326\n",
            "iteration:  12 loss: 1.19061971\n",
            "iteration:  13 loss: 1.77099979\n",
            "iteration:  14 loss: 0.36665308\n",
            "iteration:  15 loss: 0.83179086\n",
            "iteration:  16 loss: 0.06126701\n",
            "iteration:  17 loss: 0.49109480\n",
            "iteration:  18 loss: 0.43478149\n",
            "iteration:  19 loss: 1.67595327\n",
            "iteration:  20 loss: 0.74740559\n",
            "iteration:  21 loss: 0.20367068\n",
            "iteration:  22 loss: 0.30215657\n",
            "iteration:  23 loss: 1.00663793\n",
            "iteration:  24 loss: 0.18552873\n",
            "iteration:  25 loss: 0.67344463\n",
            "iteration:  26 loss: 0.45840296\n",
            "iteration:  27 loss: 0.13592198\n",
            "iteration:  28 loss: 0.23386367\n",
            "iteration:  29 loss: 0.09636164\n",
            "iteration:  30 loss: 0.33854020\n",
            "iteration:  31 loss: 0.64792937\n",
            "iteration:  32 loss: 0.99864817\n",
            "iteration:  33 loss: 0.84629315\n",
            "iteration:  34 loss: 0.49632046\n",
            "iteration:  35 loss: 0.55547994\n",
            "iteration:  36 loss: 0.19865359\n",
            "iteration:  37 loss: 0.24666870\n",
            "iteration:  38 loss: 0.98610789\n",
            "iteration:  39 loss: 0.22124508\n",
            "iteration:  40 loss: 1.62099075\n",
            "iteration:  41 loss: 1.16430938\n",
            "iteration:  42 loss: 0.41633481\n",
            "iteration:  43 loss: 0.46970469\n",
            "iteration:  44 loss: 0.16777515\n",
            "iteration:  45 loss: 0.19632232\n",
            "iteration:  46 loss: 0.24540842\n",
            "iteration:  47 loss: 0.37121585\n",
            "iteration:  48 loss: 0.20402101\n",
            "iteration:  49 loss: 0.52614385\n",
            "iteration:  50 loss: 0.43870977\n",
            "iteration:  51 loss: 0.20231223\n",
            "iteration:  52 loss: 0.48508584\n",
            "iteration:  53 loss: 0.79407781\n",
            "iteration:  54 loss: 0.06428926\n",
            "iteration:  55 loss: 0.03622650\n",
            "iteration:  56 loss: 1.26510179\n",
            "iteration:  57 loss: 0.64302295\n",
            "iteration:  58 loss: 1.24634624\n",
            "iteration:  59 loss: 1.45932114\n",
            "iteration:  60 loss: 0.12591690\n",
            "iteration:  61 loss: 0.37443447\n",
            "iteration:  62 loss: 0.39507627\n",
            "iteration:  63 loss: 0.35430539\n",
            "iteration:  64 loss: 0.29004419\n",
            "iteration:  65 loss: 0.27581781\n",
            "iteration:  66 loss: 0.11953712\n",
            "iteration:  67 loss: 0.07456832\n",
            "iteration:  68 loss: 0.08971548\n",
            "iteration:  69 loss: 0.10261402\n",
            "iteration:  70 loss: 0.31911767\n",
            "iteration:  71 loss: 0.28199103\n",
            "iteration:  72 loss: 0.19457494\n",
            "iteration:  73 loss: 0.46876428\n",
            "iteration:  74 loss: 0.81774676\n",
            "iteration:  75 loss: 1.16351342\n",
            "iteration:  76 loss: 0.97604412\n",
            "iteration:  77 loss: 0.65608120\n",
            "iteration:  78 loss: 1.16215742\n",
            "iteration:  79 loss: 0.88484782\n",
            "iteration:  80 loss: 0.08555414\n",
            "iteration:  81 loss: 0.60142827\n",
            "iteration:  82 loss: 0.48849455\n",
            "iteration:  83 loss: 0.36087662\n",
            "iteration:  84 loss: 0.72085214\n",
            "iteration:  85 loss: 0.06354181\n",
            "iteration:  86 loss: 1.81676352\n",
            "iteration:  87 loss: 0.25814685\n",
            "iteration:  88 loss: 0.14506792\n",
            "iteration:  89 loss: 0.29839528\n",
            "iteration:  90 loss: 0.40632585\n",
            "iteration:  91 loss: 0.20371132\n",
            "iteration:  92 loss: 0.60623479\n",
            "iteration:  93 loss: 1.67785132\n",
            "iteration:  94 loss: 0.36097565\n",
            "iteration:  95 loss: 0.24169448\n",
            "iteration:  96 loss: 0.16879578\n",
            "iteration:  97 loss: 1.97478807\n",
            "iteration:  98 loss: 0.49735498\n",
            "iteration:  99 loss: 0.75244844\n",
            "iteration: 100 loss: 1.27357197\n",
            "iteration: 101 loss: 0.44344756\n",
            "iteration: 102 loss: 0.24965522\n",
            "iteration: 103 loss: 0.18873218\n",
            "iteration: 104 loss: 0.10113236\n",
            "iteration: 105 loss: 0.20757347\n",
            "iteration: 106 loss: 0.37446105\n",
            "iteration: 107 loss: 0.13255893\n",
            "iteration: 108 loss: 0.14979361\n",
            "iteration: 109 loss: 0.07867605\n",
            "iteration: 110 loss: 0.36246049\n",
            "iteration: 111 loss: 1.47676623\n",
            "iteration: 112 loss: 0.47448957\n",
            "iteration: 113 loss: 0.44367191\n",
            "iteration: 114 loss: 0.20079958\n",
            "iteration: 115 loss: 3.20479155\n",
            "iteration: 116 loss: 0.17065255\n",
            "iteration: 117 loss: 0.16615683\n",
            "iteration: 118 loss: 0.21229962\n",
            "iteration: 119 loss: 0.82213789\n",
            "iteration: 120 loss: 0.24315053\n",
            "iteration: 121 loss: 0.56514436\n",
            "iteration: 122 loss: 0.30501482\n",
            "iteration: 123 loss: 0.52195901\n",
            "iteration: 124 loss: 0.17646514\n",
            "iteration: 125 loss: 0.13784279\n",
            "iteration: 126 loss: 0.16859902\n",
            "iteration: 127 loss: 1.28112650\n",
            "iteration: 128 loss: 0.11064329\n",
            "iteration: 129 loss: 1.08230579\n",
            "iteration: 130 loss: 0.42851532\n",
            "iteration: 131 loss: 0.05375028\n",
            "iteration: 132 loss: 0.65930772\n",
            "iteration: 133 loss: 0.04088302\n",
            "iteration: 134 loss: 0.23785719\n",
            "iteration: 135 loss: 0.12245230\n",
            "iteration: 136 loss: 0.27210221\n",
            "iteration: 137 loss: 0.29269910\n",
            "iteration: 138 loss: 0.23941475\n",
            "iteration: 139 loss: 2.26238918\n",
            "iteration: 140 loss: 0.18488903\n",
            "iteration: 141 loss: 1.17929673\n",
            "iteration: 142 loss: 0.48832697\n",
            "iteration: 143 loss: 0.76682162\n",
            "iteration: 144 loss: 0.74155098\n",
            "iteration: 145 loss: 0.81761426\n",
            "iteration: 146 loss: 0.09778909\n",
            "iteration: 147 loss: 0.19130857\n",
            "iteration: 148 loss: 0.37574667\n",
            "iteration: 149 loss: 0.64326328\n",
            "iteration: 150 loss: 1.11371994\n",
            "iteration: 151 loss: 0.29865143\n",
            "iteration: 152 loss: 0.52958971\n",
            "iteration: 153 loss: 0.21864417\n",
            "iteration: 154 loss: 0.56340575\n",
            "iteration: 155 loss: 0.50906402\n",
            "iteration: 156 loss: 0.47015825\n",
            "iteration: 157 loss: 0.36406139\n",
            "iteration: 158 loss: 0.95492452\n",
            "iteration: 159 loss: 0.04671257\n",
            "iteration: 160 loss: 1.80746305\n",
            "iteration: 161 loss: 1.27466214\n",
            "iteration: 162 loss: 0.28921494\n",
            "iteration: 163 loss: 0.05701850\n",
            "iteration: 164 loss: 0.35281563\n",
            "iteration: 165 loss: 0.49110162\n",
            "iteration: 166 loss: 0.21217252\n",
            "iteration: 167 loss: 0.56858617\n",
            "iteration: 168 loss: 2.29162478\n",
            "iteration: 169 loss: 0.39774263\n",
            "iteration: 170 loss: 0.24938144\n",
            "iteration: 171 loss: 0.17127672\n",
            "iteration: 172 loss: 0.05284533\n",
            "iteration: 173 loss: 2.21114254\n",
            "iteration: 174 loss: 0.49775323\n",
            "iteration: 175 loss: 0.28330776\n",
            "iteration: 176 loss: 0.92076409\n",
            "iteration: 177 loss: 0.27241436\n",
            "iteration: 178 loss: 0.32582200\n",
            "iteration: 179 loss: 1.11738884\n",
            "iteration: 180 loss: 0.79254615\n",
            "iteration: 181 loss: 0.38838243\n",
            "iteration: 182 loss: 0.06124978\n",
            "iteration: 183 loss: 0.15500291\n",
            "iteration: 184 loss: 0.18155946\n",
            "iteration: 185 loss: 0.19977959\n",
            "iteration: 186 loss: 1.68553138\n",
            "iteration: 187 loss: 0.61486912\n",
            "iteration: 188 loss: 0.42306140\n",
            "iteration: 189 loss: 0.33439729\n",
            "iteration: 190 loss: 1.31226909\n",
            "iteration: 191 loss: 1.77517295\n",
            "iteration: 192 loss: 0.11850113\n",
            "iteration: 193 loss: 0.95917517\n",
            "iteration: 194 loss: 0.54442459\n",
            "iteration: 195 loss: 1.27795601\n",
            "iteration: 196 loss: 1.99107373\n",
            "iteration: 197 loss: 0.11346820\n",
            "iteration: 198 loss: 2.57831287\n",
            "iteration: 199 loss: 0.09645814\n",
            "epoch:  87 mean loss training: 0.58690816\n",
            "epoch:  87 mean loss validation: 0.89404678\n",
            "iteration:   0 loss: 0.25460693\n",
            "iteration:   1 loss: 2.57519937\n",
            "iteration:   2 loss: 0.68052936\n",
            "iteration:   3 loss: 0.38040105\n",
            "iteration:   4 loss: 0.02845086\n",
            "iteration:   5 loss: 0.42441466\n",
            "iteration:   6 loss: 0.07838074\n",
            "iteration:   7 loss: 0.36458856\n",
            "iteration:   8 loss: 2.51341963\n",
            "iteration:   9 loss: 0.19056150\n",
            "iteration:  10 loss: 0.10748345\n",
            "iteration:  11 loss: 0.58993340\n",
            "iteration:  12 loss: 0.31188318\n",
            "iteration:  13 loss: 0.29253989\n",
            "iteration:  14 loss: 0.38620016\n",
            "iteration:  15 loss: 1.69842267\n",
            "iteration:  16 loss: 3.12423372\n",
            "iteration:  17 loss: 0.24671699\n",
            "iteration:  18 loss: 0.65403461\n",
            "iteration:  19 loss: 1.05797410\n",
            "iteration:  20 loss: 0.31494212\n",
            "iteration:  21 loss: 1.51899922\n",
            "iteration:  22 loss: 0.11745553\n",
            "iteration:  23 loss: 0.10713260\n",
            "iteration:  24 loss: 0.77594638\n",
            "iteration:  25 loss: 0.72414279\n",
            "iteration:  26 loss: 0.86154199\n",
            "iteration:  27 loss: 0.54600310\n",
            "iteration:  28 loss: 0.14408867\n",
            "iteration:  29 loss: 0.90359348\n",
            "iteration:  30 loss: 0.61169893\n",
            "iteration:  31 loss: 1.44335651\n",
            "iteration:  32 loss: 2.88018227\n",
            "iteration:  33 loss: 0.18551007\n",
            "iteration:  34 loss: 0.26560104\n",
            "iteration:  35 loss: 0.79420859\n",
            "iteration:  36 loss: 1.07487261\n",
            "iteration:  37 loss: 0.21511666\n",
            "iteration:  38 loss: 0.12990540\n",
            "iteration:  39 loss: 0.17663553\n",
            "iteration:  40 loss: 0.80655849\n",
            "iteration:  41 loss: 0.32351565\n",
            "iteration:  42 loss: 0.69791496\n",
            "iteration:  43 loss: 0.07059988\n",
            "iteration:  44 loss: 0.21355645\n",
            "iteration:  45 loss: 1.20175529\n",
            "iteration:  46 loss: 0.22206450\n",
            "iteration:  47 loss: 0.95684803\n",
            "iteration:  48 loss: 0.08774913\n",
            "iteration:  49 loss: 0.91371822\n",
            "iteration:  50 loss: 2.05023694\n",
            "iteration:  51 loss: 0.74126208\n",
            "iteration:  52 loss: 0.83633733\n",
            "iteration:  53 loss: 0.90490669\n",
            "iteration:  54 loss: 1.98759341\n",
            "iteration:  55 loss: 0.84240627\n",
            "iteration:  56 loss: 1.01187336\n",
            "iteration:  57 loss: 1.27142310\n",
            "iteration:  58 loss: 0.08773632\n",
            "iteration:  59 loss: 0.26736724\n",
            "iteration:  60 loss: 0.24386561\n",
            "iteration:  61 loss: 0.94737679\n",
            "iteration:  62 loss: 0.06644086\n",
            "iteration:  63 loss: 0.21414177\n",
            "iteration:  64 loss: 0.44080400\n",
            "iteration:  65 loss: 0.02280792\n",
            "iteration:  66 loss: 0.49285662\n",
            "iteration:  67 loss: 0.42899495\n",
            "iteration:  68 loss: 0.30888698\n",
            "iteration:  69 loss: 0.31332749\n",
            "iteration:  70 loss: 0.15838666\n",
            "iteration:  71 loss: 0.07699690\n",
            "iteration:  72 loss: 0.45060280\n",
            "iteration:  73 loss: 0.33602262\n",
            "iteration:  74 loss: 0.03876117\n",
            "iteration:  75 loss: 1.33382607\n",
            "iteration:  76 loss: 0.29287821\n",
            "iteration:  77 loss: 0.21425569\n",
            "iteration:  78 loss: 0.42798477\n",
            "iteration:  79 loss: 0.38386574\n",
            "iteration:  80 loss: 1.11398792\n",
            "iteration:  81 loss: 0.24473664\n",
            "iteration:  82 loss: 0.74312413\n",
            "iteration:  83 loss: 0.60070133\n",
            "iteration:  84 loss: 0.95568126\n",
            "iteration:  85 loss: 1.57616568\n",
            "iteration:  86 loss: 0.61293596\n",
            "iteration:  87 loss: 0.05511699\n",
            "iteration:  88 loss: 0.66332340\n",
            "iteration:  89 loss: 0.46671438\n",
            "iteration:  90 loss: 0.49538973\n",
            "iteration:  91 loss: 0.14784253\n",
            "iteration:  92 loss: 1.66462815\n",
            "iteration:  93 loss: 0.09639008\n",
            "iteration:  94 loss: 1.39696395\n",
            "iteration:  95 loss: 0.53497207\n",
            "iteration:  96 loss: 0.54022300\n",
            "iteration:  97 loss: 0.92811489\n",
            "iteration:  98 loss: 1.51018810\n",
            "iteration:  99 loss: 0.08148026\n",
            "iteration: 100 loss: 0.01961242\n",
            "iteration: 101 loss: 0.31105015\n",
            "iteration: 102 loss: 0.59418482\n",
            "iteration: 103 loss: 0.18072487\n",
            "iteration: 104 loss: 0.18342854\n",
            "iteration: 105 loss: 0.30729243\n",
            "iteration: 106 loss: 1.49720156\n",
            "iteration: 107 loss: 0.08889282\n",
            "iteration: 108 loss: 0.29998404\n",
            "iteration: 109 loss: 1.02067196\n",
            "iteration: 110 loss: 0.03037870\n",
            "iteration: 111 loss: 0.20899199\n",
            "iteration: 112 loss: 0.08793684\n",
            "iteration: 113 loss: 0.42094862\n",
            "iteration: 114 loss: 1.39778256\n",
            "iteration: 115 loss: 0.41435093\n",
            "iteration: 116 loss: 0.47433612\n",
            "iteration: 117 loss: 1.33693421\n",
            "iteration: 118 loss: 0.10563031\n",
            "iteration: 119 loss: 0.36522520\n",
            "iteration: 120 loss: 0.14866735\n",
            "iteration: 121 loss: 1.55611587\n",
            "iteration: 122 loss: 0.22322430\n",
            "iteration: 123 loss: 0.89390969\n",
            "iteration: 124 loss: 0.09474032\n",
            "iteration: 125 loss: 0.56071496\n",
            "iteration: 126 loss: 0.23240907\n",
            "iteration: 127 loss: 0.65723604\n",
            "iteration: 128 loss: 0.17249429\n",
            "iteration: 129 loss: 0.89622885\n",
            "iteration: 130 loss: 0.68904459\n",
            "iteration: 131 loss: 0.06876752\n",
            "iteration: 132 loss: 0.43537068\n",
            "iteration: 133 loss: 0.29624048\n",
            "iteration: 134 loss: 0.31236717\n",
            "iteration: 135 loss: 1.06793213\n",
            "iteration: 136 loss: 0.43016526\n",
            "iteration: 137 loss: 1.43038046\n",
            "iteration: 138 loss: 2.16989017\n",
            "iteration: 139 loss: 0.30756950\n",
            "iteration: 140 loss: 0.09557789\n",
            "iteration: 141 loss: 0.90424424\n",
            "iteration: 142 loss: 0.27940920\n",
            "iteration: 143 loss: 0.34280613\n",
            "iteration: 144 loss: 0.59097111\n",
            "iteration: 145 loss: 0.10727899\n",
            "iteration: 146 loss: 0.22539395\n",
            "iteration: 147 loss: 0.65195608\n",
            "iteration: 148 loss: 0.39222103\n",
            "iteration: 149 loss: 1.35507727\n",
            "iteration: 150 loss: 0.34548572\n",
            "iteration: 151 loss: 0.04067467\n",
            "iteration: 152 loss: 0.48170236\n",
            "iteration: 153 loss: 0.11742250\n",
            "iteration: 154 loss: 1.08033001\n",
            "iteration: 155 loss: 0.71930170\n",
            "iteration: 156 loss: 0.35214150\n",
            "iteration: 157 loss: 0.34991732\n",
            "iteration: 158 loss: 1.02359509\n",
            "iteration: 159 loss: 0.04159976\n",
            "iteration: 160 loss: 0.22639835\n",
            "iteration: 161 loss: 0.82574946\n",
            "iteration: 162 loss: 0.25630614\n",
            "iteration: 163 loss: 0.56047314\n",
            "iteration: 164 loss: 0.24014103\n",
            "iteration: 165 loss: 0.76901132\n",
            "iteration: 166 loss: 0.17990443\n",
            "iteration: 167 loss: 0.90554595\n",
            "iteration: 168 loss: 0.50156122\n",
            "iteration: 169 loss: 0.06329185\n",
            "iteration: 170 loss: 0.05738282\n",
            "iteration: 171 loss: 0.34383303\n",
            "iteration: 172 loss: 0.81119013\n",
            "iteration: 173 loss: 1.00647032\n",
            "iteration: 174 loss: 1.27679992\n",
            "iteration: 175 loss: 0.76123345\n",
            "iteration: 176 loss: 0.48845023\n",
            "iteration: 177 loss: 0.06271635\n",
            "iteration: 178 loss: 0.89194161\n",
            "iteration: 179 loss: 0.03768712\n",
            "iteration: 180 loss: 0.41742915\n",
            "iteration: 181 loss: 0.59283161\n",
            "iteration: 182 loss: 1.29820800\n",
            "iteration: 183 loss: 0.04574700\n",
            "iteration: 184 loss: 0.61857265\n",
            "iteration: 185 loss: 0.62899220\n",
            "iteration: 186 loss: 0.38713077\n",
            "iteration: 187 loss: 0.19381815\n",
            "iteration: 188 loss: 0.33691874\n",
            "iteration: 189 loss: 0.18679297\n",
            "iteration: 190 loss: 1.14007068\n",
            "iteration: 191 loss: 0.43253556\n",
            "iteration: 192 loss: 0.13443215\n",
            "iteration: 193 loss: 0.05164736\n",
            "iteration: 194 loss: 0.04810176\n",
            "iteration: 195 loss: 0.06492713\n",
            "iteration: 196 loss: 0.41050023\n",
            "iteration: 197 loss: 0.08496506\n",
            "iteration: 198 loss: 0.10048187\n",
            "iteration: 199 loss: 0.20993844\n",
            "epoch:  88 mean loss training: 0.58002180\n",
            "epoch:  88 mean loss validation: 0.83634675\n",
            "iteration:   0 loss: 0.71972543\n",
            "iteration:   1 loss: 3.03600740\n",
            "iteration:   2 loss: 0.68440723\n",
            "iteration:   3 loss: 0.08832231\n",
            "iteration:   4 loss: 0.27285033\n",
            "iteration:   5 loss: 0.12356461\n",
            "iteration:   6 loss: 0.30524328\n",
            "iteration:   7 loss: 0.81182861\n",
            "iteration:   8 loss: 0.30722946\n",
            "iteration:   9 loss: 0.16660246\n",
            "iteration:  10 loss: 0.09346094\n",
            "iteration:  11 loss: 0.07215673\n",
            "iteration:  12 loss: 0.29375646\n",
            "iteration:  13 loss: 0.13515928\n",
            "iteration:  14 loss: 0.59634447\n",
            "iteration:  15 loss: 1.88464987\n",
            "iteration:  16 loss: 0.28489015\n",
            "iteration:  17 loss: 0.13932994\n",
            "iteration:  18 loss: 0.96566850\n",
            "iteration:  19 loss: 0.40734008\n",
            "iteration:  20 loss: 0.86204559\n",
            "iteration:  21 loss: 0.84571850\n",
            "iteration:  22 loss: 0.60611594\n",
            "iteration:  23 loss: 0.24482326\n",
            "iteration:  24 loss: 0.03008686\n",
            "iteration:  25 loss: 0.21911335\n",
            "iteration:  26 loss: 0.08902182\n",
            "iteration:  27 loss: 2.08630490\n",
            "iteration:  28 loss: 0.93343377\n",
            "iteration:  29 loss: 0.39277369\n",
            "iteration:  30 loss: 0.36207727\n",
            "iteration:  31 loss: 0.23767371\n",
            "iteration:  32 loss: 1.21328890\n",
            "iteration:  33 loss: 0.02121636\n",
            "iteration:  34 loss: 0.46066177\n",
            "iteration:  35 loss: 0.75000721\n",
            "iteration:  36 loss: 1.41174388\n",
            "iteration:  37 loss: 0.14193524\n",
            "iteration:  38 loss: 0.51297414\n",
            "iteration:  39 loss: 1.10442972\n",
            "iteration:  40 loss: 0.70710564\n",
            "iteration:  41 loss: 0.42323378\n",
            "iteration:  42 loss: 0.30388457\n",
            "iteration:  43 loss: 0.82010275\n",
            "iteration:  44 loss: 1.18480051\n",
            "iteration:  45 loss: 0.04960721\n",
            "iteration:  46 loss: 1.29713845\n",
            "iteration:  47 loss: 0.20143563\n",
            "iteration:  48 loss: 0.27869585\n",
            "iteration:  49 loss: 0.27171648\n",
            "iteration:  50 loss: 0.25675458\n",
            "iteration:  51 loss: 0.34607279\n",
            "iteration:  52 loss: 0.65949970\n",
            "iteration:  53 loss: 1.24848330\n",
            "iteration:  54 loss: 0.10733031\n",
            "iteration:  55 loss: 0.20939621\n",
            "iteration:  56 loss: 1.60212505\n",
            "iteration:  57 loss: 0.18281415\n",
            "iteration:  58 loss: 0.11668076\n",
            "iteration:  59 loss: 0.26939648\n",
            "iteration:  60 loss: 0.23274161\n",
            "iteration:  61 loss: 0.22325146\n",
            "iteration:  62 loss: 2.49527407\n",
            "iteration:  63 loss: 0.23869148\n",
            "iteration:  64 loss: 0.20289405\n",
            "iteration:  65 loss: 0.45192030\n",
            "iteration:  66 loss: 0.05633495\n",
            "iteration:  67 loss: 0.29867199\n",
            "iteration:  68 loss: 0.12136318\n",
            "iteration:  69 loss: 0.34811437\n",
            "iteration:  70 loss: 0.45313448\n",
            "iteration:  71 loss: 0.43177134\n",
            "iteration:  72 loss: 0.13608694\n",
            "iteration:  73 loss: 0.42929298\n",
            "iteration:  74 loss: 0.09211766\n",
            "iteration:  75 loss: 0.10609932\n",
            "iteration:  76 loss: 0.25201333\n",
            "iteration:  77 loss: 0.10002492\n",
            "iteration:  78 loss: 1.36364138\n",
            "iteration:  79 loss: 0.87211925\n",
            "iteration:  80 loss: 0.08036698\n",
            "iteration:  81 loss: 0.32588506\n",
            "iteration:  82 loss: 0.54175794\n",
            "iteration:  83 loss: 0.43729085\n",
            "iteration:  84 loss: 0.74322850\n",
            "iteration:  85 loss: 0.11627666\n",
            "iteration:  86 loss: 0.56275433\n",
            "iteration:  87 loss: 2.33280921\n",
            "iteration:  88 loss: 1.15826035\n",
            "iteration:  89 loss: 0.08627120\n",
            "iteration:  90 loss: 0.25132394\n",
            "iteration:  91 loss: 0.56102061\n",
            "iteration:  92 loss: 0.18365632\n",
            "iteration:  93 loss: 0.42086062\n",
            "iteration:  94 loss: 3.54991198\n",
            "iteration:  95 loss: 1.20641136\n",
            "iteration:  96 loss: 0.32796335\n",
            "iteration:  97 loss: 0.96066707\n",
            "iteration:  98 loss: 2.46251655\n",
            "iteration:  99 loss: 0.14406075\n",
            "iteration: 100 loss: 0.37714708\n",
            "iteration: 101 loss: 1.00906253\n",
            "iteration: 102 loss: 3.24361992\n",
            "iteration: 103 loss: 0.37354386\n",
            "iteration: 104 loss: 0.63035232\n",
            "iteration: 105 loss: 1.04546738\n",
            "iteration: 106 loss: 0.62195259\n",
            "iteration: 107 loss: 0.20123081\n",
            "iteration: 108 loss: 1.79935336\n",
            "iteration: 109 loss: 0.08013451\n",
            "iteration: 110 loss: 0.79275405\n",
            "iteration: 111 loss: 1.15479255\n",
            "iteration: 112 loss: 0.09624610\n",
            "iteration: 113 loss: 1.37503684\n",
            "iteration: 114 loss: 0.33034280\n",
            "iteration: 115 loss: 0.75609970\n",
            "iteration: 116 loss: 0.90745640\n",
            "iteration: 117 loss: 1.85336006\n",
            "iteration: 118 loss: 1.19082403\n",
            "iteration: 119 loss: 0.40875518\n",
            "iteration: 120 loss: 1.06846929\n",
            "iteration: 121 loss: 2.09332943\n",
            "iteration: 122 loss: 0.26522213\n",
            "iteration: 123 loss: 0.57804471\n",
            "iteration: 124 loss: 0.15994346\n",
            "iteration: 125 loss: 0.43131021\n",
            "iteration: 126 loss: 0.87038201\n",
            "iteration: 127 loss: 0.41930455\n",
            "iteration: 128 loss: 0.66242200\n",
            "iteration: 129 loss: 0.35508263\n",
            "iteration: 130 loss: 0.43004680\n",
            "iteration: 131 loss: 0.11761519\n",
            "iteration: 132 loss: 0.20477271\n",
            "iteration: 133 loss: 0.53967780\n",
            "iteration: 134 loss: 0.19856173\n",
            "iteration: 135 loss: 0.32846403\n",
            "iteration: 136 loss: 0.81691420\n",
            "iteration: 137 loss: 0.25021064\n",
            "iteration: 138 loss: 0.16731000\n",
            "iteration: 139 loss: 0.69966948\n",
            "iteration: 140 loss: 0.27071017\n",
            "iteration: 141 loss: 0.43589112\n",
            "iteration: 142 loss: 0.40427977\n",
            "iteration: 143 loss: 1.33169723\n",
            "iteration: 144 loss: 0.45217940\n",
            "iteration: 145 loss: 0.28485048\n",
            "iteration: 146 loss: 0.59068280\n",
            "iteration: 147 loss: 0.45122859\n",
            "iteration: 148 loss: 0.09346315\n",
            "iteration: 149 loss: 0.10338586\n",
            "iteration: 150 loss: 0.06191181\n",
            "iteration: 151 loss: 0.22502743\n",
            "iteration: 152 loss: 1.89039218\n",
            "iteration: 153 loss: 0.91207880\n",
            "iteration: 154 loss: 0.02966169\n",
            "iteration: 155 loss: 0.34315616\n",
            "iteration: 156 loss: 0.25587371\n",
            "iteration: 157 loss: 0.16965775\n",
            "iteration: 158 loss: 0.96347398\n",
            "iteration: 159 loss: 0.19042262\n",
            "iteration: 160 loss: 0.47051054\n",
            "iteration: 161 loss: 0.38494086\n",
            "iteration: 162 loss: 0.10897320\n",
            "iteration: 163 loss: 1.15468788\n",
            "iteration: 164 loss: 0.54378945\n",
            "iteration: 165 loss: 0.47541770\n",
            "iteration: 166 loss: 1.87365866\n",
            "iteration: 167 loss: 0.54341620\n",
            "iteration: 168 loss: 1.05649304\n",
            "iteration: 169 loss: 0.67096806\n",
            "iteration: 170 loss: 0.10465612\n",
            "iteration: 171 loss: 0.55891442\n",
            "iteration: 172 loss: 0.30593652\n",
            "iteration: 173 loss: 0.26993340\n",
            "iteration: 174 loss: 0.66290414\n",
            "iteration: 175 loss: 0.05363054\n",
            "iteration: 176 loss: 0.72126848\n",
            "iteration: 177 loss: 1.11880744\n",
            "iteration: 178 loss: 0.18274428\n",
            "iteration: 179 loss: 0.13811418\n",
            "iteration: 180 loss: 1.24150026\n",
            "iteration: 181 loss: 0.12663679\n",
            "iteration: 182 loss: 0.25876525\n",
            "iteration: 183 loss: 1.86279356\n",
            "iteration: 184 loss: 0.40679350\n",
            "iteration: 185 loss: 0.44906673\n",
            "iteration: 186 loss: 0.15555951\n",
            "iteration: 187 loss: 1.75752068\n",
            "iteration: 188 loss: 0.60404396\n",
            "iteration: 189 loss: 0.11263517\n",
            "iteration: 190 loss: 0.53498465\n",
            "iteration: 191 loss: 0.24118131\n",
            "iteration: 192 loss: 2.08573580\n",
            "iteration: 193 loss: 0.47603258\n",
            "iteration: 194 loss: 0.80163372\n",
            "iteration: 195 loss: 0.97202975\n",
            "iteration: 196 loss: 0.63420552\n",
            "iteration: 197 loss: 3.32518053\n",
            "iteration: 198 loss: 0.14328310\n",
            "iteration: 199 loss: 0.36428142\n",
            "epoch:  89 mean loss training: 0.63063526\n",
            "epoch:  89 mean loss validation: 0.96458447\n",
            "iteration:   0 loss: 0.45859161\n",
            "iteration:   1 loss: 0.19811760\n",
            "iteration:   2 loss: 0.74945027\n",
            "iteration:   3 loss: 0.61110491\n",
            "iteration:   4 loss: 1.01293802\n",
            "iteration:   5 loss: 0.23864470\n",
            "iteration:   6 loss: 0.11712932\n",
            "iteration:   7 loss: 1.15833676\n",
            "iteration:   8 loss: 1.36626458\n",
            "iteration:   9 loss: 0.11831652\n",
            "iteration:  10 loss: 0.97587383\n",
            "iteration:  11 loss: 0.32385886\n",
            "iteration:  12 loss: 0.08791661\n",
            "iteration:  13 loss: 0.68872267\n",
            "iteration:  14 loss: 0.06810697\n",
            "iteration:  15 loss: 0.05263396\n",
            "iteration:  16 loss: 0.34214133\n",
            "iteration:  17 loss: 0.51307684\n",
            "iteration:  18 loss: 1.74540079\n",
            "iteration:  19 loss: 0.74619710\n",
            "iteration:  20 loss: 0.51778942\n",
            "iteration:  21 loss: 0.27105078\n",
            "iteration:  22 loss: 0.68140197\n",
            "iteration:  23 loss: 1.47840798\n",
            "iteration:  24 loss: 2.05814147\n",
            "iteration:  25 loss: 0.17658956\n",
            "iteration:  26 loss: 0.28302258\n",
            "iteration:  27 loss: 0.46519148\n",
            "iteration:  28 loss: 0.12877192\n",
            "iteration:  29 loss: 0.52713639\n",
            "iteration:  30 loss: 0.53194219\n",
            "iteration:  31 loss: 0.37551028\n",
            "iteration:  32 loss: 1.27138031\n",
            "iteration:  33 loss: 0.82017058\n",
            "iteration:  34 loss: 1.12269688\n",
            "iteration:  35 loss: 0.70664841\n",
            "iteration:  36 loss: 0.16960536\n",
            "iteration:  37 loss: 0.60849446\n",
            "iteration:  38 loss: 0.15183108\n",
            "iteration:  39 loss: 0.09942188\n",
            "iteration:  40 loss: 1.32353795\n",
            "iteration:  41 loss: 0.07975446\n",
            "iteration:  42 loss: 0.05489607\n",
            "iteration:  43 loss: 0.54416913\n",
            "iteration:  44 loss: 0.14983512\n",
            "iteration:  45 loss: 0.55541986\n",
            "iteration:  46 loss: 1.15424383\n",
            "iteration:  47 loss: 0.33153093\n",
            "iteration:  48 loss: 0.16532102\n",
            "iteration:  49 loss: 0.34432811\n",
            "iteration:  50 loss: 0.67960668\n",
            "iteration:  51 loss: 1.23833287\n",
            "iteration:  52 loss: 0.42684549\n",
            "iteration:  53 loss: 0.47240594\n",
            "iteration:  54 loss: 0.84092969\n",
            "iteration:  55 loss: 2.62533879\n",
            "iteration:  56 loss: 0.34653127\n",
            "iteration:  57 loss: 0.88141328\n",
            "iteration:  58 loss: 1.21132994\n",
            "iteration:  59 loss: 0.09794906\n",
            "iteration:  60 loss: 0.93489343\n",
            "iteration:  61 loss: 0.56435317\n",
            "iteration:  62 loss: 0.48769021\n",
            "iteration:  63 loss: 0.70309949\n",
            "iteration:  64 loss: 0.26851779\n",
            "iteration:  65 loss: 0.35555637\n",
            "iteration:  66 loss: 0.73989636\n",
            "iteration:  67 loss: 0.64674926\n",
            "iteration:  68 loss: 0.89021057\n",
            "iteration:  69 loss: 0.72972554\n",
            "iteration:  70 loss: 0.14423145\n",
            "iteration:  71 loss: 0.27261087\n",
            "iteration:  72 loss: 0.35509923\n",
            "iteration:  73 loss: 0.57640302\n",
            "iteration:  74 loss: 0.11416547\n",
            "iteration:  75 loss: 2.07337570\n",
            "iteration:  76 loss: 0.12753671\n",
            "iteration:  77 loss: 0.19803563\n",
            "iteration:  78 loss: 0.35217205\n",
            "iteration:  79 loss: 0.21196070\n",
            "iteration:  80 loss: 0.73811960\n",
            "iteration:  81 loss: 2.34532452\n",
            "iteration:  82 loss: 1.76642823\n",
            "iteration:  83 loss: 0.58296239\n",
            "iteration:  84 loss: 0.61616790\n",
            "iteration:  85 loss: 0.33374646\n",
            "iteration:  86 loss: 0.79339528\n",
            "iteration:  87 loss: 0.30040759\n",
            "iteration:  88 loss: 0.08101320\n",
            "iteration:  89 loss: 0.31892025\n",
            "iteration:  90 loss: 0.05977489\n",
            "iteration:  91 loss: 0.52031934\n",
            "iteration:  92 loss: 0.06348599\n",
            "iteration:  93 loss: 0.56650317\n",
            "iteration:  94 loss: 0.26514784\n",
            "iteration:  95 loss: 0.04394849\n",
            "iteration:  96 loss: 0.06285542\n",
            "iteration:  97 loss: 0.32679081\n",
            "iteration:  98 loss: 0.17120533\n",
            "iteration:  99 loss: 0.20129967\n",
            "iteration: 100 loss: 0.08537304\n",
            "iteration: 101 loss: 0.14131862\n",
            "iteration: 102 loss: 2.91637063\n",
            "iteration: 103 loss: 0.25688982\n",
            "iteration: 104 loss: 0.16976902\n",
            "iteration: 105 loss: 0.07227157\n",
            "iteration: 106 loss: 0.05896647\n",
            "iteration: 107 loss: 0.40251127\n",
            "iteration: 108 loss: 0.56723386\n",
            "iteration: 109 loss: 0.05432710\n",
            "iteration: 110 loss: 0.32543334\n",
            "iteration: 111 loss: 0.38855177\n",
            "iteration: 112 loss: 0.61587840\n",
            "iteration: 113 loss: 0.56872112\n",
            "iteration: 114 loss: 1.48206890\n",
            "iteration: 115 loss: 0.35360560\n",
            "iteration: 116 loss: 0.48984751\n",
            "iteration: 117 loss: 0.33785236\n",
            "iteration: 118 loss: 0.16098185\n",
            "iteration: 119 loss: 0.22371063\n",
            "iteration: 120 loss: 0.75082427\n",
            "iteration: 121 loss: 1.53176272\n",
            "iteration: 122 loss: 0.07545800\n",
            "iteration: 123 loss: 0.30666369\n",
            "iteration: 124 loss: 0.57301539\n",
            "iteration: 125 loss: 0.50813806\n",
            "iteration: 126 loss: 0.37145388\n",
            "iteration: 127 loss: 0.28771293\n",
            "iteration: 128 loss: 2.05004883\n",
            "iteration: 129 loss: 0.04555555\n",
            "iteration: 130 loss: 0.52664053\n",
            "iteration: 131 loss: 1.96744263\n",
            "iteration: 132 loss: 0.50882769\n",
            "iteration: 133 loss: 1.43333900\n",
            "iteration: 134 loss: 0.74878442\n",
            "iteration: 135 loss: 0.72934681\n",
            "iteration: 136 loss: 0.55559355\n",
            "iteration: 137 loss: 0.17367807\n",
            "iteration: 138 loss: 2.09467363\n",
            "iteration: 139 loss: 0.76754707\n",
            "iteration: 140 loss: 1.92311859\n",
            "iteration: 141 loss: 1.37025559\n",
            "iteration: 142 loss: 0.10989112\n",
            "iteration: 143 loss: 1.73690903\n",
            "iteration: 144 loss: 0.64913535\n",
            "iteration: 145 loss: 0.07146873\n",
            "iteration: 146 loss: 0.06455749\n",
            "iteration: 147 loss: 0.25410730\n",
            "iteration: 148 loss: 1.24826217\n",
            "iteration: 149 loss: 0.15439570\n",
            "iteration: 150 loss: 0.59952062\n",
            "iteration: 151 loss: 0.86066270\n",
            "iteration: 152 loss: 0.59319752\n",
            "iteration: 153 loss: 0.20079680\n",
            "iteration: 154 loss: 0.03206130\n",
            "iteration: 155 loss: 0.10500803\n",
            "iteration: 156 loss: 0.64319462\n",
            "iteration: 157 loss: 0.47078368\n",
            "iteration: 158 loss: 0.48421791\n",
            "iteration: 159 loss: 0.68511999\n",
            "iteration: 160 loss: 0.50585032\n",
            "iteration: 161 loss: 0.35714328\n",
            "iteration: 162 loss: 0.49355844\n",
            "iteration: 163 loss: 0.20157677\n",
            "iteration: 164 loss: 1.37032270\n",
            "iteration: 165 loss: 0.75466663\n",
            "iteration: 166 loss: 0.21061572\n",
            "iteration: 167 loss: 0.59361738\n",
            "iteration: 168 loss: 1.96728981\n",
            "iteration: 169 loss: 0.83338463\n",
            "iteration: 170 loss: 0.16253133\n",
            "iteration: 171 loss: 0.47159398\n",
            "iteration: 172 loss: 0.07957065\n",
            "iteration: 173 loss: 0.41829944\n",
            "iteration: 174 loss: 0.04752674\n",
            "iteration: 175 loss: 0.16345038\n",
            "iteration: 176 loss: 0.18786635\n",
            "iteration: 177 loss: 0.98303574\n",
            "iteration: 178 loss: 1.14425623\n",
            "iteration: 179 loss: 0.78779811\n",
            "iteration: 180 loss: 0.81998593\n",
            "iteration: 181 loss: 0.46127081\n",
            "iteration: 182 loss: 0.20946647\n",
            "iteration: 183 loss: 0.17023386\n",
            "iteration: 184 loss: 0.62650955\n",
            "iteration: 185 loss: 0.85403097\n",
            "iteration: 186 loss: 0.03450941\n",
            "iteration: 187 loss: 0.17831087\n",
            "iteration: 188 loss: 0.30933240\n",
            "iteration: 189 loss: 0.64297301\n",
            "iteration: 190 loss: 0.18753469\n",
            "iteration: 191 loss: 0.80033213\n",
            "iteration: 192 loss: 0.33069992\n",
            "iteration: 193 loss: 1.18604815\n",
            "iteration: 194 loss: 0.37119788\n",
            "iteration: 195 loss: 0.33249533\n",
            "iteration: 196 loss: 0.35052586\n",
            "iteration: 197 loss: 0.67740136\n",
            "iteration: 198 loss: 0.13157657\n",
            "iteration: 199 loss: 1.10801446\n",
            "epoch:  90 mean loss training: 0.58948553\n",
            "epoch:  90 mean loss validation: 0.86974758\n",
            "iteration:   0 loss: 0.35139832\n",
            "iteration:   1 loss: 0.03608020\n",
            "iteration:   2 loss: 0.13452050\n",
            "iteration:   3 loss: 1.41270733\n",
            "iteration:   4 loss: 0.35984555\n",
            "iteration:   5 loss: 0.23201109\n",
            "iteration:   6 loss: 0.11450990\n",
            "iteration:   7 loss: 0.13848718\n",
            "iteration:   8 loss: 0.81195986\n",
            "iteration:   9 loss: 1.29537225\n",
            "iteration:  10 loss: 0.67379504\n",
            "iteration:  11 loss: 3.28584170\n",
            "iteration:  12 loss: 0.40169346\n",
            "iteration:  13 loss: 0.41953081\n",
            "iteration:  14 loss: 0.17758833\n",
            "iteration:  15 loss: 0.38309222\n",
            "iteration:  16 loss: 0.55174339\n",
            "iteration:  17 loss: 1.09195805\n",
            "iteration:  18 loss: 0.29720548\n",
            "iteration:  19 loss: 0.33851808\n",
            "iteration:  20 loss: 0.19874442\n",
            "iteration:  21 loss: 0.26521888\n",
            "iteration:  22 loss: 0.84857935\n",
            "iteration:  23 loss: 0.39105591\n",
            "iteration:  24 loss: 0.42285335\n",
            "iteration:  25 loss: 0.46031570\n",
            "iteration:  26 loss: 0.11421455\n",
            "iteration:  27 loss: 0.20054933\n",
            "iteration:  28 loss: 0.25880983\n",
            "iteration:  29 loss: 0.02763727\n",
            "iteration:  30 loss: 0.08178353\n",
            "iteration:  31 loss: 0.11912818\n",
            "iteration:  32 loss: 0.06505713\n",
            "iteration:  33 loss: 0.12533973\n",
            "iteration:  34 loss: 0.35058349\n",
            "iteration:  35 loss: 1.56720901\n",
            "iteration:  36 loss: 0.58345574\n",
            "iteration:  37 loss: 0.53796422\n",
            "iteration:  38 loss: 0.46703061\n",
            "iteration:  39 loss: 1.19526172\n",
            "iteration:  40 loss: 0.16830139\n",
            "iteration:  41 loss: 0.37998524\n",
            "iteration:  42 loss: 0.45081756\n",
            "iteration:  43 loss: 0.11629682\n",
            "iteration:  44 loss: 1.86764038\n",
            "iteration:  45 loss: 0.69204569\n",
            "iteration:  46 loss: 0.13401736\n",
            "iteration:  47 loss: 0.55555719\n",
            "iteration:  48 loss: 0.14720030\n",
            "iteration:  49 loss: 0.12449361\n",
            "iteration:  50 loss: 0.67499363\n",
            "iteration:  51 loss: 0.50071061\n",
            "iteration:  52 loss: 0.11789666\n",
            "iteration:  53 loss: 0.10280149\n",
            "iteration:  54 loss: 0.22899099\n",
            "iteration:  55 loss: 0.22059029\n",
            "iteration:  56 loss: 1.83299363\n",
            "iteration:  57 loss: 1.76270747\n",
            "iteration:  58 loss: 1.40186024\n",
            "iteration:  59 loss: 0.08349813\n",
            "iteration:  60 loss: 0.18212295\n",
            "iteration:  61 loss: 1.12002707\n",
            "iteration:  62 loss: 1.19907463\n",
            "iteration:  63 loss: 0.19105490\n",
            "iteration:  64 loss: 0.47906816\n",
            "iteration:  65 loss: 1.31840205\n",
            "iteration:  66 loss: 0.20339829\n",
            "iteration:  67 loss: 1.23798883\n",
            "iteration:  68 loss: 0.36191991\n",
            "iteration:  69 loss: 0.18217528\n",
            "iteration:  70 loss: 0.18241090\n",
            "iteration:  71 loss: 0.07843985\n",
            "iteration:  72 loss: 0.51843303\n",
            "iteration:  73 loss: 0.39821377\n",
            "iteration:  74 loss: 0.35731643\n",
            "iteration:  75 loss: 0.44595641\n",
            "iteration:  76 loss: 0.58564109\n",
            "iteration:  77 loss: 0.20442028\n",
            "iteration:  78 loss: 0.07228930\n",
            "iteration:  79 loss: 2.28923917\n",
            "iteration:  80 loss: 0.08051115\n",
            "iteration:  81 loss: 2.84019828\n",
            "iteration:  82 loss: 0.03746894\n",
            "iteration:  83 loss: 0.96416318\n",
            "iteration:  84 loss: 0.48843610\n",
            "iteration:  85 loss: 0.83210766\n",
            "iteration:  86 loss: 0.16648510\n",
            "iteration:  87 loss: 0.21513766\n",
            "iteration:  88 loss: 0.57776046\n",
            "iteration:  89 loss: 0.32814345\n",
            "iteration:  90 loss: 0.31073564\n",
            "iteration:  91 loss: 1.37120187\n",
            "iteration:  92 loss: 0.58271801\n",
            "iteration:  93 loss: 0.55452555\n",
            "iteration:  94 loss: 0.45912331\n",
            "iteration:  95 loss: 0.34880072\n",
            "iteration:  96 loss: 0.85062474\n",
            "iteration:  97 loss: 1.96134245\n",
            "iteration:  98 loss: 0.84585637\n",
            "iteration:  99 loss: 0.15356241\n",
            "iteration: 100 loss: 0.23865107\n",
            "iteration: 101 loss: 0.48986375\n",
            "iteration: 102 loss: 1.67657244\n",
            "iteration: 103 loss: 0.53200340\n",
            "iteration: 104 loss: 0.27659035\n",
            "iteration: 105 loss: 0.38609764\n",
            "iteration: 106 loss: 0.54754615\n",
            "iteration: 107 loss: 0.42946392\n",
            "iteration: 108 loss: 0.17041741\n",
            "iteration: 109 loss: 0.06997552\n",
            "iteration: 110 loss: 0.48360059\n",
            "iteration: 111 loss: 0.33810702\n",
            "iteration: 112 loss: 0.08358763\n",
            "iteration: 113 loss: 1.09959280\n",
            "iteration: 114 loss: 0.06531931\n",
            "iteration: 115 loss: 0.13975540\n",
            "iteration: 116 loss: 0.09168552\n",
            "iteration: 117 loss: 0.17847209\n",
            "iteration: 118 loss: 0.56906313\n",
            "iteration: 119 loss: 0.26780483\n",
            "iteration: 120 loss: 1.43386889\n",
            "iteration: 121 loss: 0.70684141\n",
            "iteration: 122 loss: 0.34106037\n",
            "iteration: 123 loss: 0.09099146\n",
            "iteration: 124 loss: 3.56330609\n",
            "iteration: 125 loss: 1.10723388\n",
            "iteration: 126 loss: 2.21727586\n",
            "iteration: 127 loss: 0.18792507\n",
            "iteration: 128 loss: 0.39665151\n",
            "iteration: 129 loss: 1.43691659\n",
            "iteration: 130 loss: 0.31370002\n",
            "iteration: 131 loss: 0.21366426\n",
            "iteration: 132 loss: 0.24020633\n",
            "iteration: 133 loss: 0.09648563\n",
            "iteration: 134 loss: 0.69763923\n",
            "iteration: 135 loss: 0.16095217\n",
            "iteration: 136 loss: 0.29023191\n",
            "iteration: 137 loss: 0.48902842\n",
            "iteration: 138 loss: 0.30213836\n",
            "iteration: 139 loss: 0.08623915\n",
            "iteration: 140 loss: 1.58564568\n",
            "iteration: 141 loss: 0.70946896\n",
            "iteration: 142 loss: 0.32765990\n",
            "iteration: 143 loss: 3.35083604\n",
            "iteration: 144 loss: 0.37770429\n",
            "iteration: 145 loss: 0.10425431\n",
            "iteration: 146 loss: 0.72214705\n",
            "iteration: 147 loss: 0.07494274\n",
            "iteration: 148 loss: 0.10920008\n",
            "iteration: 149 loss: 0.58310932\n",
            "iteration: 150 loss: 0.50784987\n",
            "iteration: 151 loss: 0.25326270\n",
            "iteration: 152 loss: 0.48608768\n",
            "iteration: 153 loss: 0.18027171\n",
            "iteration: 154 loss: 1.25806987\n",
            "iteration: 155 loss: 0.29100019\n",
            "iteration: 156 loss: 0.47223380\n",
            "iteration: 157 loss: 0.05209761\n",
            "iteration: 158 loss: 0.31515920\n",
            "iteration: 159 loss: 1.16345465\n",
            "iteration: 160 loss: 1.27538681\n",
            "iteration: 161 loss: 0.70911902\n",
            "iteration: 162 loss: 2.06869674\n",
            "iteration: 163 loss: 0.20980504\n",
            "iteration: 164 loss: 0.04354753\n",
            "iteration: 165 loss: 0.38032657\n",
            "iteration: 166 loss: 1.67853594\n",
            "iteration: 167 loss: 0.56313914\n",
            "iteration: 168 loss: 0.17148632\n",
            "iteration: 169 loss: 0.79681993\n",
            "iteration: 170 loss: 0.83816606\n",
            "iteration: 171 loss: 0.48153734\n",
            "iteration: 172 loss: 0.91723889\n",
            "iteration: 173 loss: 1.28470838\n",
            "iteration: 174 loss: 0.95216328\n",
            "iteration: 175 loss: 0.23401222\n",
            "iteration: 176 loss: 0.21813101\n",
            "iteration: 177 loss: 1.56729555\n",
            "iteration: 178 loss: 0.18769181\n",
            "iteration: 179 loss: 0.24479240\n",
            "iteration: 180 loss: 1.14167440\n",
            "iteration: 181 loss: 0.40529591\n",
            "iteration: 182 loss: 0.61363804\n",
            "iteration: 183 loss: 0.55279511\n",
            "iteration: 184 loss: 0.27441534\n",
            "iteration: 185 loss: 1.68501747\n",
            "iteration: 186 loss: 1.56377137\n",
            "iteration: 187 loss: 1.34117675\n",
            "iteration: 188 loss: 0.17020415\n",
            "iteration: 189 loss: 0.78308207\n",
            "iteration: 190 loss: 0.74829036\n",
            "iteration: 191 loss: 0.10372868\n",
            "iteration: 192 loss: 0.11839467\n",
            "iteration: 193 loss: 0.11748751\n",
            "iteration: 194 loss: 0.35074288\n",
            "iteration: 195 loss: 0.29719606\n",
            "iteration: 196 loss: 2.15216017\n",
            "iteration: 197 loss: 0.45366549\n",
            "iteration: 198 loss: 0.09224039\n",
            "iteration: 199 loss: 0.32274941\n",
            "epoch:  91 mean loss training: 0.60734928\n",
            "epoch:  91 mean loss validation: 0.81989658\n",
            "iteration:   0 loss: 0.44370779\n",
            "iteration:   1 loss: 0.86144167\n",
            "iteration:   2 loss: 0.28230271\n",
            "iteration:   3 loss: 0.60180861\n",
            "iteration:   4 loss: 0.69691384\n",
            "iteration:   5 loss: 0.08872619\n",
            "iteration:   6 loss: 1.35629487\n",
            "iteration:   7 loss: 0.26618457\n",
            "iteration:   8 loss: 0.55880898\n",
            "iteration:   9 loss: 0.32445103\n",
            "iteration:  10 loss: 1.60486376\n",
            "iteration:  11 loss: 0.59940785\n",
            "iteration:  12 loss: 0.72045898\n",
            "iteration:  13 loss: 0.24431777\n",
            "iteration:  14 loss: 0.12124274\n",
            "iteration:  15 loss: 0.24788143\n",
            "iteration:  16 loss: 0.68853098\n",
            "iteration:  17 loss: 1.59863293\n",
            "iteration:  18 loss: 0.36046258\n",
            "iteration:  19 loss: 0.22970426\n",
            "iteration:  20 loss: 1.17772067\n",
            "iteration:  21 loss: 0.36191991\n",
            "iteration:  22 loss: 0.14853267\n",
            "iteration:  23 loss: 0.71213657\n",
            "iteration:  24 loss: 1.58280313\n",
            "iteration:  25 loss: 1.99361253\n",
            "iteration:  26 loss: 0.95238459\n",
            "iteration:  27 loss: 0.33550256\n",
            "iteration:  28 loss: 0.49016127\n",
            "iteration:  29 loss: 1.60555208\n",
            "iteration:  30 loss: 0.03899532\n",
            "iteration:  31 loss: 0.45888582\n",
            "iteration:  32 loss: 0.05877124\n",
            "iteration:  33 loss: 0.97511399\n",
            "iteration:  34 loss: 2.51283097\n",
            "iteration:  35 loss: 1.07648325\n",
            "iteration:  36 loss: 0.26598346\n",
            "iteration:  37 loss: 0.05901358\n",
            "iteration:  38 loss: 0.10917407\n",
            "iteration:  39 loss: 0.39764458\n",
            "iteration:  40 loss: 0.45223776\n",
            "iteration:  41 loss: 0.36489934\n",
            "iteration:  42 loss: 0.76036239\n",
            "iteration:  43 loss: 0.32557231\n",
            "iteration:  44 loss: 0.55267924\n",
            "iteration:  45 loss: 1.01071024\n",
            "iteration:  46 loss: 0.18586978\n",
            "iteration:  47 loss: 0.05886031\n",
            "iteration:  48 loss: 1.13066077\n",
            "iteration:  49 loss: 0.16653739\n",
            "iteration:  50 loss: 0.05074886\n",
            "iteration:  51 loss: 0.57620102\n",
            "iteration:  52 loss: 0.59306777\n",
            "iteration:  53 loss: 0.48615292\n",
            "iteration:  54 loss: 0.65448874\n",
            "iteration:  55 loss: 1.38903224\n",
            "iteration:  56 loss: 1.98987234\n",
            "iteration:  57 loss: 0.37712607\n",
            "iteration:  58 loss: 0.10969725\n",
            "iteration:  59 loss: 0.32371825\n",
            "iteration:  60 loss: 0.11035941\n",
            "iteration:  61 loss: 0.46116045\n",
            "iteration:  62 loss: 0.14901009\n",
            "iteration:  63 loss: 0.74527103\n",
            "iteration:  64 loss: 0.61381596\n",
            "iteration:  65 loss: 0.05835714\n",
            "iteration:  66 loss: 0.47116253\n",
            "iteration:  67 loss: 0.36269549\n",
            "iteration:  68 loss: 0.66666532\n",
            "iteration:  69 loss: 0.30330813\n",
            "iteration:  70 loss: 0.05295348\n",
            "iteration:  71 loss: 1.26998985\n",
            "iteration:  72 loss: 0.15522845\n",
            "iteration:  73 loss: 0.56786227\n",
            "iteration:  74 loss: 0.30322233\n",
            "iteration:  75 loss: 0.03104759\n",
            "iteration:  76 loss: 0.06777988\n",
            "iteration:  77 loss: 1.46321285\n",
            "iteration:  78 loss: 0.23551086\n",
            "iteration:  79 loss: 0.62716436\n",
            "iteration:  80 loss: 0.15562488\n",
            "iteration:  81 loss: 1.55121398\n",
            "iteration:  82 loss: 0.75860488\n",
            "iteration:  83 loss: 0.88999742\n",
            "iteration:  84 loss: 0.22761172\n",
            "iteration:  85 loss: 0.03793482\n",
            "iteration:  86 loss: 0.51693028\n",
            "iteration:  87 loss: 0.62480980\n",
            "iteration:  88 loss: 0.76775557\n",
            "iteration:  89 loss: 0.77761149\n",
            "iteration:  90 loss: 0.38141730\n",
            "iteration:  91 loss: 2.33124161\n",
            "iteration:  92 loss: 0.69630414\n",
            "iteration:  93 loss: 0.66853333\n",
            "iteration:  94 loss: 0.08280558\n",
            "iteration:  95 loss: 0.90286469\n",
            "iteration:  96 loss: 0.90588015\n",
            "iteration:  97 loss: 0.84249604\n",
            "iteration:  98 loss: 0.44587803\n",
            "iteration:  99 loss: 0.27935353\n",
            "iteration: 100 loss: 0.74918562\n",
            "iteration: 101 loss: 0.42474705\n",
            "iteration: 102 loss: 0.15105957\n",
            "iteration: 103 loss: 0.60199744\n",
            "iteration: 104 loss: 0.06221958\n",
            "iteration: 105 loss: 0.74545908\n",
            "iteration: 106 loss: 0.03312247\n",
            "iteration: 107 loss: 1.59834516\n",
            "iteration: 108 loss: 0.52472836\n",
            "iteration: 109 loss: 0.20320302\n",
            "iteration: 110 loss: 0.11523160\n",
            "iteration: 111 loss: 0.60694039\n",
            "iteration: 112 loss: 0.26468259\n",
            "iteration: 113 loss: 1.95885348\n",
            "iteration: 114 loss: 0.68868005\n",
            "iteration: 115 loss: 0.48433530\n",
            "iteration: 116 loss: 0.36209908\n",
            "iteration: 117 loss: 0.27564251\n",
            "iteration: 118 loss: 0.20278342\n",
            "iteration: 119 loss: 0.18585300\n",
            "iteration: 120 loss: 1.72961068\n",
            "iteration: 121 loss: 0.32154766\n",
            "iteration: 122 loss: 0.05686466\n",
            "iteration: 123 loss: 0.72055095\n",
            "iteration: 124 loss: 0.34700361\n",
            "iteration: 125 loss: 0.98599440\n",
            "iteration: 126 loss: 0.27079028\n",
            "iteration: 127 loss: 1.12767875\n",
            "iteration: 128 loss: 0.10592850\n",
            "iteration: 129 loss: 0.20307757\n",
            "iteration: 130 loss: 0.78728873\n",
            "iteration: 131 loss: 0.23549016\n",
            "iteration: 132 loss: 0.07411095\n",
            "iteration: 133 loss: 0.44272673\n",
            "iteration: 134 loss: 0.18961874\n",
            "iteration: 135 loss: 0.14454593\n",
            "iteration: 136 loss: 0.18501490\n",
            "iteration: 137 loss: 1.42267120\n",
            "iteration: 138 loss: 0.15726934\n",
            "iteration: 139 loss: 0.42097867\n",
            "iteration: 140 loss: 0.07086995\n",
            "iteration: 141 loss: 0.98582095\n",
            "iteration: 142 loss: 0.22008474\n",
            "iteration: 143 loss: 0.51609796\n",
            "iteration: 144 loss: 0.16357839\n",
            "iteration: 145 loss: 1.55327916\n",
            "iteration: 146 loss: 0.06721601\n",
            "iteration: 147 loss: 0.08355508\n",
            "iteration: 148 loss: 1.29182279\n",
            "iteration: 149 loss: 0.38806015\n",
            "iteration: 150 loss: 1.63794816\n",
            "iteration: 151 loss: 0.68623388\n",
            "iteration: 152 loss: 0.49684760\n",
            "iteration: 153 loss: 0.11944785\n",
            "iteration: 154 loss: 0.54513627\n",
            "iteration: 155 loss: 0.26987728\n",
            "iteration: 156 loss: 0.62940437\n",
            "iteration: 157 loss: 0.45769760\n",
            "iteration: 158 loss: 1.03046870\n",
            "iteration: 159 loss: 0.94311142\n",
            "iteration: 160 loss: 0.23154137\n",
            "iteration: 161 loss: 2.64409018\n",
            "iteration: 162 loss: 0.34934798\n",
            "iteration: 163 loss: 0.07621668\n",
            "iteration: 164 loss: 0.92182654\n",
            "iteration: 165 loss: 0.10133974\n",
            "iteration: 166 loss: 0.64044589\n",
            "iteration: 167 loss: 0.84271771\n",
            "iteration: 168 loss: 0.09504347\n",
            "iteration: 169 loss: 0.49142030\n",
            "iteration: 170 loss: 1.00277114\n",
            "iteration: 171 loss: 0.62769681\n",
            "iteration: 172 loss: 0.08843146\n",
            "iteration: 173 loss: 0.63826162\n",
            "iteration: 174 loss: 0.20884666\n",
            "iteration: 175 loss: 2.37603116\n",
            "iteration: 176 loss: 0.48786095\n",
            "iteration: 177 loss: 0.32598695\n",
            "iteration: 178 loss: 1.65205741\n",
            "iteration: 179 loss: 0.10588311\n",
            "iteration: 180 loss: 0.56213552\n",
            "iteration: 181 loss: 0.65843093\n",
            "iteration: 182 loss: 0.18041423\n",
            "iteration: 183 loss: 0.92639500\n",
            "iteration: 184 loss: 0.37504312\n",
            "iteration: 185 loss: 0.54224741\n",
            "iteration: 186 loss: 0.06764571\n",
            "iteration: 187 loss: 0.48540163\n",
            "iteration: 188 loss: 0.24265791\n",
            "iteration: 189 loss: 0.58271259\n",
            "iteration: 190 loss: 0.57446980\n",
            "iteration: 191 loss: 0.08508079\n",
            "iteration: 192 loss: 0.45519778\n",
            "iteration: 193 loss: 0.33612373\n",
            "iteration: 194 loss: 0.63610411\n",
            "iteration: 195 loss: 1.59639525\n",
            "iteration: 196 loss: 0.32773772\n",
            "iteration: 197 loss: 0.21569523\n",
            "iteration: 198 loss: 0.39637220\n",
            "iteration: 199 loss: 1.75976539\n",
            "epoch:  92 mean loss training: 0.59283423\n",
            "epoch:  92 mean loss validation: 0.80462444\n",
            "iteration:   0 loss: 0.57088667\n",
            "iteration:   1 loss: 0.07705513\n",
            "iteration:   2 loss: 0.27179748\n",
            "iteration:   3 loss: 0.76153105\n",
            "iteration:   4 loss: 2.03888988\n",
            "iteration:   5 loss: 0.06285217\n",
            "iteration:   6 loss: 0.44882941\n",
            "iteration:   7 loss: 0.11474572\n",
            "iteration:   8 loss: 0.41201806\n",
            "iteration:   9 loss: 0.23729827\n",
            "iteration:  10 loss: 0.30303717\n",
            "iteration:  11 loss: 0.13611890\n",
            "iteration:  12 loss: 0.95439929\n",
            "iteration:  13 loss: 0.20205742\n",
            "iteration:  14 loss: 0.64187372\n",
            "iteration:  15 loss: 0.98247701\n",
            "iteration:  16 loss: 0.49715069\n",
            "iteration:  17 loss: 0.06407799\n",
            "iteration:  18 loss: 0.35504690\n",
            "iteration:  19 loss: 0.66039234\n",
            "iteration:  20 loss: 0.10063049\n",
            "iteration:  21 loss: 0.52986401\n",
            "iteration:  22 loss: 0.13715801\n",
            "iteration:  23 loss: 0.31499076\n",
            "iteration:  24 loss: 1.24665630\n",
            "iteration:  25 loss: 0.55129004\n",
            "iteration:  26 loss: 1.35885215\n",
            "iteration:  27 loss: 0.47633937\n",
            "iteration:  28 loss: 0.22080944\n",
            "iteration:  29 loss: 0.21193703\n",
            "iteration:  30 loss: 0.41287220\n",
            "iteration:  31 loss: 0.27268445\n",
            "iteration:  32 loss: 0.50019974\n",
            "iteration:  33 loss: 0.32839751\n",
            "iteration:  34 loss: 0.06357303\n",
            "iteration:  35 loss: 0.51344985\n",
            "iteration:  36 loss: 1.50723076\n",
            "iteration:  37 loss: 3.06614590\n",
            "iteration:  38 loss: 0.53035486\n",
            "iteration:  39 loss: 0.08835410\n",
            "iteration:  40 loss: 0.10057121\n",
            "iteration:  41 loss: 0.41291046\n",
            "iteration:  42 loss: 0.91180688\n",
            "iteration:  43 loss: 1.32350111\n",
            "iteration:  44 loss: 0.11675167\n",
            "iteration:  45 loss: 0.47410902\n",
            "iteration:  46 loss: 0.39307860\n",
            "iteration:  47 loss: 0.17986283\n",
            "iteration:  48 loss: 1.35795212\n",
            "iteration:  49 loss: 0.30174869\n",
            "iteration:  50 loss: 0.20002097\n",
            "iteration:  51 loss: 0.09937869\n",
            "iteration:  52 loss: 0.21914569\n",
            "iteration:  53 loss: 0.43618348\n",
            "iteration:  54 loss: 0.37507799\n",
            "iteration:  55 loss: 0.73654634\n",
            "iteration:  56 loss: 1.42072546\n",
            "iteration:  57 loss: 1.03642344\n",
            "iteration:  58 loss: 1.33562922\n",
            "iteration:  59 loss: 0.13215445\n",
            "iteration:  60 loss: 1.06622279\n",
            "iteration:  61 loss: 0.21234652\n",
            "iteration:  62 loss: 0.24953759\n",
            "iteration:  63 loss: 0.87815583\n",
            "iteration:  64 loss: 2.57092261\n",
            "iteration:  65 loss: 0.15035808\n",
            "iteration:  66 loss: 0.48355687\n",
            "iteration:  67 loss: 0.63977224\n",
            "iteration:  68 loss: 0.29172626\n",
            "iteration:  69 loss: 1.24963701\n",
            "iteration:  70 loss: 2.52343941\n",
            "iteration:  71 loss: 0.08677453\n",
            "iteration:  72 loss: 0.19040969\n",
            "iteration:  73 loss: 1.33237851\n",
            "iteration:  74 loss: 0.70944911\n",
            "iteration:  75 loss: 0.01930690\n",
            "iteration:  76 loss: 0.33069474\n",
            "iteration:  77 loss: 0.56840020\n",
            "iteration:  78 loss: 0.39542052\n",
            "iteration:  79 loss: 0.13095547\n",
            "iteration:  80 loss: 0.10001376\n",
            "iteration:  81 loss: 0.03609419\n",
            "iteration:  82 loss: 0.32126540\n",
            "iteration:  83 loss: 0.08694519\n",
            "iteration:  84 loss: 0.92033648\n",
            "iteration:  85 loss: 0.18759237\n",
            "iteration:  86 loss: 0.23209333\n",
            "iteration:  87 loss: 0.55559689\n",
            "iteration:  88 loss: 2.19085550\n",
            "iteration:  89 loss: 1.71807957\n",
            "iteration:  90 loss: 0.32571214\n",
            "iteration:  91 loss: 0.27869624\n",
            "iteration:  92 loss: 0.10761405\n",
            "iteration:  93 loss: 0.61122185\n",
            "iteration:  94 loss: 0.56218749\n",
            "iteration:  95 loss: 1.66625762\n",
            "iteration:  96 loss: 0.69480228\n",
            "iteration:  97 loss: 0.15856226\n",
            "iteration:  98 loss: 0.41681817\n",
            "iteration:  99 loss: 0.99294788\n",
            "iteration: 100 loss: 0.09271457\n",
            "iteration: 101 loss: 0.72120440\n",
            "iteration: 102 loss: 0.34827998\n",
            "iteration: 103 loss: 0.59074193\n",
            "iteration: 104 loss: 0.73778844\n",
            "iteration: 105 loss: 0.85764283\n",
            "iteration: 106 loss: 0.74439150\n",
            "iteration: 107 loss: 0.22698976\n",
            "iteration: 108 loss: 0.25417066\n",
            "iteration: 109 loss: 0.49918589\n",
            "iteration: 110 loss: 0.06804556\n",
            "iteration: 111 loss: 0.05052045\n",
            "iteration: 112 loss: 0.59345162\n",
            "iteration: 113 loss: 0.03747737\n",
            "iteration: 114 loss: 0.02896409\n",
            "iteration: 115 loss: 0.03441397\n",
            "iteration: 116 loss: 1.31101024\n",
            "iteration: 117 loss: 1.78170657\n",
            "iteration: 118 loss: 0.28933069\n",
            "iteration: 119 loss: 1.10215354\n",
            "iteration: 120 loss: 1.08980381\n",
            "iteration: 121 loss: 0.27499214\n",
            "iteration: 122 loss: 1.39427865\n",
            "iteration: 123 loss: 0.91133052\n",
            "iteration: 124 loss: 0.71746761\n",
            "iteration: 125 loss: 0.23750466\n",
            "iteration: 126 loss: 0.29123175\n",
            "iteration: 127 loss: 0.28133342\n",
            "iteration: 128 loss: 0.07850247\n",
            "iteration: 129 loss: 0.35841852\n",
            "iteration: 130 loss: 0.19351797\n",
            "iteration: 131 loss: 0.21931051\n",
            "iteration: 132 loss: 0.39767617\n",
            "iteration: 133 loss: 0.02348927\n",
            "iteration: 134 loss: 0.48029450\n",
            "iteration: 135 loss: 0.47520563\n",
            "iteration: 136 loss: 1.26564503\n",
            "iteration: 137 loss: 0.41100311\n",
            "iteration: 138 loss: 0.78600246\n",
            "iteration: 139 loss: 1.71967828\n",
            "iteration: 140 loss: 0.52563816\n",
            "iteration: 141 loss: 0.05495341\n",
            "iteration: 142 loss: 0.48530304\n",
            "iteration: 143 loss: 0.13628556\n",
            "iteration: 144 loss: 0.35189512\n",
            "iteration: 145 loss: 0.94655812\n",
            "iteration: 146 loss: 0.17449269\n",
            "iteration: 147 loss: 0.46473253\n",
            "iteration: 148 loss: 0.58845967\n",
            "iteration: 149 loss: 0.85556042\n",
            "iteration: 150 loss: 0.21760118\n",
            "iteration: 151 loss: 1.63101137\n",
            "iteration: 152 loss: 0.85066444\n",
            "iteration: 153 loss: 0.98094356\n",
            "iteration: 154 loss: 0.19710217\n",
            "iteration: 155 loss: 0.46188197\n",
            "iteration: 156 loss: 0.15352713\n",
            "iteration: 157 loss: 0.21732885\n",
            "iteration: 158 loss: 0.89949590\n",
            "iteration: 159 loss: 1.34437954\n",
            "iteration: 160 loss: 0.47846887\n",
            "iteration: 161 loss: 0.45381355\n",
            "iteration: 162 loss: 0.96406710\n",
            "iteration: 163 loss: 0.03459255\n",
            "iteration: 164 loss: 0.28385183\n",
            "iteration: 165 loss: 2.35873771\n",
            "iteration: 166 loss: 1.06738639\n",
            "iteration: 167 loss: 1.16316080\n",
            "iteration: 168 loss: 0.32802880\n",
            "iteration: 169 loss: 0.54180539\n",
            "iteration: 170 loss: 0.20687009\n",
            "iteration: 171 loss: 0.25973517\n",
            "iteration: 172 loss: 0.10254385\n",
            "iteration: 173 loss: 1.27927029\n",
            "iteration: 174 loss: 1.03202891\n",
            "iteration: 175 loss: 0.16097105\n",
            "iteration: 176 loss: 0.24755891\n",
            "iteration: 177 loss: 0.81698167\n",
            "iteration: 178 loss: 0.95670360\n",
            "iteration: 179 loss: 0.25800017\n",
            "iteration: 180 loss: 2.32250643\n",
            "iteration: 181 loss: 0.96273077\n",
            "iteration: 182 loss: 0.11731203\n",
            "iteration: 183 loss: 1.45421576\n",
            "iteration: 184 loss: 0.35858381\n",
            "iteration: 185 loss: 0.77488554\n",
            "iteration: 186 loss: 0.52655923\n",
            "iteration: 187 loss: 0.26314875\n",
            "iteration: 188 loss: 0.46110192\n",
            "iteration: 189 loss: 0.91591567\n",
            "iteration: 190 loss: 0.39495307\n",
            "iteration: 191 loss: 0.07399407\n",
            "iteration: 192 loss: 0.70596766\n",
            "iteration: 193 loss: 0.90207654\n",
            "iteration: 194 loss: 1.73676419\n",
            "iteration: 195 loss: 0.54986709\n",
            "iteration: 196 loss: 0.26072165\n",
            "iteration: 197 loss: 0.96336567\n",
            "iteration: 198 loss: 0.35327286\n",
            "iteration: 199 loss: 0.23122741\n",
            "epoch:  93 mean loss training: 0.60434747\n",
            "epoch:  93 mean loss validation: 0.79216689\n",
            "iteration:   0 loss: 0.11046252\n",
            "iteration:   1 loss: 1.50969040\n",
            "iteration:   2 loss: 1.79715955\n",
            "iteration:   3 loss: 0.41500667\n",
            "iteration:   4 loss: 0.59555167\n",
            "iteration:   5 loss: 2.40640378\n",
            "iteration:   6 loss: 0.41340095\n",
            "iteration:   7 loss: 0.59872550\n",
            "iteration:   8 loss: 0.56120259\n",
            "iteration:   9 loss: 0.62735695\n",
            "iteration:  10 loss: 0.39905185\n",
            "iteration:  11 loss: 0.10253938\n",
            "iteration:  12 loss: 2.06663823\n",
            "iteration:  13 loss: 0.15795088\n",
            "iteration:  14 loss: 0.12313974\n",
            "iteration:  15 loss: 0.59440827\n",
            "iteration:  16 loss: 2.20880842\n",
            "iteration:  17 loss: 0.23445205\n",
            "iteration:  18 loss: 0.04604599\n",
            "iteration:  19 loss: 0.95383513\n",
            "iteration:  20 loss: 0.90629166\n",
            "iteration:  21 loss: 0.20010559\n",
            "iteration:  22 loss: 1.13281238\n",
            "iteration:  23 loss: 0.39747635\n",
            "iteration:  24 loss: 0.11987992\n",
            "iteration:  25 loss: 0.06253918\n",
            "iteration:  26 loss: 0.28047693\n",
            "iteration:  27 loss: 0.40956986\n",
            "iteration:  28 loss: 0.13004620\n",
            "iteration:  29 loss: 1.83791447\n",
            "iteration:  30 loss: 0.13234492\n",
            "iteration:  31 loss: 0.29475996\n",
            "iteration:  32 loss: 0.36905518\n",
            "iteration:  33 loss: 0.13273357\n",
            "iteration:  34 loss: 0.41652083\n",
            "iteration:  35 loss: 0.67240196\n",
            "iteration:  36 loss: 0.70934463\n",
            "iteration:  37 loss: 0.66030890\n",
            "iteration:  38 loss: 0.27449897\n",
            "iteration:  39 loss: 0.33382636\n",
            "iteration:  40 loss: 0.49861911\n",
            "iteration:  41 loss: 0.48733020\n",
            "iteration:  42 loss: 0.43649334\n",
            "iteration:  43 loss: 0.09871627\n",
            "iteration:  44 loss: 0.68460399\n",
            "iteration:  45 loss: 1.11872697\n",
            "iteration:  46 loss: 0.16312686\n",
            "iteration:  47 loss: 0.51855111\n",
            "iteration:  48 loss: 0.18910116\n",
            "iteration:  49 loss: 0.34931722\n",
            "iteration:  50 loss: 1.14926708\n",
            "iteration:  51 loss: 1.10534406\n",
            "iteration:  52 loss: 0.09645046\n",
            "iteration:  53 loss: 0.56553096\n",
            "iteration:  54 loss: 0.22644153\n",
            "iteration:  55 loss: 0.41047603\n",
            "iteration:  56 loss: 0.66275716\n",
            "iteration:  57 loss: 0.19083810\n",
            "iteration:  58 loss: 0.29696998\n",
            "iteration:  59 loss: 0.05249397\n",
            "iteration:  60 loss: 0.12111654\n",
            "iteration:  61 loss: 0.55686677\n",
            "iteration:  62 loss: 1.60551476\n",
            "iteration:  63 loss: 0.13194332\n",
            "iteration:  64 loss: 0.39969623\n",
            "iteration:  65 loss: 1.05128384\n",
            "iteration:  66 loss: 0.37348178\n",
            "iteration:  67 loss: 0.40667331\n",
            "iteration:  68 loss: 0.22420108\n",
            "iteration:  69 loss: 0.41247079\n",
            "iteration:  70 loss: 2.27234960\n",
            "iteration:  71 loss: 0.56795967\n",
            "iteration:  72 loss: 0.43217409\n",
            "iteration:  73 loss: 0.71560133\n",
            "iteration:  74 loss: 0.47516373\n",
            "iteration:  75 loss: 0.10663706\n",
            "iteration:  76 loss: 0.10621292\n",
            "iteration:  77 loss: 0.41840884\n",
            "iteration:  78 loss: 0.30017671\n",
            "iteration:  79 loss: 0.22558898\n",
            "iteration:  80 loss: 0.27148178\n",
            "iteration:  81 loss: 0.34774294\n",
            "iteration:  82 loss: 0.31530973\n",
            "iteration:  83 loss: 0.41872609\n",
            "iteration:  84 loss: 0.34002727\n",
            "iteration:  85 loss: 0.13781421\n",
            "iteration:  86 loss: 1.41309655\n",
            "iteration:  87 loss: 0.88669258\n",
            "iteration:  88 loss: 0.21927685\n",
            "iteration:  89 loss: 0.26380444\n",
            "iteration:  90 loss: 0.16503064\n",
            "iteration:  91 loss: 0.31710640\n",
            "iteration:  92 loss: 0.50340700\n",
            "iteration:  93 loss: 0.13412678\n",
            "iteration:  94 loss: 0.09310890\n",
            "iteration:  95 loss: 0.12617037\n",
            "iteration:  96 loss: 0.28454044\n",
            "iteration:  97 loss: 0.21801117\n",
            "iteration:  98 loss: 0.11373012\n",
            "iteration:  99 loss: 0.15869425\n",
            "iteration: 100 loss: 0.93368763\n",
            "iteration: 101 loss: 0.03687097\n",
            "iteration: 102 loss: 0.98541892\n",
            "iteration: 103 loss: 0.33251062\n",
            "iteration: 104 loss: 0.08071488\n",
            "iteration: 105 loss: 1.49681330\n",
            "iteration: 106 loss: 1.05550849\n",
            "iteration: 107 loss: 0.32948321\n",
            "iteration: 108 loss: 0.52701575\n",
            "iteration: 109 loss: 0.66745156\n",
            "iteration: 110 loss: 0.85802543\n",
            "iteration: 111 loss: 0.33874846\n",
            "iteration: 112 loss: 2.28821993\n",
            "iteration: 113 loss: 1.57613456\n",
            "iteration: 114 loss: 0.07664538\n",
            "iteration: 115 loss: 0.41091725\n",
            "iteration: 116 loss: 1.84207439\n",
            "iteration: 117 loss: 0.03867939\n",
            "iteration: 118 loss: 0.25865710\n",
            "iteration: 119 loss: 0.43161973\n",
            "iteration: 120 loss: 0.85278785\n",
            "iteration: 121 loss: 0.51894766\n",
            "iteration: 122 loss: 0.07635655\n",
            "iteration: 123 loss: 0.25559300\n",
            "iteration: 124 loss: 0.69911134\n",
            "iteration: 125 loss: 0.17963342\n",
            "iteration: 126 loss: 0.68746340\n",
            "iteration: 127 loss: 1.94456279\n",
            "iteration: 128 loss: 0.10813975\n",
            "iteration: 129 loss: 0.73392707\n",
            "iteration: 130 loss: 0.23988055\n",
            "iteration: 131 loss: 1.30923855\n",
            "iteration: 132 loss: 0.76538402\n",
            "iteration: 133 loss: 0.23238286\n",
            "iteration: 134 loss: 1.23234212\n",
            "iteration: 135 loss: 0.32486680\n",
            "iteration: 136 loss: 0.47817156\n",
            "iteration: 137 loss: 0.33594775\n",
            "iteration: 138 loss: 0.56377643\n",
            "iteration: 139 loss: 0.93523479\n",
            "iteration: 140 loss: 0.24191858\n",
            "iteration: 141 loss: 0.73474622\n",
            "iteration: 142 loss: 0.27697903\n",
            "iteration: 143 loss: 0.43913957\n",
            "iteration: 144 loss: 0.29539081\n",
            "iteration: 145 loss: 0.40071455\n",
            "iteration: 146 loss: 1.38165450\n",
            "iteration: 147 loss: 0.51553822\n",
            "iteration: 148 loss: 0.22579314\n",
            "iteration: 149 loss: 1.92565215\n",
            "iteration: 150 loss: 1.89629292\n",
            "iteration: 151 loss: 0.15273461\n",
            "iteration: 152 loss: 0.04543247\n",
            "iteration: 153 loss: 0.71451157\n",
            "iteration: 154 loss: 0.03257826\n",
            "iteration: 155 loss: 0.59123343\n",
            "iteration: 156 loss: 0.46501803\n",
            "iteration: 157 loss: 0.33392543\n",
            "iteration: 158 loss: 0.64343411\n",
            "iteration: 159 loss: 0.03317074\n",
            "iteration: 160 loss: 0.04728776\n",
            "iteration: 161 loss: 1.33528769\n",
            "iteration: 162 loss: 0.69386816\n",
            "iteration: 163 loss: 0.16711624\n",
            "iteration: 164 loss: 0.37891224\n",
            "iteration: 165 loss: 2.11870790\n",
            "iteration: 166 loss: 0.30725375\n",
            "iteration: 167 loss: 0.19940190\n",
            "iteration: 168 loss: 0.08574816\n",
            "iteration: 169 loss: 0.16773325\n",
            "iteration: 170 loss: 0.40590641\n",
            "iteration: 171 loss: 1.97492290\n",
            "iteration: 172 loss: 0.29182765\n",
            "iteration: 173 loss: 0.17895271\n",
            "iteration: 174 loss: 1.65295303\n",
            "iteration: 175 loss: 1.94431829\n",
            "iteration: 176 loss: 0.06167572\n",
            "iteration: 177 loss: 0.08376943\n",
            "iteration: 178 loss: 0.34274024\n",
            "iteration: 179 loss: 0.33363247\n",
            "iteration: 180 loss: 0.57911742\n",
            "iteration: 181 loss: 0.32544333\n",
            "iteration: 182 loss: 0.44974491\n",
            "iteration: 183 loss: 0.39446062\n",
            "iteration: 184 loss: 0.29429021\n",
            "iteration: 185 loss: 0.03886662\n",
            "iteration: 186 loss: 0.67023027\n",
            "iteration: 187 loss: 0.05445983\n",
            "iteration: 188 loss: 0.30833074\n",
            "iteration: 189 loss: 0.34887683\n",
            "iteration: 190 loss: 0.68175632\n",
            "iteration: 191 loss: 0.90491140\n",
            "iteration: 192 loss: 0.54868329\n",
            "iteration: 193 loss: 0.20455398\n",
            "iteration: 194 loss: 1.90343571\n",
            "iteration: 195 loss: 0.32427862\n",
            "iteration: 196 loss: 2.32081962\n",
            "iteration: 197 loss: 0.27257076\n",
            "iteration: 198 loss: 0.94953531\n",
            "iteration: 199 loss: 0.49076059\n",
            "epoch:  94 mean loss training: 0.57774627\n",
            "epoch:  94 mean loss validation: 0.98222828\n",
            "iteration:   0 loss: 0.97626328\n",
            "iteration:   1 loss: 0.19462591\n",
            "iteration:   2 loss: 0.48007426\n",
            "iteration:   3 loss: 0.14705439\n",
            "iteration:   4 loss: 0.35705149\n",
            "iteration:   5 loss: 1.13982856\n",
            "iteration:   6 loss: 0.54694349\n",
            "iteration:   7 loss: 0.13293368\n",
            "iteration:   8 loss: 0.47222510\n",
            "iteration:   9 loss: 0.03341589\n",
            "iteration:  10 loss: 1.24980140\n",
            "iteration:  11 loss: 0.21884328\n",
            "iteration:  12 loss: 1.62602818\n",
            "iteration:  13 loss: 0.28664428\n",
            "iteration:  14 loss: 0.35824776\n",
            "iteration:  15 loss: 0.43632177\n",
            "iteration:  16 loss: 0.61598700\n",
            "iteration:  17 loss: 0.58467656\n",
            "iteration:  18 loss: 1.19902790\n",
            "iteration:  19 loss: 0.27136329\n",
            "iteration:  20 loss: 0.52584946\n",
            "iteration:  21 loss: 0.73609132\n",
            "iteration:  22 loss: 0.66344559\n",
            "iteration:  23 loss: 0.39694694\n",
            "iteration:  24 loss: 1.19692683\n",
            "iteration:  25 loss: 0.17696729\n",
            "iteration:  26 loss: 0.63040125\n",
            "iteration:  27 loss: 0.11966064\n",
            "iteration:  28 loss: 3.33818030\n",
            "iteration:  29 loss: 0.58628148\n",
            "iteration:  30 loss: 0.25195715\n",
            "iteration:  31 loss: 0.33502129\n",
            "iteration:  32 loss: 0.14043285\n",
            "iteration:  33 loss: 0.12261527\n",
            "iteration:  34 loss: 0.27505717\n",
            "iteration:  35 loss: 0.90865660\n",
            "iteration:  36 loss: 0.18276395\n",
            "iteration:  37 loss: 0.33219221\n",
            "iteration:  38 loss: 0.06431855\n",
            "iteration:  39 loss: 1.46174514\n",
            "iteration:  40 loss: 0.69804895\n",
            "iteration:  41 loss: 1.59171033\n",
            "iteration:  42 loss: 0.71832752\n",
            "iteration:  43 loss: 0.05670735\n",
            "iteration:  44 loss: 0.51623029\n",
            "iteration:  45 loss: 1.27615178\n",
            "iteration:  46 loss: 2.07842731\n",
            "iteration:  47 loss: 0.24820678\n",
            "iteration:  48 loss: 0.22788249\n",
            "iteration:  49 loss: 0.12634791\n",
            "iteration:  50 loss: 1.18969440\n",
            "iteration:  51 loss: 0.68989301\n",
            "iteration:  52 loss: 0.04489163\n",
            "iteration:  53 loss: 2.35647511\n",
            "iteration:  54 loss: 0.23661247\n",
            "iteration:  55 loss: 0.71483082\n",
            "iteration:  56 loss: 1.00418544\n",
            "iteration:  57 loss: 0.86630350\n",
            "iteration:  58 loss: 0.49039614\n",
            "iteration:  59 loss: 0.09902625\n",
            "iteration:  60 loss: 0.38079923\n",
            "iteration:  61 loss: 0.09871650\n",
            "iteration:  62 loss: 1.47570908\n",
            "iteration:  63 loss: 0.30862322\n",
            "iteration:  64 loss: 0.17918974\n",
            "iteration:  65 loss: 0.11170256\n",
            "iteration:  66 loss: 0.28978044\n",
            "iteration:  67 loss: 0.21715076\n",
            "iteration:  68 loss: 0.18330970\n",
            "iteration:  69 loss: 0.19379765\n",
            "iteration:  70 loss: 0.14937150\n",
            "iteration:  71 loss: 1.69088328\n",
            "iteration:  72 loss: 0.10663577\n",
            "iteration:  73 loss: 0.23439781\n",
            "iteration:  74 loss: 1.70120263\n",
            "iteration:  75 loss: 0.22291161\n",
            "iteration:  76 loss: 0.62299675\n",
            "iteration:  77 loss: 0.23061964\n",
            "iteration:  78 loss: 0.26587236\n",
            "iteration:  79 loss: 0.21715525\n",
            "iteration:  80 loss: 0.61724514\n",
            "iteration:  81 loss: 0.63601142\n",
            "iteration:  82 loss: 2.12654877\n",
            "iteration:  83 loss: 0.37747562\n",
            "iteration:  84 loss: 1.45605564\n",
            "iteration:  85 loss: 1.66654909\n",
            "iteration:  86 loss: 0.36700010\n",
            "iteration:  87 loss: 0.27451161\n",
            "iteration:  88 loss: 0.42875898\n",
            "iteration:  89 loss: 0.81451958\n",
            "iteration:  90 loss: 0.30069759\n",
            "iteration:  91 loss: 0.22656959\n",
            "iteration:  92 loss: 0.58088344\n",
            "iteration:  93 loss: 0.45204580\n",
            "iteration:  94 loss: 0.28692195\n",
            "iteration:  95 loss: 1.41225767\n",
            "iteration:  96 loss: 0.61070794\n",
            "iteration:  97 loss: 0.62229276\n",
            "iteration:  98 loss: 0.31070331\n",
            "iteration:  99 loss: 0.61926335\n",
            "iteration: 100 loss: 0.75273412\n",
            "iteration: 101 loss: 0.34973261\n",
            "iteration: 102 loss: 0.39666614\n",
            "iteration: 103 loss: 0.58799988\n",
            "iteration: 104 loss: 0.04778921\n",
            "iteration: 105 loss: 0.26349857\n",
            "iteration: 106 loss: 0.04490403\n",
            "iteration: 107 loss: 0.20489387\n",
            "iteration: 108 loss: 0.69915038\n",
            "iteration: 109 loss: 0.81714594\n",
            "iteration: 110 loss: 0.53121525\n",
            "iteration: 111 loss: 1.43042338\n",
            "iteration: 112 loss: 0.28665626\n",
            "iteration: 113 loss: 0.88167560\n",
            "iteration: 114 loss: 0.95066035\n",
            "iteration: 115 loss: 1.18159103\n",
            "iteration: 116 loss: 0.05489418\n",
            "iteration: 117 loss: 0.37128425\n",
            "iteration: 118 loss: 0.06559341\n",
            "iteration: 119 loss: 1.29868793\n",
            "iteration: 120 loss: 0.51400042\n",
            "iteration: 121 loss: 0.15915516\n",
            "iteration: 122 loss: 0.30736354\n",
            "iteration: 123 loss: 0.20401515\n",
            "iteration: 124 loss: 1.63932943\n",
            "iteration: 125 loss: 0.16844408\n",
            "iteration: 126 loss: 0.61105049\n",
            "iteration: 127 loss: 0.18997279\n",
            "iteration: 128 loss: 0.51497447\n",
            "iteration: 129 loss: 0.25190142\n",
            "iteration: 130 loss: 0.62894386\n",
            "iteration: 131 loss: 0.50487357\n",
            "iteration: 132 loss: 0.21985652\n",
            "iteration: 133 loss: 0.17994975\n",
            "iteration: 134 loss: 0.38315311\n",
            "iteration: 135 loss: 1.35872841\n",
            "iteration: 136 loss: 0.11627638\n",
            "iteration: 137 loss: 0.40364394\n",
            "iteration: 138 loss: 0.82037193\n",
            "iteration: 139 loss: 0.63750148\n",
            "iteration: 140 loss: 0.03432902\n",
            "iteration: 141 loss: 0.52599984\n",
            "iteration: 142 loss: 0.25812027\n",
            "iteration: 143 loss: 1.16166329\n",
            "iteration: 144 loss: 0.18945378\n",
            "iteration: 145 loss: 0.99924731\n",
            "iteration: 146 loss: 0.51496065\n",
            "iteration: 147 loss: 0.59835178\n",
            "iteration: 148 loss: 0.81362337\n",
            "iteration: 149 loss: 0.39701697\n",
            "iteration: 150 loss: 0.85094076\n",
            "iteration: 151 loss: 0.27077487\n",
            "iteration: 152 loss: 0.54780334\n",
            "iteration: 153 loss: 0.90194905\n",
            "iteration: 154 loss: 1.88791502\n",
            "iteration: 155 loss: 0.16991737\n",
            "iteration: 156 loss: 0.10219665\n",
            "iteration: 157 loss: 0.35381505\n",
            "iteration: 158 loss: 0.04700685\n",
            "iteration: 159 loss: 0.55382395\n",
            "iteration: 160 loss: 0.06271359\n",
            "iteration: 161 loss: 1.36976314\n",
            "iteration: 162 loss: 0.56383860\n",
            "iteration: 163 loss: 0.21374962\n",
            "iteration: 164 loss: 2.73060107\n",
            "iteration: 165 loss: 0.33304280\n",
            "iteration: 166 loss: 0.43148190\n",
            "iteration: 167 loss: 1.91147590\n",
            "iteration: 168 loss: 0.05529512\n",
            "iteration: 169 loss: 0.06875846\n",
            "iteration: 170 loss: 0.98539698\n",
            "iteration: 171 loss: 0.47094077\n",
            "iteration: 172 loss: 0.27051052\n",
            "iteration: 173 loss: 1.73477554\n",
            "iteration: 174 loss: 0.29229844\n",
            "iteration: 175 loss: 0.21737280\n",
            "iteration: 176 loss: 0.25528154\n",
            "iteration: 177 loss: 0.53719223\n",
            "iteration: 178 loss: 0.46353266\n",
            "iteration: 179 loss: 2.57582808\n",
            "iteration: 180 loss: 0.13714778\n",
            "iteration: 181 loss: 0.37619165\n",
            "iteration: 182 loss: 2.01733208\n",
            "iteration: 183 loss: 0.08791243\n",
            "iteration: 184 loss: 0.16051777\n",
            "iteration: 185 loss: 0.80487686\n",
            "iteration: 186 loss: 2.76659918\n",
            "iteration: 187 loss: 0.17082141\n",
            "iteration: 188 loss: 0.34728810\n",
            "iteration: 189 loss: 2.49417353\n",
            "iteration: 190 loss: 0.44287476\n",
            "iteration: 191 loss: 0.44232154\n",
            "iteration: 192 loss: 0.08564087\n",
            "iteration: 193 loss: 1.07405400\n",
            "iteration: 194 loss: 1.50153339\n",
            "iteration: 195 loss: 0.36382982\n",
            "iteration: 196 loss: 0.19184163\n",
            "iteration: 197 loss: 0.41512185\n",
            "iteration: 198 loss: 0.05086160\n",
            "iteration: 199 loss: 0.52017403\n",
            "epoch:  95 mean loss training: 0.62137818\n",
            "epoch:  95 mean loss validation: 0.86835295\n",
            "iteration:   0 loss: 1.89246428\n",
            "iteration:   1 loss: 0.61594856\n",
            "iteration:   2 loss: 0.57176876\n",
            "iteration:   3 loss: 0.06892300\n",
            "iteration:   4 loss: 1.25681651\n",
            "iteration:   5 loss: 0.06930943\n",
            "iteration:   6 loss: 0.47653756\n",
            "iteration:   7 loss: 0.15861969\n",
            "iteration:   8 loss: 1.61578548\n",
            "iteration:   9 loss: 0.28618628\n",
            "iteration:  10 loss: 0.13618255\n",
            "iteration:  11 loss: 0.09284948\n",
            "iteration:  12 loss: 0.14990024\n",
            "iteration:  13 loss: 0.24903478\n",
            "iteration:  14 loss: 0.06212557\n",
            "iteration:  15 loss: 0.05003564\n",
            "iteration:  16 loss: 0.15249251\n",
            "iteration:  17 loss: 1.65427864\n",
            "iteration:  18 loss: 0.70307273\n",
            "iteration:  19 loss: 0.31079125\n",
            "iteration:  20 loss: 0.56107777\n",
            "iteration:  21 loss: 0.07776304\n",
            "iteration:  22 loss: 0.45008108\n",
            "iteration:  23 loss: 0.67495817\n",
            "iteration:  24 loss: 1.14711356\n",
            "iteration:  25 loss: 0.20693403\n",
            "iteration:  26 loss: 0.39126396\n",
            "iteration:  27 loss: 0.67757118\n",
            "iteration:  28 loss: 0.30799735\n",
            "iteration:  29 loss: 0.23743047\n",
            "iteration:  30 loss: 0.77165061\n",
            "iteration:  31 loss: 0.22882673\n",
            "iteration:  32 loss: 0.08982644\n",
            "iteration:  33 loss: 0.08511899\n",
            "iteration:  34 loss: 0.06877191\n",
            "iteration:  35 loss: 0.82124358\n",
            "iteration:  36 loss: 1.91999781\n",
            "iteration:  37 loss: 0.59232938\n",
            "iteration:  38 loss: 0.37487677\n",
            "iteration:  39 loss: 0.73598295\n",
            "iteration:  40 loss: 0.09126821\n",
            "iteration:  41 loss: 1.68685889\n",
            "iteration:  42 loss: 0.27437970\n",
            "iteration:  43 loss: 0.15816794\n",
            "iteration:  44 loss: 1.14244020\n",
            "iteration:  45 loss: 0.90642607\n",
            "iteration:  46 loss: 0.39922953\n",
            "iteration:  47 loss: 0.51868117\n",
            "iteration:  48 loss: 0.28764778\n",
            "iteration:  49 loss: 3.08149052\n",
            "iteration:  50 loss: 0.77806526\n",
            "iteration:  51 loss: 0.08153765\n",
            "iteration:  52 loss: 0.40814027\n",
            "iteration:  53 loss: 0.22145082\n",
            "iteration:  54 loss: 1.75204098\n",
            "iteration:  55 loss: 0.12879907\n",
            "iteration:  56 loss: 0.83005297\n",
            "iteration:  57 loss: 0.27951339\n",
            "iteration:  58 loss: 0.07693537\n",
            "iteration:  59 loss: 0.44104978\n",
            "iteration:  60 loss: 0.21409725\n",
            "iteration:  61 loss: 0.19164668\n",
            "iteration:  62 loss: 0.45220640\n",
            "iteration:  63 loss: 0.18448302\n",
            "iteration:  64 loss: 0.43044841\n",
            "iteration:  65 loss: 0.54467219\n",
            "iteration:  66 loss: 0.60349208\n",
            "iteration:  67 loss: 0.38032317\n",
            "iteration:  68 loss: 0.60702258\n",
            "iteration:  69 loss: 0.06900288\n",
            "iteration:  70 loss: 0.26177502\n",
            "iteration:  71 loss: 1.59811509\n",
            "iteration:  72 loss: 0.29130414\n",
            "iteration:  73 loss: 0.72540408\n",
            "iteration:  74 loss: 0.14558522\n",
            "iteration:  75 loss: 0.08274122\n",
            "iteration:  76 loss: 1.34375000\n",
            "iteration:  77 loss: 0.16498958\n",
            "iteration:  78 loss: 0.35069901\n",
            "iteration:  79 loss: 0.83332813\n",
            "iteration:  80 loss: 0.16861050\n",
            "iteration:  81 loss: 0.19528353\n",
            "iteration:  82 loss: 1.50429034\n",
            "iteration:  83 loss: 0.20505549\n",
            "iteration:  84 loss: 0.13933575\n",
            "iteration:  85 loss: 0.67125946\n",
            "iteration:  86 loss: 1.25632429\n",
            "iteration:  87 loss: 1.63184416\n",
            "iteration:  88 loss: 2.96825099\n",
            "iteration:  89 loss: 0.90849155\n",
            "iteration:  90 loss: 0.23010103\n",
            "iteration:  91 loss: 0.36141470\n",
            "iteration:  92 loss: 0.29334366\n",
            "iteration:  93 loss: 0.03018665\n",
            "iteration:  94 loss: 0.85354757\n",
            "iteration:  95 loss: 0.02914093\n",
            "iteration:  96 loss: 1.41966522\n",
            "iteration:  97 loss: 0.37809804\n",
            "iteration:  98 loss: 0.17683473\n",
            "iteration:  99 loss: 0.30756840\n",
            "iteration: 100 loss: 0.37225854\n",
            "iteration: 101 loss: 0.06092476\n",
            "iteration: 102 loss: 0.21388096\n",
            "iteration: 103 loss: 0.79936904\n",
            "iteration: 104 loss: 1.36387455\n",
            "iteration: 105 loss: 0.16462676\n",
            "iteration: 106 loss: 0.22231670\n",
            "iteration: 107 loss: 0.38495448\n",
            "iteration: 108 loss: 0.26980615\n",
            "iteration: 109 loss: 0.46669397\n",
            "iteration: 110 loss: 1.00482011\n",
            "iteration: 111 loss: 0.88471651\n",
            "iteration: 112 loss: 1.05671263\n",
            "iteration: 113 loss: 0.04111971\n",
            "iteration: 114 loss: 0.22480476\n",
            "iteration: 115 loss: 1.62312806\n",
            "iteration: 116 loss: 2.22016168\n",
            "iteration: 117 loss: 0.17061174\n",
            "iteration: 118 loss: 0.74737555\n",
            "iteration: 119 loss: 1.93687153\n",
            "iteration: 120 loss: 0.01833970\n",
            "iteration: 121 loss: 0.28627646\n",
            "iteration: 122 loss: 0.30184922\n",
            "iteration: 123 loss: 2.95640588\n",
            "iteration: 124 loss: 0.15481925\n",
            "iteration: 125 loss: 0.60031766\n",
            "iteration: 126 loss: 0.10927589\n",
            "iteration: 127 loss: 0.50245410\n",
            "iteration: 128 loss: 0.23739851\n",
            "iteration: 129 loss: 0.58531475\n",
            "iteration: 130 loss: 0.49799597\n",
            "iteration: 131 loss: 0.68803555\n",
            "iteration: 132 loss: 0.04228985\n",
            "iteration: 133 loss: 0.28571123\n",
            "iteration: 134 loss: 0.16722265\n",
            "iteration: 135 loss: 0.96906424\n",
            "iteration: 136 loss: 1.15947437\n",
            "iteration: 137 loss: 0.54207933\n",
            "iteration: 138 loss: 0.20495385\n",
            "iteration: 139 loss: 0.02497552\n",
            "iteration: 140 loss: 2.67401505\n",
            "iteration: 141 loss: 0.33909512\n",
            "iteration: 142 loss: 0.35565525\n",
            "iteration: 143 loss: 1.02540982\n",
            "iteration: 144 loss: 0.67875940\n",
            "iteration: 145 loss: 0.12543772\n",
            "iteration: 146 loss: 0.07657874\n",
            "iteration: 147 loss: 0.06634223\n",
            "iteration: 148 loss: 0.13868448\n",
            "iteration: 149 loss: 0.01466260\n",
            "iteration: 150 loss: 0.79184043\n",
            "iteration: 151 loss: 1.68039870\n",
            "iteration: 152 loss: 1.81263661\n",
            "iteration: 153 loss: 0.29998422\n",
            "iteration: 154 loss: 0.46211392\n",
            "iteration: 155 loss: 0.91910499\n",
            "iteration: 156 loss: 0.10168035\n",
            "iteration: 157 loss: 0.06915963\n",
            "iteration: 158 loss: 0.18534520\n",
            "iteration: 159 loss: 1.23015904\n",
            "iteration: 160 loss: 0.45311534\n",
            "iteration: 161 loss: 0.57958996\n",
            "iteration: 162 loss: 1.26787293\n",
            "iteration: 163 loss: 0.34312168\n",
            "iteration: 164 loss: 0.19352497\n",
            "iteration: 165 loss: 0.37350518\n",
            "iteration: 166 loss: 0.58804637\n",
            "iteration: 167 loss: 0.15205801\n",
            "iteration: 168 loss: 0.22257179\n",
            "iteration: 169 loss: 0.73103613\n",
            "iteration: 170 loss: 0.54213381\n",
            "iteration: 171 loss: 0.75600839\n",
            "iteration: 172 loss: 0.22721358\n",
            "iteration: 173 loss: 0.36189032\n",
            "iteration: 174 loss: 0.57173574\n",
            "iteration: 175 loss: 0.16407682\n",
            "iteration: 176 loss: 1.71923065\n",
            "iteration: 177 loss: 1.22338891\n",
            "iteration: 178 loss: 0.81348646\n",
            "iteration: 179 loss: 0.37164748\n",
            "iteration: 180 loss: 0.28102496\n",
            "iteration: 181 loss: 0.46140176\n",
            "iteration: 182 loss: 0.33204108\n",
            "iteration: 183 loss: 0.05632196\n",
            "iteration: 184 loss: 0.29753909\n",
            "iteration: 185 loss: 1.34819436\n",
            "iteration: 186 loss: 0.38094765\n",
            "iteration: 187 loss: 0.64531946\n",
            "iteration: 188 loss: 0.18598175\n",
            "iteration: 189 loss: 0.14172904\n",
            "iteration: 190 loss: 0.31020394\n",
            "iteration: 191 loss: 0.59188759\n",
            "iteration: 192 loss: 1.22640598\n",
            "iteration: 193 loss: 0.88521236\n",
            "iteration: 194 loss: 0.11065038\n",
            "iteration: 195 loss: 0.59147912\n",
            "iteration: 196 loss: 1.69420314\n",
            "iteration: 197 loss: 0.62343240\n",
            "iteration: 198 loss: 1.61120903\n",
            "iteration: 199 loss: 0.20107648\n",
            "epoch:  96 mean loss training: 0.59491861\n",
            "epoch:  96 mean loss validation: 0.82144225\n",
            "iteration:   0 loss: 0.89496720\n",
            "iteration:   1 loss: 0.07301188\n",
            "iteration:   2 loss: 0.25033361\n",
            "iteration:   3 loss: 0.46365535\n",
            "iteration:   4 loss: 0.07618655\n",
            "iteration:   5 loss: 1.76622272\n",
            "iteration:   6 loss: 0.15536137\n",
            "iteration:   7 loss: 0.04631646\n",
            "iteration:   8 loss: 1.66658735\n",
            "iteration:   9 loss: 2.17435598\n",
            "iteration:  10 loss: 2.72412777\n",
            "iteration:  11 loss: 0.03027563\n",
            "iteration:  12 loss: 0.37801662\n",
            "iteration:  13 loss: 0.37770325\n",
            "iteration:  14 loss: 0.13412477\n",
            "iteration:  15 loss: 0.34557199\n",
            "iteration:  16 loss: 0.96975499\n",
            "iteration:  17 loss: 0.14967345\n",
            "iteration:  18 loss: 0.59276789\n",
            "iteration:  19 loss: 0.78303385\n",
            "iteration:  20 loss: 0.17679867\n",
            "iteration:  21 loss: 1.25429583\n",
            "iteration:  22 loss: 1.07660699\n",
            "iteration:  23 loss: 0.04947241\n",
            "iteration:  24 loss: 1.06901324\n",
            "iteration:  25 loss: 0.62755668\n",
            "iteration:  26 loss: 0.57375062\n",
            "iteration:  27 loss: 0.59016621\n",
            "iteration:  28 loss: 0.07976347\n",
            "iteration:  29 loss: 0.74282086\n",
            "iteration:  30 loss: 0.24572565\n",
            "iteration:  31 loss: 0.35907212\n",
            "iteration:  32 loss: 0.66795892\n",
            "iteration:  33 loss: 1.60397005\n",
            "iteration:  34 loss: 0.11954653\n",
            "iteration:  35 loss: 0.15108649\n",
            "iteration:  36 loss: 3.15676212\n",
            "iteration:  37 loss: 0.38690415\n",
            "iteration:  38 loss: 0.19820856\n",
            "iteration:  39 loss: 0.73677099\n",
            "iteration:  40 loss: 0.35064405\n",
            "iteration:  41 loss: 0.22040005\n",
            "iteration:  42 loss: 0.34715411\n",
            "iteration:  43 loss: 1.01034093\n",
            "iteration:  44 loss: 0.54642969\n",
            "iteration:  45 loss: 0.08056427\n",
            "iteration:  46 loss: 0.71921039\n",
            "iteration:  47 loss: 0.30088848\n",
            "iteration:  48 loss: 0.40771857\n",
            "iteration:  49 loss: 0.63544726\n",
            "iteration:  50 loss: 0.20818499\n",
            "iteration:  51 loss: 0.10975527\n",
            "iteration:  52 loss: 0.09470943\n",
            "iteration:  53 loss: 0.04447161\n",
            "iteration:  54 loss: 1.15461254\n",
            "iteration:  55 loss: 0.29107711\n",
            "iteration:  56 loss: 1.77739644\n",
            "iteration:  57 loss: 0.23391382\n",
            "iteration:  58 loss: 0.07362889\n",
            "iteration:  59 loss: 0.49272069\n",
            "iteration:  60 loss: 0.47528175\n",
            "iteration:  61 loss: 0.41724855\n",
            "iteration:  62 loss: 0.15545754\n",
            "iteration:  63 loss: 0.09009276\n",
            "iteration:  64 loss: 0.37723649\n",
            "iteration:  65 loss: 1.07970679\n",
            "iteration:  66 loss: 0.61801869\n",
            "iteration:  67 loss: 1.18932104\n",
            "iteration:  68 loss: 0.11212683\n",
            "iteration:  69 loss: 0.09839183\n",
            "iteration:  70 loss: 0.31217119\n",
            "iteration:  71 loss: 0.36015925\n",
            "iteration:  72 loss: 1.54756129\n",
            "iteration:  73 loss: 0.35179517\n",
            "iteration:  74 loss: 1.79710841\n",
            "iteration:  75 loss: 0.83305264\n",
            "iteration:  76 loss: 0.05607221\n",
            "iteration:  77 loss: 0.54532850\n",
            "iteration:  78 loss: 0.07937672\n",
            "iteration:  79 loss: 0.47514403\n",
            "iteration:  80 loss: 0.07419495\n",
            "iteration:  81 loss: 0.34683263\n",
            "iteration:  82 loss: 0.03464044\n",
            "iteration:  83 loss: 0.95048028\n",
            "iteration:  84 loss: 0.21249767\n",
            "iteration:  85 loss: 0.28489536\n",
            "iteration:  86 loss: 0.45917884\n",
            "iteration:  87 loss: 0.05018740\n",
            "iteration:  88 loss: 0.54768133\n",
            "iteration:  89 loss: 0.69060045\n",
            "iteration:  90 loss: 0.97724527\n",
            "iteration:  91 loss: 0.37659669\n",
            "iteration:  92 loss: 0.03429510\n",
            "iteration:  93 loss: 1.05448067\n",
            "iteration:  94 loss: 0.04440331\n",
            "iteration:  95 loss: 0.58120072\n",
            "iteration:  96 loss: 1.22068441\n",
            "iteration:  97 loss: 0.53167492\n",
            "iteration:  98 loss: 1.15536284\n",
            "iteration:  99 loss: 1.48183870\n",
            "iteration: 100 loss: 0.12539223\n",
            "iteration: 101 loss: 0.41664898\n",
            "iteration: 102 loss: 0.11789056\n",
            "iteration: 103 loss: 0.42722929\n",
            "iteration: 104 loss: 0.35898492\n",
            "iteration: 105 loss: 1.23449433\n",
            "iteration: 106 loss: 0.25535727\n",
            "iteration: 107 loss: 0.38108122\n",
            "iteration: 108 loss: 1.10330057\n",
            "iteration: 109 loss: 0.48920247\n",
            "iteration: 110 loss: 0.10556007\n",
            "iteration: 111 loss: 0.09328723\n",
            "iteration: 112 loss: 0.67259032\n",
            "iteration: 113 loss: 1.90794718\n",
            "iteration: 114 loss: 0.37598941\n",
            "iteration: 115 loss: 0.64698541\n",
            "iteration: 116 loss: 0.20770265\n",
            "iteration: 117 loss: 0.11968458\n",
            "iteration: 118 loss: 0.81892842\n",
            "iteration: 119 loss: 0.44079840\n",
            "iteration: 120 loss: 0.28085274\n",
            "iteration: 121 loss: 0.87753701\n",
            "iteration: 122 loss: 0.77951545\n",
            "iteration: 123 loss: 0.09908834\n",
            "iteration: 124 loss: 1.87653458\n",
            "iteration: 125 loss: 0.30812800\n",
            "iteration: 126 loss: 0.61348671\n",
            "iteration: 127 loss: 0.35931411\n",
            "iteration: 128 loss: 0.55838138\n",
            "iteration: 129 loss: 1.60446084\n",
            "iteration: 130 loss: 0.09527169\n",
            "iteration: 131 loss: 0.47234723\n",
            "iteration: 132 loss: 1.03860891\n",
            "iteration: 133 loss: 0.19327685\n",
            "iteration: 134 loss: 0.26288322\n",
            "iteration: 135 loss: 0.69333404\n",
            "iteration: 136 loss: 1.66866279\n",
            "iteration: 137 loss: 1.02302468\n",
            "iteration: 138 loss: 0.41132000\n",
            "iteration: 139 loss: 0.53694504\n",
            "iteration: 140 loss: 0.60404700\n",
            "iteration: 141 loss: 0.13746072\n",
            "iteration: 142 loss: 0.74112546\n",
            "iteration: 143 loss: 0.24330813\n",
            "iteration: 144 loss: 0.45644712\n",
            "iteration: 145 loss: 0.37450510\n",
            "iteration: 146 loss: 0.28270203\n",
            "iteration: 147 loss: 2.25897264\n",
            "iteration: 148 loss: 0.38964573\n",
            "iteration: 149 loss: 0.25582662\n",
            "iteration: 150 loss: 0.10112434\n",
            "iteration: 151 loss: 1.51013434\n",
            "iteration: 152 loss: 0.90721387\n",
            "iteration: 153 loss: 0.92234820\n",
            "iteration: 154 loss: 0.22934829\n",
            "iteration: 155 loss: 0.14913379\n",
            "iteration: 156 loss: 0.52277058\n",
            "iteration: 157 loss: 0.03390404\n",
            "iteration: 158 loss: 0.52124506\n",
            "iteration: 159 loss: 0.56177449\n",
            "iteration: 160 loss: 0.05501220\n",
            "iteration: 161 loss: 1.37859523\n",
            "iteration: 162 loss: 1.08904421\n",
            "iteration: 163 loss: 0.14801191\n",
            "iteration: 164 loss: 0.09829485\n",
            "iteration: 165 loss: 1.15278220\n",
            "iteration: 166 loss: 0.09838118\n",
            "iteration: 167 loss: 0.43382359\n",
            "iteration: 168 loss: 0.27080029\n",
            "iteration: 169 loss: 0.07581657\n",
            "iteration: 170 loss: 1.04520667\n",
            "iteration: 171 loss: 0.14787786\n",
            "iteration: 172 loss: 0.12983872\n",
            "iteration: 173 loss: 1.96788561\n",
            "iteration: 174 loss: 0.69411212\n",
            "iteration: 175 loss: 0.12059222\n",
            "iteration: 176 loss: 0.13002649\n",
            "iteration: 177 loss: 0.59645784\n",
            "iteration: 178 loss: 0.51271558\n",
            "iteration: 179 loss: 0.05650414\n",
            "iteration: 180 loss: 0.09755208\n",
            "iteration: 181 loss: 0.60891849\n",
            "iteration: 182 loss: 0.22195028\n",
            "iteration: 183 loss: 0.68483686\n",
            "iteration: 184 loss: 0.31970659\n",
            "iteration: 185 loss: 0.49054426\n",
            "iteration: 186 loss: 0.24531879\n",
            "iteration: 187 loss: 0.66663897\n",
            "iteration: 188 loss: 0.13687456\n",
            "iteration: 189 loss: 1.62422240\n",
            "iteration: 190 loss: 0.13479976\n",
            "iteration: 191 loss: 1.46606338\n",
            "iteration: 192 loss: 0.12403749\n",
            "iteration: 193 loss: 0.14261310\n",
            "iteration: 194 loss: 1.20458961\n",
            "iteration: 195 loss: 1.46876371\n",
            "iteration: 196 loss: 0.61851186\n",
            "iteration: 197 loss: 0.04427899\n",
            "iteration: 198 loss: 0.27170208\n",
            "iteration: 199 loss: 0.43268892\n",
            "epoch:  97 mean loss training: 0.57779330\n",
            "epoch:  97 mean loss validation: 0.82391280\n",
            "iteration:   0 loss: 0.04447704\n",
            "iteration:   1 loss: 0.90460986\n",
            "iteration:   2 loss: 0.86139274\n",
            "iteration:   3 loss: 1.77571392\n",
            "iteration:   4 loss: 0.21641171\n",
            "iteration:   5 loss: 0.83761978\n",
            "iteration:   6 loss: 0.29049531\n",
            "iteration:   7 loss: 0.33897820\n",
            "iteration:   8 loss: 0.33811611\n",
            "iteration:   9 loss: 0.19751130\n",
            "iteration:  10 loss: 1.39256120\n",
            "iteration:  11 loss: 0.13138670\n",
            "iteration:  12 loss: 0.97413421\n",
            "iteration:  13 loss: 0.11484693\n",
            "iteration:  14 loss: 0.60754764\n",
            "iteration:  15 loss: 0.92085850\n",
            "iteration:  16 loss: 0.05423408\n",
            "iteration:  17 loss: 0.17047715\n",
            "iteration:  18 loss: 2.09889102\n",
            "iteration:  19 loss: 0.64687079\n",
            "iteration:  20 loss: 0.89659685\n",
            "iteration:  21 loss: 0.63901585\n",
            "iteration:  22 loss: 0.03937827\n",
            "iteration:  23 loss: 0.05971286\n",
            "iteration:  24 loss: 0.05772578\n",
            "iteration:  25 loss: 0.61900824\n",
            "iteration:  26 loss: 0.67926681\n",
            "iteration:  27 loss: 0.61374009\n",
            "iteration:  28 loss: 1.64965200\n",
            "iteration:  29 loss: 0.06685147\n",
            "iteration:  30 loss: 0.12088217\n",
            "iteration:  31 loss: 1.30349040\n",
            "iteration:  32 loss: 0.82963496\n",
            "iteration:  33 loss: 0.39998800\n",
            "iteration:  34 loss: 0.14122309\n",
            "iteration:  35 loss: 1.85265207\n",
            "iteration:  36 loss: 0.20036478\n",
            "iteration:  37 loss: 0.68824911\n",
            "iteration:  38 loss: 0.49128792\n",
            "iteration:  39 loss: 0.07041128\n",
            "iteration:  40 loss: 0.23201956\n",
            "iteration:  41 loss: 0.98958552\n",
            "iteration:  42 loss: 0.50872564\n",
            "iteration:  43 loss: 0.82566118\n",
            "iteration:  44 loss: 0.34137246\n",
            "iteration:  45 loss: 0.15498690\n",
            "iteration:  46 loss: 0.88694096\n",
            "iteration:  47 loss: 0.21815719\n",
            "iteration:  48 loss: 0.51208472\n",
            "iteration:  49 loss: 0.35908216\n",
            "iteration:  50 loss: 0.56337661\n",
            "iteration:  51 loss: 0.08203945\n",
            "iteration:  52 loss: 0.49228904\n",
            "iteration:  53 loss: 0.08629627\n",
            "iteration:  54 loss: 0.61362940\n",
            "iteration:  55 loss: 0.60149831\n",
            "iteration:  56 loss: 0.42800635\n",
            "iteration:  57 loss: 1.06158841\n",
            "iteration:  58 loss: 0.34718457\n",
            "iteration:  59 loss: 4.21915388\n",
            "iteration:  60 loss: 0.51024592\n",
            "iteration:  61 loss: 0.25092736\n",
            "iteration:  62 loss: 0.23340815\n",
            "iteration:  63 loss: 0.31218389\n",
            "iteration:  64 loss: 1.97038054\n",
            "iteration:  65 loss: 2.21324086\n",
            "iteration:  66 loss: 0.08318728\n",
            "iteration:  67 loss: 0.72365969\n",
            "iteration:  68 loss: 0.55147928\n",
            "iteration:  69 loss: 0.32288533\n",
            "iteration:  70 loss: 0.57736272\n",
            "iteration:  71 loss: 0.90488815\n",
            "iteration:  72 loss: 0.09344094\n",
            "iteration:  73 loss: 1.68657470\n",
            "iteration:  74 loss: 1.61320639\n",
            "iteration:  75 loss: 0.31857255\n",
            "iteration:  76 loss: 0.12406330\n",
            "iteration:  77 loss: 0.75165415\n",
            "iteration:  78 loss: 0.26754892\n",
            "iteration:  79 loss: 0.06773906\n",
            "iteration:  80 loss: 0.06237568\n",
            "iteration:  81 loss: 0.01983787\n",
            "iteration:  82 loss: 0.15283246\n",
            "iteration:  83 loss: 0.52336764\n",
            "iteration:  84 loss: 1.24515891\n",
            "iteration:  85 loss: 1.04367220\n",
            "iteration:  86 loss: 0.04750257\n",
            "iteration:  87 loss: 0.60398775\n",
            "iteration:  88 loss: 0.59542239\n",
            "iteration:  89 loss: 0.93599135\n",
            "iteration:  90 loss: 0.09833670\n",
            "iteration:  91 loss: 0.06563714\n",
            "iteration:  92 loss: 0.50154120\n",
            "iteration:  93 loss: 0.57216358\n",
            "iteration:  94 loss: 1.31978846\n",
            "iteration:  95 loss: 0.06091596\n",
            "iteration:  96 loss: 0.17862204\n",
            "iteration:  97 loss: 0.34796703\n",
            "iteration:  98 loss: 0.57515961\n",
            "iteration:  99 loss: 0.27351937\n",
            "iteration: 100 loss: 0.74616140\n",
            "iteration: 101 loss: 0.57919371\n",
            "iteration: 102 loss: 0.54926878\n",
            "iteration: 103 loss: 0.38478440\n",
            "iteration: 104 loss: 1.55097520\n",
            "iteration: 105 loss: 0.15865867\n",
            "iteration: 106 loss: 0.62886482\n",
            "iteration: 107 loss: 0.11833635\n",
            "iteration: 108 loss: 1.20645893\n",
            "iteration: 109 loss: 1.23090684\n",
            "iteration: 110 loss: 0.29240116\n",
            "iteration: 111 loss: 1.09370089\n",
            "iteration: 112 loss: 0.70115143\n",
            "iteration: 113 loss: 0.26216364\n",
            "iteration: 114 loss: 0.97056508\n",
            "iteration: 115 loss: 0.39476648\n",
            "iteration: 116 loss: 0.33631700\n",
            "iteration: 117 loss: 0.39243242\n",
            "iteration: 118 loss: 0.41601971\n",
            "iteration: 119 loss: 2.28329182\n",
            "iteration: 120 loss: 0.20909859\n",
            "iteration: 121 loss: 1.75704980\n",
            "iteration: 122 loss: 0.47978017\n",
            "iteration: 123 loss: 1.48041368\n",
            "iteration: 124 loss: 0.13596122\n",
            "iteration: 125 loss: 0.24794286\n",
            "iteration: 126 loss: 0.39282838\n",
            "iteration: 127 loss: 0.25895303\n",
            "iteration: 128 loss: 0.15603915\n",
            "iteration: 129 loss: 0.48971164\n",
            "iteration: 130 loss: 0.33811182\n",
            "iteration: 131 loss: 0.34369838\n",
            "iteration: 132 loss: 0.10354118\n",
            "iteration: 133 loss: 0.51688623\n",
            "iteration: 134 loss: 0.13177599\n",
            "iteration: 135 loss: 0.22425362\n",
            "iteration: 136 loss: 0.24901645\n",
            "iteration: 137 loss: 0.63547164\n",
            "iteration: 138 loss: 0.10534881\n",
            "iteration: 139 loss: 0.63907015\n",
            "iteration: 140 loss: 0.44884828\n",
            "iteration: 141 loss: 0.04502262\n",
            "iteration: 142 loss: 0.41006696\n",
            "iteration: 143 loss: 1.18261886\n",
            "iteration: 144 loss: 0.61599272\n",
            "iteration: 145 loss: 0.38704857\n",
            "iteration: 146 loss: 0.52279902\n",
            "iteration: 147 loss: 1.84425616\n",
            "iteration: 148 loss: 0.19676670\n",
            "iteration: 149 loss: 0.03118686\n",
            "iteration: 150 loss: 0.64694482\n",
            "iteration: 151 loss: 0.20378084\n",
            "iteration: 152 loss: 0.38833016\n",
            "iteration: 153 loss: 1.76806390\n",
            "iteration: 154 loss: 0.54489416\n",
            "iteration: 155 loss: 2.10172272\n",
            "iteration: 156 loss: 0.45195785\n",
            "iteration: 157 loss: 0.10393760\n",
            "iteration: 158 loss: 0.31553385\n",
            "iteration: 159 loss: 0.17771809\n",
            "iteration: 160 loss: 0.12984569\n",
            "iteration: 161 loss: 0.15101869\n",
            "iteration: 162 loss: 0.47427464\n",
            "iteration: 163 loss: 0.97448349\n",
            "iteration: 164 loss: 0.02176431\n",
            "iteration: 165 loss: 0.61503196\n",
            "iteration: 166 loss: 0.26394659\n",
            "iteration: 167 loss: 0.42659694\n",
            "iteration: 168 loss: 0.03798532\n",
            "iteration: 169 loss: 0.12119325\n",
            "iteration: 170 loss: 0.24090250\n",
            "iteration: 171 loss: 0.25592095\n",
            "iteration: 172 loss: 0.22204135\n",
            "iteration: 173 loss: 0.08243674\n",
            "iteration: 174 loss: 0.28579536\n",
            "iteration: 175 loss: 0.19607517\n",
            "iteration: 176 loss: 0.59728694\n",
            "iteration: 177 loss: 0.62285644\n",
            "iteration: 178 loss: 0.50126958\n",
            "iteration: 179 loss: 0.08057375\n",
            "iteration: 180 loss: 0.91693258\n",
            "iteration: 181 loss: 0.53971797\n",
            "iteration: 182 loss: 1.25455570\n",
            "iteration: 183 loss: 0.24669568\n",
            "iteration: 184 loss: 0.70787060\n",
            "iteration: 185 loss: 1.48134840\n",
            "iteration: 186 loss: 0.12351675\n",
            "iteration: 187 loss: 0.03959407\n",
            "iteration: 188 loss: 0.18485999\n",
            "iteration: 189 loss: 0.06447863\n",
            "iteration: 190 loss: 2.00504947\n",
            "iteration: 191 loss: 0.39100471\n",
            "iteration: 192 loss: 1.46014225\n",
            "iteration: 193 loss: 0.58717781\n",
            "iteration: 194 loss: 0.19163233\n",
            "iteration: 195 loss: 0.08215518\n",
            "iteration: 196 loss: 0.43518883\n",
            "iteration: 197 loss: 0.10101538\n",
            "iteration: 198 loss: 0.09588339\n",
            "iteration: 199 loss: 0.35644570\n",
            "epoch:  98 mean loss training: 0.56902909\n",
            "epoch:  98 mean loss validation: 0.87549275\n",
            "iteration:   0 loss: 0.30749065\n",
            "iteration:   1 loss: 0.99845868\n",
            "iteration:   2 loss: 0.45591438\n",
            "iteration:   3 loss: 0.27488577\n",
            "iteration:   4 loss: 0.04897291\n",
            "iteration:   5 loss: 1.34952605\n",
            "iteration:   6 loss: 0.37915948\n",
            "iteration:   7 loss: 0.05011046\n",
            "iteration:   8 loss: 1.85851419\n",
            "iteration:   9 loss: 1.58751369\n",
            "iteration:  10 loss: 0.06636565\n",
            "iteration:  11 loss: 0.16518417\n",
            "iteration:  12 loss: 0.24936558\n",
            "iteration:  13 loss: 0.96704739\n",
            "iteration:  14 loss: 0.05490740\n",
            "iteration:  15 loss: 0.57339126\n",
            "iteration:  16 loss: 0.35897437\n",
            "iteration:  17 loss: 2.65861654\n",
            "iteration:  18 loss: 0.22002193\n",
            "iteration:  19 loss: 0.27723390\n",
            "iteration:  20 loss: 1.99549317\n",
            "iteration:  21 loss: 0.51957971\n",
            "iteration:  22 loss: 0.44548583\n",
            "iteration:  23 loss: 0.94435823\n",
            "iteration:  24 loss: 0.94297892\n",
            "iteration:  25 loss: 0.08990655\n",
            "iteration:  26 loss: 0.59493405\n",
            "iteration:  27 loss: 0.53371519\n",
            "iteration:  28 loss: 0.22418658\n",
            "iteration:  29 loss: 0.47014368\n",
            "iteration:  30 loss: 0.61519986\n",
            "iteration:  31 loss: 0.35391626\n",
            "iteration:  32 loss: 1.33015311\n",
            "iteration:  33 loss: 0.07401215\n",
            "iteration:  34 loss: 0.99605143\n",
            "iteration:  35 loss: 0.44106308\n",
            "iteration:  36 loss: 0.75320709\n",
            "iteration:  37 loss: 0.75710833\n",
            "iteration:  38 loss: 1.78174138\n",
            "iteration:  39 loss: 1.02461743\n",
            "iteration:  40 loss: 0.81888074\n",
            "iteration:  41 loss: 0.09822821\n",
            "iteration:  42 loss: 0.89516294\n",
            "iteration:  43 loss: 0.79549813\n",
            "iteration:  44 loss: 0.82969695\n",
            "iteration:  45 loss: 0.61940390\n",
            "iteration:  46 loss: 0.26227814\n",
            "iteration:  47 loss: 0.43670237\n",
            "iteration:  48 loss: 0.08564650\n",
            "iteration:  49 loss: 1.00595117\n",
            "iteration:  50 loss: 0.69472682\n",
            "iteration:  51 loss: 0.38272008\n",
            "iteration:  52 loss: 0.21525341\n",
            "iteration:  53 loss: 0.07245512\n",
            "iteration:  54 loss: 0.50290686\n",
            "iteration:  55 loss: 0.17805685\n",
            "iteration:  56 loss: 0.53981626\n",
            "iteration:  57 loss: 0.75839615\n",
            "iteration:  58 loss: 0.34482604\n",
            "iteration:  59 loss: 1.68677652\n",
            "iteration:  60 loss: 0.08176171\n",
            "iteration:  61 loss: 0.16471793\n",
            "iteration:  62 loss: 0.29815125\n",
            "iteration:  63 loss: 0.08873096\n",
            "iteration:  64 loss: 0.14300370\n",
            "iteration:  65 loss: 0.09421696\n",
            "iteration:  66 loss: 0.15970472\n",
            "iteration:  67 loss: 0.46541923\n",
            "iteration:  68 loss: 2.61963105\n",
            "iteration:  69 loss: 0.76088095\n",
            "iteration:  70 loss: 0.16551894\n",
            "iteration:  71 loss: 0.34212658\n",
            "iteration:  72 loss: 0.26624873\n",
            "iteration:  73 loss: 0.07982495\n",
            "iteration:  74 loss: 0.20749539\n",
            "iteration:  75 loss: 0.59677762\n",
            "iteration:  76 loss: 1.22145319\n",
            "iteration:  77 loss: 0.42790246\n",
            "iteration:  78 loss: 0.18610106\n",
            "iteration:  79 loss: 0.56936848\n",
            "iteration:  80 loss: 0.61745703\n",
            "iteration:  81 loss: 1.22821653\n",
            "iteration:  82 loss: 0.46365106\n",
            "iteration:  83 loss: 0.58887565\n",
            "iteration:  84 loss: 0.22694372\n",
            "iteration:  85 loss: 0.09900706\n",
            "iteration:  86 loss: 0.32514584\n",
            "iteration:  87 loss: 0.21379635\n",
            "iteration:  88 loss: 0.46040165\n",
            "iteration:  89 loss: 0.91616952\n",
            "iteration:  90 loss: 0.28145260\n",
            "iteration:  91 loss: 0.28494397\n",
            "iteration:  92 loss: 0.60690171\n",
            "iteration:  93 loss: 0.18472335\n",
            "iteration:  94 loss: 0.27759153\n",
            "iteration:  95 loss: 0.47128555\n",
            "iteration:  96 loss: 0.11648541\n",
            "iteration:  97 loss: 0.22342780\n",
            "iteration:  98 loss: 0.88069969\n",
            "iteration:  99 loss: 0.02969878\n",
            "iteration: 100 loss: 2.20483947\n",
            "iteration: 101 loss: 0.80890501\n",
            "iteration: 102 loss: 0.17239377\n",
            "iteration: 103 loss: 1.65089130\n",
            "iteration: 104 loss: 0.09191465\n",
            "iteration: 105 loss: 0.57460099\n",
            "iteration: 106 loss: 0.12968600\n",
            "iteration: 107 loss: 0.35806537\n",
            "iteration: 108 loss: 2.04896879\n",
            "iteration: 109 loss: 0.36787915\n",
            "iteration: 110 loss: 1.43741167\n",
            "iteration: 111 loss: 0.10394359\n",
            "iteration: 112 loss: 0.36979079\n",
            "iteration: 113 loss: 0.18039772\n",
            "iteration: 114 loss: 0.25638682\n",
            "iteration: 115 loss: 0.15678056\n",
            "iteration: 116 loss: 1.07892311\n",
            "iteration: 117 loss: 0.89380819\n",
            "iteration: 118 loss: 0.76473695\n",
            "iteration: 119 loss: 0.76656687\n",
            "iteration: 120 loss: 0.35513860\n",
            "iteration: 121 loss: 0.33554649\n",
            "iteration: 122 loss: 0.19363065\n",
            "iteration: 123 loss: 0.91995388\n",
            "iteration: 124 loss: 0.29694602\n",
            "iteration: 125 loss: 0.40003046\n",
            "iteration: 126 loss: 0.26951268\n",
            "iteration: 127 loss: 1.28270972\n",
            "iteration: 128 loss: 0.32207048\n",
            "iteration: 129 loss: 0.57749021\n",
            "iteration: 130 loss: 0.05609370\n",
            "iteration: 131 loss: 0.09367828\n",
            "iteration: 132 loss: 0.29612666\n",
            "iteration: 133 loss: 0.37593156\n",
            "iteration: 134 loss: 0.18390033\n",
            "iteration: 135 loss: 0.85410154\n",
            "iteration: 136 loss: 2.21893835\n",
            "iteration: 137 loss: 0.37498710\n",
            "iteration: 138 loss: 0.54558951\n",
            "iteration: 139 loss: 0.07614989\n",
            "iteration: 140 loss: 2.95587969\n",
            "iteration: 141 loss: 0.85200864\n",
            "iteration: 142 loss: 0.41009822\n",
            "iteration: 143 loss: 0.57786554\n",
            "iteration: 144 loss: 0.58192044\n",
            "iteration: 145 loss: 0.33778295\n",
            "iteration: 146 loss: 0.30232805\n",
            "iteration: 147 loss: 0.52414459\n",
            "iteration: 148 loss: 0.96730477\n",
            "iteration: 149 loss: 0.45676270\n",
            "iteration: 150 loss: 1.86146581\n",
            "iteration: 151 loss: 0.23029594\n",
            "iteration: 152 loss: 0.31997299\n",
            "iteration: 153 loss: 0.41063476\n",
            "iteration: 154 loss: 1.18985999\n",
            "iteration: 155 loss: 0.51122218\n",
            "iteration: 156 loss: 0.94234747\n",
            "iteration: 157 loss: 0.64250708\n",
            "iteration: 158 loss: 1.38005173\n",
            "iteration: 159 loss: 0.23274191\n",
            "iteration: 160 loss: 0.26212117\n",
            "iteration: 161 loss: 1.65637767\n",
            "iteration: 162 loss: 0.45657104\n",
            "iteration: 163 loss: 0.29295874\n",
            "iteration: 164 loss: 0.71208090\n",
            "iteration: 165 loss: 0.35488772\n",
            "iteration: 166 loss: 1.01125312\n",
            "iteration: 167 loss: 1.88633406\n",
            "iteration: 168 loss: 0.69918495\n",
            "iteration: 169 loss: 0.91197449\n",
            "iteration: 170 loss: 0.38090670\n",
            "iteration: 171 loss: 0.07022162\n",
            "iteration: 172 loss: 0.09701075\n",
            "iteration: 173 loss: 0.47128466\n",
            "iteration: 174 loss: 0.09851815\n",
            "iteration: 175 loss: 0.32117611\n",
            "iteration: 176 loss: 2.20742202\n",
            "iteration: 177 loss: 0.21883628\n",
            "iteration: 178 loss: 0.42962766\n",
            "iteration: 179 loss: 0.30807981\n",
            "iteration: 180 loss: 0.05246321\n",
            "iteration: 181 loss: 2.20156264\n",
            "iteration: 182 loss: 0.05229478\n",
            "iteration: 183 loss: 0.56183469\n",
            "iteration: 184 loss: 1.44224977\n",
            "iteration: 185 loss: 0.06582228\n",
            "iteration: 186 loss: 0.57234406\n",
            "iteration: 187 loss: 0.36181006\n",
            "iteration: 188 loss: 0.12931009\n",
            "iteration: 189 loss: 0.22893213\n",
            "iteration: 190 loss: 0.23264663\n",
            "iteration: 191 loss: 0.37271988\n",
            "iteration: 192 loss: 0.34929037\n",
            "iteration: 193 loss: 0.76976854\n",
            "iteration: 194 loss: 0.29395664\n",
            "iteration: 195 loss: 0.64917970\n",
            "iteration: 196 loss: 0.30875131\n",
            "iteration: 197 loss: 0.68779129\n",
            "iteration: 198 loss: 0.45712149\n",
            "iteration: 199 loss: 0.49151194\n",
            "epoch:  99 mean loss training: 0.59854805\n",
            "epoch:  99 mean loss validation: 0.83866215\n",
            "iteration:   0 loss: 0.70516092\n",
            "iteration:   1 loss: 0.51383877\n",
            "iteration:   2 loss: 0.45050302\n",
            "iteration:   3 loss: 0.94143432\n",
            "iteration:   4 loss: 0.67015368\n",
            "iteration:   5 loss: 0.28297684\n",
            "iteration:   6 loss: 1.17977679\n",
            "iteration:   7 loss: 0.11880221\n",
            "iteration:   8 loss: 0.25854710\n",
            "iteration:   9 loss: 0.50403398\n",
            "iteration:  10 loss: 1.01235890\n",
            "iteration:  11 loss: 0.41536194\n",
            "iteration:  12 loss: 0.10124478\n",
            "iteration:  13 loss: 0.36029994\n",
            "iteration:  14 loss: 0.72145033\n",
            "iteration:  15 loss: 0.23906122\n",
            "iteration:  16 loss: 0.03560866\n",
            "iteration:  17 loss: 0.65744746\n",
            "iteration:  18 loss: 0.31575266\n",
            "iteration:  19 loss: 0.13400201\n",
            "iteration:  20 loss: 0.22364002\n",
            "iteration:  21 loss: 0.10701066\n",
            "iteration:  22 loss: 0.12036745\n",
            "iteration:  23 loss: 0.31940031\n",
            "iteration:  24 loss: 0.32243118\n",
            "iteration:  25 loss: 1.78836644\n",
            "iteration:  26 loss: 0.47238556\n",
            "iteration:  27 loss: 0.67117125\n",
            "iteration:  28 loss: 1.84123576\n",
            "iteration:  29 loss: 0.24384750\n",
            "iteration:  30 loss: 0.44356877\n",
            "iteration:  31 loss: 1.25674450\n",
            "iteration:  32 loss: 0.28841093\n",
            "iteration:  33 loss: 1.71027017\n",
            "iteration:  34 loss: 0.32529050\n",
            "iteration:  35 loss: 0.29567069\n",
            "iteration:  36 loss: 0.15008408\n",
            "iteration:  37 loss: 0.46875903\n",
            "iteration:  38 loss: 0.24659544\n",
            "iteration:  39 loss: 0.62290788\n",
            "iteration:  40 loss: 1.18253517\n",
            "iteration:  41 loss: 1.84632587\n",
            "iteration:  42 loss: 0.25181967\n",
            "iteration:  43 loss: 0.06256526\n",
            "iteration:  44 loss: 0.23006292\n",
            "iteration:  45 loss: 0.28049019\n",
            "iteration:  46 loss: 0.74063236\n",
            "iteration:  47 loss: 0.25563994\n",
            "iteration:  48 loss: 1.03913879\n",
            "iteration:  49 loss: 0.20207068\n",
            "iteration:  50 loss: 0.50093329\n",
            "iteration:  51 loss: 0.68403906\n",
            "iteration:  52 loss: 0.59243941\n",
            "iteration:  53 loss: 0.47236615\n",
            "iteration:  54 loss: 0.58850259\n",
            "iteration:  55 loss: 0.15134552\n",
            "iteration:  56 loss: 1.07857287\n",
            "iteration:  57 loss: 0.08257528\n",
            "iteration:  58 loss: 0.52909565\n",
            "iteration:  59 loss: 0.63081568\n",
            "iteration:  60 loss: 0.31592181\n",
            "iteration:  61 loss: 0.34902373\n",
            "iteration:  62 loss: 0.07544962\n",
            "iteration:  63 loss: 0.32360798\n",
            "iteration:  64 loss: 0.02096812\n",
            "iteration:  65 loss: 0.34209174\n",
            "iteration:  66 loss: 2.57847238\n",
            "iteration:  67 loss: 0.93630481\n",
            "iteration:  68 loss: 0.59525073\n",
            "iteration:  69 loss: 0.29134455\n",
            "iteration:  70 loss: 0.49572670\n",
            "iteration:  71 loss: 0.10378826\n",
            "iteration:  72 loss: 0.20157206\n",
            "iteration:  73 loss: 0.22498743\n",
            "iteration:  74 loss: 0.93376863\n",
            "iteration:  75 loss: 0.90919489\n",
            "iteration:  76 loss: 0.39574820\n",
            "iteration:  77 loss: 0.22644065\n",
            "iteration:  78 loss: 0.18113846\n",
            "iteration:  79 loss: 0.81161702\n",
            "iteration:  80 loss: 2.67764688\n",
            "iteration:  81 loss: 0.23940955\n",
            "iteration:  82 loss: 0.10878798\n",
            "iteration:  83 loss: 0.47199512\n",
            "iteration:  84 loss: 0.32673204\n",
            "iteration:  85 loss: 0.64703983\n",
            "iteration:  86 loss: 0.39281589\n",
            "iteration:  87 loss: 1.69100475\n",
            "iteration:  88 loss: 1.19348133\n",
            "iteration:  89 loss: 0.54743886\n",
            "iteration:  90 loss: 2.02220917\n",
            "iteration:  91 loss: 0.36459684\n",
            "iteration:  92 loss: 0.35020399\n",
            "iteration:  93 loss: 0.08200611\n",
            "iteration:  94 loss: 0.28017125\n",
            "iteration:  95 loss: 0.37902802\n",
            "iteration:  96 loss: 1.78685832\n",
            "iteration:  97 loss: 0.44747004\n",
            "iteration:  98 loss: 0.42567647\n",
            "iteration:  99 loss: 0.49517894\n",
            "iteration: 100 loss: 0.40036285\n",
            "iteration: 101 loss: 0.96626282\n",
            "iteration: 102 loss: 0.15714817\n",
            "iteration: 103 loss: 0.45503107\n",
            "iteration: 104 loss: 0.08814486\n",
            "iteration: 105 loss: 0.35600859\n",
            "iteration: 106 loss: 0.64135158\n",
            "iteration: 107 loss: 0.39414150\n",
            "iteration: 108 loss: 0.35452124\n",
            "iteration: 109 loss: 1.36630416\n",
            "iteration: 110 loss: 0.84058082\n",
            "iteration: 111 loss: 0.28850713\n",
            "iteration: 112 loss: 0.07256703\n",
            "iteration: 113 loss: 0.58015543\n",
            "iteration: 114 loss: 0.38611650\n",
            "iteration: 115 loss: 0.99209970\n",
            "iteration: 116 loss: 1.68980694\n",
            "iteration: 117 loss: 1.57074678\n",
            "iteration: 118 loss: 1.57576787\n",
            "iteration: 119 loss: 0.12349129\n",
            "iteration: 120 loss: 0.95953345\n",
            "iteration: 121 loss: 0.45173129\n",
            "iteration: 122 loss: 2.49672842\n",
            "iteration: 123 loss: 0.21761672\n",
            "iteration: 124 loss: 0.08861576\n",
            "iteration: 125 loss: 0.35281318\n",
            "iteration: 126 loss: 0.65825260\n",
            "iteration: 127 loss: 0.03323764\n",
            "iteration: 128 loss: 0.23066035\n",
            "iteration: 129 loss: 0.63551879\n",
            "iteration: 130 loss: 2.40199876\n",
            "iteration: 131 loss: 0.10053363\n",
            "iteration: 132 loss: 0.90806693\n",
            "iteration: 133 loss: 0.40148607\n",
            "iteration: 134 loss: 1.83255148\n",
            "iteration: 135 loss: 0.95716250\n",
            "iteration: 136 loss: 1.34927261\n",
            "iteration: 137 loss: 1.00443876\n",
            "iteration: 138 loss: 1.27340281\n",
            "iteration: 139 loss: 0.98417830\n",
            "iteration: 140 loss: 1.90994167\n",
            "iteration: 141 loss: 0.12128426\n",
            "iteration: 142 loss: 1.94038355\n",
            "iteration: 143 loss: 0.19548219\n",
            "iteration: 144 loss: 0.14716142\n",
            "iteration: 145 loss: 0.96089727\n",
            "iteration: 146 loss: 0.21206109\n",
            "iteration: 147 loss: 0.25615421\n",
            "iteration: 148 loss: 0.11518055\n",
            "iteration: 149 loss: 0.72422868\n",
            "iteration: 150 loss: 0.57259500\n",
            "iteration: 151 loss: 0.26191390\n",
            "iteration: 152 loss: 0.19976112\n",
            "iteration: 153 loss: 0.54606259\n",
            "iteration: 154 loss: 0.18845063\n",
            "iteration: 155 loss: 0.21805528\n",
            "iteration: 156 loss: 0.24976879\n",
            "iteration: 157 loss: 0.78385657\n",
            "iteration: 158 loss: 0.45952609\n",
            "iteration: 159 loss: 2.38074279\n",
            "iteration: 160 loss: 0.21855746\n",
            "iteration: 161 loss: 0.39305180\n",
            "iteration: 162 loss: 0.70401597\n",
            "iteration: 163 loss: 0.14622590\n",
            "iteration: 164 loss: 0.20890516\n",
            "iteration: 165 loss: 0.78526813\n",
            "iteration: 166 loss: 0.09146168\n",
            "iteration: 167 loss: 0.32904023\n",
            "iteration: 168 loss: 0.78042471\n",
            "iteration: 169 loss: 0.28943220\n",
            "iteration: 170 loss: 1.68132710\n",
            "iteration: 171 loss: 0.57929975\n",
            "iteration: 172 loss: 1.10682082\n",
            "iteration: 173 loss: 0.33519751\n",
            "iteration: 174 loss: 0.84471905\n",
            "iteration: 175 loss: 0.91090989\n",
            "iteration: 176 loss: 0.19605958\n",
            "iteration: 177 loss: 0.77512127\n",
            "iteration: 178 loss: 0.06211498\n",
            "iteration: 179 loss: 1.31291246\n",
            "iteration: 180 loss: 0.09611842\n",
            "iteration: 181 loss: 0.15445574\n",
            "iteration: 182 loss: 1.14396381\n",
            "iteration: 183 loss: 0.32630989\n",
            "iteration: 184 loss: 0.18787794\n",
            "iteration: 185 loss: 0.09156596\n",
            "iteration: 186 loss: 0.87874836\n",
            "iteration: 187 loss: 1.61306036\n",
            "iteration: 188 loss: 0.43464723\n",
            "iteration: 189 loss: 0.09606418\n",
            "iteration: 190 loss: 0.64345139\n",
            "iteration: 191 loss: 1.02218533\n",
            "iteration: 192 loss: 0.52731109\n",
            "iteration: 193 loss: 0.30420616\n",
            "iteration: 194 loss: 0.56476790\n",
            "iteration: 195 loss: 0.06323845\n",
            "iteration: 196 loss: 0.48434126\n",
            "iteration: 197 loss: 0.25199372\n",
            "iteration: 198 loss: 0.52711755\n",
            "iteration: 199 loss: 1.14958572\n",
            "epoch: 100 mean loss training: 0.61786199\n",
            "epoch: 100 mean loss validation: 0.84307921\n"
          ]
        }
      ],
      "source": [
        "#Training the data\n",
        "epochs = 100 #The number of times the dataset will be used to train the model\n",
        "batch_size = 10 #Besaran kumpulan atau pecahan data dari dataset\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_valid = []\n",
        "\n",
        "best_loss_valid = np.inf\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    aggregated_losses_train = []\n",
        "    aggregated_losses_valid = []\n",
        "    i += 1\n",
        "#added random permutation for shuffle data training\n",
        "    idxs_train = np.random.permutation(train_records)\n",
        "    for j in range((train_records//batch_size)+1):\n",
        "        start_train = j*batch_size\n",
        "        end_train = start_train+batch_size\n",
        "\n",
        "        idxs_batch_train = idxs_train[start_train:end_train] #for shuffle training dataset\n",
        "\n",
        "        #input is replaced with categorical_train_data and numerical_train_data\n",
        "        train, train_embed = model(numerical_train_data[idxs_batch_train], categorical_train_data[idxs_batch_train])\n",
        "        \n",
        "        logits, mlogits = lmcl_loss(train_embed, train_outputs[idxs_batch_train])\n",
        "\n",
        "        train_loss = loss_function(mlogits, train_outputs[idxs_batch_train])\n",
        "        aggregated_losses_train.append(train_loss)\n",
        "\n",
        "        print(f'iteration: {j:3} loss: {train_loss.item():10.8f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_lmcl.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_lmcl.step()\n",
        "        mean_loss_train = torch.mean(torch.stack(aggregated_losses_train))\n",
        "        \n",
        "    print(f'epoch: {i:3} mean loss training: {mean_loss_train.item():10.8f}')\n",
        "    mean_losses_train.append(mean_loss_train)\n",
        "#==============================================================================================\n",
        "#validation\n",
        "#==============================================================================================        \n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        idxs_valid = np.random.permutation(valid_records)\n",
        "        for k in range((valid_records//batch_size)+1):\n",
        "            start_valid = k*batch_size\n",
        "            end_valid = start_valid+batch_size\n",
        "\n",
        "            idxs_batch_valid = idxs_valid[start_valid:end_valid] #for shuffle validation dataset\n",
        "\n",
        "            #input is replaced with categorical_valid_data and numerical_valid_data\n",
        "            valid, valid_embed = model(numerical_valid_data[idxs_batch_valid], categorical_valid_data[idxs_batch_valid])\n",
        "            logits, mlogits = lmcl_loss(valid_embed, valid_outputs[idxs_batch_valid])\n",
        "            \n",
        "            #loss name is differentiated between train and validation, outputs is changed to valid_outputs\n",
        "            valid_loss = loss_function(mlogits, valid_outputs[idxs_batch_valid])\n",
        "            aggregated_losses_valid.append(valid_loss)\n",
        "\n",
        "    mean_loss_valid = torch.mean(torch.stack(aggregated_losses_valid))\n",
        "\n",
        "    print(f'epoch: {i:3} mean loss validation: {mean_loss_valid:.8f}')\n",
        "\n",
        "#======================================================================\n",
        "#The model is saved when the loss is lowest not at the end of the epoch\n",
        "#======================================================================\n",
        "    if mean_loss_valid.cpu().numpy()[()] < best_loss_valid:\n",
        "        best_loss_valid = mean_loss_valid\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCL.pth\".format(churn_percentage))\n",
        "        torch.save(lmcl_loss.state_dict(), \"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCLOSS.pth\".format(churn_percentage))\n",
        "        best_epoch = i\n",
        "\n",
        "    mean_losses_valid.append(mean_loss_valid)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceKFzS_PhFPT",
        "outputId": "66adff3f-e875-439b-80f3-d0af3b33a176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Load training model\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCL.pth\".format(churn_percentage)))\n",
        "lmcl_loss.load_state_dict(torch.load(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCLOSS.pth\".format(churn_percentage)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INWvsnGGhFPU",
        "outputId": "6ee1df2b-05d7-44f4-e2d9-ac00e967671c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.84197229\n"
          ]
        }
      ],
      "source": [
        "#Creating predictions\n",
        "with torch.no_grad():\n",
        "    valid, valid_embed = model(numerical_valid_data, categorical_valid_data)\n",
        "    logits, mlogits = lmcl_loss(valid_embed, valid_outputs)\n",
        "    valid_loss = loss_function(mlogits, valid_outputs)\n",
        "    total_valid_loss = valid_loss\n",
        "print(f'Loss: {total_valid_loss:.8f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryLNu5EYhFPU",
        "outputId": "04613d17-20ca-4f2a-bf88-6d7967155449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[541  23]\n",
            " [ 24  78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       564\n",
            "           1       0.77      0.76      0.77       102\n",
            "\n",
            "    accuracy                           0.93       666\n",
            "   macro avg       0.86      0.86      0.86       666\n",
            "weighted avg       0.93      0.93      0.93       666\n",
            "\n",
            "Accuracy:  0.9294294294294294\n",
            "F1 Score:  0.8634215727768154\n"
          ]
        }
      ],
      "source": [
        "#=============================================================================================\n",
        "#the main result seen is the F1 Score, because\n",
        "#the misleading accuracy metric is used for imbalance data\n",
        "#=============================================================================================\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "valid_val = np.argmax(logits.data, axis=1)\n",
        "print(confusion_matrix(valid_outputs, valid_val))\n",
        "print(classification_report(valid_outputs, valid_val))\n",
        "print(\"Accuracy: \", accuracy_score(valid_outputs, valid_val))\n",
        "print(\"F1 Score: \", f1_score(valid_outputs, valid_val, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function for smoothing the loss plot by using exponential moving average\n",
        "#https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar\n",
        "def smooth(scalars, weight):\n",
        "    last = scalars[0]  # First value in the plot (first timestep)\n",
        "    smoothed = list()\n",
        "    for point in scalars:\n",
        "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
        "        smoothed.append(smoothed_val)                        # Save it\n",
        "        last = smoothed_val                                  # Anchor the last smoothed value\n",
        "\n",
        "    return smoothed"
      ],
      "metadata": {
        "id": "ghwl4CbNvEuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "me_losses_train = []\n",
        "for l in mean_losses_train:\n",
        "  me_losses_train.append(l.detach().numpy())\n",
        "\n",
        "print (me_losses_train)\n",
        "print (mean_losses_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj6Gmgc6vOC5",
        "outputId": "58e19109-2e8a-4a0a-ecd3-fd97f396b62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(1.8500252, dtype=float32), array(1.2968166, dtype=float32), array(1.1473203, dtype=float32), array(1.0358256, dtype=float32), array(0.9656413, dtype=float32), array(0.9585012, dtype=float32), array(0.88371557, dtype=float32), array(0.85299337, dtype=float32), array(0.8776589, dtype=float32), array(0.80568755, dtype=float32), array(0.8088623, dtype=float32), array(0.76407427, dtype=float32), array(0.82411987, dtype=float32), array(0.7525364, dtype=float32), array(0.7627035, dtype=float32), array(0.7372519, dtype=float32), array(0.7198498, dtype=float32), array(0.709865, dtype=float32), array(0.6816058, dtype=float32), array(0.70940936, dtype=float32), array(0.6766739, dtype=float32), array(0.68686074, dtype=float32), array(0.667416, dtype=float32), array(0.6354763, dtype=float32), array(0.6751758, dtype=float32), array(0.6496592, dtype=float32), array(0.65353966, dtype=float32), array(0.6255463, dtype=float32), array(0.65573347, dtype=float32), array(0.6448337, dtype=float32), array(0.6065363, dtype=float32), array(0.6157565, dtype=float32), array(0.60805756, dtype=float32), array(0.5942186, dtype=float32), array(0.6218776, dtype=float32), array(0.62150836, dtype=float32), array(0.63827133, dtype=float32), array(0.6179061, dtype=float32), array(0.6291132, dtype=float32), array(0.5887181, dtype=float32), array(0.6139445, dtype=float32), array(0.6385355, dtype=float32), array(0.6004377, dtype=float32), array(0.57347834, dtype=float32), array(0.6019932, dtype=float32), array(0.61737865, dtype=float32), array(0.59158695, dtype=float32), array(0.57575667, dtype=float32), array(0.59996724, dtype=float32), array(0.6281389, dtype=float32), array(0.6239619, dtype=float32), array(0.58203715, dtype=float32), array(0.6109886, dtype=float32), array(0.600146, dtype=float32), array(0.5880411, dtype=float32), array(0.59937006, dtype=float32), array(0.5879728, dtype=float32), array(0.59201056, dtype=float32), array(0.58789045, dtype=float32), array(0.59103644, dtype=float32), array(0.6021736, dtype=float32), array(0.58331263, dtype=float32), array(0.5943839, dtype=float32), array(0.58523226, dtype=float32), array(0.56969684, dtype=float32), array(0.62768185, dtype=float32), array(0.6063639, dtype=float32), array(0.5662424, dtype=float32), array(0.6549097, dtype=float32), array(0.5971464, dtype=float32), array(0.60324323, dtype=float32), array(0.5782076, dtype=float32), array(0.6114233, dtype=float32), array(0.58228564, dtype=float32), array(0.60528594, dtype=float32), array(0.588105, dtype=float32), array(0.5897795, dtype=float32), array(0.5977676, dtype=float32), array(0.58404815, dtype=float32), array(0.55671406, dtype=float32), array(0.60626465, dtype=float32), array(0.6006418, dtype=float32), array(0.62951887, dtype=float32), array(0.5459395, dtype=float32), array(0.5842001, dtype=float32), array(0.58356476, dtype=float32), array(0.58690816, dtype=float32), array(0.5800218, dtype=float32), array(0.63063526, dtype=float32), array(0.5894855, dtype=float32), array(0.6073493, dtype=float32), array(0.59283423, dtype=float32), array(0.60434747, dtype=float32), array(0.5777463, dtype=float32), array(0.6213782, dtype=float32), array(0.5949186, dtype=float32), array(0.5777933, dtype=float32), array(0.5690291, dtype=float32), array(0.59854805, dtype=float32), array(0.617862, dtype=float32)]\n",
            "[tensor(1.1410), tensor(1.0447), tensor(1.0255), tensor(0.8782), tensor(0.8904), tensor(0.8751), tensor(0.8288), tensor(0.8867), tensor(0.8750), tensor(0.8826), tensor(0.8901), tensor(0.8490), tensor(0.9283), tensor(0.8516), tensor(0.8466), tensor(0.9029), tensor(0.8379), tensor(0.8383), tensor(0.8143), tensor(0.8978), tensor(0.8781), tensor(0.8177), tensor(0.8972), tensor(0.8577), tensor(0.8249), tensor(0.8493), tensor(0.8733), tensor(0.8817), tensor(0.8122), tensor(0.8163), tensor(0.9227), tensor(0.8677), tensor(0.8525), tensor(0.8300), tensor(0.8316), tensor(0.8136), tensor(0.8407), tensor(0.8477), tensor(0.8222), tensor(0.8805), tensor(0.8955), tensor(0.8453), tensor(0.8137), tensor(0.8475), tensor(0.8583), tensor(0.9198), tensor(0.9350), tensor(0.8273), tensor(0.8317), tensor(0.8327), tensor(0.8468), tensor(0.8349), tensor(0.8220), tensor(0.9503), tensor(0.8796), tensor(0.8634), tensor(0.8014), tensor(0.8243), tensor(0.8785), tensor(0.8715), tensor(0.9294), tensor(0.8198), tensor(0.8981), tensor(0.8334), tensor(0.8840), tensor(0.8397), tensor(0.8322), tensor(0.8451), tensor(0.8941), tensor(0.8046), tensor(0.8484), tensor(0.8388), tensor(0.8798), tensor(0.8375), tensor(0.9109), tensor(0.8478), tensor(0.8615), tensor(0.8481), tensor(0.8664), tensor(0.8388), tensor(0.8688), tensor(0.8201), tensor(0.8202), tensor(0.8817), tensor(0.8226), tensor(0.8812), tensor(0.8940), tensor(0.8363), tensor(0.9646), tensor(0.8697), tensor(0.8199), tensor(0.8046), tensor(0.7922), tensor(0.9822), tensor(0.8684), tensor(0.8214), tensor(0.8239), tensor(0.8755), tensor(0.8387), tensor(0.8431)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5j7ZbFxhFPV",
        "outputId": "1696aa97-b3d0-4c00-a614-e5b96897635c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xV9f3H8dc3e5OQwQoQRth7CYK4Aa27Cu5Rlbpqa7WtHb+qXdpqrXtXrVuKuw5cCDhQwpQhe4UACQlZZCff3x/fsDNukntzM97PxyMPSO45534y7j3v813HWGsRERERkeYV4O8CRERERNojhTARERERP1AIExEREfEDhTARERERP1AIExEREfEDhTARERERP1AIE5EWxRizxRhzir/r8IW2/L2JSMMphImIiIj4gUKYiIgHjDFB/q5BRNoWhTARabGMMaHGmAeMMRnVHw8YY0KrH0swxvzPGJNrjMkxxiwwxgRUP/YbY8wOY0yBMWatMebkWo4fb4x5zxiTb4xZZIz5izHmy0Met8aYG40x64H11V970BizvXqfxcaY4w7Z/k5jzGxjzOvVz73EGDP8iKcdYYxZYYzJq94uzNs/NxFpHRTCRKQl+z0wHhgBDAfGAX+ofuxWIB1IBDoBvwOsMaY/cBMw1lobDUwFttRy/EeBfUBn4IrqjyOdAxwDDKr+fFF1PR2BV4D/HhGkzgb+e8jjbxtjgg95fDowDegFDAOurPtHICJtlUKYiLRklwB/stZmWmuzgLuAy6ofKwe6AD2tteXW2gXW3Qy3EggFBhljgq21W6y1G488sDEmEPgxcIe1tshauxr4Tw013G2tzbHWFgNYa1+y1mZbayustf+sfq7+h2y/2Fo721pbDtwPhOGC5H4PWWszrLU5wHu4QCci7ZBCmIi0ZF2BrYd8vrX6awD3AhuAj40xm4wxtwNYazcAvwDuBDKNMa8ZY7pytEQgCNh+yNe217DdYV8zxtxmjFlT3Z2YC3QAEmra3lpbhWutO/T5dx3y/yIgqobnFJF2QCFMRFqyDKDnIZ/3qP4a1toCa+2t1trewFnAL/eP/bLWvmKtnVS9rwX+XsOxs4AKIPmQr3WvYTu7/z/V479+jetSjLPWxgJ5gKnpGNVj1JL31ywiciiFMBFpyV4F/mCMSTTGJAB/BF4CMMacYYzpa4wxuCBUCVQZY/obY06qHsBfAhQDVUce2FpbCbwJ3GmMiTDGDAAur6eeaFxwywKCjDF/BGKO2Ga0Mea86tmUvwBKgYWN+u5FpE1TCBORluwvQBqwAvgeWFL9NYBU4FOgEPgGeMxaOxc3RuseYA+u6y8J+G0tx78J1524C3gRF/pK66hnDvARsA7XNVrC0V2Y7wAzgL248WvnVY8PExE5jHHjWEVExBjzd6CztbamWZKe7H8n0Ndae6lXCxORNkktYSLSbhljBhhjhhlnHHA18Ja/6xKR9kErQItIexaN64LsCuwG/onrThQR8Tl1R4qIiIj4gbojRURERPxAIUxERETED1rdmLCEhASbkpLi7zJERERE6rV48eI91trEmh5rdSEsJSWFtLQ0f5chIiIiUi9jzNbaHlN3pIiIiIgfKISJiIiI+IFCmIiIiIgftLoxYSIiItJ05eXlpKenU1JS4u9S2oSwsDCSk5MJDg72eB+FMBERkXYoPT2d6OhoUlJSMMb4u5xWzVpLdnY26enp9OrVy+P91B0pIiLSDpWUlBAfH68A5gXGGOLj4xvcqqgQJiIi0k4pgHlPY36WCmEiIiLS7HJzc3nssccavN/pp59Obm6uDypqfgphIiIi0uxqC2EVFRV17vfBBx8QGxvrq7KalULYEbILS3nl222k7y3ydykiIiJt1u23387GjRsZMWIEY8eO5bjjjuOss85i0KBBAJxzzjmMHj2awYMH89RTTx3YLyUlhT179rBlyxYGDhzItddey+DBg5kyZQrFxcX++nYaRSHsCJkFpfzure9ZkZ7n71JERETarHvuuYc+ffqwbNky7r33XpYsWcKDDz7IunXrAHj22WdZvHgxaWlpPPTQQ2RnZx91jPXr13PjjTeyatUqYmNjeeONN5r722gSLVFxhKToUAAy87VuioiItA93vbeK1Rn5Xj3moK4x3HHmYI+3Hzdu3GHLOzz00EO89dZbAGzfvp3169cTHx9/2D69evVixIgRAIwePZotW7Y0vfBmpBB2hLiIEIICDFmFpf4uRUREpN2IjIw88P8vvviCTz/9lG+++YaIiAhOOOGEGpd/CA0NPfD/wMDAVtcdqRB2hIAAQ0JUKJn5CmEiItI+NKTFyluio6MpKCio8bG8vDzi4uKIiIjghx9+YOHChc1cXfNQCKtBYnSoWsJERER8KD4+nokTJzJkyBDCw8Pp1KnTgcemTZvGE088wcCBA+nfvz/jx4/3Y6W+oxBWg6ToUHbmaUyYiIiIL73yyis1fj00NJQPP/ywxsf2j/tKSEhg5cqVB75+2223eb0+X/PZ7EhjzLPGmExjzMpaHu9gjHnPGLPcGLPKGHOVr2ppKLWEiYiIiK/5comK54FpdTx+I7DaWjscOAH4pzEmxIf1eCwpOpTswlIqq6y/SxEREZE2ymchzFo7H8ipaxMg2ribLUVVb1v3MrnNJDE6lCrrFm4VERER8QV/Ltb6CDAQyAC+B35ura3yYz0HJEaHAW7hVhERERFf8GcImwosA7oCI4BHjDExNW1ojJlpjEkzxqRlZWX5vLDE6gVbsxTCRERExEf8GcKuAt60zgZgMzCgpg2ttU9Za8dYa8ckJib6vLAkhTARERHxMX+GsG3AyQDGmE5Af2CTH+s5YH9LWGaBlqkQERFpCaKiogDIyMjg/PPPr3GbE044gbS0tDqP88ADD1BUVHTg89NPP53c3FzvFdoAvlyi4lXgG6C/MSbdGHO1MeY6Y8x11Zv8GTjWGPM98BnwG2vtHl/V0xBhwYHEhAWpJUxERKSF6dq1K7Nnz270/keGsA8++IDY2FhvlNZgvpwdeZG1tou1Ntham2yt/be19glr7RPVj2dYa6dYa4daa4dYa1/yVS2NkRgdqoH5IiIiPnL77bfz6KOPHvj8zjvv5C9/+Qsnn3wyo0aNYujQobzzzjtH7bdlyxaGDBkCQHFxMRdeeCEDBw7k3HPPPezekddffz1jxoxh8ODB3HHHHYC7KXhGRgYnnngiJ554IgApKSns2ePagO6//36GDBnCkCFDeOCBBw4838CBA7n22msZPHgwU6ZM8do9Kv3ZHdmiJUWHqSVMRETER2bMmMGsWbMOfD5r1iyuuOIK3nrrLZYsWcLcuXO59dZbsbb2NTsff/xxIiIiWLNmDXfddReLFy8+8Nhf//pX0tLSWLFiBfPmzWPFihXcfPPNdO3alblz5zJ37tzDjrV48WKee+45vv32WxYuXMjTTz/N0qVLAVi/fj033ngjq1atIjY2ljfeeMMrPwPdtqgWidGhLNvunz5iERGRZvXh7bDre+8es/NQOO2eWh8eOXIkmZmZZGRkkJWVRVxcHJ07d+aWW25h/vz5BAQEsGPHDnbv3k3nzp1rPMb8+fO5+eabARg2bBjDhg078NisWbN46qmnqKioYOfOnaxevfqwx4/05Zdfcu655xIZGQnAeeedx4IFCzjrrLPo1asXI0aMAGD06NEHbp3UVAphtUiKDiWroBRrLW49WREREfGmCy64gNmzZ7Nr1y5mzJjByy+/TFZWFosXLyY4OJiUlBRKSho+SW7z5s3cd999LFq0iLi4OK688spGHWe/0NDQA/8PDAz0WnekQlgtEqNDKS6vpLC0guiwYH+XIyIi4jt1tFj50owZM7j22mvZs2cP8+bNY9asWSQlJREcHMzcuXPZunVrnftPnjyZV155hZNOOomVK1eyYsUKAPLz84mMjKRDhw7s3r2bDz/8kBNOOAGA6OhoCgoKSEhIOOxYxx13HFdeeSW333471lreeustXnzxRZ983/sphNUiKebgWmEKYSIiIt43ePBgCgoK6NatG126dOGSSy7hzDPPZOjQoYwZM4YBA2pcPvSA66+/nquuuoqBAwcycOBARo8eDcDw4cMZOXIkAwYMoHv37kycOPHAPjNnzmTatGkHxobtN2rUKK688krGjRsHwDXXXMPIkSO91vVYE1PXgLeWaMyYMba+NUC84cv1e7j039/y2szxjO8d7/PnExERaU5r1qxh4MCB/i6jTanpZ2qMWWytHVPT9podWYtDW8JEREREvE0hrBaJUftXzVcIExEREe9TCKtFbEQwwYFGLWEiIiLiEwphtTDGkBgVqvtHiohIm9XaxoW3ZI35WSqE1SExRqvmi4hI2xQWFkZ2draCmBdYa8nOziYsLKxB+2mJijokRoWSvreo/g1FRERameTkZNLT08nKyvJ3KW1CWFgYycnJDdpHIawOSTGhLN22199liIiIeF1wcDC9evXydxntmroj65AYFUr2vjLKK6v8XYqIiIi0MQphddi/Vlh2YZmfKxEREZG2RiGsDgfXCtMMSREREfEuhbA6JMW4WQ6aISkiIiLephBWh8RorZovIiIivqEQVoeEqBBALWEiIiLifQphdQgNCiQ2IlhjwkRERMTrFMLqkRQdqpYwERER8TqFsHokRodqTJiIiIh4nUJYPZKidf9IERER8T6FsHrsbwnTDU5FRETEmxTC6pEUHUpZRRX5JRX+LkVERETaEIWweuxfKyxLMyRFRETEixTC6qEFW0VERMQXFMLqkRStWxeJiIiI9ymE1eNgd6RCmIiIiHiPQlg9YsKCCA0KUHekiIiIeJXPQpgx5lljTKYxZmUd25xgjFlmjFlljJnnq1qawhhDolbNFxERES/zZUvY88C02h40xsQCjwFnWWsHAxf4sJYmSYoO1f0jRURExKt8FsKstfOBnDo2uRh401q7rXr7TF/V0lRqCRMRERFv8+eYsH5AnDHmC2PMYmPM5X6spU5J0WEaEyYiIiJeFeTn5x4NnAyEA98YYxZaa9cduaExZiYwE6BHjx7NWiS4lrDconJKKyoJDQps9ucXERGRtsefLWHpwBxr7T5r7R5gPjC8pg2ttU9Za8dYa8ckJiY2a5HgxoQB7Cksa/bnFhERkbbJnyHsHWCSMSbIGBMBHAOs8WM9tTqwan6+BueLiIiId/isO9IY8ypwApBgjEkH7gCCAay1T1hr1xhjPgJWAFXAM9baWpez8Cetmi8iIiLe5rMQZq29yINt7gXu9VUN3qL7R4qIiIi3acV8DyREhRAUYNiZV+zvUkRERKSNUAjzQFBgAF1iw0jfqxAmIiIi3qEQ5qFuseEKYSIiIuI1CmEeSo6LIH1vkb/LEBERkTZCIcxDyXHhZBaUUlpR6e9SREREpA1QCPNQclwE1sLOXK0VJiIiIk2nEOah5LhwAI0LExEREa9QCPPQwRCmcWEiIiLSdAphHuocE0ZggFFLmIiIiHiFQpiHggID6BwTxo5chTARERFpOoWwBkiOC1d3pIiIiHiFQlgDuLXC1BImIiIiTacQ1gDJceHsyi+hrKLK36WIiIhIK6cQ1gDd4sLdWmG6kbeIiIg0kUJYA2itMBEREfEWhbAG6B4XAcAOhTARERFpIoWwBujcIYwAowVbRUREpOkUwhogODCALh3C1R0pIiIiTaYQ1kDd4hTCREREpOkUwhooOVYLtoqIiEjTKYQ10P61wsortVaYiIiINJ5CWAMlx0VQZWFXXom/SxEREZFWTCGsgfavFbZdXZIiIiLSBAphDZRcvVaYBueLiIhIUyiENdDBtcIUwkRERKTxFMIaKCQogE4xYZohKSIiIk2iENYIyXHhunWRiIiINIlCWCMkx0WoO1JERESaRCGsEfavFVahtcJERESkkRTCGiE5LpzKKstOrRUmIiIijeSzEGaMedYYk2mMWVnPdmONMRXGmPN9VYu3aZkKERERaSpftoQ9D0yrawNjTCDwd+BjH9bhdd1i3YKtmiEpIiIijeWzEGatnQ/k1LPZz4A3gExf1eELXWLDMAZ25KolTERERBrHb2PCjDHdgHOBxz3YdqYxJs0Yk5aVleX74uoRGhRIp+gwdUeKiIhIo/lzYP4DwG+stfVOMbTWPmWtHWOtHZOYmNgMpdUvOS5c3ZEiIiLSaEF+fO4xwGvGGIAE4HRjTIW19m0/1uSx5Lhw0rbu9XcZIiIi0kr5rSXMWtvLWptirU0BZgM3tJYABm6G5M48rRUmIiIijeOzljBjzKvACUCCMSYduAMIBrDWPuGr520u3arXCttdUHpgtqSIiIiIp3wWwqy1FzVg2yt9VYevJMdVL1ORU6QQJiIiIg2mFfMbSQu2ioiISFMohDVS19gwALZrhqSIiIg0gkJYI4UGBdK1QxhbsxXCREREpOEUwpqgT1IU6zML/F2GiIiItEIKYU2QmhTNxsx9VFVZf5ciIiIirYxCWBOkdoqiuLxS95AUERGRBlMIa4LUpCgANmQW+rkSERERaW0Uwpqgb3UI07gwERERaSiFsCaIjQghISpULWEiIiLSYAphTZSaFMV6hTARERFpIIWwJkrtFMWG3YVYqxmSIiIi4jmFsCZKTYqioLSC3fml/i5FREREWhGFsCbqo8H5IiIi0ggKYU2UmhQNaJkKERERaRiFsCZKiAohNiJYg/NFRESkQRTCmsgYQ2qSG5wvIiIi4imFMC/oqxt5i4iISAMphHlB36Ro9haVk12oGZIiIiLiGYUwL0g9MENSXZIiIiLiGYUwL0jtpBAmIiIiDaMQ5gWdY8KICg1iw26NCxMRERHPKIR5gTGGPklRbMhSS5iIiIh4RiHMS1KTolivZSpERETEQwphXpKaFEVmQSl5ReX+LkVERERaAY9CmDHm58aYGOP82xizxBgzxdfFtSZ9q2dIbsjSuDARERGpn6ctYT+x1uYDU4A44DLgHp9V1Qrtv4ekuiRFRETEE56GMFP97+nAi9baVYd8TYBuceGEBQfoRt4iIiLiEU9D2GJjzMe4EDbHGBMNVPmurNYnMMDQJzFKa4WJiIiIR4I83O5qYASwyVpbZIzpCFzlu7Jap9SkKBZt2evvMkRERKQV8LQlbAKw1lqba4y5FPgDkFfXDsaYZ40xmcaYlbU8fokxZoUx5ntjzNfGmOENK73l6ZsUxY7cYvaVVvi7FBEREWnhPA1hjwNF1UHpVmAj8EI9+zwPTKvj8c3A8dbaocCfgac8rKXF6ls9OH+jFm0VERGRengawiqstRY4G3jEWvsoEF3XDtba+UBOHY9/ba3d33e3EEj2sJYW68A9JDVDUkREROrh6ZiwAmPMb3FLUxxnjAkAgr1Yx9XAh148nl/07BhBcKDR4HwRERGpl6ctYTOAUtx6YbtwrVb3eqMAY8yJuBD2mzq2mWmMSTPGpGVlZXnjaX0iKDCA3glR/LAr39+liIiISAvnUQirDl4vAx2MMWcAJdba+saE1csYMwx4BjjbWptdx/M/Za0dY60dk5iY2NSn9akR3WNZtj2Xqirr71JERESkBfP0tkXTge+AC4DpwLfGmPOb8sTGmB7Am8Bl1tp1TTlWSzI6JY7conI27VGXpIiIiNTO0zFhvwfGWmszAYwxicCnwOzadjDGvAqcACQYY9KBO6geR2atfQL4IxAPPGaMATf4f0zjvo2WY3TPOAAWb917YLakiIiIyJE8DWEB+wNYtWzqaUWz1l5Uz+PXANd4+PytRu+ESOIigknbspcZY3v4uxwRERFpoTwNYR8ZY+YAr1Z/PgP4wDcltW7GGEb3jGPxNq2cLyIiIrXzdGD+r3CLqQ6r/njKWlvrbMb2blTPODZl7SNnX5m/SxEREZEWytOWMKy1bwBv+LCWNmN0DzcubMnWvZwyqJOfqxEREZGWqM6WMGNMgTEmv4aPAmOMFsOqxfDusQQFGHVJioiISK3qbAmz1mp6XyOEBQcyuFsHFm9VCBMREZGaebpifvthLWyaB5UVTTrM6B5xLN+eS1lFlZcKExERkbZEIexIW7+CF86CZS816TCje8ZRWlHF6p3qtRUREZGjKYQdqedE6D4e5v4NShu/6v2YlIOLtoqIiIgcSSHsSMbAlD9D4W745pFGH6ZTTBjdYsNZvDXHi8WJiIhIW6EQVpPu42DQ2fDVQ1Cwu9GHGd0zjsVb92KtbuYtIiIih1MIq83Jd0BlKXxxd6MPMSYljt35pezILfZiYSIiItIWKITVJr4PjLkalrwAWWsbdYhRPTQuTERERGqmEFaX438DIZHwyR2N2n1A52giQgIVwkREROQoCmF1iYyHSbfAug9hy5cN3j0oMICRPWIVwkREROQoCmH1GX89xHSDj/8AVQ1feHV0jzjW7MynsLRpi7+KiIhI26IQVp/gcDjpD5CxFJa/0uDdR/WMo8rC8u25PihOREREWiuFME8MmwE9J8H7t8Gu7xu068gecRgDaVvUJSkiIiIHKYR5IiAQLngOwmPh9Uuh2PNA1SE8mH5J0aRp0VYRERE5hEKYp6KSYPoLkLcD3pzZoPFhJwxI5OuN2WRovTARERGpphDWEN3HwbS7Yf3HMO/vHu926TE9sdby0sKtPixOREREWhOFsIYaew0Mvxjm3QNrP/Jol+4dIzh1UCde/W4bJeWVPi5QREREWgOFsIYyBs64HzoPc92S2Rs92u3KY3uxt6icd5dl+LhAERERaQ0UwhojOBxmvAi2Cubf59Eu43t3ZEDnaJ77eotu6C0iIiIKYY0WlwL9prrxYR4M0jfGcMWxKazZmc93mzVTUkREpL1TCGuKflOhaI9byNUD54zoRofwYJ7/eotv6xIREZEWTyGsKfqeAiYA1s/xaPPwkEAuHNedOat2sUPLVYiIiLRrCmFNEdERksfCOs9CGMBl43sC8OI3Wq5CRESkPVMIa6rUKbBzGRTs8mjz5LgIpgzqzGuLtFyFiIhIe6YQ1lT9prp/13/i8S5XTkwht6icd5bt8FFRIiIi0tIphDVVpyEQ083jcWEAx/SqXq7iKy1XISIi0l75LIQZY541xmQaY1bW8rgxxjxkjNlgjFlhjBnlq1p8yhhIPRU2fgEVZR7uYrhsQk9+2FXA8vQ839YnIiIiLZIvW8KeB6bV8fhpQGr1x0zgcR/W4lupU6GsALZ97fEuZw3vSnhwIK8v2u7DwkRERKSl8lkIs9bOB+palfRs4AXrLARijTFdfFWPT/WaDIEhDRoXFh0WzOlDu/De8gyKyip8WJyIiIi0RP4cE9YNOLQZKL36a61PaBSkTGrQUhUAM8Z2p7C0gvdX7PRRYSIiItJStYqB+caYmcaYNGNMWlZWlr/LqVnqVMheDzmbPN5lbEocvRMimZWmLkkREZH2xp8hbAfQ/ZDPk6u/dhRr7VPW2jHW2jGJiYnNUlyD9Zvi/l33sce7GGOYPrY7i7bsZUNmoY8KExERkZbInyHsXeDy6lmS44E8a23r7Zfr2BviUxu0VAXAeaO6ERRg+K9aw0RERNoVXy5R8SrwDdDfGJNujLnaGHOdMea66k0+ADYBG4CngRt8VUuz6TcVtnwJpZ63aiVFh3HSgCTeWJJOeWWVD4sTERGRliTIVwe21l5Uz+MWuNFXz+8XqVPgm0dg8zwY8COPd5sxtjsfr97NZ2symTaksw8LFBERkZaiVQzMbzV6TICQaFj9ToN2O75fIp1iQjVAX0REpB1RCPOmoBAYfQWseB02zfN8t8AAzh+dzBdrM9mVV+LDAkVERKSlUAjzthN/Dx37wDs3QUm+x7tNH9OdKguzF6s1TEREpD1QCPO2kAg49wnIT4ePf+/xbj3jI5nQO55ZaelUVemm3iIiIm2dQpgvdB8Hx94MS15o0K2MLh3fk205Rby7PMOHxYmIiEhLoBDmKyf+DhIHwrs/g+K9Hu1y2pDODO4awz8/WUtZhZarEBERacsUwnwlKBTOfRwKM+HD33i0S0CA4dfTBrA9p5hXvt3q4wJFRETEnxTCfKnrSJj8Kzdbcs17Hu0yOTWBCb3jefjzDRSWVvi4QBEREfEXhTBfm3wbdB4Gb98AO5bUu7kxht+cNoDsfWU8s8Dzm4GLiIhI66IQ5muBwXDRqxAeBy+eAztX1LvLiO6xnDakM0/P38SewtJmKFJERESam0JYc+iQDFe851bTf+Fs2L263l1um9qfkooqHvl8QzMUKCIiIs1NIay5xPWEK951A/ZfOAuy1tW5eZ/EKKaPSeblb7eyLbuomYoUaWWyN8KKWWC1tp6IVxXlwKd3Qa4WEPclhbDmFN8HLn8XMPCfM90JpA4/P7kfAcZw/ydrm6c+kdYkYxn8+1R481pY866/qxFpWxb8E768H56cDBs+9Xc1bZZCWHNL7AeXvwOVZfDCOXWuIda5QxhXTezF28sy+OWsZazbXeC7uqoqoVKzMaWV2LbQXcgER0DSIPjgVx6vxyci9SjMgkX/hr6nQnRneOl8+OLvUKX1K71NIcwfOg2CS/4LBRnw3s/r7Er5+cmp/GRiLz78fhdT/jWfa/6TxuKtXj7ZlBfDc6fDI6MhS61u0sJt/BxePBeikuAnH8E5j8O+PfDJH/1dWctiLXz8f/Dpnf6uRFqbbx6GylKYdjdc8ykMmw5f/A1eucB1U3rTlw+488/Gz7173FZCIcxfkse4m32vfgeWvlTrZuEhgfzxzEF8fftJ/OKUVNK25vDjx79m+pPfsDu/pOl1VFXBWz+F7d9CSZ7r3tnyZdOPK+ILa96DV2ZAxz5w1Udu0kvXETDhRnebsM0L/Fvf6ndh1hWuK2fjXCjO9V8tCx+Drx+CL/8Fq972Xx3SuuzLhu+egcHnQUIqhETCuU/Cj+6HzfNd9+QeL00Yq6qChY/D1q/chdV/zvJoKae2xNhWNqB1zJgxNi0tzd9leEdVpZstuWMx/HS++4OvR1FZBa99t51756xlYt8Enr58NMaYwzeyFvZugYwlsHsVpBwHfU6s+YCf3AFfPQBT/gIDz4SXp0POJjjnMXf14/H3UgUByvTiQyvfhDeugW6j4ZJZbtmX/cqK4PEJYALh+q8gOLz566sshweHu5aCiuKDXyXBlU8AACAASURBVO/YB3pMgJGXQo/xcOTr1Rc2zXMntf6nQX4G7N0MNyx0XUsidfnsz+4i4oZvIGng4Y/tWOy6JqO7uBaykIimPVf6YnjmJDjzISjbBwvug6JsGHQ2nPRHSOjbtOO3EMaYxdbaMTU9prOmPwUEwnlPuRmTs38CFfWvCRYREsRPJvXillNT+WzNTr5YtBy2fAVLX4bP/gQv/Rj+0RseGuGOueCfbn2yd2+GkvzDD7b4eRfAxlwNE26CuBS4eo47Ubx5Lcy/t/5ZZ1WV7kV7Tw9Xg4gv7F7tFjzuPg4ue+vwAAbuZHDGA5CzEeb9wz81rn4H8nfA9P/Ab7a4Ok/6P3ciW/MuPDcNHj8Wvnv66NeiN+Vug9lXQXxfOPcJ14pRXgzv3FT767k41010aE8qyuDjP8DKN9rX7Nqqytr//or3wrdPuhB0ZAADdwF03tOQuQo+/HXTa1n7vrtwGngmTLgBbl4Gx98OGz5zLW7t4G9SLWEtwQ/vw2sXw7E/cy1S4N4Utn3jgtL6j93XAoLd4q+BwVgM5XvTCaH84HFMICQOgG4j3Yul6yg3I3P+fa5bIqYbnPWwaxXb8Bm8fIH7/0WvQ2DQweNUlLobj694HYZOhyl/rvkKujjXtUxs+ARie7g3/wk3wal/cgHT26xtnlaE5rDqbfj8zxDbExL6uSu++FT3xheV5O/qGmbtRzDvHhh+EYz5ifsb9abSAnjqRCjNh58ugOhOtW/79g2w/DX46TzoPNS7ddTFWnj6RCgthBu/O7pVuGwffD8b0v4NO5dDcCSMuBim/tVdhHlLeTH8ewrs3QrXfn6wJeHbJ91J84x/ud/RodLT4L9XQt52dwI84fa28zqrjbUulC6rHgrSazKc/k83caoty1jm3tuzN7iLluEzDn987t3utXzdV9B5SO3H+exP7gL/3Cdh+IWNr+fRYyAyEa783+Ffz9sBz05z49Ku+Qxiuzf+OVqAulrCFMJaiv/90r1Bn/8cFOx04WvPOgiNcVcJIZGuu6OyHKrKoaqS3Saeh5aU02/AUK740QnQoXvtJ8Dti+Dt6yF7vTtZrvmfW7vsqg8hLObo7a2FeX93rWEBwTD+eph488EWiN2r4fVL3Boyp/8DRl4Gc34H3z0FqVPgx89AWIem/Uyshcw1bozalvmuxS++L1z0GkTG177fN4+5n2XnYZAyyXXHJqR678RSnOtO9EtfdGH3jAca1hVbWggPj3a/q/A494ZYfshacD2OdW+Og86B8Fjv1OwLFWXw2V3wzSPu+yjeCwn9YdrfoO8p3nkOa+GNq2HVW255l17H1b19UQ48Os6NFbvyffe6aQ5bv3EtXT+6H8ZeXfe2O5a4mWfLXnIXLVP/6p0arHXjO1fMgotfh35TDz5WVQUvnefGfl73pbs4s9aNx/nkjxDTxV20rX4bhvwYzn706C5da2Hth65Vb+gF0Pdk79S94r8QFOJaX5rLgn+6IDH5V+4C87M/uS7tY29yX2uuv5vmUlbkBtZ/86gLPbE9IH2R6wWZdre7ECjJg38Nda+xC+vp1aiscENpMpbAzC8gsX/Da8reCA+Pgmn3uPPLkTLXwL+nQkxX10PT1POJHymEtQZlRe5KOusH93nyOBh9JQw+p843hDveWckLC7fy1g0TGdG9nhN2eTF8/hf3Qozu7K4wOnSre5/sjfDF3e4qPiwGJv7Chb33fg6hUTD9Bdd9uV/as265gI594OLXoGNvz77/Q1nrTu5LXoSiPe5rHXpAj2PcwOyEVHdCjuh49L4L7nf7dhkBhbtdoAWITILeJ8Apd7gTdGNq2rHEfX8r33Bjfjr2cd1fDT2Rfv5XmP8PuPpT6D7WnSALMmDPetcq8f0sF8ADQ92YnuEXuWDbksbc5W6D/14FO9Jg7LWuBXfjZzDn9278UepU9zPxYJxjnb57Gj64DU7+Ixx3q2f7rHrbtex07OW6TpJrfO/zrtcvdRcLt6z2fJzMB79yFy2XzIbUU5v2/MW57nU9/x9uws/xNXQV5WfAY+Ndy+vFs1yLyA//g/6nuzGgYbFuEP9nd0HyWLjwlYOtslu/drMst38LgSFuiZ3Uqe733tjWI2vdRd7cv4IJcD8HbwW7uqx803XXDjnfXSwa45Zk+OSPsPwV9/426Gz3vhsS6ZZBCYl072Xdxhzea9AabPwc3vsF5G5155RT7oKQKPd7/vohF76n/weWvw5z/wIz57nJLvXJ3wlPTHKh7trPGh5cv37YdQf/fIVrEKjJpnnu4qHnRPf3ERTSsOdoIRTCWos9G2DFazD4XOg02KNdCkrKOfX++cRFhvDuTRMJDvTgRL1rpbuqaEgT766Vrvts3Ufu8+SxMP1FdwV9pM0LYNZl7v/H/sy1kjWki+3rR+Dj37uTw4AfudasuBT32IZP4dWLXLfd5e8cPjZo/n2uxiHnu2bygEA3yWDLl272zZr/uRfxuU8e3kpQl/1X//P/ARlL3Rvy0Atcl06X4e5EuuhpdzI69mf1Hy8vHR4eAwNOh/Ofrf05M5a4N8WVs91A1eRx8KN/QpdhntXtSz+871pVrXXd24PPOfhYRanr+pp/r2vdm/o3OOanjXueHUvg2akuPF/0esNC6OYFrsb8DNeyMfk273eT7pez2V3RT/yFC/meKi+Bp09yFwvXf9XwQfP5O92YmjX/gy0LoKrChYfzn6/9Z/X9bNeyGBLtLiROudNdRBzaSrz6XXhzJkQmuN/f0pdg/RyI6uy6KodNdy15+3/HY6+B439T80VRbax1J+BvHoFhM9z7S346XPN50wdjr3zDtYb3Pt7VFtP14GPbF8HzP4KuI937R3DY4ftu/dq16Getg/J9Rx87NMZ1XfY5yX1EJsLOZe5vNWOJ+7e8yHXRjbnaXQgcqSjHtThmLIVjrndLFjWUJ0MzKkrho9vdhWN8Xzf4PWXi4dus+Z97nZgAwLoJJBe/7nkdGz+HF89zXevnPNaw7+HZ09xQg+vrmY2/7BVX44hL4exHPOvRKNgFEQktJjArhLVxH63cxXUvLea3pw3gp8f38e2TbVvoWmvGXVv3WJacTe7qa/M815058EwXXFIm1f0i2vRF9ayu013Iq+lksu5j1xXaaTBc9rbrspt3r7uKGzrdrRtV04tvzwaYfSXs+t6deE6+o/Yrq/3h64u7YdcKFwIn3OROQIc2i1dVuqvq1e+4Vpf6ZpS+OdO11PwszXUJ1Key3I3N++QOKM5xrU4n/d4/TfPWuqA79y+upfGC52pv6SzMdK2laz9w4fSUPzUsRBXvdQNzrXUzhxtygt+vJA8++LW7sOk6yv1+fDHb6sPbYdEz8Ivva74oqUvmD/DUCa6V99K3PPsZFex2E2c2z3Ofd+wDA8+AAWe67vH6jvH2DS6k/vgZ97w1yVjqLnYKdrq/tUm3wLifHt7KV5jlurgWP+/CybiZ7jVe38+gqhL+9wu3pMi4mTDt75C3zY37i4h3rSqN+fsuK3KhY8l/XMt53nZ3ITbwLDjmOhdynznZtdhc85kLmXXWWeUCVXmRCwu7V7rQseFzV++ROvRw43GrKt17h61yLZxjr3Xvexs+cV3F6+a4ISUBwa6+0/4Boy73fLjE3q2udSimq+v+rqm1OW8HzLrctVQf+zM48Q9HB879sje6bXevdCE4ebRndey3v2W/tm7FmuzLhvv6ugukE39X//b7x6qd8FsX+Gv7WZXtc63xi59zIWzQWW6pjZ7H+macsocUwto4ay3XvrCYLzdk8f7Nx9EnMcrfJR20Zz2kPQfLXoaSXNcVcvIfXSg70t6t7oQUleSmP4dG137ctR/C65e51qjex7sxHsMudFdjdb3YyktcK9uiZ1zXwvnPuqbwilK34Oe+TPem9NWD1eGrl3ujGDa99paU8hI3K3X7QtfNU1uXyo7FruVj0i8b1mICLpR8/hfXAhGZ6Frehp7ffG8sVVXu57bwMfdzPuuh+geUV1XCh79xLYWDz3Mz9erap6rSBfx1H7qgmpcOP5nT8JPCkVa9Bf+7xXXHDzjDtRalnuqdZSxK8uD+Qa7F9rynGneMxc+7wHrKXTDpF3Vvu2eDOwHvy3LBaOBZbjxOQ8Y7Wus+6gtr+Ttdd+XQ84+ejXqo3avcDOl1Hx0Sen4K3Y85uq6KMjdubdWbcNxtcNIfDm6zeYGbyd37RNca05C/7cwfXBd01hrXInnSH9zfz6Jn3LCG0jwICncXXVd/2rQB+Na694iNn7v3tC4jXMtaVOLBbfJ2uN/r4ufde4oJBFsJUZ1cS/qw6W6ZhzevdReeQ853kyZqGp97qJxNbi2t0urZjeXFrpt+0i0HX1tbvoL/XuEeO+dxF0TqU17sljWqaUZkfaoqXYj74X+uZXXSLfXvs/RleOcGz7s+rXWtYctfda11J98BPSccvk3GUjdRLHujuxgoznGThiqKXSvu4HPchXtkknsPjUxw55pmWM5GIawdyMgt5rQHF1BlLX//8TBOH9rAK3JfKy92J9avH3bTmwefC6fde/CNq6wInp0Ce7fBzLlu4HB9fnjfvfirKty4qbMf9fyNe9XbbkxMVYULVyV5hz8e18uNqxk63bMm7ZI8t+pzzma44r2jg4O1brZPzka4eWndAbMuGUvh/VtdoAuPc111+7tGGjPWbb+qKneSqCloVla4n9XyV1z3ydS/ed6qZa0LtJ/eAT0nwYUvHTyhV1W5k8rOZW627vo5rus1IMi90Y6/wXXbekP+Tnclvfpd9+YcHAn9prhQZgIOjh8s2OX+H5noboeUNMh1F3XoXnPQ2T+uxdOTSU2sdSfNH96Hn3xce+hMT4NXqltaL/5v08Opt+VschcJS190r4fOw1w3WEUpVJS4fwt3uYkop/4JJv786GMs+je8/0v32Kl/cl+z1gWqHWluIlB4rPsbCu/oWki3f+fCfkgknPfk0ZNCSgtda+iqt113asok3/8s9qsogx/ec7+71FOh1/GHv0dVVbn7M879q2ttP/+52v+O9mxwt+qqKIHL33Yh7qPfuiEL8alw5gOuW/fj37tjzXgZkgY0x3fpWuzfus7VMvlXblxiXRcGr17sZgnfstLzC4jKCtfKOe/v7jWaOtVd0CcNdO8xc//qAtZ5T7ouY3AtY+s+cuMA13/iZlseacxPXAD2IYWwdmJ7ThE3vbqU5dtzueSYHvzfGYMIC/ZfE2yNKsvdC2be393g0NPvdbOx3rjGjeW4eJY7OXpqw6cumEz6ZcNbhXI2u3XSAkOqr4wS3ZVRVCd3ddvQ8QT5O93yAPnp7nua9MuD4z1Wve1OtGc+6AbHNkVVFax5x3XLbvzcndjAtTIOv9CNg/GkOyd3O2ya61Z23/SFe3Pvf7pr+ehzsms1KC9x682tfd+9sU7+VeNmma74r7uSje/jWjp2LnctjWWF7vGwDm7yQb9p7iTqq1mhlRWw9Uv3+1jz3sGJH+D+DqI7u7+Dgt3u97hfSLTr0hh0tguG4XHuWA+NcMuMXPV+0+oq3gtPHOf+hqf81Y3dObT1ad0c19ITmejWH/PkIsVfyva5brelL7r1qILCXCtNUJj7mxo2o+5lDfbPFB97jQvF6WkH/8Zr02uy625urYvRbv0aZl/t/h6HnO9+PinHHbzYyVrrAlhVJVzx7uFjhtd/6oJr7lb3ef/TXatzcw9ZOLSbefwN7mKtpveKsiK3luXIS+FH9zX8ecqK4Lsn3SSSknw3JCJno5tNfsa/ah+6UF7iWiULs1xL8v6PToM9HyPcSAph7UhZRRX//HgtT87fxIDO0Txy8Uj6JjWy1cWXMn+Ad250V7edhrjxCCf9nxtA3ZoVZroZR4uedQN7+//ITXt/6zoXOq9b4N0uxP3LeGya67potyyoHp9zrWu1OrSLpCTfTVLY+LnbPrv61iNRnd16cUGhB1uKwmJdN0b2JhdaTrsXjpnZtFo3z3ddyJVlbg2vLsMPfiQO8N3A+dpUVbowGBTmTt7hcYefNEry3M82c7VrYVj/iRsLFBDkWiDjermu1gtf9U6L3bZv3dp9pXmAcZMwUo5zv4sv7nbrNl0yu/WtI9dQleVuXOiWBe4EmzzWDR1IHuPCZ0m++xstynH/BgS77mA/jvnxin3Z8Pmf4Ps3oKzAtb4Om+G6dd+5ATCulb2m1q2yIndxGxoF42/030xqa13r3LePu3FuZzxw9O/lhw/gtYvceN7a7uTiieK97nte9ZYbJzb8oha7vp1CWDs0d20mt85aTnFZJQ9eOIIpg1vgFWJVpVun6PM/uxaQC55vsS+iBivKcbMEv33CjRuBpr/peCJjmbtCXP2OCxejLndjHzZ+7tYFqqpwMzxTJrkWqT4nugC0/+deWe5axlbOdt1j5cXuqroht7CqS2W56/5rjSfM/bNWV73tfr65W92g+JvSvHfSqyh1Xc2bF7jQmv6dC629T4QZLza+G7u1qShzraSNmZDR2pUVuQkty191r1tb5boer3iv6Uu+NAdrXdfg/Htdl+HZjx5+MfjOjbD6Pfj1xua/8PIThbB2and+CTNfXMzaXfnMvu5YhnRroYvdleS57p6WtA6Wt5QWuMG5FaXN28q3Zz18+YAbC1NV6caZ9DnJncy7j/NslfayIjdGq5WvVu0T1rru1PA4z2a5NlZ5sWuxTBzYYqbbSzMq2OUGvPc9tfa1tFqq7552MxXDOrgJAqmnuPei+/q5luTz/+3vCpuNQlg7llVQytmPfIkF3rlpIknRtUxTlrZp3x7X8tQeWxRExL92r3LjfTNXu2VC+p/mVto//1k3brad8NsNvI0x04wxa40xG4wxt9fweA9jzFxjzFJjzApjjJemQsl+idGhPH3FGHKLyrnuxcWUVlTWuF1haQX5JeU1PiatWGSCApiI+Eenwe4epsdc54ZmvHyBG8PXt4l3iGhDfBbCjDGBwKPAacAg4CJjzJFLA/8BmGWtHQlcCDRwyV3xxOCuHbh/+nCWbMvld2+u5NDWz6KyCh6du4Fj7/6MKffPJ31vUR1HEhERaYDgcDjt725SSVgs9J9W/3po7YgvW8LGARustZustWXAa8CRd2i1wP7fRgcgw4f1tGunDe3Cz09O5Y0l6TyzYDNlFVW88M0WJv/jC+6ds5ZRPeMoKqvg8n9/R3ZhDWupiIiINFbqqXDLKvhx+xkL5glfjvTsBmw/5PN04Mh7ZNwJfGyM+RkQCRyxyp54089PTmXd7gLu/nANz321mYy8Esb16siTl41idM+OLN6awyXPfMuVzy3ilWuPITqsfcxcERGRZtBKb8DtS/6ejnYR8Ly1Nhk4HXjRGHNUTcaYmcaYNGNMWlZWVrMX2VYEBBj+OX04w7vHkhAdyvNXjeX1meMZ3dONGRrdsyOPXzKaNTvzmfnCYkrKax4/JiIiIk3ns9mRxpgJwJ3W2qnVn/8WwFp79yHbrAKmWWu3V3++CRhvrc2s7biaHel7by/dwS9eX8bUwZ149OJRBAX6O6uLiIi0Tv6aHbkISDXG9DLGhOAG3r97xDbbgJOrixwIhAFq6vKzc0Z2449nDGLOqt3c/NpSVu7Io7UtZSIiItLS+WxMmLW2whhzEzAHCASetdauMsb8CUiz1r4L3Ao8bYy5BTdI/0qrs32L8JNJvdhXWsFDn6/ng+930ScxkrNHdOOs4V1JSYj0d3kiIiKtnhZrlTrlFpXx4cpdvL10B99tycFaGJsSx51nDWZw1xa6Ar+IiEgLoRXzxSsycot5b3kGz3y5mb37yrjhhD7ceFJfQoNa4X0ARUREmoHfVsyXtqVrbDg/Pb4Pn9wymbNHdOOhzzdw5sNfsmx7rr9LExERaXUUwqTBYiNC+Of04Tx31VgKSio477Gv+NsHa7SkhYiISAMohEmjndg/iY9vmcyMsT14av4mTntwAYu25Pi7LBERkVZBIUyaJDosmLvPG8rL1xxDeWUV05/8hjvfXcW+0gp/lyYiItKiKYSJV0zsm8CcX0zmigkp/OebLUx9YD5fbdjj77JERERaLIUw8ZrI0CDuPGsws346geDAAC555ltmvpDG2l0F/i5NRESkxVEIE68bm9KRD39+HLec0o9vNmYz7cH53PzqUjbv2efv0kRERFoMrRMmPpVbVMaT8zfx/FdbKKus4sejuvGrqQNIjA71d2kiIiI+p3XCxG9iI0L4zbQBzP/1iVwxIYW3l2Vw5sNfsnTbXn+XJiIi4lcKYdIsEqND+eOZg3j7hokEBxlmPLmQV77d5u+yRERE/EYhTJrVoK4xvHfTJMb3ied3b33P7W+soLTi6EVeW1s3uYiISEMF+bsAaX9iI0J47sqx3P/JWh6du5E1uwqYMqgT6XuL2J5TTPreInbkFtOvUzS/PW0gk1IT/F2yiIiI12lgvvjVRyt3cdt/l1NYWkF8ZAjJHSPoHhdOlw5hfLhyF+l7izmhfyK/PW0g/TtH+7tcERGRBqlrYL5CmPhdUVkF1rp1xg5VWlHJC19v5eHP11NYWsH0Md254tgUOsWEERseTECA8VPFIiIinlEIk1Zt774yHv58Ay8u3EJ5pft7DQwwxEWEEB8ZQp+kSKaP6c7k1EQFMxERaVEUwqRN2J5TxJJte8nZV0Z2YRnZ+0rZU1jGkq17yd5XRo+OEVx8TA8uGJ1MfJTWIRMREf9TCJM2rbSikjmrdvPSwq18tzmHkMAAzh7Rlf87cxAxYcH+Lk9ERNqxukKYZkdKqxcaFMhZw7ty1vCurNtdwMsLt/Lyt9tYvG0vT18+hj6JUf4uUURE5ChaJ0zalH6dornr7CG8fM0x5BWVc84jX/HZmt01bltSXsl3m3PILylv5ipFRETUHSlt2I7cYn76YhqrMvK59dR+3HhiX8orLV9uyOK95Tv5eNUu9pVVEhxoGN87nlMHdeKUgZ3oGhvu79JFRKSN0JgwabeKyyq5/c0VvLMsgxHdY9mSvY/conI6hAdz2pDOTO6XyPL0XD5ZvZtNWfsAGNw1hj6JUXSMDKFjZAhxkSEkRIYwoU88sREhfv6ORESkNVEIk3bNWsszCzbz/NdbGJsSx5nDu3JcaiIhQYf3xm/ILOST1buZty6TnXkl5Owro6Ck4sDjMWFB/PyUflw2vudR+4qIiNREIUykkcoqqthbVMb2nCIe/Gw9C9bvoXdCJL//0UBOGpCEMQZrLRuz9vHF2kzmrcsiq6CUmLBgYsKDiQkPIiYsmMFdYzh/dDLGaB0zEZH2RCFMxAustcxdm8lf3l/Dpqx9TOqbQK+ESL5Yl8n2nGIA+iRG0ishioKScvJLKsgvLievuJzC0gpOGZjEfRcMV5emiEg7ohAm4kXllVW8vHAr//p0PWUVVUzsG8/x/ZM4oV8i3TtGHLW9tZbnv97C3z5YQ1J0GI9cPJKRPeL8ULmIiDQ3hTARHyirqMJiCQ0K9Gj7ZdtzufHlJWQWlHD7aQP5ycQUdU+KiLRxCmEiLUReUTm3zV7OJ6t3c1xqAicPSGJY91gGdYkhLNizMNdYJeWVfLRyF68v2k7OvjIGdolmUNcYBnXpwMAu0cRGhLA7v4SM3GJ25BaTkVtCcKDh0vE9G1ybtZYfdhXQMz6CiBCtCS0i7ZdCmEgLYq3l319u5ol5m9hTWAq4G5L36xTN8OQOjE3pyDG9O5Icd3TXZmOs3VXAq99t462lO8grLqdnfAR9EqNYszOfnXklB7YLMFBVw9tBv05RPDBjJIO6xnj0fF9v3MO9c9aydFsusRHBXDa+J5dPSCExWvfzlKOtSM8lt6icSX0TCAhQy7C0PX4LYcaYacCDQCDwjLX2nhq2mQ7cCVhgubX24rqOqRAmbYW1ll35JaxIz+P79DxW7Mhj+fZc8ordCv7dYsMZ3zue8b07MmVQZzpE1H0fzJLySjZkFrIxq5CNmYVszNrHut0FrM8sJCQwgKlDOnPR2O6M7x1/4GS3d18Za3bms3pnPrlF5XSJDaNbbDjdYsPpEhvOoi05/Hr2CvKKyvnV1P5cPalXrSfKFem53DtnLQvW76FzTBhXT+rFoi05fLJmN8GBAfx4VDLXHNerRd5GantOEd9uzuHbTdmsSM9jUmoCt03pT3iIb1sn27uNWYWc9fCX7CurpHdCJFdNTOG8UclEhqr1VNoOv4QwY0wgsA44FUgHFgEXWWtXH7JNKjALOMlau9cYk2StzazruAph0pZVVbluvG83Z/Ptphy+25JDzr4yQoMCmDakM9PHdGfCISEqv6Scz9dk8v73O5m3LouyiirAtWr16OhavCb0iee8Ucl0jGzcrMzswlJuf/N7Plm9m2P7xHPfBcMJCQogfW8x6XuLSN9bzOKte/lk9W7iIoK58cS+h3Vhbswq5JkFm3ljSTrllVVcckwP/vCjQT7vfq3PzrxiHvhkPV9u2MOOXDe7NTYimH6dovlucw4p8RHce8FwxqZ09GudbVVxWSXnPPoVmQUl/GrqAF5ftI3l6XlEhwVx0bgenD2iK8mxEcSEB7XbsZNfb9jDPR/9wKXH9GT62O7+LkcayV8hbAJwp7V2avXnvwWw1t59yDb/ANZZa5/x9LgKYdKeVFVZVmbkMXtxOm8v3UF+SQXJceGcMawrGzILmL9uD2WVVXSOCWPakM6MTelI36QoesZHeDXkWGuZlbadu95bTVFZ5VGPx0eGcOn4nlxzXC+iw2pusdtTWMqjczfw3FdbGNA5mkcuHknfpOh6nzuvuJz3V+xk9uLtZBWWMjk1kVMGdmJCn/hGfY9VVZZXvtvGPR/+QEVVFScNSOKYXvEc07sj/ZKiCQgwfL1xD7+evYIducX8ZGKvw1rFCkrK+T49j2XpuewrrSApOoxOMaEkRoeRFB1K5w5hBAe2r8V8t+cUsTOvhMLScgpKKigsraC4rJJTBnYiJSHyqO2ttdz63+W8tXQHz181juP7JWKtbS7/hAAAErFJREFUZcm2XJ79ajMfrdxFZXXfeGhQAJ1iwugcE0ZqpyhmTu5Nz/ijj9mW5BWXc/cHa3ht0XZCAgOospZXZ45v1guCZdtzefyLDQzq0oFpQzrTr1NUrWG4ssoSYPBaWP56wx6+3ZxD/87RDOnage4dw30SxNP3FhEUEEDnDmFeP/ah/BXCzgemWWuvqf78MuAYa+1Nh2zzNq61bCKuy/JOa+1HdR1XIUzaq5LySuas2sWstO18tSGbrh3COG1oF04f2pmR3eOaZTzNlj37eHPpDjpGBJMcF0FyR9d1WVvwqsnctZncOms5xWWV3HX2YC6oYRHbyirLVxv2MHtxOnNW7aK0oorUpCh6xkfy9cY9FJVVEh4cyMS+CUwd3Ikzh3f1KJBt3rOP37yxgu825zCxbzx3nzuMHvE1j73bV1rBPR/+wIsLt9IrIZJRPeJYnp7LxqxC9r9tBgaYA2Fhv/DgQMb37sjkfolM7pdI74TIGk8g1to20cLz3Febueu91TU+FhESyJ/PHsKPRycf9vXXvtvG7W9+z80np/LLU/sdtV9Grmtd3Z1fQmZBKbvyStidX8Ly9FwqKi0zxnbnZyelHnbytNayemc+by7ZwXebc5jQJ54fj0qmf+eag35ZRRU/7MonLiKE5DjfnOQbY86qXfzf2yvJ3lfGzMm9uWpiCjOeXEhBSQXv/WwiXTr4/t6289dlcd1LiwkMMBSWVmAt9E6IZOqQzpzYP4ncojJ+2FXA2l0F/LArny3ZRcRFBDOwSwyDusQwsPqjylp2HNJiviO3mG6x4Vw+IaXG192uvBL+/P5q3l+x87CvdwgPZki3GEZ2j+OCMclNDuF5ReU8+sUGnv9qC2cM78L900c06Xj1ackh7H9AOTAdSAbmA0OttblHHGsmMBOgR48eo7du3eqTmkVai/yScqJDW283ze78En7x2jK+2fT/7d15lFT1lcDx762q3uileqFp7GqgQXZoaZAgKkZHEhfChEXMokaOcU6cE8+JZvRk1IlnEqM5SSZRk9EsMy7BiXE5CG5jiEqIJhBAZJG92aEbet+3Wu/88R4MsgWR5jVd93NOne736vWPX/WP23Xrt7xfA7PLi5lVHmJHbRvbq9vZUdvGjpp2uqJxghkpzCovZt7FJZSFgogI3dE4q/Y0snRrDUu31lLV3EW+2xN366VD6J91/AKAQy1dLFpbxS+W7iA14OPBL4zlxsmnt4PBip31PLB4I23dMcoH5TJhUC7lg3K5qCRITnoKDR0RatucZKG2tZstB1t5f0c9e+qdvUhDuRmUhYK0h2M0dUZo7ozS1BkhEkswtH8moy/IYfTAbEYVZTOiKIuMFD9+n+D3CT6fkOLzkZ7iO+22buyIsP5AE+v2N7NufzM+n3DJ0HymDsunLJR71rbcUlV+9nYFTyzbyTVji7j10lKy0wNkpQfITgvQFY3znYUfsWpPI3MmhvjB7PFkpQXYfLCFOb9cwZTSfBZ8fQr+T/Dhoba1myeW7eSF1fvxiTD/slLmXVzCsm21LFpbxfaaNlL8wrjiIJuqWogllHHFOcydVMKMsoEcbO5i5e5GVu5uYM3eJrqiTq9ubr8UykJBxoeClIWCDMrrRzAjhWBGCtnpAXw+Z2eM5s6ou3LYeQCMDwUZW5xzxquA4wllV107Gw44e9i+vaWGMRfk8JMbLqKsJAjAjpo2Zj+5nOFF2bx8x9TTvi3OmXhtfRX3vLyBEUXZLPj6Z0Dh7S01/HFzNSt2NXzsQ8fg/H6MGpjN8AFZ1LeF2VrdSkV1O5F44rhy01N8FOdmsL+hk7gqnx9TxO3ThjJlaD6xhPLb5Xt5/N0KYgnlm1cN57Zppeyt72BTVSsbq1rYfLCFzQdbSahy1chCbr2slCtHFH6iD5/hWJz/+ds+/vNPO2ntjjJ3Ygn3XDOS4tyeTWx783Dkr4FVqvqse7wUuE9VPzhZudYTZkzfEE8oTy7byePvVhxZlVmYnXYkGflMaT7Txww45RuOqrJydyNP/WU3S7fVkhrwMXdiiC+WF7OrroM1extZs7fpyJyva8YW8YPZ4ynK+eTDD5+05+pAYyfvVdTxfkUdO+vaCWakkNcv1X2kEPD72FnbzrbqViqbuk5ZVqrfR0GWs6F8QVYaBZmp+ESIJxJEE0o8rkTjCXbVtbO3oRNweulGFWUTTyjba9oA543w4iF5DM7PpKkjQkNHmIb2CPXtYcKxBKG8DAbn92NQXj8G5WdwYWEWlw/vf1wvYzyhfPfVTbywej9fnTKIh2eXnTCZOrqNB+X344dzynhg8UbC0QT/+61pFJwgYT7d3+1j71aweF3VkV7JiYNzmTuphJllF5CXmUpDe5g3Nhxk0boqPqps+djPjx6YzdRhBUwuzaO5M8qmqhY2VrVQUdNGNP7x90QRyElPIRJLHEnajuUTGD4gi/GhIJcOK2DOxBCBUwxJ17R28+zyvazb38SmqhY63CH+7LQAd1w5jDuuvPC4Ie0lmw7xz79by5cnD+JHN5T1yAewZ/66h4fe3MIlQ/P57/mTyTmmh7u5M8KqPY0UZqcxsiibrBMsoIjGE+yu62DroVYCfnF6zPMyKMhMRUSoae3mub/t5flV+2nujDI+lEMklqCipp2rRw/ge/847qS90zWt3fx+1X5+v3o/dW1hSgv6ccvUIX93zms4FuetjYd49J0KDjR2ccWI/tx3/WjGFQc/1e/rdHmVhAVwhhqnA1U4E/NvUtXNR11zHc5k/fki0h9YB5SrasPJyrUkzJi+ZUdNG40dEUYWZZN3hosHwNmA/Znle3jlw0rC7gKFAdlpTC7NY/KQfKYMzWdccU6v7D1s645SUdPGrroOIrEE8YQSTygJVSLxBC1dURraIzR2RGjoiNDYESaRgIBfCPiEgM+H3yeE8jKYNDiPSYNzKSsJHumdaeyIsHpPAyt3N7JqTyO1rd0UZKVSkJlGQVYq/bPSCPiEquYuDjR1sr+hk1Z38/rstADXlw1kdnmIS4YVEEskuPvF9fxhUzV3/sOF3HvNqL/7O/1gbyN3vbCOgy3d+H3CS9+YyuSzML9pR00by3fWc+WoAQw9wdyzo69buq2W0oJ+TBlacNI37HAsTkV1O9Wt3bR0RWnujNDaFaW5K0qq3+nJcR7pFOdmEIsrG90EblNVCx9VtlDfHmZ8KIdHZpcxYVDux8qPJ5TnV+3jJ0u2E47FGVscZEJJkItKcplQEmRYYdYpewb/44/beHLZLh6ZM56bLxly3POqSjiWoDsapzvqfG0Px478n2nsiNLYESYcTVCQlUb/rFT6Z6dRmJXGWxsP8cs/7+LacUX8/CsTe3zhTFck7swJXLGHWFy5f8YYPj+26LR+NhJLsGRzNc+t2MuafU2k+IXpo4u4cXIJV44sJOD3oaqsO9DMorWVvLHhEC1dUUYPzOaBGWP47MjCHn1tx/LyFhUzgMdx5ns9o6qPiMhDwBpVfV2cyP0ZcB0QBx5R1RdPVaYlYcaYU2loD7NmXxNjBub02ITeZNDS5SxAeG19FX/YVE17OMbAnHQKslLZfLCVB2eO5fZpQ0+7vObOCD9eso2Lh+Qz75g5Yn2FqvLWxmq+/8Zm6trD3HLJEO69dhTBjBS2Hmrl/kUbWX+gmWnD+/Pw7PEnXLRwKvGEcvuCD1i+s56bpgymqTNKXVuY2rZu6trCtLnzt07F7xNS/EJ39Pghw1P1avZW26pbWbimklfXV1HfHqF/VhqfGzOA1Xsa2V3fQVrAx7XjBjJ3UogrRhR68trsZq3GGGPOWHc0zrtba3h13UFW7WngoVnjmDOxbyZSZ0Nbd5RH36lgwYq95GemMX30AF5ZW0kwI4UHZ45lVnnxGX84aOmMctNTK9ld18GAHKcX6/DXYEYK6al+0gN+0lP8pKf4yEwLUJDpDmVnph2Z49YZiVHfFqGuPUx9e5hUv4+rRhWetx9aovEEf95ex8IPD7BsWx3lg3O5YVKIGWUXfKKFQz3BkjBjjDHmHNtU1cK/Ld7IhsoWvjS5hPuvH/OphtwPO/y+fb4mTMnmVEmY3ZbYGGOM6QHjQ0EWffNyalq7z+oKPEu++o7kuqOgMcYYcw75fdLjt0Aw5y9LwowxxhhjPGBJmDHGGGOMBywJM8YYY4zxgCVhxhhjjDEesCTMGGOMMcYDloQZY4wxxnjAkjBjjDHGGA9YEmaMMcYY4wFLwowxxhhjPGBJmDHGGGOMB867DbxFpA7YdxaL7A/Un8XyzNljbdM7Wbv0XtY2vZO1S+91LtpmiKoWnuiJ8y4JO9tEZM3Jdjc33rK26Z2sXXova5veydql9/K6bWw40hhjjDHGA5aEGWOMMcZ4wJIw+C+vK2BOytqmd7J26b2sbXona5fey9O2Sfo5YcYYY4wxXrCeMGOMMcYYDyR1EiYi14nIdhHZKSL3eV2fZCUig0RkmYhsEZHNInKXez5fRN4RkR3u1zyv65qsRMQvIutE5E33eKiIrHJj5yURSfW6jslGRHJFZKGIbBORrSJyqcVM7yAi33b/lm0SkRdEJN1ixhsi8oyI1IrIpqPOnTBOxPELt40+EpFJPV2/pE3CRMQPPAlcD4wFvioiY72tVdKKAfeo6lhgKnCn2xb3AUtVdQSw1D023rgL2HrU8Y+Bx1R1ONAE3O5JrZLbz4ElqjoamIDTPhYzHhOREPAtYLKqjgf8wFewmPHKb4Hrjjl3sji5HhjhPr4B/KqnK5e0SRgwBdipqrtVNQK8CMzyuE5JSVUPqepa9/s2nDeTEE57LHAvWwDM9qaGyU1ESoAvAE+5xwJcDSx0L7G2OcdEJAh8FngaQFUjqtqMxUxvEQAyRCQA9AMOYTHjCVV9H2g85vTJ4mQW8Jw6VgK5InJBT9YvmZOwEHDgqONK95zxkIiUAhOBVUCRqh5yn6oGijyqVrJ7HPgOkHCPC4BmVY25xxY7595QoA541h0mfkpEMrGY8ZyqVgE/BfbjJF8twIdYzPQmJ4uTc54XJHMSZnoZEckCXgHuVtXWo59TZxmvLeU9x0RkJlCrqh96XRfzMQFgEvArVZ0IdHDM0KPFjDfc+UWzcBLlYiCT44fDTC/hdZwkcxJWBQw66rjEPWc8ICIpOAnY86q6yD1dc7gr2P1a61X9ktjlwBdFZC/OkP3VOHORct2hFrDY8UIlUKmqq9zjhThJmcWM9z4H7FHVOlWNAotw4shipvc4WZyc87wgmZOwD4AR7oqVVJyJk697XKek5M4xehrYqqqPHvXU68B89/v5wGvnum7JTlXvV9USVS3FiZE/qerNwDJgnnuZtc05pqrVwAERGeWemg5swWKmN9gPTBWRfu7ftsNtYzHTe5wsTl4HbnVXSU4FWo4atuwRSX2zVhGZgTPfxQ88o6qPeFylpCQi04C/ABv5/3lHD+DMC3sZGAzsA76kqsdOsDTniIhcBdyrqjNFZBhOz1g+sA64RVXDXtYv2YhIOc5iiVRgN3AbzgdrixmPicj3gS/jrPxeB/wTztwii5lzTEReAK4C+gM1wL8Dr3KCOHGT5idwho87gdtUdU2P1i+ZkzBjjDHGGK8k83CkMcYYY4xnLAkzxhhjjPGAJWHGGGOMMR6wJMwYY4wxxgOWhBljjDHGeMCSMGOMOQURuUpE3vS6HsaYvseSMGOMMcYYD1gSZozpE0TkFhFZLSLrReQ3IuIXkXYReUxENovIUhEpdK8tF5GVIvKRiCx29/tDRIaLyLsiskFE1orIhW7xWSKyUES2icjz7k0dEZEficgWt5yfevTSjTHnKUvCjDHnPREZg3OH8stVtRyIAzfjbJ68RlXHAe/h3C0b4DngX1X1IpydGg6ffx54UlUnAJcBh7csmQjcDYwFhgGXi0gBMAcY55bzcM++SmNMX2NJmDGmL5gOXAx8ICLr3eNhONtgveRe8ztgmogEgVxVfc89vwD4rIhkAyFVXQygqt2q2ules1pVK1U1AawHSoEWoBt4WkTm4mxzYowxp82SMGNMXyDAAlUtdx+jVPV7J7juTPdpO3qPvzgQUNUYMAVYCMwElpxh2caYJGVJmDGmL1gKzBORAQAiki8iQ3D+xs1zr7kJ+KuqtgBNInKFe/5rwHuq2gZUishst4w0Eel3sn9QRLKAoKq+BXwbmNATL8wY03cFvK6AMcZ8Wqq6RUS+C7wtIj4gCtwJdABT3OdqceaNAcwHfu0mWbuB29zzXwN+IyIPuWXceIp/Nht4TUTScXri/uUsvyxjTB8nqmfaO2+MMb2biLSrapbX9TDGmBOx4UhjjDHGGA9YT5gxxhhjjAesJ8wYY4wxxgOWhBljjDHGeMCSMGOMMcYYD1gSZowxxhjjAUvCjDHGGGM8YEmYMcYYY4wH/g/ynNEVd7wXbwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93\n"
          ]
        }
      ],
      "source": [
        "epochs_to_show = 100\n",
        "smoothing = 0.5\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(me_losses_train[0:epochs_to_show],smoothing), label = 'train')\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(mean_losses_valid[0:epochs_to_show],smoothing), label = 'validation')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('loss graph')\n",
        "plt.show()\n",
        "print(best_epoch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "LMCL_1st_Run_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}