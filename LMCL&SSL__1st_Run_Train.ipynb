{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DinneRatj/Vector-Embedding-Model/blob/main/LMCL%26SSL__1st_Run_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meTBh7oChFPJ",
        "outputId": "f347aa51-4c80-4109-8c85-b60167f9b129"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f362960b950>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Imporing libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "#Added so that the random numbers are always the same when the program is run, so the results are always the same\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting to Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k-EbN3UGtd3",
        "outputId": "bf79707d-9f51-414b-ffc8-ad0b39d75cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "fp0MZCw2hFPM",
        "outputId": "5385298c-6808-4e5c-e031-cc7bd211a219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     state  account length  area code phone number international plan  \\\n",
              "0       KS             128        415     382-4657                 no   \n",
              "1       OH             107        415     371-7191                 no   \n",
              "2       NJ             137        415     358-1921                 no   \n",
              "3       OH              84        408     375-9999                yes   \n",
              "4       OK              75        415     330-6626                yes   \n",
              "...    ...             ...        ...          ...                ...   \n",
              "3328    AZ             192        415     414-4276                 no   \n",
              "3329    WV              68        415     370-3271                 no   \n",
              "3330    RI              28        510     328-8230                 no   \n",
              "3331    CT             184        510     364-6381                yes   \n",
              "3332    TN              74        415     400-4344                 no   \n",
              "\n",
              "     voice mail plan  number vmail messages  total day minutes  \\\n",
              "0                yes                     25              265.1   \n",
              "1                yes                     26              161.6   \n",
              "2                 no                      0              243.4   \n",
              "3                 no                      0              299.4   \n",
              "4                 no                      0              166.7   \n",
              "...              ...                    ...                ...   \n",
              "3328             yes                     36              156.2   \n",
              "3329              no                      0              231.1   \n",
              "3330              no                      0              180.8   \n",
              "3331              no                      0              213.8   \n",
              "3332             yes                     25              234.4   \n",
              "\n",
              "      total day calls  total day charge  total eve minutes  total eve calls  \\\n",
              "0                 110             45.07              197.4               99   \n",
              "1                 123             27.47              195.5              103   \n",
              "2                 114             41.38              121.2              110   \n",
              "3                  71             50.90               61.9               88   \n",
              "4                 113             28.34              148.3              122   \n",
              "...               ...               ...                ...              ...   \n",
              "3328               77             26.55              215.5              126   \n",
              "3329               57             39.29              153.4               55   \n",
              "3330              109             30.74              288.8               58   \n",
              "3331              105             36.35              159.6               84   \n",
              "3332              113             39.85              265.9               82   \n",
              "\n",
              "      total eve charge  total night minutes  total night calls  \\\n",
              "0                16.78                244.7                 91   \n",
              "1                16.62                254.4                103   \n",
              "2                10.30                162.6                104   \n",
              "3                 5.26                196.9                 89   \n",
              "4                12.61                186.9                121   \n",
              "...                ...                  ...                ...   \n",
              "3328             18.32                279.1                 83   \n",
              "3329             13.04                191.3                123   \n",
              "3330             24.55                191.9                 91   \n",
              "3331             13.57                139.2                137   \n",
              "3332             22.60                241.4                 77   \n",
              "\n",
              "      total night charge  total intl minutes  total intl calls  \\\n",
              "0                  11.01                10.0                 3   \n",
              "1                  11.45                13.7                 3   \n",
              "2                   7.32                12.2                 5   \n",
              "3                   8.86                 6.6                 7   \n",
              "4                   8.41                10.1                 3   \n",
              "...                  ...                 ...               ...   \n",
              "3328               12.56                 9.9                 6   \n",
              "3329                8.61                 9.6                 4   \n",
              "3330                8.64                14.1                 6   \n",
              "3331                6.26                 5.0                10   \n",
              "3332               10.86                13.7                 4   \n",
              "\n",
              "      total intl charge  customer service calls  churn  \n",
              "0                  2.70                       1  False  \n",
              "1                  3.70                       1  False  \n",
              "2                  3.29                       0  False  \n",
              "3                  1.78                       2  False  \n",
              "4                  2.73                       3  False  \n",
              "...                 ...                     ...    ...  \n",
              "3328               2.67                       2  False  \n",
              "3329               2.59                       3  False  \n",
              "3330               3.81                       2  False  \n",
              "3331               1.35                       2  False  \n",
              "3332               3.70                       0  False  \n",
              "\n",
              "[3333 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a570342-5c81-4977-ae90-4a458bce7891\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>account length</th>\n",
              "      <th>area code</th>\n",
              "      <th>phone number</th>\n",
              "      <th>international plan</th>\n",
              "      <th>voice mail plan</th>\n",
              "      <th>number vmail messages</th>\n",
              "      <th>total day minutes</th>\n",
              "      <th>total day calls</th>\n",
              "      <th>total day charge</th>\n",
              "      <th>total eve minutes</th>\n",
              "      <th>total eve calls</th>\n",
              "      <th>total eve charge</th>\n",
              "      <th>total night minutes</th>\n",
              "      <th>total night calls</th>\n",
              "      <th>total night charge</th>\n",
              "      <th>total intl minutes</th>\n",
              "      <th>total intl calls</th>\n",
              "      <th>total intl charge</th>\n",
              "      <th>customer service calls</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KS</td>\n",
              "      <td>128</td>\n",
              "      <td>415</td>\n",
              "      <td>382-4657</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>265.1</td>\n",
              "      <td>110</td>\n",
              "      <td>45.07</td>\n",
              "      <td>197.4</td>\n",
              "      <td>99</td>\n",
              "      <td>16.78</td>\n",
              "      <td>244.7</td>\n",
              "      <td>91</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OH</td>\n",
              "      <td>107</td>\n",
              "      <td>415</td>\n",
              "      <td>371-7191</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>26</td>\n",
              "      <td>161.6</td>\n",
              "      <td>123</td>\n",
              "      <td>27.47</td>\n",
              "      <td>195.5</td>\n",
              "      <td>103</td>\n",
              "      <td>16.62</td>\n",
              "      <td>254.4</td>\n",
              "      <td>103</td>\n",
              "      <td>11.45</td>\n",
              "      <td>13.7</td>\n",
              "      <td>3</td>\n",
              "      <td>3.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NJ</td>\n",
              "      <td>137</td>\n",
              "      <td>415</td>\n",
              "      <td>358-1921</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>243.4</td>\n",
              "      <td>114</td>\n",
              "      <td>41.38</td>\n",
              "      <td>121.2</td>\n",
              "      <td>110</td>\n",
              "      <td>10.30</td>\n",
              "      <td>162.6</td>\n",
              "      <td>104</td>\n",
              "      <td>7.32</td>\n",
              "      <td>12.2</td>\n",
              "      <td>5</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OH</td>\n",
              "      <td>84</td>\n",
              "      <td>408</td>\n",
              "      <td>375-9999</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>299.4</td>\n",
              "      <td>71</td>\n",
              "      <td>50.90</td>\n",
              "      <td>61.9</td>\n",
              "      <td>88</td>\n",
              "      <td>5.26</td>\n",
              "      <td>196.9</td>\n",
              "      <td>89</td>\n",
              "      <td>8.86</td>\n",
              "      <td>6.6</td>\n",
              "      <td>7</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OK</td>\n",
              "      <td>75</td>\n",
              "      <td>415</td>\n",
              "      <td>330-6626</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>166.7</td>\n",
              "      <td>113</td>\n",
              "      <td>28.34</td>\n",
              "      <td>148.3</td>\n",
              "      <td>122</td>\n",
              "      <td>12.61</td>\n",
              "      <td>186.9</td>\n",
              "      <td>121</td>\n",
              "      <td>8.41</td>\n",
              "      <td>10.1</td>\n",
              "      <td>3</td>\n",
              "      <td>2.73</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>AZ</td>\n",
              "      <td>192</td>\n",
              "      <td>415</td>\n",
              "      <td>414-4276</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>36</td>\n",
              "      <td>156.2</td>\n",
              "      <td>77</td>\n",
              "      <td>26.55</td>\n",
              "      <td>215.5</td>\n",
              "      <td>126</td>\n",
              "      <td>18.32</td>\n",
              "      <td>279.1</td>\n",
              "      <td>83</td>\n",
              "      <td>12.56</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "      <td>2.67</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3329</th>\n",
              "      <td>WV</td>\n",
              "      <td>68</td>\n",
              "      <td>415</td>\n",
              "      <td>370-3271</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>231.1</td>\n",
              "      <td>57</td>\n",
              "      <td>39.29</td>\n",
              "      <td>153.4</td>\n",
              "      <td>55</td>\n",
              "      <td>13.04</td>\n",
              "      <td>191.3</td>\n",
              "      <td>123</td>\n",
              "      <td>8.61</td>\n",
              "      <td>9.6</td>\n",
              "      <td>4</td>\n",
              "      <td>2.59</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>RI</td>\n",
              "      <td>28</td>\n",
              "      <td>510</td>\n",
              "      <td>328-8230</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>180.8</td>\n",
              "      <td>109</td>\n",
              "      <td>30.74</td>\n",
              "      <td>288.8</td>\n",
              "      <td>58</td>\n",
              "      <td>24.55</td>\n",
              "      <td>191.9</td>\n",
              "      <td>91</td>\n",
              "      <td>8.64</td>\n",
              "      <td>14.1</td>\n",
              "      <td>6</td>\n",
              "      <td>3.81</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3331</th>\n",
              "      <td>CT</td>\n",
              "      <td>184</td>\n",
              "      <td>510</td>\n",
              "      <td>364-6381</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>213.8</td>\n",
              "      <td>105</td>\n",
              "      <td>36.35</td>\n",
              "      <td>159.6</td>\n",
              "      <td>84</td>\n",
              "      <td>13.57</td>\n",
              "      <td>139.2</td>\n",
              "      <td>137</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3332</th>\n",
              "      <td>TN</td>\n",
              "      <td>74</td>\n",
              "      <td>415</td>\n",
              "      <td>400-4344</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>234.4</td>\n",
              "      <td>113</td>\n",
              "      <td>39.85</td>\n",
              "      <td>265.9</td>\n",
              "      <td>82</td>\n",
              "      <td>22.60</td>\n",
              "      <td>241.4</td>\n",
              "      <td>77</td>\n",
              "      <td>10.86</td>\n",
              "      <td>13.7</td>\n",
              "      <td>4</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3333 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a570342-5c81-4977-ae90-4a458bce7891')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a570342-5c81-4977-ae90-4a458bce7891 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a570342-5c81-4977-ae90-4a458bce7891');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Reading data\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/bigml_59c28831336c6604c800002a.csv\")\n",
        "pd.options.display.max_columns = None\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuZRNrsYhFPN"
      },
      "outputs": [],
      "source": [
        "#Defining columns\n",
        "numerical_columns = ['number vmail messages', 'total day minutes', 'total day calls',\n",
        "                     'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes',\n",
        "                     'total night calls', 'total night charge', 'total intl minutes', 'total intl calls',\n",
        "                     'total intl charge', 'customer service calls']\n",
        "categorical_columns = ['state', 'international plan', 'voice mail plan','area code']\n",
        "outputs = ['churn']\n",
        "\n",
        "#Input >14 Numerical coloums and 4 Categorical coloums\n",
        "#Output > 1 Categorical coloum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "churn_data = dataset[dataset['churn'] == 'True']\n",
        "notchurn_data = dataset[dataset['churn'] == 'False']"
      ],
      "metadata": {
        "id": "SyY-utDEHaey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34Hmwl3ChFPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99c73f4-fd57-4935-f180-efa032575c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2349,  1.5668,  0.4766,  ..., -0.6012, -0.0857, -0.4279],\n",
            "        [ 1.3079, -0.3337,  1.1245,  ..., -0.6012,  1.2412, -0.4279],\n",
            "        [-0.5918,  1.1683,  0.6760,  ...,  0.2115,  0.6972, -1.1882],\n",
            "        ...,\n",
            "        [-0.5918,  0.0188,  0.4268,  ...,  0.6179,  1.3871,  0.3324],\n",
            "        [-0.5918,  0.6248,  0.2275,  ...,  2.2434, -1.8770,  0.3324],\n",
            "        [ 1.2349,  1.0030,  0.6261,  ..., -0.1948,  1.2412, -1.1882]])\n",
            "torch.float32\n",
            "torch.Size([3333, 14])\n",
            "___________________________________________________________________________\n",
            "tensor([[16,  0,  1,  1],\n",
            "        [35,  0,  1,  1],\n",
            "        [31,  0,  0,  1],\n",
            "        ...,\n",
            "        [39,  0,  0,  2],\n",
            "        [ 6,  1,  0,  2],\n",
            "        [42,  0,  1,  1]])\n",
            "torch.int64\n",
            "torch.Size([3333, 4])\n",
            "___________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Processing columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Numerical\n",
        "#Convert our numerical columns to tensors\n",
        "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
        "\n",
        "#Fixed how to use scaler\n",
        "numerical_data = scaler.fit_transform(numerical_data)\n",
        "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
        "\n",
        "#Categorical\n",
        "#Convert the types for categorical columns to category\n",
        "for category in categorical_columns:\n",
        "    dataset[category] = dataset[category].astype('category')\n",
        "\n",
        "#Convert data in the four categorical columns into numpy arrays and then stack all the columns horizontally \n",
        "st = dataset['state'].cat.codes.values\n",
        "ip = dataset['international plan'].cat.codes.values\n",
        "vm = dataset['voice mail plan'].cat.codes.values\n",
        "ac = dataset['area code'].cat.codes.values\n",
        "\n",
        "categorical_data = np.stack([st, ip, vm, ac], 1)\n",
        "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
        "\n",
        "#Outputs\n",
        "#Convert the output numpy array into a tensor object\n",
        "dataset[outputs] = dataset[outputs].astype(int)\n",
        "outputs = torch.tensor(dataset[outputs].values).flatten()\n",
        "outputs = outputs.long()\n",
        "\n",
        "#Print Outputs\n",
        "print(numerical_data)\n",
        "print(numerical_data.dtype)\n",
        "print(numerical_data.shape)\n",
        "print('_' * 75)\n",
        "\n",
        "print(categorical_data)\n",
        "print(categorical_data.dtype)\n",
        "print(categorical_data.shape)\n",
        "print('_' * 75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDn5akU9hFPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d041ebb9-d0d0-43fe-cf44-48de7c47cb56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999\n",
            "666\n",
            "666\n"
          ]
        }
      ],
      "source": [
        "#Dividing dataset into Training, Valid and Test\n",
        "total_records = 3333\n",
        "\n",
        "train_records = int(total_records * .6)\n",
        "valid_records = int(total_records * .2)\n",
        "test_records = int(total_records * .2)\n",
        "\n",
        "numerical_train_data = numerical_data[:train_records]\n",
        "numerical_valid_data = numerical_data[train_records:train_records+valid_records]\n",
        "numerical_test_data = numerical_data[train_records+valid_records:total_records]\n",
        "\n",
        "categorical_train_data = categorical_data[:train_records]\n",
        "categorical_valid_data = categorical_data[train_records:train_records+valid_records]\n",
        "categorical_test_data = categorical_data[train_records+valid_records:total_records]\n",
        "\n",
        "train_outputs = outputs[:train_records]\n",
        "valid_outputs = outputs[train_records:train_records+valid_records]\n",
        "test_outputs = outputs[train_records+valid_records:total_records]\n",
        "\n",
        "#Print divide dataset\n",
        "print(train_records)\n",
        "print(valid_records)\n",
        "print(test_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyVTP22PhFPQ"
      },
      "outputs": [],
      "source": [
        "#Define a class named Model, which will be used to train the model\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "#Creating the Neural Network\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(14, 100) #Numerical\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm1d(100)\n",
        "\n",
        "# =============================================================================================\n",
        "#weights 1 and 2 don't exist in this model because they already exist\n",
        "#as the self.centers attribute in the LMC_Loss model in model_utils.py.\n",
        "# =============================================================================================\n",
        "        \n",
        "        #Categorical\n",
        "        self.layer1_1 = nn.Embedding(51, 5) #51 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_1 = nn.BatchNorm1d(5)\n",
        "        self.layer1_2 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_2 = nn.BatchNorm1d(5)\n",
        "        self.layer1_3 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_3 = nn.BatchNorm1d(5)\n",
        "        self.layer1_4 = nn.Embedding(3, 5) #3 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_4 = nn.BatchNorm1d(5)\n",
        "        \n",
        "        self.layer2 = nn.Linear(120, 120)\n",
        "        self.bn2 = nn.BatchNorm1d(120)\n",
        "\n",
        "        #Decoder\n",
        "        self.decoder_categorical_1 = nn.Linear(120,51)\n",
        "        self.decoder_categorical_2 = nn.Linear(120,2)\n",
        "        self.decoder_categorical_3 = nn.Linear(120,2)\n",
        "        self.decoder_categorical_4 = nn.Linear(120,3)\n",
        "                \n",
        "        self.decoder_numerical = nn.Linear(120,14)\n",
        "        \n",
        "    def forward(self, x_numerical, x_categorical):\n",
        "        x1 = self.layer1(x_numerical)\n",
        "        x1 = self.relu(x1)\n",
        "        x1 = self.bn1(x1)\n",
        "        \n",
        "        #Decoder\n",
        "        x1_embedding = self.layer1_1(x_categorical[:,0])\n",
        "        x1_embedding = self.relu(x1_embedding)\n",
        "        x1_embedding = self.bn1_1(x1_embedding)\n",
        "        \n",
        "        x2_embedding = self.layer1_2(x_categorical[:,1])\n",
        "        x2_embedding = self.relu(x2_embedding)\n",
        "        x2_embedding = self.bn1_2(x2_embedding)\n",
        "        \n",
        "        x3_embedding = self.layer1_3(x_categorical[:,2])\n",
        "        x3_embedding = self.relu(x3_embedding)\n",
        "        x3_embedding = self.bn1_3(x3_embedding)\n",
        "        \n",
        "        x4_embedding = self.layer1_4(x_categorical[:,3])\n",
        "        x4_embedding = self.relu(x4_embedding)\n",
        "        x4_embedding = self.bn1_4(x4_embedding)\n",
        "        \n",
        "        x_embedding = torch.cat([x1_embedding,x2_embedding,x3_embedding,x4_embedding], 1)\n",
        "                \n",
        "        x1 = torch.cat([x1, x_embedding], 1)\n",
        "                \n",
        "        #Decoder\n",
        "        x2 = self.layer2(x1)\n",
        "        emb = self.relu(x2)\n",
        "        x2 = self.bn2(emb)\n",
        "                      \n",
        "        categorical_1_decoded = self.decoder_categorical_1(x2)\n",
        "        categorical_2_decoded = self.decoder_categorical_2(x2)\n",
        "        categorical_3_decoded = self.decoder_categorical_3(x2)\n",
        "        categorical_4_decoded = self.decoder_categorical_4(x2)\n",
        "            \n",
        "        numerical_decoded = self.decoder_numerical(x2)\n",
        "\n",
        "        return emb, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbROfHsjhFPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bd9cbf-b808-4525-ea7c-afc4813d2364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=120, out_features=14, bias=True)\n"
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "print(model.decoder_numerical)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/model_lmcl.py /content"
      ],
      "metadata": {
        "id": "2BQd7UZrW5lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG381vELhFPS"
      },
      "outputs": [],
      "source": [
        "# =============================================================================================\n",
        "#loss function is replaced by using Large Margin Cosine (LMC) Loss implementation\n",
        "#in the following YirongMao repo: https://github.com/YirongMao/softmax_variants\n",
        "#\n",
        "#LMC Loss is a loss function that in theory will be implemented from the start.\n",
        "#Paper LMC Loss: https://arxiv.org/abs/1801.09414\n",
        "#\n",
        "#Cosine Embedding Loss which was used was not actually the same mathematical as LMC Loss,\n",
        "#therefore the implementation was replaced with the implementation of YirongMao \n",
        "# =============================================================================================\n",
        "# =============================================================================================\n",
        "#Added a new loss with the LMCLoss library from YirongMao. Cross entropy loss persists,\n",
        "#because YirongMao's LMCLoss implementation still utilizes Pytorch's original Cross Entropy Loss.\n",
        "#Check the implementation method at: https://github.com/YirongMao/softmax_variants/blob/master/train_mnist_LMCL.py\n",
        "#\n",
        "#margin (parameter m) uses a value of 0.35,\n",
        "#according to the results of research from the original LMCLoss paper which found that the optimal value was 0.35 or 4.\n",
        "# =============================================================================================\n",
        "import model_lmcl\n",
        "\n",
        "lmcl_loss = model_lmcl.LMCL_loss(num_classes=2, feat_dim=120, m=0.35)\n",
        "optimizer_lmcl = torch.optim.Adam(lmcl_loss.parameters(), lr=0.001)\n",
        "# =============================================================================================\n",
        "#For cross entropy loss, weight parameter was added,\n",
        "#because it turned out to be imbalance data (not churn data is much more than churn, with a ratio of around 85:15).\n",
        "#This imbalance data causes the prediction results for the churn class to be not good.\n",
        "#The solution is to use the weight parameter in the cross entropy loss,\n",
        "#so the penalty for being wrong in the churn class is much greater than for being wrong in the not churn class.\n",
        "#Penalty is increased by the inverse ratio of the ratio of the amount of data (15:85)\n",
        "#===============================================================================================\n",
        "#Defining churn:loyal weight ratio. churn_percentage=0.7 means churn:loyal weight ratio of 7:3.\n",
        "#===============================================================================================\n",
        "churn_percentage = 0.7\n",
        "\n",
        "#Defining loss function\n",
        "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1-churn_percentage, churn_percentage]))\n",
        "loss_function_autoencoder = nn.CrossEntropyLoss() #Classification - Categorical\n",
        "loss_function_mse = nn.MSELoss() #Regression - Numerical\n",
        "\n",
        "#Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Added learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the data\n",
        "epochs = 100 #The number of times the dataset will be used to train the model\n",
        "batch_size = 10 #Besaran kumpulan atau pecahan data dari dataset\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_valid = []\n",
        "\n",
        "mean_losses_train_autoencoder = []\n",
        "mean_losses_valid_autoencoder = []\n",
        "\n",
        "mean_losses_train_classification = []\n",
        "mean_losses_valid_classification = []\n",
        "\n",
        "best_loss_valid = np.inf\n",
        "\n",
        "#==============================================================================================================\n",
        "#the loss ratio of the autoencoder is made following a function that decreases exponentially starting from 0.5.\n",
        "#The goal is that the longer it takes, the stronger the effect of loss classification becomes.\n",
        "#Decreasing this ratio is needed because the main purpose of the model is classification\n",
        "#while the autoencoder is just an addition to strengthen the classification ability\n",
        "#==============================================================================================================\n",
        "ae_ratio = 0.5\n",
        "ae_scale = 0.1\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    aggregated_losses_train = []\n",
        "    aggregated_losses_valid = []\n",
        "    \n",
        "    aggregated_losses_autoencoder = []\n",
        "    aggregated_losses_valid_autoencoder = []\n",
        "    \n",
        "    aggregated_losses_classification = []\n",
        "    aggregated_losses_valid_classification = []\n",
        "    \n",
        "    mean_losses = []\n",
        "    i += 1\n",
        "#added random permutation for shuffle data training\n",
        "    idxs_train = np.random.permutation(train_records)\n",
        "    for j in range((train_records//batch_size)+1):\n",
        "        start_train = j*batch_size\n",
        "        end_train = start_train+batch_size\n",
        "        \n",
        "        idxs_batch_train = idxs_train[start_train:end_train] #for shuffle training dataset\n",
        "               \n",
        "        #input is replaced with categorical_train_data and numerical_train_data\n",
        "        train, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, train_embed = model(numerical_train_data[idxs_batch_train], categorical_train_data[idxs_batch_train])\n",
        "        \n",
        "        logits, mlogits = lmcl_loss(train_embed, train_outputs[idxs_batch_train])\n",
        "        \n",
        "        #loss name is differentiated between train and validation, outputs is changed to train_outputs\n",
        "        classification_loss_train = loss_function(mlogits, train_outputs[idxs_batch_train])      \n",
        "        \n",
        "        categorical_1_loss_train = loss_function_autoencoder(categorical_1_decoded, categorical_data[idxs_batch_train][:,0])\n",
        "        categorical_2_loss_train = loss_function_autoencoder(categorical_2_decoded, categorical_data[idxs_batch_train][:,1])\n",
        "        categorical_3_loss_train = loss_function_autoencoder(categorical_3_decoded, categorical_data[idxs_batch_train][:,2])\n",
        "        categorical_4_loss_train = loss_function_autoencoder(categorical_4_decoded, categorical_data[idxs_batch_train][:,3])\n",
        "\n",
        "        numerical_loss_train = loss_function_mse(numerical_decoded, numerical_train_data[idxs_batch_train])\n",
        "   \n",
        "        autoencoder_loss_train = ae_scale*(categorical_1_loss_train + categorical_2_loss_train + categorical_3_loss_train + categorical_4_loss_train + numerical_loss_train)\n",
        "        train_loss = ((1-ae_ratio)*classification_loss_train) + (ae_ratio*autoencoder_loss_train)\n",
        "        \n",
        "        aggregated_losses_autoencoder.append(autoencoder_loss_train)\n",
        "        aggregated_losses_classification.append(classification_loss_train)\n",
        "        aggregated_losses_train.append(train_loss)\n",
        "\n",
        "        print(f'iteration: {j:3} loss: {train_loss.item():10.8f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_lmcl.zero_grad()\n",
        "        \n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_lmcl.step()\n",
        "\n",
        "    aggregated_losses_autoencoder = torch.stack(aggregated_losses_autoencoder)\n",
        "    aggregated_losses_classification = torch.stack(aggregated_losses_classification)\n",
        "    aggregated_losses_train = torch.stack(aggregated_losses_train)\n",
        "        \n",
        "    mean_losses_autoencoder = torch.mean(aggregated_losses_autoencoder)\n",
        "    mean_losses_classification = torch.mean(aggregated_losses_classification)\n",
        "    mean_losses = torch.mean(aggregated_losses_train)\n",
        "        \n",
        "    print(f'epoch: {i:3} mean loss training: {mean_losses.item():10.8f}')\n",
        "        \n",
        "    mean_losses_train_autoencoder.append(mean_losses_autoencoder)\n",
        "    mean_losses_train_classification.append(mean_losses_classification)\n",
        "    mean_losses_train.append(mean_losses)\n",
        "# ==============================================================================================\n",
        "# validation\n",
        "# ==============================================================================================\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        idxs_valid = np.random.permutation(valid_records)\n",
        "        for k in range((valid_records//batch_size)+1):\n",
        "            start_valid = k*batch_size\n",
        "            end_valid = start_valid+batch_size\n",
        "\n",
        "            idxs_batch_valid = idxs_valid[start_valid:end_valid] #for shuffle validation dataset\n",
        "            \n",
        "            #input is replaced with categorical_valid_data and numerical_valid_data\n",
        "            valid, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, valid_embed = model(numerical_valid_data[idxs_batch_valid], categorical_valid_data[idxs_batch_valid])\n",
        "            \n",
        "            logits, mlogits = lmcl_loss(valid_embed, valid_outputs[idxs_batch_valid])\n",
        "            \n",
        "            #loss name is differentiated between train and validation, outputs is changed to valid_outputs\n",
        "            classification_loss_valid = loss_function(mlogits, valid_outputs[idxs_batch_valid])\n",
        " \n",
        "            categorical_1_loss_valid = loss_function_autoencoder(categorical_1_decoded, categorical_data[idxs_batch_valid][:,0])\n",
        "            categorical_2_loss_valid = loss_function_autoencoder(categorical_2_decoded, categorical_data[idxs_batch_valid][:,1])\n",
        "            categorical_3_loss_valid = loss_function_autoencoder(categorical_3_decoded, categorical_data[idxs_batch_valid][:,2])\n",
        "            categorical_4_loss_valid = loss_function_autoencoder(categorical_4_decoded, categorical_data[idxs_batch_valid][:,3])\n",
        "\n",
        "            numerical_loss_valid = loss_function_mse(numerical_decoded, numerical_valid_data[idxs_batch_valid])\n",
        "            \n",
        "            autoencoder_loss_valid = ae_scale*(categorical_1_loss_valid + categorical_2_loss_valid + categorical_3_loss_valid + categorical_4_loss_valid + numerical_loss_valid)\n",
        "            valid_loss = ((1-ae_ratio)*classification_loss_valid) + (ae_ratio*autoencoder_loss_valid)\n",
        "\n",
        "            aggregated_losses_valid_autoencoder.append(autoencoder_loss_valid)\n",
        "            aggregated_losses_valid_classification.append(classification_loss_valid)\n",
        "            aggregated_losses_valid.append(valid_loss)\n",
        "\n",
        "    mean_loss_valid = torch.mean(torch.stack(aggregated_losses_valid))\n",
        "    mean_loss_valid_autoencoder = torch.mean(torch.stack(aggregated_losses_valid_autoencoder))\n",
        "    mean_loss_valid_classification = torch.mean(torch.stack(aggregated_losses_valid_classification))\n",
        "    \n",
        "    print(f'epoch: {i:3} mean loss validation: {mean_loss_valid:.8f}')\n",
        "  \n",
        "#======================================================================\n",
        "#The model is saved when the loss is lowest not at the end of the epoch\n",
        "#======================================================================  \n",
        "    if mean_loss_valid.cpu().numpy()[()] < best_loss_valid:\n",
        "        best_loss_valid = mean_loss_valid\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCL&SSL.pth\".format(churn_percentage))\n",
        "        torch.save(lmcl_loss.state_dict(), \"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCLOSS&SSL.pth\".format(churn_percentage))\n",
        "        best_epoch = i        \n",
        "    \n",
        "    mean_losses_valid.append(mean_loss_valid)\n",
        "    mean_losses_valid_autoencoder.append(mean_loss_valid_autoencoder)\n",
        "    mean_losses_valid_classification.append(mean_loss_valid_classification)\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuiuC3sN0fbm",
        "outputId": "7dc7204d-04af-4acc-90ac-d33777baa779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration:  50 loss: 0.10493910\n",
            "iteration:  51 loss: 0.83668458\n",
            "iteration:  52 loss: 0.26255655\n",
            "iteration:  53 loss: 0.51096064\n",
            "iteration:  54 loss: 0.12901108\n",
            "iteration:  55 loss: 0.19760962\n",
            "iteration:  56 loss: 0.60961354\n",
            "iteration:  57 loss: 0.70829719\n",
            "iteration:  58 loss: 0.12336376\n",
            "iteration:  59 loss: 0.23036274\n",
            "iteration:  60 loss: 0.12211277\n",
            "iteration:  61 loss: 0.19282831\n",
            "iteration:  62 loss: 0.13486856\n",
            "iteration:  63 loss: 0.61160779\n",
            "iteration:  64 loss: 0.22176707\n",
            "iteration:  65 loss: 0.44001555\n",
            "iteration:  66 loss: 0.30922282\n",
            "iteration:  67 loss: 0.63722819\n",
            "iteration:  68 loss: 0.29704016\n",
            "iteration:  69 loss: 0.77859902\n",
            "iteration:  70 loss: 0.75853640\n",
            "iteration:  71 loss: 0.22061014\n",
            "iteration:  72 loss: 0.67298007\n",
            "iteration:  73 loss: 0.49086145\n",
            "iteration:  74 loss: 0.16226885\n",
            "iteration:  75 loss: 1.35823941\n",
            "iteration:  76 loss: 0.56534076\n",
            "iteration:  77 loss: 0.28132927\n",
            "iteration:  78 loss: 0.14213134\n",
            "iteration:  79 loss: 0.29471427\n",
            "iteration:  80 loss: 0.38497013\n",
            "iteration:  81 loss: 0.14259343\n",
            "iteration:  82 loss: 0.47672737\n",
            "iteration:  83 loss: 0.17040867\n",
            "iteration:  84 loss: 0.19542935\n",
            "iteration:  85 loss: 0.17749107\n",
            "iteration:  86 loss: 0.59962302\n",
            "iteration:  87 loss: 0.66933256\n",
            "iteration:  88 loss: 0.29205823\n",
            "iteration:  89 loss: 0.29708263\n",
            "iteration:  90 loss: 0.47445795\n",
            "iteration:  91 loss: 0.31449258\n",
            "iteration:  92 loss: 0.21942751\n",
            "iteration:  93 loss: 0.16638504\n",
            "iteration:  94 loss: 0.29627794\n",
            "iteration:  95 loss: 0.88614589\n",
            "iteration:  96 loss: 1.37510121\n",
            "iteration:  97 loss: 0.27080655\n",
            "iteration:  98 loss: 0.42723781\n",
            "iteration:  99 loss: 0.13084418\n",
            "iteration: 100 loss: 0.71540028\n",
            "iteration: 101 loss: 0.18538973\n",
            "iteration: 102 loss: 0.50973904\n",
            "iteration: 103 loss: 0.08540542\n",
            "iteration: 104 loss: 0.46233892\n",
            "iteration: 105 loss: 0.12936480\n",
            "iteration: 106 loss: 0.13667648\n",
            "iteration: 107 loss: 0.32717264\n",
            "iteration: 108 loss: 0.07812734\n",
            "iteration: 109 loss: 0.14119804\n",
            "iteration: 110 loss: 0.15659174\n",
            "iteration: 111 loss: 0.36811382\n",
            "iteration: 112 loss: 0.32814160\n",
            "iteration: 113 loss: 0.57048440\n",
            "iteration: 114 loss: 0.25569618\n",
            "iteration: 115 loss: 0.45725760\n",
            "iteration: 116 loss: 0.08296816\n",
            "iteration: 117 loss: 0.21783087\n",
            "iteration: 118 loss: 0.33271560\n",
            "iteration: 119 loss: 0.28321549\n",
            "iteration: 120 loss: 0.06936263\n",
            "iteration: 121 loss: 0.44621176\n",
            "iteration: 122 loss: 0.23206198\n",
            "iteration: 123 loss: 0.12424777\n",
            "iteration: 124 loss: 0.28234425\n",
            "iteration: 125 loss: 0.41959417\n",
            "iteration: 126 loss: 0.35591224\n",
            "iteration: 127 loss: 0.12020724\n",
            "iteration: 128 loss: 0.19237089\n",
            "iteration: 129 loss: 0.14144829\n",
            "iteration: 130 loss: 0.18957372\n",
            "iteration: 131 loss: 0.13431874\n",
            "iteration: 132 loss: 0.25834823\n",
            "iteration: 133 loss: 0.14218771\n",
            "iteration: 134 loss: 0.60659379\n",
            "iteration: 135 loss: 0.14401223\n",
            "iteration: 136 loss: 0.11826694\n",
            "iteration: 137 loss: 0.19332775\n",
            "iteration: 138 loss: 0.18275595\n",
            "iteration: 139 loss: 0.17127526\n",
            "iteration: 140 loss: 0.13684224\n",
            "iteration: 141 loss: 0.22835882\n",
            "iteration: 142 loss: 0.28398469\n",
            "iteration: 143 loss: 0.34619310\n",
            "iteration: 144 loss: 0.17003551\n",
            "iteration: 145 loss: 0.31841224\n",
            "iteration: 146 loss: 0.15907025\n",
            "iteration: 147 loss: 0.83295006\n",
            "iteration: 148 loss: 0.28708100\n",
            "iteration: 149 loss: 0.50694185\n",
            "iteration: 150 loss: 0.21700327\n",
            "iteration: 151 loss: 1.34006691\n",
            "iteration: 152 loss: 0.94522738\n",
            "iteration: 153 loss: 0.21464795\n",
            "iteration: 154 loss: 0.61405641\n",
            "iteration: 155 loss: 1.19714630\n",
            "iteration: 156 loss: 0.23753464\n",
            "iteration: 157 loss: 0.37618211\n",
            "iteration: 158 loss: 0.09832782\n",
            "iteration: 159 loss: 0.11237940\n",
            "iteration: 160 loss: 0.16334832\n",
            "iteration: 161 loss: 0.09632504\n",
            "iteration: 162 loss: 0.08128518\n",
            "iteration: 163 loss: 0.39616480\n",
            "iteration: 164 loss: 0.11912727\n",
            "iteration: 165 loss: 0.26034147\n",
            "iteration: 166 loss: 0.25582021\n",
            "iteration: 167 loss: 0.23140894\n",
            "iteration: 168 loss: 0.17947808\n",
            "iteration: 169 loss: 0.29351446\n",
            "iteration: 170 loss: 0.32343987\n",
            "iteration: 171 loss: 1.08440185\n",
            "iteration: 172 loss: 0.85523021\n",
            "iteration: 173 loss: 0.14208946\n",
            "iteration: 174 loss: 0.27273062\n",
            "iteration: 175 loss: 0.09194521\n",
            "iteration: 176 loss: 0.41783202\n",
            "iteration: 177 loss: 0.57169938\n",
            "iteration: 178 loss: 0.20709403\n",
            "iteration: 179 loss: 0.59774917\n",
            "iteration: 180 loss: 0.33421782\n",
            "iteration: 181 loss: 0.40149468\n",
            "iteration: 182 loss: 0.21707883\n",
            "iteration: 183 loss: 0.29619396\n",
            "iteration: 184 loss: 0.31203765\n",
            "iteration: 185 loss: 0.25292417\n",
            "iteration: 186 loss: 0.23130554\n",
            "iteration: 187 loss: 0.20767488\n",
            "iteration: 188 loss: 0.18359977\n",
            "iteration: 189 loss: 0.69870669\n",
            "iteration: 190 loss: 0.33666423\n",
            "iteration: 191 loss: 0.44805884\n",
            "iteration: 192 loss: 0.14756674\n",
            "iteration: 193 loss: 0.19176379\n",
            "iteration: 194 loss: 0.28482854\n",
            "iteration: 195 loss: 0.54230392\n",
            "iteration: 196 loss: 0.27081278\n",
            "iteration: 197 loss: 0.24117205\n",
            "iteration: 198 loss: 0.09418391\n",
            "iteration: 199 loss: 0.55063534\n",
            "epoch:  76 mean loss training: 0.36717525\n",
            "epoch:  76 mean loss validation: 1.24075699\n",
            "iteration:   0 loss: 0.78747094\n",
            "iteration:   1 loss: 0.20302299\n",
            "iteration:   2 loss: 0.25521141\n",
            "iteration:   3 loss: 0.32247382\n",
            "iteration:   4 loss: 1.08679879\n",
            "iteration:   5 loss: 0.24528842\n",
            "iteration:   6 loss: 0.34938633\n",
            "iteration:   7 loss: 0.62003344\n",
            "iteration:   8 loss: 0.35447553\n",
            "iteration:   9 loss: 0.69133180\n",
            "iteration:  10 loss: 0.40483534\n",
            "iteration:  11 loss: 0.14141279\n",
            "iteration:  12 loss: 0.17039475\n",
            "iteration:  13 loss: 0.18411204\n",
            "iteration:  14 loss: 0.24323028\n",
            "iteration:  15 loss: 0.22942986\n",
            "iteration:  16 loss: 0.29238954\n",
            "iteration:  17 loss: 0.18182258\n",
            "iteration:  18 loss: 0.34972879\n",
            "iteration:  19 loss: 0.17810006\n",
            "iteration:  20 loss: 0.16138110\n",
            "iteration:  21 loss: 0.17172194\n",
            "iteration:  22 loss: 0.47947800\n",
            "iteration:  23 loss: 0.63633764\n",
            "iteration:  24 loss: 0.17225745\n",
            "iteration:  25 loss: 0.29941514\n",
            "iteration:  26 loss: 0.11446504\n",
            "iteration:  27 loss: 0.28055936\n",
            "iteration:  28 loss: 0.27693877\n",
            "iteration:  29 loss: 0.12316687\n",
            "iteration:  30 loss: 0.17141065\n",
            "iteration:  31 loss: 0.19767036\n",
            "iteration:  32 loss: 0.31734633\n",
            "iteration:  33 loss: 0.27855799\n",
            "iteration:  34 loss: 0.10927130\n",
            "iteration:  35 loss: 0.34563538\n",
            "iteration:  36 loss: 0.09686532\n",
            "iteration:  37 loss: 0.33221233\n",
            "iteration:  38 loss: 0.42330286\n",
            "iteration:  39 loss: 0.32768500\n",
            "iteration:  40 loss: 0.18431906\n",
            "iteration:  41 loss: 0.19228649\n",
            "iteration:  42 loss: 0.47980446\n",
            "iteration:  43 loss: 0.33589444\n",
            "iteration:  44 loss: 0.12184111\n",
            "iteration:  45 loss: 0.20690548\n",
            "iteration:  46 loss: 0.89407194\n",
            "iteration:  47 loss: 0.34858227\n",
            "iteration:  48 loss: 0.18326330\n",
            "iteration:  49 loss: 0.21162108\n",
            "iteration:  50 loss: 0.11405800\n",
            "iteration:  51 loss: 0.63613725\n",
            "iteration:  52 loss: 0.23481667\n",
            "iteration:  53 loss: 0.18815242\n",
            "iteration:  54 loss: 0.22213084\n",
            "iteration:  55 loss: 0.10083005\n",
            "iteration:  56 loss: 0.39724785\n",
            "iteration:  57 loss: 0.48259941\n",
            "iteration:  58 loss: 0.24325615\n",
            "iteration:  59 loss: 0.19757895\n",
            "iteration:  60 loss: 0.14714342\n",
            "iteration:  61 loss: 0.36079055\n",
            "iteration:  62 loss: 0.38019449\n",
            "iteration:  63 loss: 0.47384608\n",
            "iteration:  64 loss: 0.11706915\n",
            "iteration:  65 loss: 0.86177438\n",
            "iteration:  66 loss: 0.34296063\n",
            "iteration:  67 loss: 0.16599229\n",
            "iteration:  68 loss: 0.15622383\n",
            "iteration:  69 loss: 0.23761858\n",
            "iteration:  70 loss: 0.15913145\n",
            "iteration:  71 loss: 0.79969233\n",
            "iteration:  72 loss: 0.39146712\n",
            "iteration:  73 loss: 0.16992812\n",
            "iteration:  74 loss: 0.44174570\n",
            "iteration:  75 loss: 0.91925484\n",
            "iteration:  76 loss: 0.38291281\n",
            "iteration:  77 loss: 0.24099636\n",
            "iteration:  78 loss: 0.43987262\n",
            "iteration:  79 loss: 0.08794609\n",
            "iteration:  80 loss: 0.22523391\n",
            "iteration:  81 loss: 0.09991104\n",
            "iteration:  82 loss: 0.68780839\n",
            "iteration:  83 loss: 0.62212616\n",
            "iteration:  84 loss: 0.17991242\n",
            "iteration:  85 loss: 0.16525383\n",
            "iteration:  86 loss: 0.29252401\n",
            "iteration:  87 loss: 0.50524342\n",
            "iteration:  88 loss: 1.21562314\n",
            "iteration:  89 loss: 0.27127099\n",
            "iteration:  90 loss: 0.12148099\n",
            "iteration:  91 loss: 0.69977158\n",
            "iteration:  92 loss: 0.37795350\n",
            "iteration:  93 loss: 0.25147426\n",
            "iteration:  94 loss: 0.53271931\n",
            "iteration:  95 loss: 0.21293199\n",
            "iteration:  96 loss: 0.25126585\n",
            "iteration:  97 loss: 1.27512777\n",
            "iteration:  98 loss: 0.30940196\n",
            "iteration:  99 loss: 0.22874659\n",
            "iteration: 100 loss: 0.17030904\n",
            "iteration: 101 loss: 0.19170204\n",
            "iteration: 102 loss: 0.15688744\n",
            "iteration: 103 loss: 0.14978327\n",
            "iteration: 104 loss: 0.98566937\n",
            "iteration: 105 loss: 0.51022708\n",
            "iteration: 106 loss: 0.17679451\n",
            "iteration: 107 loss: 0.35030115\n",
            "iteration: 108 loss: 0.12799746\n",
            "iteration: 109 loss: 0.32405183\n",
            "iteration: 110 loss: 1.42538941\n",
            "iteration: 111 loss: 0.64007038\n",
            "iteration: 112 loss: 0.18708280\n",
            "iteration: 113 loss: 0.10646160\n",
            "iteration: 114 loss: 0.30274951\n",
            "iteration: 115 loss: 1.25902128\n",
            "iteration: 116 loss: 0.57973439\n",
            "iteration: 117 loss: 0.14019166\n",
            "iteration: 118 loss: 0.18075126\n",
            "iteration: 119 loss: 0.40545809\n",
            "iteration: 120 loss: 0.09721004\n",
            "iteration: 121 loss: 0.09564570\n",
            "iteration: 122 loss: 0.72020638\n",
            "iteration: 123 loss: 1.74001634\n",
            "iteration: 124 loss: 0.17877471\n",
            "iteration: 125 loss: 0.33367854\n",
            "iteration: 126 loss: 0.30260482\n",
            "iteration: 127 loss: 0.34650329\n",
            "iteration: 128 loss: 0.49494106\n",
            "iteration: 129 loss: 0.63559872\n",
            "iteration: 130 loss: 0.10162698\n",
            "iteration: 131 loss: 0.49339923\n",
            "iteration: 132 loss: 0.11005282\n",
            "iteration: 133 loss: 0.16420451\n",
            "iteration: 134 loss: 0.20795794\n",
            "iteration: 135 loss: 0.36096779\n",
            "iteration: 136 loss: 0.81539392\n",
            "iteration: 137 loss: 0.36167523\n",
            "iteration: 138 loss: 0.24061668\n",
            "iteration: 139 loss: 0.34768131\n",
            "iteration: 140 loss: 0.96350873\n",
            "iteration: 141 loss: 0.23733795\n",
            "iteration: 142 loss: 0.23914282\n",
            "iteration: 143 loss: 0.26747307\n",
            "iteration: 144 loss: 0.36605754\n",
            "iteration: 145 loss: 0.38507152\n",
            "iteration: 146 loss: 0.85727286\n",
            "iteration: 147 loss: 0.87864351\n",
            "iteration: 148 loss: 0.13082963\n",
            "iteration: 149 loss: 0.37199089\n",
            "iteration: 150 loss: 0.38991469\n",
            "iteration: 151 loss: 0.13701889\n",
            "iteration: 152 loss: 0.76523429\n",
            "iteration: 153 loss: 0.14855140\n",
            "iteration: 154 loss: 0.10282934\n",
            "iteration: 155 loss: 0.17482457\n",
            "iteration: 156 loss: 0.39252937\n",
            "iteration: 157 loss: 0.77191991\n",
            "iteration: 158 loss: 1.30551708\n",
            "iteration: 159 loss: 0.44651806\n",
            "iteration: 160 loss: 0.22911194\n",
            "iteration: 161 loss: 0.53869396\n",
            "iteration: 162 loss: 0.24066323\n",
            "iteration: 163 loss: 0.18875173\n",
            "iteration: 164 loss: 0.24155289\n",
            "iteration: 165 loss: 0.20930804\n",
            "iteration: 166 loss: 0.43975961\n",
            "iteration: 167 loss: 0.13634810\n",
            "iteration: 168 loss: 0.75894582\n",
            "iteration: 169 loss: 0.34290248\n",
            "iteration: 170 loss: 0.33887979\n",
            "iteration: 171 loss: 0.58764279\n",
            "iteration: 172 loss: 0.94713730\n",
            "iteration: 173 loss: 0.32838595\n",
            "iteration: 174 loss: 0.62644380\n",
            "iteration: 175 loss: 0.24624157\n",
            "iteration: 176 loss: 0.11706579\n",
            "iteration: 177 loss: 0.44663054\n",
            "iteration: 178 loss: 0.21631882\n",
            "iteration: 179 loss: 0.68006617\n",
            "iteration: 180 loss: 0.53932929\n",
            "iteration: 181 loss: 0.55321932\n",
            "iteration: 182 loss: 0.38017035\n",
            "iteration: 183 loss: 0.16251311\n",
            "iteration: 184 loss: 0.11158124\n",
            "iteration: 185 loss: 0.40528005\n",
            "iteration: 186 loss: 0.19249627\n",
            "iteration: 187 loss: 0.18426010\n",
            "iteration: 188 loss: 0.51421607\n",
            "iteration: 189 loss: 0.23138997\n",
            "iteration: 190 loss: 0.22802766\n",
            "iteration: 191 loss: 0.29916155\n",
            "iteration: 192 loss: 0.11923352\n",
            "iteration: 193 loss: 0.21997887\n",
            "iteration: 194 loss: 0.12001684\n",
            "iteration: 195 loss: 0.21755542\n",
            "iteration: 196 loss: 0.51766628\n",
            "iteration: 197 loss: 0.07326904\n",
            "iteration: 198 loss: 0.16239025\n",
            "iteration: 199 loss: 0.31798381\n",
            "epoch:  77 mean loss training: 0.36823264\n",
            "epoch:  77 mean loss validation: 1.22620463\n",
            "iteration:   0 loss: 0.35495609\n",
            "iteration:   1 loss: 0.28543258\n",
            "iteration:   2 loss: 0.18658021\n",
            "iteration:   3 loss: 1.23642051\n",
            "iteration:   4 loss: 0.25870079\n",
            "iteration:   5 loss: 0.18163854\n",
            "iteration:   6 loss: 0.96956384\n",
            "iteration:   7 loss: 0.25882840\n",
            "iteration:   8 loss: 0.35565364\n",
            "iteration:   9 loss: 0.08451323\n",
            "iteration:  10 loss: 0.44606709\n",
            "iteration:  11 loss: 0.27112007\n",
            "iteration:  12 loss: 0.18600181\n",
            "iteration:  13 loss: 0.34121636\n",
            "iteration:  14 loss: 0.15427640\n",
            "iteration:  15 loss: 0.89993924\n",
            "iteration:  16 loss: 0.78516203\n",
            "iteration:  17 loss: 0.23190887\n",
            "iteration:  18 loss: 0.24760820\n",
            "iteration:  19 loss: 0.33842173\n",
            "iteration:  20 loss: 0.16949785\n",
            "iteration:  21 loss: 0.09608715\n",
            "iteration:  22 loss: 0.25134185\n",
            "iteration:  23 loss: 0.28161195\n",
            "iteration:  24 loss: 0.58094215\n",
            "iteration:  25 loss: 0.14102650\n",
            "iteration:  26 loss: 0.41649440\n",
            "iteration:  27 loss: 0.15534584\n",
            "iteration:  28 loss: 0.22532004\n",
            "iteration:  29 loss: 0.17890462\n",
            "iteration:  30 loss: 0.14358521\n",
            "iteration:  31 loss: 0.50400579\n",
            "iteration:  32 loss: 0.35075238\n",
            "iteration:  33 loss: 0.20658511\n",
            "iteration:  34 loss: 0.25842202\n",
            "iteration:  35 loss: 1.39161837\n",
            "iteration:  36 loss: 0.19200477\n",
            "iteration:  37 loss: 0.38630331\n",
            "iteration:  38 loss: 0.94912708\n",
            "iteration:  39 loss: 0.14137059\n",
            "iteration:  40 loss: 0.13997188\n",
            "iteration:  41 loss: 0.31320968\n",
            "iteration:  42 loss: 0.44335404\n",
            "iteration:  43 loss: 0.14996871\n",
            "iteration:  44 loss: 0.84796220\n",
            "iteration:  45 loss: 0.28528598\n",
            "iteration:  46 loss: 0.12530926\n",
            "iteration:  47 loss: 0.15040219\n",
            "iteration:  48 loss: 0.14203891\n",
            "iteration:  49 loss: 0.14247729\n",
            "iteration:  50 loss: 0.15499887\n",
            "iteration:  51 loss: 0.09534170\n",
            "iteration:  52 loss: 0.27401698\n",
            "iteration:  53 loss: 0.80868047\n",
            "iteration:  54 loss: 0.10990853\n",
            "iteration:  55 loss: 0.89196879\n",
            "iteration:  56 loss: 0.83883762\n",
            "iteration:  57 loss: 0.08525749\n",
            "iteration:  58 loss: 0.16572224\n",
            "iteration:  59 loss: 0.54317117\n",
            "iteration:  60 loss: 0.22641806\n",
            "iteration:  61 loss: 0.11859576\n",
            "iteration:  62 loss: 0.63983959\n",
            "iteration:  63 loss: 0.32837951\n",
            "iteration:  64 loss: 1.18612659\n",
            "iteration:  65 loss: 0.24035941\n",
            "iteration:  66 loss: 1.35848916\n",
            "iteration:  67 loss: 0.80106139\n",
            "iteration:  68 loss: 0.56479275\n",
            "iteration:  69 loss: 0.19681498\n",
            "iteration:  70 loss: 0.26269916\n",
            "iteration:  71 loss: 0.37417465\n",
            "iteration:  72 loss: 0.85452485\n",
            "iteration:  73 loss: 0.20029736\n",
            "iteration:  74 loss: 0.39525169\n",
            "iteration:  75 loss: 0.20311919\n",
            "iteration:  76 loss: 0.07369367\n",
            "iteration:  77 loss: 0.35561362\n",
            "iteration:  78 loss: 0.33738226\n",
            "iteration:  79 loss: 0.90883601\n",
            "iteration:  80 loss: 0.81080288\n",
            "iteration:  81 loss: 0.11793649\n",
            "iteration:  82 loss: 0.13454680\n",
            "iteration:  83 loss: 0.09445006\n",
            "iteration:  84 loss: 0.23715763\n",
            "iteration:  85 loss: 0.61451626\n",
            "iteration:  86 loss: 0.30962455\n",
            "iteration:  87 loss: 0.11214363\n",
            "iteration:  88 loss: 0.17798050\n",
            "iteration:  89 loss: 0.31943661\n",
            "iteration:  90 loss: 0.29746032\n",
            "iteration:  91 loss: 0.23594484\n",
            "iteration:  92 loss: 0.13253078\n",
            "iteration:  93 loss: 0.15029106\n",
            "iteration:  94 loss: 0.21976268\n",
            "iteration:  95 loss: 0.25381276\n",
            "iteration:  96 loss: 0.34221324\n",
            "iteration:  97 loss: 0.13399293\n",
            "iteration:  98 loss: 0.31314397\n",
            "iteration:  99 loss: 0.31566346\n",
            "iteration: 100 loss: 0.19911197\n",
            "iteration: 101 loss: 0.48443386\n",
            "iteration: 102 loss: 1.44280672\n",
            "iteration: 103 loss: 0.48280358\n",
            "iteration: 104 loss: 0.58973712\n",
            "iteration: 105 loss: 0.08337247\n",
            "iteration: 106 loss: 0.94928128\n",
            "iteration: 107 loss: 0.07803421\n",
            "iteration: 108 loss: 0.62378377\n",
            "iteration: 109 loss: 0.13489972\n",
            "iteration: 110 loss: 1.14767456\n",
            "iteration: 111 loss: 0.22018312\n",
            "iteration: 112 loss: 0.45324931\n",
            "iteration: 113 loss: 0.21147124\n",
            "iteration: 114 loss: 0.34232730\n",
            "iteration: 115 loss: 0.31509238\n",
            "iteration: 116 loss: 0.39485076\n",
            "iteration: 117 loss: 0.49279058\n",
            "iteration: 118 loss: 0.27940881\n",
            "iteration: 119 loss: 0.29653654\n",
            "iteration: 120 loss: 0.22544783\n",
            "iteration: 121 loss: 0.15522289\n",
            "iteration: 122 loss: 0.18796086\n",
            "iteration: 123 loss: 0.51105225\n",
            "iteration: 124 loss: 0.37688509\n",
            "iteration: 125 loss: 0.44430178\n",
            "iteration: 126 loss: 0.19503582\n",
            "iteration: 127 loss: 0.20298338\n",
            "iteration: 128 loss: 0.38930130\n",
            "iteration: 129 loss: 0.19065495\n",
            "iteration: 130 loss: 0.10206366\n",
            "iteration: 131 loss: 0.33144259\n",
            "iteration: 132 loss: 0.39733997\n",
            "iteration: 133 loss: 0.36100036\n",
            "iteration: 134 loss: 0.51033354\n",
            "iteration: 135 loss: 0.85151255\n",
            "iteration: 136 loss: 0.34023756\n",
            "iteration: 137 loss: 0.31617665\n",
            "iteration: 138 loss: 0.81133258\n",
            "iteration: 139 loss: 0.12589680\n",
            "iteration: 140 loss: 0.12787555\n",
            "iteration: 141 loss: 0.25907630\n",
            "iteration: 142 loss: 0.96940744\n",
            "iteration: 143 loss: 0.26815456\n",
            "iteration: 144 loss: 0.27149832\n",
            "iteration: 145 loss: 0.12799595\n",
            "iteration: 146 loss: 0.22485083\n",
            "iteration: 147 loss: 0.24202918\n",
            "iteration: 148 loss: 0.28662664\n",
            "iteration: 149 loss: 0.64125723\n",
            "iteration: 150 loss: 0.19718176\n",
            "iteration: 151 loss: 0.34799436\n",
            "iteration: 152 loss: 0.15143798\n",
            "iteration: 153 loss: 0.08442857\n",
            "iteration: 154 loss: 1.82658839\n",
            "iteration: 155 loss: 0.11166147\n",
            "iteration: 156 loss: 0.20797622\n",
            "iteration: 157 loss: 0.67985040\n",
            "iteration: 158 loss: 0.21442945\n",
            "iteration: 159 loss: 0.75548822\n",
            "iteration: 160 loss: 0.24183428\n",
            "iteration: 161 loss: 0.24746712\n",
            "iteration: 162 loss: 0.65551960\n",
            "iteration: 163 loss: 0.28385708\n",
            "iteration: 164 loss: 0.15733099\n",
            "iteration: 165 loss: 0.14059955\n",
            "iteration: 166 loss: 0.60970724\n",
            "iteration: 167 loss: 0.83636504\n",
            "iteration: 168 loss: 0.62971634\n",
            "iteration: 169 loss: 0.40324149\n",
            "iteration: 170 loss: 0.12468982\n",
            "iteration: 171 loss: 0.32614076\n",
            "iteration: 172 loss: 0.92916256\n",
            "iteration: 173 loss: 0.36192024\n",
            "iteration: 174 loss: 0.39859241\n",
            "iteration: 175 loss: 0.17571262\n",
            "iteration: 176 loss: 0.23710912\n",
            "iteration: 177 loss: 0.66809839\n",
            "iteration: 178 loss: 0.21607746\n",
            "iteration: 179 loss: 0.91301823\n",
            "iteration: 180 loss: 0.16339904\n",
            "iteration: 181 loss: 0.25499135\n",
            "iteration: 182 loss: 0.18392515\n",
            "iteration: 183 loss: 0.26869071\n",
            "iteration: 184 loss: 0.21244094\n",
            "iteration: 185 loss: 0.19372571\n",
            "iteration: 186 loss: 0.22560270\n",
            "iteration: 187 loss: 0.83465320\n",
            "iteration: 188 loss: 0.19419739\n",
            "iteration: 189 loss: 0.27992135\n",
            "iteration: 190 loss: 0.20231648\n",
            "iteration: 191 loss: 0.11949930\n",
            "iteration: 192 loss: 0.32379016\n",
            "iteration: 193 loss: 0.19487515\n",
            "iteration: 194 loss: 0.34903112\n",
            "iteration: 195 loss: 0.23987898\n",
            "iteration: 196 loss: 0.08970090\n",
            "iteration: 197 loss: 0.27555963\n",
            "iteration: 198 loss: 0.35030648\n",
            "iteration: 199 loss: 0.13140787\n",
            "epoch:  78 mean loss training: 0.37320018\n",
            "epoch:  78 mean loss validation: 1.24444258\n",
            "iteration:   0 loss: 0.12112442\n",
            "iteration:   1 loss: 0.20541921\n",
            "iteration:   2 loss: 0.43663207\n",
            "iteration:   3 loss: 0.28387484\n",
            "iteration:   4 loss: 0.14999771\n",
            "iteration:   5 loss: 0.50426114\n",
            "iteration:   6 loss: 0.12179081\n",
            "iteration:   7 loss: 0.16551331\n",
            "iteration:   8 loss: 0.13016553\n",
            "iteration:   9 loss: 0.15692072\n",
            "iteration:  10 loss: 0.33404106\n",
            "iteration:  11 loss: 0.55199921\n",
            "iteration:  12 loss: 0.09385033\n",
            "iteration:  13 loss: 0.51109892\n",
            "iteration:  14 loss: 0.12215973\n",
            "iteration:  15 loss: 0.20098662\n",
            "iteration:  16 loss: 0.55187005\n",
            "iteration:  17 loss: 0.25585666\n",
            "iteration:  18 loss: 0.15131873\n",
            "iteration:  19 loss: 0.25628778\n",
            "iteration:  20 loss: 0.26656634\n",
            "iteration:  21 loss: 0.17793554\n",
            "iteration:  22 loss: 0.36264682\n",
            "iteration:  23 loss: 1.65037024\n",
            "iteration:  24 loss: 0.58722478\n",
            "iteration:  25 loss: 0.30916846\n",
            "iteration:  26 loss: 0.17722045\n",
            "iteration:  27 loss: 0.34887016\n",
            "iteration:  28 loss: 0.37366626\n",
            "iteration:  29 loss: 0.20922993\n",
            "iteration:  30 loss: 0.17370693\n",
            "iteration:  31 loss: 0.49420285\n",
            "iteration:  32 loss: 0.29053423\n",
            "iteration:  33 loss: 0.53832603\n",
            "iteration:  34 loss: 0.11977415\n",
            "iteration:  35 loss: 0.29294199\n",
            "iteration:  36 loss: 0.10132560\n",
            "iteration:  37 loss: 0.12843308\n",
            "iteration:  38 loss: 0.21580628\n",
            "iteration:  39 loss: 0.94063902\n",
            "iteration:  40 loss: 0.59495294\n",
            "iteration:  41 loss: 0.16564243\n",
            "iteration:  42 loss: 0.33111387\n",
            "iteration:  43 loss: 0.07811604\n",
            "iteration:  44 loss: 0.22030944\n",
            "iteration:  45 loss: 0.37248117\n",
            "iteration:  46 loss: 0.80135119\n",
            "iteration:  47 loss: 0.80484498\n",
            "iteration:  48 loss: 0.40379533\n",
            "iteration:  49 loss: 0.09870116\n",
            "iteration:  50 loss: 0.77356625\n",
            "iteration:  51 loss: 0.60269219\n",
            "iteration:  52 loss: 0.66121125\n",
            "iteration:  53 loss: 0.09732525\n",
            "iteration:  54 loss: 0.07785194\n",
            "iteration:  55 loss: 1.08753955\n",
            "iteration:  56 loss: 0.23569538\n",
            "iteration:  57 loss: 0.74592316\n",
            "iteration:  58 loss: 0.11534464\n",
            "iteration:  59 loss: 1.61038470\n",
            "iteration:  60 loss: 0.72126919\n",
            "iteration:  61 loss: 0.15143549\n",
            "iteration:  62 loss: 0.40062666\n",
            "iteration:  63 loss: 0.37872440\n",
            "iteration:  64 loss: 0.21865401\n",
            "iteration:  65 loss: 0.88232404\n",
            "iteration:  66 loss: 0.80649078\n",
            "iteration:  67 loss: 0.39284921\n",
            "iteration:  68 loss: 0.69987035\n",
            "iteration:  69 loss: 0.34596798\n",
            "iteration:  70 loss: 0.30716139\n",
            "iteration:  71 loss: 0.37180668\n",
            "iteration:  72 loss: 0.23988366\n",
            "iteration:  73 loss: 0.88181448\n",
            "iteration:  74 loss: 0.24501880\n",
            "iteration:  75 loss: 0.23601966\n",
            "iteration:  76 loss: 0.34744412\n",
            "iteration:  77 loss: 0.17798972\n",
            "iteration:  78 loss: 0.72803104\n",
            "iteration:  79 loss: 0.82510054\n",
            "iteration:  80 loss: 0.25673622\n",
            "iteration:  81 loss: 0.31122014\n",
            "iteration:  82 loss: 0.11406475\n",
            "iteration:  83 loss: 0.19450241\n",
            "iteration:  84 loss: 0.44121978\n",
            "iteration:  85 loss: 0.12820163\n",
            "iteration:  86 loss: 0.22348610\n",
            "iteration:  87 loss: 0.40180096\n",
            "iteration:  88 loss: 0.10507323\n",
            "iteration:  89 loss: 0.15482220\n",
            "iteration:  90 loss: 0.45823860\n",
            "iteration:  91 loss: 0.22497582\n",
            "iteration:  92 loss: 0.85018688\n",
            "iteration:  93 loss: 0.49629331\n",
            "iteration:  94 loss: 0.15887097\n",
            "iteration:  95 loss: 0.29413387\n",
            "iteration:  96 loss: 0.12229639\n",
            "iteration:  97 loss: 0.18036836\n",
            "iteration:  98 loss: 0.19566780\n",
            "iteration:  99 loss: 0.96207267\n",
            "iteration: 100 loss: 0.33968019\n",
            "iteration: 101 loss: 0.31689754\n",
            "iteration: 102 loss: 0.12196958\n",
            "iteration: 103 loss: 0.18987995\n",
            "iteration: 104 loss: 0.64713866\n",
            "iteration: 105 loss: 0.66790509\n",
            "iteration: 106 loss: 0.17400181\n",
            "iteration: 107 loss: 0.19020198\n",
            "iteration: 108 loss: 0.25028354\n",
            "iteration: 109 loss: 0.30887443\n",
            "iteration: 110 loss: 0.12949155\n",
            "iteration: 111 loss: 0.09869692\n",
            "iteration: 112 loss: 0.25732696\n",
            "iteration: 113 loss: 0.11251459\n",
            "iteration: 114 loss: 0.93436933\n",
            "iteration: 115 loss: 0.23996684\n",
            "iteration: 116 loss: 0.33347476\n",
            "iteration: 117 loss: 0.42932463\n",
            "iteration: 118 loss: 0.13568929\n",
            "iteration: 119 loss: 0.15771291\n",
            "iteration: 120 loss: 0.23620915\n",
            "iteration: 121 loss: 0.85066825\n",
            "iteration: 122 loss: 0.13998163\n",
            "iteration: 123 loss: 0.20624039\n",
            "iteration: 124 loss: 0.13522603\n",
            "iteration: 125 loss: 0.13425757\n",
            "iteration: 126 loss: 0.64662981\n",
            "iteration: 127 loss: 0.51010764\n",
            "iteration: 128 loss: 0.15653463\n",
            "iteration: 129 loss: 0.59487766\n",
            "iteration: 130 loss: 0.16191714\n",
            "iteration: 131 loss: 0.11738039\n",
            "iteration: 132 loss: 0.50208783\n",
            "iteration: 133 loss: 0.17215271\n",
            "iteration: 134 loss: 0.33135569\n",
            "iteration: 135 loss: 0.41987756\n",
            "iteration: 136 loss: 0.59615034\n",
            "iteration: 137 loss: 1.21848071\n",
            "iteration: 138 loss: 0.29389027\n",
            "iteration: 139 loss: 0.33089486\n",
            "iteration: 140 loss: 0.29848045\n",
            "iteration: 141 loss: 0.18247853\n",
            "iteration: 142 loss: 0.31255987\n",
            "iteration: 143 loss: 0.48660907\n",
            "iteration: 144 loss: 0.77617055\n",
            "iteration: 145 loss: 0.15990207\n",
            "iteration: 146 loss: 1.21628392\n",
            "iteration: 147 loss: 0.14017795\n",
            "iteration: 148 loss: 0.56554002\n",
            "iteration: 149 loss: 0.24381219\n",
            "iteration: 150 loss: 0.18353114\n",
            "iteration: 151 loss: 0.19399145\n",
            "iteration: 152 loss: 0.23256680\n",
            "iteration: 153 loss: 0.99441069\n",
            "iteration: 154 loss: 0.10066693\n",
            "iteration: 155 loss: 0.35941085\n",
            "iteration: 156 loss: 0.50250584\n",
            "iteration: 157 loss: 0.10359000\n",
            "iteration: 158 loss: 0.26948047\n",
            "iteration: 159 loss: 0.21845102\n",
            "iteration: 160 loss: 0.14463210\n",
            "iteration: 161 loss: 0.95670825\n",
            "iteration: 162 loss: 0.31227934\n",
            "iteration: 163 loss: 0.13079372\n",
            "iteration: 164 loss: 1.70746708\n",
            "iteration: 165 loss: 0.25486249\n",
            "iteration: 166 loss: 0.21125813\n",
            "iteration: 167 loss: 0.42572802\n",
            "iteration: 168 loss: 0.14838047\n",
            "iteration: 169 loss: 0.23074678\n",
            "iteration: 170 loss: 0.15368289\n",
            "iteration: 171 loss: 0.30192924\n",
            "iteration: 172 loss: 0.33610719\n",
            "iteration: 173 loss: 0.18992625\n",
            "iteration: 174 loss: 0.98369586\n",
            "iteration: 175 loss: 0.16339467\n",
            "iteration: 176 loss: 0.20653006\n",
            "iteration: 177 loss: 0.26134411\n",
            "iteration: 178 loss: 0.21213110\n",
            "iteration: 179 loss: 0.16561453\n",
            "iteration: 180 loss: 0.77608067\n",
            "iteration: 181 loss: 0.81968099\n",
            "iteration: 182 loss: 0.18675564\n",
            "iteration: 183 loss: 0.43637243\n",
            "iteration: 184 loss: 0.49018872\n",
            "iteration: 185 loss: 0.58905971\n",
            "iteration: 186 loss: 0.16731289\n",
            "iteration: 187 loss: 0.26631013\n",
            "iteration: 188 loss: 0.39555517\n",
            "iteration: 189 loss: 0.56620318\n",
            "iteration: 190 loss: 0.28366876\n",
            "iteration: 191 loss: 0.68334472\n",
            "iteration: 192 loss: 0.20415354\n",
            "iteration: 193 loss: 0.13354711\n",
            "iteration: 194 loss: 0.21106334\n",
            "iteration: 195 loss: 0.17981875\n",
            "iteration: 196 loss: 0.25341073\n",
            "iteration: 197 loss: 0.48671243\n",
            "iteration: 198 loss: 0.21093467\n",
            "iteration: 199 loss: 0.23531073\n",
            "epoch:  79 mean loss training: 0.37489480\n",
            "epoch:  79 mean loss validation: 1.25844872\n",
            "iteration:   0 loss: 0.53897703\n",
            "iteration:   1 loss: 0.21052937\n",
            "iteration:   2 loss: 0.64082730\n",
            "iteration:   3 loss: 0.51744896\n",
            "iteration:   4 loss: 0.14270574\n",
            "iteration:   5 loss: 0.73455822\n",
            "iteration:   6 loss: 0.33386567\n",
            "iteration:   7 loss: 0.21794306\n",
            "iteration:   8 loss: 0.18310064\n",
            "iteration:   9 loss: 0.61954296\n",
            "iteration:  10 loss: 0.19137348\n",
            "iteration:  11 loss: 0.19917145\n",
            "iteration:  12 loss: 0.16346385\n",
            "iteration:  13 loss: 0.14302373\n",
            "iteration:  14 loss: 0.96072638\n",
            "iteration:  15 loss: 0.12011171\n",
            "iteration:  16 loss: 0.84317684\n",
            "iteration:  17 loss: 0.23193043\n",
            "iteration:  18 loss: 0.31615207\n",
            "iteration:  19 loss: 0.12879704\n",
            "iteration:  20 loss: 0.15424055\n",
            "iteration:  21 loss: 0.13368219\n",
            "iteration:  22 loss: 0.36307463\n",
            "iteration:  23 loss: 0.10688676\n",
            "iteration:  24 loss: 0.19118470\n",
            "iteration:  25 loss: 0.27893490\n",
            "iteration:  26 loss: 0.27982813\n",
            "iteration:  27 loss: 0.12334503\n",
            "iteration:  28 loss: 0.38469690\n",
            "iteration:  29 loss: 0.32678020\n",
            "iteration:  30 loss: 0.57420456\n",
            "iteration:  31 loss: 0.33306918\n",
            "iteration:  32 loss: 0.29558420\n",
            "iteration:  33 loss: 0.27131170\n",
            "iteration:  34 loss: 0.27442586\n",
            "iteration:  35 loss: 0.41927165\n",
            "iteration:  36 loss: 0.27212825\n",
            "iteration:  37 loss: 0.26362777\n",
            "iteration:  38 loss: 0.15014234\n",
            "iteration:  39 loss: 0.38336241\n",
            "iteration:  40 loss: 0.27049616\n",
            "iteration:  41 loss: 0.13723917\n",
            "iteration:  42 loss: 1.00997043\n",
            "iteration:  43 loss: 0.26224780\n",
            "iteration:  44 loss: 1.09695959\n",
            "iteration:  45 loss: 0.21414451\n",
            "iteration:  46 loss: 0.20847201\n",
            "iteration:  47 loss: 0.58695024\n",
            "iteration:  48 loss: 0.57155955\n",
            "iteration:  49 loss: 0.27567807\n",
            "iteration:  50 loss: 0.21134119\n",
            "iteration:  51 loss: 0.31424153\n",
            "iteration:  52 loss: 0.16188462\n",
            "iteration:  53 loss: 0.15124160\n",
            "iteration:  54 loss: 0.40152705\n",
            "iteration:  55 loss: 0.19112317\n",
            "iteration:  56 loss: 0.17997405\n",
            "iteration:  57 loss: 0.56773132\n",
            "iteration:  58 loss: 0.43795297\n",
            "iteration:  59 loss: 0.96174765\n",
            "iteration:  60 loss: 0.62888253\n",
            "iteration:  61 loss: 0.31523818\n",
            "iteration:  62 loss: 0.28170598\n",
            "iteration:  63 loss: 0.08496291\n",
            "iteration:  64 loss: 0.63777965\n",
            "iteration:  65 loss: 0.73702037\n",
            "iteration:  66 loss: 0.14949709\n",
            "iteration:  67 loss: 0.23800792\n",
            "iteration:  68 loss: 0.28038949\n",
            "iteration:  69 loss: 0.22861254\n",
            "iteration:  70 loss: 0.30161995\n",
            "iteration:  71 loss: 0.12355841\n",
            "iteration:  72 loss: 0.28631991\n",
            "iteration:  73 loss: 0.76396388\n",
            "iteration:  74 loss: 0.30982172\n",
            "iteration:  75 loss: 0.12282464\n",
            "iteration:  76 loss: 0.25733790\n",
            "iteration:  77 loss: 0.15490063\n",
            "iteration:  78 loss: 0.82866842\n",
            "iteration:  79 loss: 0.25002062\n",
            "iteration:  80 loss: 0.13932127\n",
            "iteration:  81 loss: 0.12113632\n",
            "iteration:  82 loss: 0.17122225\n",
            "iteration:  83 loss: 0.18304594\n",
            "iteration:  84 loss: 0.71623111\n",
            "iteration:  85 loss: 0.13310412\n",
            "iteration:  86 loss: 0.34070799\n",
            "iteration:  87 loss: 0.08422168\n",
            "iteration:  88 loss: 0.33629897\n",
            "iteration:  89 loss: 0.10083127\n",
            "iteration:  90 loss: 0.11617377\n",
            "iteration:  91 loss: 0.35897750\n",
            "iteration:  92 loss: 0.16830845\n",
            "iteration:  93 loss: 0.14118209\n",
            "iteration:  94 loss: 0.22134250\n",
            "iteration:  95 loss: 1.28479981\n",
            "iteration:  96 loss: 0.72063476\n",
            "iteration:  97 loss: 0.14677186\n",
            "iteration:  98 loss: 0.12654354\n",
            "iteration:  99 loss: 0.31239223\n",
            "iteration: 100 loss: 0.39092276\n",
            "iteration: 101 loss: 0.74436659\n",
            "iteration: 102 loss: 0.15386121\n",
            "iteration: 103 loss: 0.10950474\n",
            "iteration: 104 loss: 0.25570998\n",
            "iteration: 105 loss: 0.12720113\n",
            "iteration: 106 loss: 0.27280357\n",
            "iteration: 107 loss: 1.76105690\n",
            "iteration: 108 loss: 0.20142066\n",
            "iteration: 109 loss: 0.12146477\n",
            "iteration: 110 loss: 0.53591275\n",
            "iteration: 111 loss: 0.28129119\n",
            "iteration: 112 loss: 0.25145006\n",
            "iteration: 113 loss: 0.30458927\n",
            "iteration: 114 loss: 0.08891353\n",
            "iteration: 115 loss: 0.14516586\n",
            "iteration: 116 loss: 0.22895783\n",
            "iteration: 117 loss: 0.11700615\n",
            "iteration: 118 loss: 0.51946187\n",
            "iteration: 119 loss: 0.58912295\n",
            "iteration: 120 loss: 0.26737413\n",
            "iteration: 121 loss: 0.39668161\n",
            "iteration: 122 loss: 0.91398728\n",
            "iteration: 123 loss: 0.91329050\n",
            "iteration: 124 loss: 0.21462223\n",
            "iteration: 125 loss: 0.30977458\n",
            "iteration: 126 loss: 0.23835850\n",
            "iteration: 127 loss: 0.37283057\n",
            "iteration: 128 loss: 0.18874966\n",
            "iteration: 129 loss: 0.19947940\n",
            "iteration: 130 loss: 0.37492281\n",
            "iteration: 131 loss: 0.19718832\n",
            "iteration: 132 loss: 0.19647816\n",
            "iteration: 133 loss: 0.53322560\n",
            "iteration: 134 loss: 0.70860660\n",
            "iteration: 135 loss: 1.07544506\n",
            "iteration: 136 loss: 0.18989684\n",
            "iteration: 137 loss: 0.22274415\n",
            "iteration: 138 loss: 0.18667048\n",
            "iteration: 139 loss: 0.28071108\n",
            "iteration: 140 loss: 0.47097450\n",
            "iteration: 141 loss: 0.84058887\n",
            "iteration: 142 loss: 0.99952376\n",
            "iteration: 143 loss: 0.43490666\n",
            "iteration: 144 loss: 0.68141288\n",
            "iteration: 145 loss: 0.10905506\n",
            "iteration: 146 loss: 0.18292920\n",
            "iteration: 147 loss: 0.22315954\n",
            "iteration: 148 loss: 0.43461686\n",
            "iteration: 149 loss: 0.19846722\n",
            "iteration: 150 loss: 0.23441204\n",
            "iteration: 151 loss: 0.53520966\n",
            "iteration: 152 loss: 0.28503782\n",
            "iteration: 153 loss: 0.18153815\n",
            "iteration: 154 loss: 0.13279462\n",
            "iteration: 155 loss: 0.39960605\n",
            "iteration: 156 loss: 0.16744213\n",
            "iteration: 157 loss: 0.13145423\n",
            "iteration: 158 loss: 0.14275837\n",
            "iteration: 159 loss: 0.16744457\n",
            "iteration: 160 loss: 0.28497502\n",
            "iteration: 161 loss: 0.17819230\n",
            "iteration: 162 loss: 1.19726217\n",
            "iteration: 163 loss: 0.57483387\n",
            "iteration: 164 loss: 0.42854971\n",
            "iteration: 165 loss: 0.36763376\n",
            "iteration: 166 loss: 0.17089200\n",
            "iteration: 167 loss: 0.37695801\n",
            "iteration: 168 loss: 0.15506425\n",
            "iteration: 169 loss: 0.44473666\n",
            "iteration: 170 loss: 0.12011497\n",
            "iteration: 171 loss: 0.72422552\n",
            "iteration: 172 loss: 0.86564261\n",
            "iteration: 173 loss: 0.50094742\n",
            "iteration: 174 loss: 0.16913077\n",
            "iteration: 175 loss: 0.25112945\n",
            "iteration: 176 loss: 0.23157007\n",
            "iteration: 177 loss: 0.11790754\n",
            "iteration: 178 loss: 0.45675030\n",
            "iteration: 179 loss: 0.38524359\n",
            "iteration: 180 loss: 0.24010755\n",
            "iteration: 181 loss: 1.07068026\n",
            "iteration: 182 loss: 0.38552687\n",
            "iteration: 183 loss: 0.10305811\n",
            "iteration: 184 loss: 1.15794170\n",
            "iteration: 185 loss: 0.14277942\n",
            "iteration: 186 loss: 0.11597645\n",
            "iteration: 187 loss: 0.21573964\n",
            "iteration: 188 loss: 0.40352067\n",
            "iteration: 189 loss: 0.14398748\n",
            "iteration: 190 loss: 0.69882345\n",
            "iteration: 191 loss: 0.10233721\n",
            "iteration: 192 loss: 0.30498031\n",
            "iteration: 193 loss: 0.75601310\n",
            "iteration: 194 loss: 0.13787329\n",
            "iteration: 195 loss: 0.84879619\n",
            "iteration: 196 loss: 0.44641265\n",
            "iteration: 197 loss: 0.35036746\n",
            "iteration: 198 loss: 0.15946719\n",
            "iteration: 199 loss: 0.37864220\n",
            "epoch:  80 mean loss training: 0.36216709\n",
            "epoch:  80 mean loss validation: 1.24681711\n",
            "iteration:   0 loss: 0.69325298\n",
            "iteration:   1 loss: 0.32591915\n",
            "iteration:   2 loss: 0.31410420\n",
            "iteration:   3 loss: 0.09871368\n",
            "iteration:   4 loss: 0.34169483\n",
            "iteration:   5 loss: 0.91658539\n",
            "iteration:   6 loss: 0.31000447\n",
            "iteration:   7 loss: 0.11339408\n",
            "iteration:   8 loss: 0.16645971\n",
            "iteration:   9 loss: 0.47022966\n",
            "iteration:  10 loss: 0.29799828\n",
            "iteration:  11 loss: 0.12373077\n",
            "iteration:  12 loss: 0.10268065\n",
            "iteration:  13 loss: 0.46803486\n",
            "iteration:  14 loss: 0.34264541\n",
            "iteration:  15 loss: 0.32984594\n",
            "iteration:  16 loss: 1.12562370\n",
            "iteration:  17 loss: 0.16779178\n",
            "iteration:  18 loss: 0.19413067\n",
            "iteration:  19 loss: 0.65306830\n",
            "iteration:  20 loss: 0.21588990\n",
            "iteration:  21 loss: 0.31515852\n",
            "iteration:  22 loss: 1.76115847\n",
            "iteration:  23 loss: 0.09423606\n",
            "iteration:  24 loss: 0.81489217\n",
            "iteration:  25 loss: 0.77732033\n",
            "iteration:  26 loss: 0.11958519\n",
            "iteration:  27 loss: 0.13786176\n",
            "iteration:  28 loss: 0.28660360\n",
            "iteration:  29 loss: 0.17558065\n",
            "iteration:  30 loss: 1.17154300\n",
            "iteration:  31 loss: 0.90105277\n",
            "iteration:  32 loss: 0.49483439\n",
            "iteration:  33 loss: 1.35774887\n",
            "iteration:  34 loss: 0.45536390\n",
            "iteration:  35 loss: 0.09127051\n",
            "iteration:  36 loss: 0.35180256\n",
            "iteration:  37 loss: 0.33133432\n",
            "iteration:  38 loss: 0.23314892\n",
            "iteration:  39 loss: 0.30499125\n",
            "iteration:  40 loss: 0.52541530\n",
            "iteration:  41 loss: 0.24204431\n",
            "iteration:  42 loss: 0.47739050\n",
            "iteration:  43 loss: 0.15400997\n",
            "iteration:  44 loss: 0.78057605\n",
            "iteration:  45 loss: 0.68320876\n",
            "iteration:  46 loss: 0.65336818\n",
            "iteration:  47 loss: 0.18300448\n",
            "iteration:  48 loss: 0.47643563\n",
            "iteration:  49 loss: 0.10337586\n",
            "iteration:  50 loss: 0.22276381\n",
            "iteration:  51 loss: 0.12221849\n",
            "iteration:  52 loss: 0.21735075\n",
            "iteration:  53 loss: 1.17139053\n",
            "iteration:  54 loss: 0.26585254\n",
            "iteration:  55 loss: 0.39340413\n",
            "iteration:  56 loss: 0.36955851\n",
            "iteration:  57 loss: 0.25394684\n",
            "iteration:  58 loss: 0.89700162\n",
            "iteration:  59 loss: 0.09518512\n",
            "iteration:  60 loss: 0.34482920\n",
            "iteration:  61 loss: 0.09846602\n",
            "iteration:  62 loss: 0.23601443\n",
            "iteration:  63 loss: 0.30866811\n",
            "iteration:  64 loss: 0.09698670\n",
            "iteration:  65 loss: 0.13344309\n",
            "iteration:  66 loss: 0.34509763\n",
            "iteration:  67 loss: 0.10877402\n",
            "iteration:  68 loss: 0.35405716\n",
            "iteration:  69 loss: 0.45820251\n",
            "iteration:  70 loss: 0.19770113\n",
            "iteration:  71 loss: 0.75706351\n",
            "iteration:  72 loss: 0.95891106\n",
            "iteration:  73 loss: 0.84567958\n",
            "iteration:  74 loss: 0.10283154\n",
            "iteration:  75 loss: 0.31277204\n",
            "iteration:  76 loss: 0.10572261\n",
            "iteration:  77 loss: 0.98743683\n",
            "iteration:  78 loss: 0.11411192\n",
            "iteration:  79 loss: 0.28382927\n",
            "iteration:  80 loss: 0.67725784\n",
            "iteration:  81 loss: 0.32860079\n",
            "iteration:  82 loss: 0.71440971\n",
            "iteration:  83 loss: 0.55791402\n",
            "iteration:  84 loss: 0.51459187\n",
            "iteration:  85 loss: 0.20502132\n",
            "iteration:  86 loss: 0.34169662\n",
            "iteration:  87 loss: 0.12592697\n",
            "iteration:  88 loss: 0.44228491\n",
            "iteration:  89 loss: 0.22395220\n",
            "iteration:  90 loss: 0.11371018\n",
            "iteration:  91 loss: 0.37678367\n",
            "iteration:  92 loss: 0.17321636\n",
            "iteration:  93 loss: 0.14755528\n",
            "iteration:  94 loss: 0.90690446\n",
            "iteration:  95 loss: 0.34300011\n",
            "iteration:  96 loss: 0.11391155\n",
            "iteration:  97 loss: 0.19286188\n",
            "iteration:  98 loss: 0.10316989\n",
            "iteration:  99 loss: 0.27752745\n",
            "iteration: 100 loss: 0.68647945\n",
            "iteration: 101 loss: 0.40189543\n",
            "iteration: 102 loss: 0.25044617\n",
            "iteration: 103 loss: 0.14764811\n",
            "iteration: 104 loss: 0.68604839\n",
            "iteration: 105 loss: 0.63600314\n",
            "iteration: 106 loss: 0.48875111\n",
            "iteration: 107 loss: 0.59435934\n",
            "iteration: 108 loss: 0.67751634\n",
            "iteration: 109 loss: 0.52007830\n",
            "iteration: 110 loss: 0.42188787\n",
            "iteration: 111 loss: 0.09996217\n",
            "iteration: 112 loss: 0.17789765\n",
            "iteration: 113 loss: 0.12591740\n",
            "iteration: 114 loss: 0.23318604\n",
            "iteration: 115 loss: 0.12525055\n",
            "iteration: 116 loss: 0.64491820\n",
            "iteration: 117 loss: 0.57686114\n",
            "iteration: 118 loss: 0.80430770\n",
            "iteration: 119 loss: 0.13528486\n",
            "iteration: 120 loss: 0.10961962\n",
            "iteration: 121 loss: 0.19278483\n",
            "iteration: 122 loss: 0.68368143\n",
            "iteration: 123 loss: 0.72069556\n",
            "iteration: 124 loss: 0.19429101\n",
            "iteration: 125 loss: 0.44574517\n",
            "iteration: 126 loss: 0.09385536\n",
            "iteration: 127 loss: 1.14271832\n",
            "iteration: 128 loss: 0.28734991\n",
            "iteration: 129 loss: 0.19818772\n",
            "iteration: 130 loss: 0.79241347\n",
            "iteration: 131 loss: 0.12127969\n",
            "iteration: 132 loss: 0.09302300\n",
            "iteration: 133 loss: 0.32195956\n",
            "iteration: 134 loss: 0.26784357\n",
            "iteration: 135 loss: 0.53900635\n",
            "iteration: 136 loss: 0.14682272\n",
            "iteration: 137 loss: 0.19272499\n",
            "iteration: 138 loss: 0.27095157\n",
            "iteration: 139 loss: 0.70831770\n",
            "iteration: 140 loss: 0.48601517\n",
            "iteration: 141 loss: 0.18662089\n",
            "iteration: 142 loss: 1.16104341\n",
            "iteration: 143 loss: 0.10829107\n",
            "iteration: 144 loss: 0.09929238\n",
            "iteration: 145 loss: 0.25920424\n",
            "iteration: 146 loss: 0.27150843\n",
            "iteration: 147 loss: 0.15651537\n",
            "iteration: 148 loss: 0.15502729\n",
            "iteration: 149 loss: 0.34214705\n",
            "iteration: 150 loss: 0.31074366\n",
            "iteration: 151 loss: 0.28360030\n",
            "iteration: 152 loss: 0.69534129\n",
            "iteration: 153 loss: 0.27957475\n",
            "iteration: 154 loss: 0.38954201\n",
            "iteration: 155 loss: 0.49420923\n",
            "iteration: 156 loss: 0.25466904\n",
            "iteration: 157 loss: 0.19675371\n",
            "iteration: 158 loss: 1.00021374\n",
            "iteration: 159 loss: 0.54312277\n",
            "iteration: 160 loss: 0.36288410\n",
            "iteration: 161 loss: 0.12598246\n",
            "iteration: 162 loss: 0.27413392\n",
            "iteration: 163 loss: 1.23673904\n",
            "iteration: 164 loss: 0.30222884\n",
            "iteration: 165 loss: 0.20392740\n",
            "iteration: 166 loss: 1.03800154\n",
            "iteration: 167 loss: 0.24288073\n",
            "iteration: 168 loss: 0.31037453\n",
            "iteration: 169 loss: 0.26319134\n",
            "iteration: 170 loss: 0.15218124\n",
            "iteration: 171 loss: 0.81504327\n",
            "iteration: 172 loss: 0.10598192\n",
            "iteration: 173 loss: 0.29388118\n",
            "iteration: 174 loss: 0.71534890\n",
            "iteration: 175 loss: 0.43551826\n",
            "iteration: 176 loss: 0.13114162\n",
            "iteration: 177 loss: 0.68050730\n",
            "iteration: 178 loss: 0.20064685\n",
            "iteration: 179 loss: 0.17085382\n",
            "iteration: 180 loss: 0.24958554\n",
            "iteration: 181 loss: 0.13899453\n",
            "iteration: 182 loss: 0.98741752\n",
            "iteration: 183 loss: 0.38917702\n",
            "iteration: 184 loss: 0.24614856\n",
            "iteration: 185 loss: 0.79944968\n",
            "iteration: 186 loss: 0.09843796\n",
            "iteration: 187 loss: 0.54913151\n",
            "iteration: 188 loss: 0.16168730\n",
            "iteration: 189 loss: 0.16196176\n",
            "iteration: 190 loss: 0.42453006\n",
            "iteration: 191 loss: 0.18613905\n",
            "iteration: 192 loss: 0.06981435\n",
            "iteration: 193 loss: 0.35861835\n",
            "iteration: 194 loss: 0.44663233\n",
            "iteration: 195 loss: 0.46143466\n",
            "iteration: 196 loss: 0.35238189\n",
            "iteration: 197 loss: 0.28544086\n",
            "iteration: 198 loss: 0.10029984\n",
            "iteration: 199 loss: 0.53775603\n",
            "epoch:  81 mean loss training: 0.39481789\n",
            "epoch:  81 mean loss validation: 1.26226950\n",
            "iteration:   0 loss: 0.30443090\n",
            "iteration:   1 loss: 0.18410492\n",
            "iteration:   2 loss: 0.89793551\n",
            "iteration:   3 loss: 0.33312151\n",
            "iteration:   4 loss: 0.11084159\n",
            "iteration:   5 loss: 0.26930231\n",
            "iteration:   6 loss: 1.46461844\n",
            "iteration:   7 loss: 0.17737505\n",
            "iteration:   8 loss: 0.74593574\n",
            "iteration:   9 loss: 0.14337760\n",
            "iteration:  10 loss: 0.33864850\n",
            "iteration:  11 loss: 0.11460142\n",
            "iteration:  12 loss: 0.23428565\n",
            "iteration:  13 loss: 0.44610506\n",
            "iteration:  14 loss: 0.20012370\n",
            "iteration:  15 loss: 0.40425611\n",
            "iteration:  16 loss: 0.07982248\n",
            "iteration:  17 loss: 0.25772941\n",
            "iteration:  18 loss: 0.17567235\n",
            "iteration:  19 loss: 0.09257789\n",
            "iteration:  20 loss: 0.10300456\n",
            "iteration:  21 loss: 0.13938519\n",
            "iteration:  22 loss: 0.12238487\n",
            "iteration:  23 loss: 0.23631504\n",
            "iteration:  24 loss: 0.61160642\n",
            "iteration:  25 loss: 0.30711225\n",
            "iteration:  26 loss: 0.35785812\n",
            "iteration:  27 loss: 0.25113240\n",
            "iteration:  28 loss: 0.45095700\n",
            "iteration:  29 loss: 0.09020127\n",
            "iteration:  30 loss: 0.13247646\n",
            "iteration:  31 loss: 1.01170313\n",
            "iteration:  32 loss: 0.19899881\n",
            "iteration:  33 loss: 0.18756834\n",
            "iteration:  34 loss: 1.61657417\n",
            "iteration:  35 loss: 0.26610151\n",
            "iteration:  36 loss: 0.50862068\n",
            "iteration:  37 loss: 0.24804170\n",
            "iteration:  38 loss: 0.32863426\n",
            "iteration:  39 loss: 0.36977461\n",
            "iteration:  40 loss: 0.12336977\n",
            "iteration:  41 loss: 0.63359141\n",
            "iteration:  42 loss: 0.12984221\n",
            "iteration:  43 loss: 0.48022684\n",
            "iteration:  44 loss: 0.79396367\n",
            "iteration:  45 loss: 0.29637778\n",
            "iteration:  46 loss: 0.99426168\n",
            "iteration:  47 loss: 0.39601862\n",
            "iteration:  48 loss: 0.25704592\n",
            "iteration:  49 loss: 0.57701075\n",
            "iteration:  50 loss: 0.11806181\n",
            "iteration:  51 loss: 1.44977498\n",
            "iteration:  52 loss: 0.26250008\n",
            "iteration:  53 loss: 0.23113565\n",
            "iteration:  54 loss: 0.15128261\n",
            "iteration:  55 loss: 0.76738989\n",
            "iteration:  56 loss: 0.95121771\n",
            "iteration:  57 loss: 0.11357768\n",
            "iteration:  58 loss: 0.31237799\n",
            "iteration:  59 loss: 0.71478182\n",
            "iteration:  60 loss: 1.05462611\n",
            "iteration:  61 loss: 0.11704708\n",
            "iteration:  62 loss: 0.48638454\n",
            "iteration:  63 loss: 0.73394144\n",
            "iteration:  64 loss: 0.73419416\n",
            "iteration:  65 loss: 0.12638983\n",
            "iteration:  66 loss: 0.24816814\n",
            "iteration:  67 loss: 0.70931125\n",
            "iteration:  68 loss: 0.39501619\n",
            "iteration:  69 loss: 0.10625207\n",
            "iteration:  70 loss: 0.63488418\n",
            "iteration:  71 loss: 0.09606299\n",
            "iteration:  72 loss: 0.35811466\n",
            "iteration:  73 loss: 0.28800577\n",
            "iteration:  74 loss: 1.22825539\n",
            "iteration:  75 loss: 0.67621744\n",
            "iteration:  76 loss: 0.93976718\n",
            "iteration:  77 loss: 0.28913993\n",
            "iteration:  78 loss: 0.10322018\n",
            "iteration:  79 loss: 0.12832732\n",
            "iteration:  80 loss: 1.02558684\n",
            "iteration:  81 loss: 0.18220459\n",
            "iteration:  82 loss: 0.73315674\n",
            "iteration:  83 loss: 0.19338255\n",
            "iteration:  84 loss: 0.15500310\n",
            "iteration:  85 loss: 0.19055903\n",
            "iteration:  86 loss: 0.29330489\n",
            "iteration:  87 loss: 0.70771861\n",
            "iteration:  88 loss: 0.92875558\n",
            "iteration:  89 loss: 0.72965384\n",
            "iteration:  90 loss: 0.31486127\n",
            "iteration:  91 loss: 0.09517515\n",
            "iteration:  92 loss: 0.62582153\n",
            "iteration:  93 loss: 0.38010606\n",
            "iteration:  94 loss: 0.17545368\n",
            "iteration:  95 loss: 0.14753237\n",
            "iteration:  96 loss: 0.21748307\n",
            "iteration:  97 loss: 0.14603007\n",
            "iteration:  98 loss: 0.19536217\n",
            "iteration:  99 loss: 0.16661231\n",
            "iteration: 100 loss: 1.20461607\n",
            "iteration: 101 loss: 0.64370579\n",
            "iteration: 102 loss: 0.21543074\n",
            "iteration: 103 loss: 0.42013538\n",
            "iteration: 104 loss: 0.11087073\n",
            "iteration: 105 loss: 0.27796188\n",
            "iteration: 106 loss: 0.20630750\n",
            "iteration: 107 loss: 0.33801320\n",
            "iteration: 108 loss: 0.74596572\n",
            "iteration: 109 loss: 0.57133633\n",
            "iteration: 110 loss: 0.16607371\n",
            "iteration: 111 loss: 0.21776316\n",
            "iteration: 112 loss: 0.14464186\n",
            "iteration: 113 loss: 0.23849156\n",
            "iteration: 114 loss: 0.13391504\n",
            "iteration: 115 loss: 0.62567878\n",
            "iteration: 116 loss: 0.60994631\n",
            "iteration: 117 loss: 0.33844498\n",
            "iteration: 118 loss: 0.26754004\n",
            "iteration: 119 loss: 0.20583923\n",
            "iteration: 120 loss: 0.22303700\n",
            "iteration: 121 loss: 0.38435853\n",
            "iteration: 122 loss: 0.08931293\n",
            "iteration: 123 loss: 0.18306407\n",
            "iteration: 124 loss: 0.78677583\n",
            "iteration: 125 loss: 0.69956690\n",
            "iteration: 126 loss: 0.25870934\n",
            "iteration: 127 loss: 0.55976731\n",
            "iteration: 128 loss: 0.12651379\n",
            "iteration: 129 loss: 0.94590282\n",
            "iteration: 130 loss: 0.11724512\n",
            "iteration: 131 loss: 0.23718716\n",
            "iteration: 132 loss: 0.18086278\n",
            "iteration: 133 loss: 0.18841580\n",
            "iteration: 134 loss: 0.24215952\n",
            "iteration: 135 loss: 0.82977104\n",
            "iteration: 136 loss: 0.11063501\n",
            "iteration: 137 loss: 0.31905842\n",
            "iteration: 138 loss: 0.13515712\n",
            "iteration: 139 loss: 0.11398895\n",
            "iteration: 140 loss: 0.17672056\n",
            "iteration: 141 loss: 0.75394672\n",
            "iteration: 142 loss: 0.32249844\n",
            "iteration: 143 loss: 0.39858818\n",
            "iteration: 144 loss: 0.35135287\n",
            "iteration: 145 loss: 0.27304894\n",
            "iteration: 146 loss: 0.47653401\n",
            "iteration: 147 loss: 0.44965446\n",
            "iteration: 148 loss: 0.23462638\n",
            "iteration: 149 loss: 0.25710595\n",
            "iteration: 150 loss: 0.62713230\n",
            "iteration: 151 loss: 0.21126823\n",
            "iteration: 152 loss: 0.21738279\n",
            "iteration: 153 loss: 0.12931262\n",
            "iteration: 154 loss: 0.18266062\n",
            "iteration: 155 loss: 0.16448808\n",
            "iteration: 156 loss: 0.22143012\n",
            "iteration: 157 loss: 0.63358217\n",
            "iteration: 158 loss: 0.86000764\n",
            "iteration: 159 loss: 0.12637317\n",
            "iteration: 160 loss: 0.21071677\n",
            "iteration: 161 loss: 0.29936746\n",
            "iteration: 162 loss: 0.22184446\n",
            "iteration: 163 loss: 0.24408340\n",
            "iteration: 164 loss: 0.14185296\n",
            "iteration: 165 loss: 0.41980720\n",
            "iteration: 166 loss: 0.36927703\n",
            "iteration: 167 loss: 0.18319632\n",
            "iteration: 168 loss: 0.20122088\n",
            "iteration: 169 loss: 0.13587411\n",
            "iteration: 170 loss: 0.42611414\n",
            "iteration: 171 loss: 0.12507255\n",
            "iteration: 172 loss: 0.47282299\n",
            "iteration: 173 loss: 0.10447719\n",
            "iteration: 174 loss: 0.71248794\n",
            "iteration: 175 loss: 0.22682700\n",
            "iteration: 176 loss: 0.11983524\n",
            "iteration: 177 loss: 0.09270439\n",
            "iteration: 178 loss: 0.13250506\n",
            "iteration: 179 loss: 0.57885355\n",
            "iteration: 180 loss: 0.74439919\n",
            "iteration: 181 loss: 0.17534682\n",
            "iteration: 182 loss: 0.12903492\n",
            "iteration: 183 loss: 0.20668057\n",
            "iteration: 184 loss: 0.21555063\n",
            "iteration: 185 loss: 0.42812496\n",
            "iteration: 186 loss: 0.39700741\n",
            "iteration: 187 loss: 0.23727477\n",
            "iteration: 188 loss: 0.34024704\n",
            "iteration: 189 loss: 0.12213029\n",
            "iteration: 190 loss: 0.16229916\n",
            "iteration: 191 loss: 0.41174269\n",
            "iteration: 192 loss: 0.21596061\n",
            "iteration: 193 loss: 0.61784559\n",
            "iteration: 194 loss: 0.20713905\n",
            "iteration: 195 loss: 0.20377108\n",
            "iteration: 196 loss: 0.10193717\n",
            "iteration: 197 loss: 0.67193651\n",
            "iteration: 198 loss: 0.35161361\n",
            "iteration: 199 loss: 0.65344465\n",
            "epoch:  82 mean loss training: 0.37409264\n",
            "epoch:  82 mean loss validation: 1.24074948\n",
            "iteration:   0 loss: 0.31157374\n",
            "iteration:   1 loss: 0.22517726\n",
            "iteration:   2 loss: 0.09822378\n",
            "iteration:   3 loss: 0.24356180\n",
            "iteration:   4 loss: 1.04762101\n",
            "iteration:   5 loss: 0.30305061\n",
            "iteration:   6 loss: 0.34319669\n",
            "iteration:   7 loss: 0.28311288\n",
            "iteration:   8 loss: 0.60139626\n",
            "iteration:   9 loss: 0.31232113\n",
            "iteration:  10 loss: 0.13932173\n",
            "iteration:  11 loss: 1.23717487\n",
            "iteration:  12 loss: 0.44810665\n",
            "iteration:  13 loss: 0.87673867\n",
            "iteration:  14 loss: 0.40570515\n",
            "iteration:  15 loss: 0.23028117\n",
            "iteration:  16 loss: 0.30676088\n",
            "iteration:  17 loss: 0.33433974\n",
            "iteration:  18 loss: 0.52946264\n",
            "iteration:  19 loss: 0.15861034\n",
            "iteration:  20 loss: 0.24384677\n",
            "iteration:  21 loss: 0.34918416\n",
            "iteration:  22 loss: 0.21736756\n",
            "iteration:  23 loss: 0.25301850\n",
            "iteration:  24 loss: 0.52814949\n",
            "iteration:  25 loss: 0.93720829\n",
            "iteration:  26 loss: 0.20126241\n",
            "iteration:  27 loss: 0.10349815\n",
            "iteration:  28 loss: 0.25397488\n",
            "iteration:  29 loss: 0.30608666\n",
            "iteration:  30 loss: 0.12322630\n",
            "iteration:  31 loss: 0.24048442\n",
            "iteration:  32 loss: 0.32394662\n",
            "iteration:  33 loss: 0.11019674\n",
            "iteration:  34 loss: 0.10460463\n",
            "iteration:  35 loss: 0.66483474\n",
            "iteration:  36 loss: 0.13883126\n",
            "iteration:  37 loss: 0.87545925\n",
            "iteration:  38 loss: 0.20492028\n",
            "iteration:  39 loss: 1.02933705\n",
            "iteration:  40 loss: 0.32550877\n",
            "iteration:  41 loss: 0.16743509\n",
            "iteration:  42 loss: 0.34358966\n",
            "iteration:  43 loss: 0.62281567\n",
            "iteration:  44 loss: 0.61789340\n",
            "iteration:  45 loss: 0.26877779\n",
            "iteration:  46 loss: 0.28277668\n",
            "iteration:  47 loss: 0.40541086\n",
            "iteration:  48 loss: 0.29740587\n",
            "iteration:  49 loss: 0.48103029\n",
            "iteration:  50 loss: 0.13967834\n",
            "iteration:  51 loss: 0.85238558\n",
            "iteration:  52 loss: 0.48038480\n",
            "iteration:  53 loss: 1.02719641\n",
            "iteration:  54 loss: 0.31455415\n",
            "iteration:  55 loss: 0.32930657\n",
            "iteration:  56 loss: 0.12251177\n",
            "iteration:  57 loss: 0.27465531\n",
            "iteration:  58 loss: 0.18357828\n",
            "iteration:  59 loss: 0.36839288\n",
            "iteration:  60 loss: 0.14609730\n",
            "iteration:  61 loss: 0.48729599\n",
            "iteration:  62 loss: 0.15346171\n",
            "iteration:  63 loss: 0.14405003\n",
            "iteration:  64 loss: 0.31660005\n",
            "iteration:  65 loss: 0.49775332\n",
            "iteration:  66 loss: 0.58795393\n",
            "iteration:  67 loss: 0.57480425\n",
            "iteration:  68 loss: 0.44749501\n",
            "iteration:  69 loss: 0.12709808\n",
            "iteration:  70 loss: 0.31808621\n",
            "iteration:  71 loss: 0.13465536\n",
            "iteration:  72 loss: 0.15758091\n",
            "iteration:  73 loss: 0.13010393\n",
            "iteration:  74 loss: 0.20594504\n",
            "iteration:  75 loss: 0.12840074\n",
            "iteration:  76 loss: 1.09878516\n",
            "iteration:  77 loss: 0.54692930\n",
            "iteration:  78 loss: 0.15707980\n",
            "iteration:  79 loss: 0.22102439\n",
            "iteration:  80 loss: 0.30573666\n",
            "iteration:  81 loss: 0.34954414\n",
            "iteration:  82 loss: 0.98066533\n",
            "iteration:  83 loss: 0.20902202\n",
            "iteration:  84 loss: 0.37184417\n",
            "iteration:  85 loss: 0.29946202\n",
            "iteration:  86 loss: 0.42154014\n",
            "iteration:  87 loss: 0.11826161\n",
            "iteration:  88 loss: 0.29832503\n",
            "iteration:  89 loss: 0.11183634\n",
            "iteration:  90 loss: 0.25840962\n",
            "iteration:  91 loss: 0.12342903\n",
            "iteration:  92 loss: 0.38960281\n",
            "iteration:  93 loss: 0.65262771\n",
            "iteration:  94 loss: 0.35639203\n",
            "iteration:  95 loss: 0.09588248\n",
            "iteration:  96 loss: 0.14350414\n",
            "iteration:  97 loss: 0.20673403\n",
            "iteration:  98 loss: 0.67872638\n",
            "iteration:  99 loss: 0.24843413\n",
            "iteration: 100 loss: 1.28516245\n",
            "iteration: 101 loss: 0.21300873\n",
            "iteration: 102 loss: 0.13956660\n",
            "iteration: 103 loss: 0.31505591\n",
            "iteration: 104 loss: 0.11459489\n",
            "iteration: 105 loss: 0.27569228\n",
            "iteration: 106 loss: 1.16299665\n",
            "iteration: 107 loss: 0.31420529\n",
            "iteration: 108 loss: 0.94559640\n",
            "iteration: 109 loss: 0.99899179\n",
            "iteration: 110 loss: 0.37763822\n",
            "iteration: 111 loss: 0.37627530\n",
            "iteration: 112 loss: 0.36270469\n",
            "iteration: 113 loss: 0.12906809\n",
            "iteration: 114 loss: 0.15182266\n",
            "iteration: 115 loss: 0.23828658\n",
            "iteration: 116 loss: 0.13666239\n",
            "iteration: 117 loss: 0.91841805\n",
            "iteration: 118 loss: 0.17418937\n",
            "iteration: 119 loss: 0.17060964\n",
            "iteration: 120 loss: 0.33123428\n",
            "iteration: 121 loss: 0.42072731\n",
            "iteration: 122 loss: 0.40730458\n",
            "iteration: 123 loss: 0.41107795\n",
            "iteration: 124 loss: 0.25483799\n",
            "iteration: 125 loss: 0.20298895\n",
            "iteration: 126 loss: 0.11389095\n",
            "iteration: 127 loss: 0.33116758\n",
            "iteration: 128 loss: 0.08381541\n",
            "iteration: 129 loss: 0.15842888\n",
            "iteration: 130 loss: 1.02213323\n",
            "iteration: 131 loss: 0.16544294\n",
            "iteration: 132 loss: 0.49563491\n",
            "iteration: 133 loss: 0.21476361\n",
            "iteration: 134 loss: 0.89027566\n",
            "iteration: 135 loss: 0.68934429\n",
            "iteration: 136 loss: 0.21384460\n",
            "iteration: 137 loss: 0.31182590\n",
            "iteration: 138 loss: 0.28946301\n",
            "iteration: 139 loss: 0.76904845\n",
            "iteration: 140 loss: 1.38260245\n",
            "iteration: 141 loss: 0.28823254\n",
            "iteration: 142 loss: 0.79154170\n",
            "iteration: 143 loss: 0.13003883\n",
            "iteration: 144 loss: 0.31876445\n",
            "iteration: 145 loss: 0.69873977\n",
            "iteration: 146 loss: 0.12908022\n",
            "iteration: 147 loss: 0.42261028\n",
            "iteration: 148 loss: 0.22861484\n",
            "iteration: 149 loss: 0.24254477\n",
            "iteration: 150 loss: 0.51669210\n",
            "iteration: 151 loss: 0.46331358\n",
            "iteration: 152 loss: 0.67488217\n",
            "iteration: 153 loss: 1.26298583\n",
            "iteration: 154 loss: 0.45257756\n",
            "iteration: 155 loss: 0.56812167\n",
            "iteration: 156 loss: 1.05859888\n",
            "iteration: 157 loss: 0.25756231\n",
            "iteration: 158 loss: 0.70119858\n",
            "iteration: 159 loss: 0.68999940\n",
            "iteration: 160 loss: 0.21439719\n",
            "iteration: 161 loss: 0.26803011\n",
            "iteration: 162 loss: 0.48590428\n",
            "iteration: 163 loss: 0.36585799\n",
            "iteration: 164 loss: 0.18082735\n",
            "iteration: 165 loss: 0.61223108\n",
            "iteration: 166 loss: 0.47130811\n",
            "iteration: 167 loss: 0.21785140\n",
            "iteration: 168 loss: 0.53219396\n",
            "iteration: 169 loss: 0.45515198\n",
            "iteration: 170 loss: 0.12657259\n",
            "iteration: 171 loss: 0.30507872\n",
            "iteration: 172 loss: 0.15811744\n",
            "iteration: 173 loss: 0.16045704\n",
            "iteration: 174 loss: 0.39812922\n",
            "iteration: 175 loss: 0.24116620\n",
            "iteration: 176 loss: 0.40977436\n",
            "iteration: 177 loss: 0.15317538\n",
            "iteration: 178 loss: 0.79410321\n",
            "iteration: 179 loss: 0.27251321\n",
            "iteration: 180 loss: 0.98504263\n",
            "iteration: 181 loss: 0.20354189\n",
            "iteration: 182 loss: 1.04819500\n",
            "iteration: 183 loss: 0.15295649\n",
            "iteration: 184 loss: 0.13209659\n",
            "iteration: 185 loss: 0.09476234\n",
            "iteration: 186 loss: 0.49664068\n",
            "iteration: 187 loss: 0.28905410\n",
            "iteration: 188 loss: 0.39584345\n",
            "iteration: 189 loss: 0.12945552\n",
            "iteration: 190 loss: 0.17620459\n",
            "iteration: 191 loss: 0.36646727\n",
            "iteration: 192 loss: 0.47836110\n",
            "iteration: 193 loss: 0.25586414\n",
            "iteration: 194 loss: 0.14909969\n",
            "iteration: 195 loss: 0.76375133\n",
            "iteration: 196 loss: 0.15339214\n",
            "iteration: 197 loss: 0.29322150\n",
            "iteration: 198 loss: 0.09407495\n",
            "iteration: 199 loss: 0.09231452\n",
            "epoch:  83 mean loss training: 0.39090660\n",
            "epoch:  83 mean loss validation: 1.23882031\n",
            "iteration:   0 loss: 0.24330238\n",
            "iteration:   1 loss: 0.27179748\n",
            "iteration:   2 loss: 0.43270844\n",
            "iteration:   3 loss: 0.13281177\n",
            "iteration:   4 loss: 0.74847525\n",
            "iteration:   5 loss: 0.11151610\n",
            "iteration:   6 loss: 0.15718582\n",
            "iteration:   7 loss: 0.20815825\n",
            "iteration:   8 loss: 0.32152429\n",
            "iteration:   9 loss: 0.31263652\n",
            "iteration:  10 loss: 0.70452625\n",
            "iteration:  11 loss: 1.00517988\n",
            "iteration:  12 loss: 0.18196273\n",
            "iteration:  13 loss: 0.28336522\n",
            "iteration:  14 loss: 0.26588303\n",
            "iteration:  15 loss: 1.15901351\n",
            "iteration:  16 loss: 0.18144400\n",
            "iteration:  17 loss: 0.45935178\n",
            "iteration:  18 loss: 0.42165583\n",
            "iteration:  19 loss: 0.16861181\n",
            "iteration:  20 loss: 0.48630020\n",
            "iteration:  21 loss: 0.45530781\n",
            "iteration:  22 loss: 0.51076496\n",
            "iteration:  23 loss: 0.33045912\n",
            "iteration:  24 loss: 0.20459065\n",
            "iteration:  25 loss: 0.93482131\n",
            "iteration:  26 loss: 0.10733593\n",
            "iteration:  27 loss: 0.27683604\n",
            "iteration:  28 loss: 0.13431351\n",
            "iteration:  29 loss: 0.20304793\n",
            "iteration:  30 loss: 0.18835205\n",
            "iteration:  31 loss: 0.29993904\n",
            "iteration:  32 loss: 0.57257921\n",
            "iteration:  33 loss: 0.26467204\n",
            "iteration:  34 loss: 0.30679634\n",
            "iteration:  35 loss: 0.33583796\n",
            "iteration:  36 loss: 0.47330278\n",
            "iteration:  37 loss: 0.18189415\n",
            "iteration:  38 loss: 0.35277030\n",
            "iteration:  39 loss: 0.11385178\n",
            "iteration:  40 loss: 0.80805421\n",
            "iteration:  41 loss: 0.21797518\n",
            "iteration:  42 loss: 0.89543492\n",
            "iteration:  43 loss: 0.32454786\n",
            "iteration:  44 loss: 0.22471678\n",
            "iteration:  45 loss: 0.19722071\n",
            "iteration:  46 loss: 0.21206968\n",
            "iteration:  47 loss: 0.29723692\n",
            "iteration:  48 loss: 0.13514630\n",
            "iteration:  49 loss: 0.40539324\n",
            "iteration:  50 loss: 0.33798987\n",
            "iteration:  51 loss: 0.09101076\n",
            "iteration:  52 loss: 0.09767973\n",
            "iteration:  53 loss: 0.99587870\n",
            "iteration:  54 loss: 0.20560761\n",
            "iteration:  55 loss: 0.46873653\n",
            "iteration:  56 loss: 0.11854214\n",
            "iteration:  57 loss: 0.49742663\n",
            "iteration:  58 loss: 0.15527505\n",
            "iteration:  59 loss: 0.63510382\n",
            "iteration:  60 loss: 0.75622863\n",
            "iteration:  61 loss: 1.01811659\n",
            "iteration:  62 loss: 0.34529978\n",
            "iteration:  63 loss: 0.81964225\n",
            "iteration:  64 loss: 0.32096586\n",
            "iteration:  65 loss: 0.27929312\n",
            "iteration:  66 loss: 0.24780750\n",
            "iteration:  67 loss: 0.36441520\n",
            "iteration:  68 loss: 0.23755255\n",
            "iteration:  69 loss: 0.12681368\n",
            "iteration:  70 loss: 0.37183896\n",
            "iteration:  71 loss: 0.68906558\n",
            "iteration:  72 loss: 0.23046058\n",
            "iteration:  73 loss: 0.52702254\n",
            "iteration:  74 loss: 0.29218188\n",
            "iteration:  75 loss: 0.23491293\n",
            "iteration:  76 loss: 0.23367186\n",
            "iteration:  77 loss: 0.49297935\n",
            "iteration:  78 loss: 0.12992741\n",
            "iteration:  79 loss: 0.58354038\n",
            "iteration:  80 loss: 0.28250736\n",
            "iteration:  81 loss: 0.33502159\n",
            "iteration:  82 loss: 0.14132425\n",
            "iteration:  83 loss: 0.80001390\n",
            "iteration:  84 loss: 0.27544731\n",
            "iteration:  85 loss: 0.30870864\n",
            "iteration:  86 loss: 0.30435643\n",
            "iteration:  87 loss: 0.43328583\n",
            "iteration:  88 loss: 0.13031514\n",
            "iteration:  89 loss: 0.34892824\n",
            "iteration:  90 loss: 0.16210224\n",
            "iteration:  91 loss: 0.42893687\n",
            "iteration:  92 loss: 0.21874937\n",
            "iteration:  93 loss: 0.26182386\n",
            "iteration:  94 loss: 0.30251530\n",
            "iteration:  95 loss: 0.33299521\n",
            "iteration:  96 loss: 0.11073641\n",
            "iteration:  97 loss: 0.81019193\n",
            "iteration:  98 loss: 0.96907103\n",
            "iteration:  99 loss: 0.57152736\n",
            "iteration: 100 loss: 0.19858608\n",
            "iteration: 101 loss: 0.23214263\n",
            "iteration: 102 loss: 0.15787789\n",
            "iteration: 103 loss: 0.17677331\n",
            "iteration: 104 loss: 0.68256515\n",
            "iteration: 105 loss: 0.89601296\n",
            "iteration: 106 loss: 0.11068083\n",
            "iteration: 107 loss: 0.38917047\n",
            "iteration: 108 loss: 0.40880218\n",
            "iteration: 109 loss: 0.28275415\n",
            "iteration: 110 loss: 0.65576804\n",
            "iteration: 111 loss: 0.41332290\n",
            "iteration: 112 loss: 0.27490923\n",
            "iteration: 113 loss: 0.34489170\n",
            "iteration: 114 loss: 0.13001129\n",
            "iteration: 115 loss: 0.23472238\n",
            "iteration: 116 loss: 0.92596906\n",
            "iteration: 117 loss: 0.24531591\n",
            "iteration: 118 loss: 0.28810292\n",
            "iteration: 119 loss: 0.17633867\n",
            "iteration: 120 loss: 0.24935699\n",
            "iteration: 121 loss: 0.84805459\n",
            "iteration: 122 loss: 0.21232691\n",
            "iteration: 123 loss: 0.38424248\n",
            "iteration: 124 loss: 0.18924189\n",
            "iteration: 125 loss: 0.66731173\n",
            "iteration: 126 loss: 0.16955450\n",
            "iteration: 127 loss: 0.14792085\n",
            "iteration: 128 loss: 0.08727103\n",
            "iteration: 129 loss: 0.43590334\n",
            "iteration: 130 loss: 0.29511231\n",
            "iteration: 131 loss: 0.17678341\n",
            "iteration: 132 loss: 0.34610748\n",
            "iteration: 133 loss: 0.24095771\n",
            "iteration: 134 loss: 0.21596479\n",
            "iteration: 135 loss: 0.13312903\n",
            "iteration: 136 loss: 0.27586544\n",
            "iteration: 137 loss: 1.29375362\n",
            "iteration: 138 loss: 0.15369067\n",
            "iteration: 139 loss: 0.49538234\n",
            "iteration: 140 loss: 0.33923197\n",
            "iteration: 141 loss: 0.09875758\n",
            "iteration: 142 loss: 0.16801706\n",
            "iteration: 143 loss: 0.27736413\n",
            "iteration: 144 loss: 0.24804360\n",
            "iteration: 145 loss: 1.13763785\n",
            "iteration: 146 loss: 0.32695356\n",
            "iteration: 147 loss: 0.40109026\n",
            "iteration: 148 loss: 0.20946181\n",
            "iteration: 149 loss: 0.10617899\n",
            "iteration: 150 loss: 0.22134466\n",
            "iteration: 151 loss: 0.65566230\n",
            "iteration: 152 loss: 0.08663362\n",
            "iteration: 153 loss: 0.11620319\n",
            "iteration: 154 loss: 0.13935664\n",
            "iteration: 155 loss: 0.64328676\n",
            "iteration: 156 loss: 0.44468242\n",
            "iteration: 157 loss: 0.20237461\n",
            "iteration: 158 loss: 0.09317412\n",
            "iteration: 159 loss: 0.17911738\n",
            "iteration: 160 loss: 0.23585905\n",
            "iteration: 161 loss: 0.12239215\n",
            "iteration: 162 loss: 0.13798822\n",
            "iteration: 163 loss: 0.25843674\n",
            "iteration: 164 loss: 0.67300755\n",
            "iteration: 165 loss: 0.24105518\n",
            "iteration: 166 loss: 0.20417520\n",
            "iteration: 167 loss: 0.25276831\n",
            "iteration: 168 loss: 0.70589513\n",
            "iteration: 169 loss: 0.20959780\n",
            "iteration: 170 loss: 0.16229516\n",
            "iteration: 171 loss: 0.27915710\n",
            "iteration: 172 loss: 0.98936164\n",
            "iteration: 173 loss: 0.85730034\n",
            "iteration: 174 loss: 0.33131880\n",
            "iteration: 175 loss: 0.07416935\n",
            "iteration: 176 loss: 0.13499352\n",
            "iteration: 177 loss: 0.23583680\n",
            "iteration: 178 loss: 0.66818869\n",
            "iteration: 179 loss: 0.11302146\n",
            "iteration: 180 loss: 0.13180959\n",
            "iteration: 181 loss: 0.24283952\n",
            "iteration: 182 loss: 0.21093363\n",
            "iteration: 183 loss: 0.21829602\n",
            "iteration: 184 loss: 0.54304242\n",
            "iteration: 185 loss: 0.33347499\n",
            "iteration: 186 loss: 0.41683227\n",
            "iteration: 187 loss: 0.29995558\n",
            "iteration: 188 loss: 0.10191801\n",
            "iteration: 189 loss: 0.45549163\n",
            "iteration: 190 loss: 0.25344396\n",
            "iteration: 191 loss: 0.25790557\n",
            "iteration: 192 loss: 0.35570326\n",
            "iteration: 193 loss: 1.03071368\n",
            "iteration: 194 loss: 0.10302107\n",
            "iteration: 195 loss: 0.28006288\n",
            "iteration: 196 loss: 0.09535308\n",
            "iteration: 197 loss: 0.12474114\n",
            "iteration: 198 loss: 0.11774287\n",
            "iteration: 199 loss: 0.13632317\n",
            "epoch:  84 mean loss training: 0.35294932\n",
            "epoch:  84 mean loss validation: 1.27006567\n",
            "iteration:   0 loss: 0.12479205\n",
            "iteration:   1 loss: 0.81315637\n",
            "iteration:   2 loss: 0.34310153\n",
            "iteration:   3 loss: 0.20006143\n",
            "iteration:   4 loss: 0.12017072\n",
            "iteration:   5 loss: 0.15112421\n",
            "iteration:   6 loss: 0.31055033\n",
            "iteration:   7 loss: 1.07307124\n",
            "iteration:   8 loss: 0.24022324\n",
            "iteration:   9 loss: 0.27047786\n",
            "iteration:  10 loss: 0.28853983\n",
            "iteration:  11 loss: 0.31760246\n",
            "iteration:  12 loss: 0.41381150\n",
            "iteration:  13 loss: 0.40729916\n",
            "iteration:  14 loss: 0.13468352\n",
            "iteration:  15 loss: 0.15881491\n",
            "iteration:  16 loss: 0.52084571\n",
            "iteration:  17 loss: 0.21382776\n",
            "iteration:  18 loss: 1.08081412\n",
            "iteration:  19 loss: 0.26661891\n",
            "iteration:  20 loss: 0.26907057\n",
            "iteration:  21 loss: 0.82999146\n",
            "iteration:  22 loss: 0.45748338\n",
            "iteration:  23 loss: 0.83597785\n",
            "iteration:  24 loss: 0.83771443\n",
            "iteration:  25 loss: 0.30136415\n",
            "iteration:  26 loss: 0.31413376\n",
            "iteration:  27 loss: 0.23274916\n",
            "iteration:  28 loss: 0.34931117\n",
            "iteration:  29 loss: 0.09068429\n",
            "iteration:  30 loss: 0.16930464\n",
            "iteration:  31 loss: 0.88153958\n",
            "iteration:  32 loss: 1.40004992\n",
            "iteration:  33 loss: 0.22736341\n",
            "iteration:  34 loss: 0.22595569\n",
            "iteration:  35 loss: 0.60225481\n",
            "iteration:  36 loss: 0.15976997\n",
            "iteration:  37 loss: 0.23717707\n",
            "iteration:  38 loss: 0.36062887\n",
            "iteration:  39 loss: 0.78444147\n",
            "iteration:  40 loss: 0.32908642\n",
            "iteration:  41 loss: 0.19260111\n",
            "iteration:  42 loss: 0.07938336\n",
            "iteration:  43 loss: 0.26526049\n",
            "iteration:  44 loss: 0.59741837\n",
            "iteration:  45 loss: 0.14668906\n",
            "iteration:  46 loss: 0.15025905\n",
            "iteration:  47 loss: 0.70228410\n",
            "iteration:  48 loss: 0.40914744\n",
            "iteration:  49 loss: 0.72626871\n",
            "iteration:  50 loss: 0.26344776\n",
            "iteration:  51 loss: 0.41251874\n",
            "iteration:  52 loss: 0.77721846\n",
            "iteration:  53 loss: 0.20091796\n",
            "iteration:  54 loss: 0.60380208\n",
            "iteration:  55 loss: 0.99706864\n",
            "iteration:  56 loss: 0.19066095\n",
            "iteration:  57 loss: 0.28942877\n",
            "iteration:  58 loss: 0.17671672\n",
            "iteration:  59 loss: 0.41403684\n",
            "iteration:  60 loss: 0.15626128\n",
            "iteration:  61 loss: 0.19714347\n",
            "iteration:  62 loss: 0.38853028\n",
            "iteration:  63 loss: 0.22797352\n",
            "iteration:  64 loss: 0.11539162\n",
            "iteration:  65 loss: 0.12411945\n",
            "iteration:  66 loss: 0.23626959\n",
            "iteration:  67 loss: 0.15116414\n",
            "iteration:  68 loss: 0.35937744\n",
            "iteration:  69 loss: 0.16545802\n",
            "iteration:  70 loss: 0.60057074\n",
            "iteration:  71 loss: 0.11733113\n",
            "iteration:  72 loss: 0.13811281\n",
            "iteration:  73 loss: 0.29481053\n",
            "iteration:  74 loss: 0.07321176\n",
            "iteration:  75 loss: 0.20356558\n",
            "iteration:  76 loss: 0.24733709\n",
            "iteration:  77 loss: 0.37730724\n",
            "iteration:  78 loss: 0.70425153\n",
            "iteration:  79 loss: 0.15346467\n",
            "iteration:  80 loss: 0.12224545\n",
            "iteration:  81 loss: 0.24860841\n",
            "iteration:  82 loss: 0.61699605\n",
            "iteration:  83 loss: 0.19715318\n",
            "iteration:  84 loss: 0.20208198\n",
            "iteration:  85 loss: 0.43569878\n",
            "iteration:  86 loss: 0.16311219\n",
            "iteration:  87 loss: 0.30328488\n",
            "iteration:  88 loss: 0.15055709\n",
            "iteration:  89 loss: 0.23593450\n",
            "iteration:  90 loss: 0.12383382\n",
            "iteration:  91 loss: 0.70516330\n",
            "iteration:  92 loss: 0.14868933\n",
            "iteration:  93 loss: 0.12835532\n",
            "iteration:  94 loss: 0.21127388\n",
            "iteration:  95 loss: 0.25544956\n",
            "iteration:  96 loss: 0.17962717\n",
            "iteration:  97 loss: 0.25241840\n",
            "iteration:  98 loss: 0.36373100\n",
            "iteration:  99 loss: 0.13853677\n",
            "iteration: 100 loss: 0.13559511\n",
            "iteration: 101 loss: 0.30705976\n",
            "iteration: 102 loss: 0.18590578\n",
            "iteration: 103 loss: 0.12535715\n",
            "iteration: 104 loss: 0.17642872\n",
            "iteration: 105 loss: 0.76953125\n",
            "iteration: 106 loss: 0.56155097\n",
            "iteration: 107 loss: 0.16425413\n",
            "iteration: 108 loss: 1.20540392\n",
            "iteration: 109 loss: 0.08289652\n",
            "iteration: 110 loss: 0.30753416\n",
            "iteration: 111 loss: 0.50486231\n",
            "iteration: 112 loss: 0.24591249\n",
            "iteration: 113 loss: 0.88133627\n",
            "iteration: 114 loss: 0.25470772\n",
            "iteration: 115 loss: 0.38377434\n",
            "iteration: 116 loss: 0.69902599\n",
            "iteration: 117 loss: 0.14608864\n",
            "iteration: 118 loss: 0.67925763\n",
            "iteration: 119 loss: 0.38874447\n",
            "iteration: 120 loss: 0.69384718\n",
            "iteration: 121 loss: 0.54527080\n",
            "iteration: 122 loss: 0.43979940\n",
            "iteration: 123 loss: 0.27879775\n",
            "iteration: 124 loss: 0.54772621\n",
            "iteration: 125 loss: 0.44296950\n",
            "iteration: 126 loss: 0.21985525\n",
            "iteration: 127 loss: 0.33317918\n",
            "iteration: 128 loss: 0.37255102\n",
            "iteration: 129 loss: 0.14206074\n",
            "iteration: 130 loss: 0.16609877\n",
            "iteration: 131 loss: 0.19789222\n",
            "iteration: 132 loss: 0.38909465\n",
            "iteration: 133 loss: 0.38526967\n",
            "iteration: 134 loss: 0.15637760\n",
            "iteration: 135 loss: 0.56736416\n",
            "iteration: 136 loss: 0.29555851\n",
            "iteration: 137 loss: 0.24744686\n",
            "iteration: 138 loss: 0.10130522\n",
            "iteration: 139 loss: 0.30641717\n",
            "iteration: 140 loss: 0.87478161\n",
            "iteration: 141 loss: 0.36894923\n",
            "iteration: 142 loss: 0.50186867\n",
            "iteration: 143 loss: 0.17553975\n",
            "iteration: 144 loss: 0.20777792\n",
            "iteration: 145 loss: 0.59851909\n",
            "iteration: 146 loss: 0.45866081\n",
            "iteration: 147 loss: 0.08446536\n",
            "iteration: 148 loss: 0.17933193\n",
            "iteration: 149 loss: 0.14901531\n",
            "iteration: 150 loss: 0.33381766\n",
            "iteration: 151 loss: 0.88037878\n",
            "iteration: 152 loss: 0.29024783\n",
            "iteration: 153 loss: 0.44029042\n",
            "iteration: 154 loss: 0.53102398\n",
            "iteration: 155 loss: 0.33351699\n",
            "iteration: 156 loss: 0.26618192\n",
            "iteration: 157 loss: 0.19127515\n",
            "iteration: 158 loss: 0.33946949\n",
            "iteration: 159 loss: 0.14492494\n",
            "iteration: 160 loss: 0.26115972\n",
            "iteration: 161 loss: 0.12093471\n",
            "iteration: 162 loss: 0.67727119\n",
            "iteration: 163 loss: 0.13863450\n",
            "iteration: 164 loss: 0.16955599\n",
            "iteration: 165 loss: 0.59870946\n",
            "iteration: 166 loss: 1.26914644\n",
            "iteration: 167 loss: 0.39794654\n",
            "iteration: 168 loss: 0.33934462\n",
            "iteration: 169 loss: 0.29926276\n",
            "iteration: 170 loss: 1.30311692\n",
            "iteration: 171 loss: 0.09204313\n",
            "iteration: 172 loss: 0.14910774\n",
            "iteration: 173 loss: 0.67527652\n",
            "iteration: 174 loss: 0.16921738\n",
            "iteration: 175 loss: 0.75514793\n",
            "iteration: 176 loss: 0.40540135\n",
            "iteration: 177 loss: 0.57220107\n",
            "iteration: 178 loss: 0.28353354\n",
            "iteration: 179 loss: 0.15326552\n",
            "iteration: 180 loss: 0.41719070\n",
            "iteration: 181 loss: 0.19270447\n",
            "iteration: 182 loss: 0.94254750\n",
            "iteration: 183 loss: 0.12935355\n",
            "iteration: 184 loss: 0.21854049\n",
            "iteration: 185 loss: 0.73799187\n",
            "iteration: 186 loss: 0.41409540\n",
            "iteration: 187 loss: 0.31581378\n",
            "iteration: 188 loss: 0.19416496\n",
            "iteration: 189 loss: 0.13124225\n",
            "iteration: 190 loss: 0.26668191\n",
            "iteration: 191 loss: 0.34468594\n",
            "iteration: 192 loss: 0.18187506\n",
            "iteration: 193 loss: 0.22371635\n",
            "iteration: 194 loss: 0.67425096\n",
            "iteration: 195 loss: 0.26709169\n",
            "iteration: 196 loss: 0.24436928\n",
            "iteration: 197 loss: 0.26100871\n",
            "iteration: 198 loss: 0.28706920\n",
            "iteration: 199 loss: 0.23350638\n",
            "epoch:  85 mean loss training: 0.36523750\n",
            "epoch:  85 mean loss validation: 1.23940647\n",
            "iteration:   0 loss: 0.10355574\n",
            "iteration:   1 loss: 0.33132249\n",
            "iteration:   2 loss: 0.25008804\n",
            "iteration:   3 loss: 0.12253016\n",
            "iteration:   4 loss: 0.32201064\n",
            "iteration:   5 loss: 0.22855242\n",
            "iteration:   6 loss: 0.27547535\n",
            "iteration:   7 loss: 0.31806988\n",
            "iteration:   8 loss: 0.69557101\n",
            "iteration:   9 loss: 1.01211751\n",
            "iteration:  10 loss: 0.23978509\n",
            "iteration:  11 loss: 0.09878726\n",
            "iteration:  12 loss: 0.52067375\n",
            "iteration:  13 loss: 0.61339152\n",
            "iteration:  14 loss: 0.07935650\n",
            "iteration:  15 loss: 0.16437912\n",
            "iteration:  16 loss: 0.42430758\n",
            "iteration:  17 loss: 0.73241585\n",
            "iteration:  18 loss: 0.19863123\n",
            "iteration:  19 loss: 0.25427467\n",
            "iteration:  20 loss: 0.65342212\n",
            "iteration:  21 loss: 0.88566738\n",
            "iteration:  22 loss: 0.13235867\n",
            "iteration:  23 loss: 0.89927894\n",
            "iteration:  24 loss: 0.17969877\n",
            "iteration:  25 loss: 0.15234259\n",
            "iteration:  26 loss: 0.91609979\n",
            "iteration:  27 loss: 0.62490135\n",
            "iteration:  28 loss: 0.13334307\n",
            "iteration:  29 loss: 0.35441354\n",
            "iteration:  30 loss: 0.25926745\n",
            "iteration:  31 loss: 0.35043937\n",
            "iteration:  32 loss: 0.14446442\n",
            "iteration:  33 loss: 0.11929594\n",
            "iteration:  34 loss: 0.26511821\n",
            "iteration:  35 loss: 0.16466892\n",
            "iteration:  36 loss: 0.84514487\n",
            "iteration:  37 loss: 0.16393323\n",
            "iteration:  38 loss: 0.64488029\n",
            "iteration:  39 loss: 0.48979867\n",
            "iteration:  40 loss: 0.36654738\n",
            "iteration:  41 loss: 0.15624465\n",
            "iteration:  42 loss: 0.25872096\n",
            "iteration:  43 loss: 0.41113281\n",
            "iteration:  44 loss: 0.21437129\n",
            "iteration:  45 loss: 0.40024236\n",
            "iteration:  46 loss: 0.12150718\n",
            "iteration:  47 loss: 0.12968250\n",
            "iteration:  48 loss: 0.25410861\n",
            "iteration:  49 loss: 0.18068512\n",
            "iteration:  50 loss: 0.20363146\n",
            "iteration:  51 loss: 0.40386209\n",
            "iteration:  52 loss: 0.13385770\n",
            "iteration:  53 loss: 0.20538202\n",
            "iteration:  54 loss: 0.10579747\n",
            "iteration:  55 loss: 0.11471338\n",
            "iteration:  56 loss: 0.29974228\n",
            "iteration:  57 loss: 0.23166144\n",
            "iteration:  58 loss: 0.31310558\n",
            "iteration:  59 loss: 0.69346142\n",
            "iteration:  60 loss: 0.88107800\n",
            "iteration:  61 loss: 1.38213110\n",
            "iteration:  62 loss: 0.77681857\n",
            "iteration:  63 loss: 0.39776272\n",
            "iteration:  64 loss: 0.19893427\n",
            "iteration:  65 loss: 0.30003244\n",
            "iteration:  66 loss: 0.19462693\n",
            "iteration:  67 loss: 0.95204204\n",
            "iteration:  68 loss: 0.78336781\n",
            "iteration:  69 loss: 0.25065032\n",
            "iteration:  70 loss: 0.17708965\n",
            "iteration:  71 loss: 0.21650140\n",
            "iteration:  72 loss: 0.68474358\n",
            "iteration:  73 loss: 0.15253235\n",
            "iteration:  74 loss: 0.47147521\n",
            "iteration:  75 loss: 0.31692952\n",
            "iteration:  76 loss: 0.29421481\n",
            "iteration:  77 loss: 0.10789316\n",
            "iteration:  78 loss: 0.37400192\n",
            "iteration:  79 loss: 0.24979380\n",
            "iteration:  80 loss: 0.11234652\n",
            "iteration:  81 loss: 0.37496737\n",
            "iteration:  82 loss: 0.37991792\n",
            "iteration:  83 loss: 0.39576283\n",
            "iteration:  84 loss: 0.43041769\n",
            "iteration:  85 loss: 0.27144149\n",
            "iteration:  86 loss: 0.10863499\n",
            "iteration:  87 loss: 0.38365838\n",
            "iteration:  88 loss: 1.02354288\n",
            "iteration:  89 loss: 0.32299656\n",
            "iteration:  90 loss: 0.13725100\n",
            "iteration:  91 loss: 0.16853741\n",
            "iteration:  92 loss: 0.21558884\n",
            "iteration:  93 loss: 0.25191379\n",
            "iteration:  94 loss: 0.13302007\n",
            "iteration:  95 loss: 0.18453610\n",
            "iteration:  96 loss: 1.29673696\n",
            "iteration:  97 loss: 0.79915875\n",
            "iteration:  98 loss: 0.49681357\n",
            "iteration:  99 loss: 0.37731457\n",
            "iteration: 100 loss: 0.51725525\n",
            "iteration: 101 loss: 0.25070962\n",
            "iteration: 102 loss: 0.99485832\n",
            "iteration: 103 loss: 0.11202729\n",
            "iteration: 104 loss: 0.32556087\n",
            "iteration: 105 loss: 0.09332292\n",
            "iteration: 106 loss: 0.24563423\n",
            "iteration: 107 loss: 0.39831716\n",
            "iteration: 108 loss: 0.17620844\n",
            "iteration: 109 loss: 0.15462060\n",
            "iteration: 110 loss: 0.18973097\n",
            "iteration: 111 loss: 0.79096293\n",
            "iteration: 112 loss: 0.26255548\n",
            "iteration: 113 loss: 0.95828098\n",
            "iteration: 114 loss: 0.77718931\n",
            "iteration: 115 loss: 0.61870688\n",
            "iteration: 116 loss: 0.10007355\n",
            "iteration: 117 loss: 0.25830865\n",
            "iteration: 118 loss: 0.23268963\n",
            "iteration: 119 loss: 0.26077712\n",
            "iteration: 120 loss: 0.15800062\n",
            "iteration: 121 loss: 0.23619770\n",
            "iteration: 122 loss: 0.10268086\n",
            "iteration: 123 loss: 0.24503322\n",
            "iteration: 124 loss: 0.08426233\n",
            "iteration: 125 loss: 0.22243196\n",
            "iteration: 126 loss: 0.29858604\n",
            "iteration: 127 loss: 0.31986025\n",
            "iteration: 128 loss: 0.29429391\n",
            "iteration: 129 loss: 0.80242401\n",
            "iteration: 130 loss: 0.21871495\n",
            "iteration: 131 loss: 0.31948966\n",
            "iteration: 132 loss: 0.16470899\n",
            "iteration: 133 loss: 0.18460870\n",
            "iteration: 134 loss: 0.10929041\n",
            "iteration: 135 loss: 0.50350213\n",
            "iteration: 136 loss: 0.25037181\n",
            "iteration: 137 loss: 0.48960873\n",
            "iteration: 138 loss: 0.09892805\n",
            "iteration: 139 loss: 0.16873239\n",
            "iteration: 140 loss: 0.59687996\n",
            "iteration: 141 loss: 0.15880635\n",
            "iteration: 142 loss: 0.44375700\n",
            "iteration: 143 loss: 0.32143939\n",
            "iteration: 144 loss: 0.19705006\n",
            "iteration: 145 loss: 0.81227028\n",
            "iteration: 146 loss: 0.12117377\n",
            "iteration: 147 loss: 0.25475410\n",
            "iteration: 148 loss: 1.56063199\n",
            "iteration: 149 loss: 0.16227642\n",
            "iteration: 150 loss: 0.12651165\n",
            "iteration: 151 loss: 0.10357051\n",
            "iteration: 152 loss: 0.46573305\n",
            "iteration: 153 loss: 0.41570535\n",
            "iteration: 154 loss: 0.38688397\n",
            "iteration: 155 loss: 0.18920128\n",
            "iteration: 156 loss: 0.11107675\n",
            "iteration: 157 loss: 0.24297830\n",
            "iteration: 158 loss: 0.20543157\n",
            "iteration: 159 loss: 0.19925305\n",
            "iteration: 160 loss: 0.20592417\n",
            "iteration: 161 loss: 0.18211934\n",
            "iteration: 162 loss: 0.56350362\n",
            "iteration: 163 loss: 0.25530726\n",
            "iteration: 164 loss: 0.97847205\n",
            "iteration: 165 loss: 0.11497556\n",
            "iteration: 166 loss: 0.10444331\n",
            "iteration: 167 loss: 0.29744717\n",
            "iteration: 168 loss: 0.11498131\n",
            "iteration: 169 loss: 0.18968463\n",
            "iteration: 170 loss: 0.44662744\n",
            "iteration: 171 loss: 0.38636208\n",
            "iteration: 172 loss: 0.09165421\n",
            "iteration: 173 loss: 0.35931367\n",
            "iteration: 174 loss: 0.38647819\n",
            "iteration: 175 loss: 0.29472980\n",
            "iteration: 176 loss: 0.08506588\n",
            "iteration: 177 loss: 1.95398951\n",
            "iteration: 178 loss: 0.29115516\n",
            "iteration: 179 loss: 0.34312385\n",
            "iteration: 180 loss: 0.09301811\n",
            "iteration: 181 loss: 0.67024821\n",
            "iteration: 182 loss: 0.37284642\n",
            "iteration: 183 loss: 0.20483051\n",
            "iteration: 184 loss: 0.15204085\n",
            "iteration: 185 loss: 0.97644800\n",
            "iteration: 186 loss: 0.79493940\n",
            "iteration: 187 loss: 0.11021425\n",
            "iteration: 188 loss: 0.10926045\n",
            "iteration: 189 loss: 0.42184225\n",
            "iteration: 190 loss: 0.50902915\n",
            "iteration: 191 loss: 0.15966946\n",
            "iteration: 192 loss: 0.12730898\n",
            "iteration: 193 loss: 0.53179985\n",
            "iteration: 194 loss: 0.78336239\n",
            "iteration: 195 loss: 0.50524575\n",
            "iteration: 196 loss: 0.20117933\n",
            "iteration: 197 loss: 0.23690817\n",
            "iteration: 198 loss: 0.18234333\n",
            "iteration: 199 loss: 0.17397539\n",
            "epoch:  86 mean loss training: 0.36308122\n",
            "epoch:  86 mean loss validation: 1.28786588\n",
            "iteration:   0 loss: 1.08667397\n",
            "iteration:   1 loss: 0.10021600\n",
            "iteration:   2 loss: 0.21810958\n",
            "iteration:   3 loss: 0.29481286\n",
            "iteration:   4 loss: 0.10585621\n",
            "iteration:   5 loss: 0.14946954\n",
            "iteration:   6 loss: 0.10394073\n",
            "iteration:   7 loss: 0.93139374\n",
            "iteration:   8 loss: 0.31978935\n",
            "iteration:   9 loss: 0.53906274\n",
            "iteration:  10 loss: 0.29588404\n",
            "iteration:  11 loss: 0.70450616\n",
            "iteration:  12 loss: 0.57138306\n",
            "iteration:  13 loss: 0.76713932\n",
            "iteration:  14 loss: 0.21313666\n",
            "iteration:  15 loss: 0.61871797\n",
            "iteration:  16 loss: 0.10893430\n",
            "iteration:  17 loss: 0.45752576\n",
            "iteration:  18 loss: 0.24337706\n",
            "iteration:  19 loss: 0.82960558\n",
            "iteration:  20 loss: 0.24928525\n",
            "iteration:  21 loss: 0.16416012\n",
            "iteration:  22 loss: 0.40900561\n",
            "iteration:  23 loss: 0.77597773\n",
            "iteration:  24 loss: 0.16688085\n",
            "iteration:  25 loss: 0.22875768\n",
            "iteration:  26 loss: 0.22600830\n",
            "iteration:  27 loss: 0.14968657\n",
            "iteration:  28 loss: 0.21838179\n",
            "iteration:  29 loss: 0.18005209\n",
            "iteration:  30 loss: 0.18852469\n",
            "iteration:  31 loss: 0.46550170\n",
            "iteration:  32 loss: 0.65463531\n",
            "iteration:  33 loss: 0.62188309\n",
            "iteration:  34 loss: 0.27245519\n",
            "iteration:  35 loss: 0.28391030\n",
            "iteration:  36 loss: 0.16781041\n",
            "iteration:  37 loss: 0.30147523\n",
            "iteration:  38 loss: 0.57454318\n",
            "iteration:  39 loss: 0.29046908\n",
            "iteration:  40 loss: 0.76397699\n",
            "iteration:  41 loss: 0.88163304\n",
            "iteration:  42 loss: 0.38454333\n",
            "iteration:  43 loss: 0.36718944\n",
            "iteration:  44 loss: 0.25630125\n",
            "iteration:  45 loss: 0.19935927\n",
            "iteration:  46 loss: 0.26053235\n",
            "iteration:  47 loss: 0.21431823\n",
            "iteration:  48 loss: 0.17160967\n",
            "iteration:  49 loss: 0.30224988\n",
            "iteration:  50 loss: 0.16522786\n",
            "iteration:  51 loss: 0.21573956\n",
            "iteration:  52 loss: 0.27218401\n",
            "iteration:  53 loss: 0.63614845\n",
            "iteration:  54 loss: 0.10048413\n",
            "iteration:  55 loss: 0.12120914\n",
            "iteration:  56 loss: 0.72825748\n",
            "iteration:  57 loss: 0.34646174\n",
            "iteration:  58 loss: 0.79392457\n",
            "iteration:  59 loss: 0.78588814\n",
            "iteration:  60 loss: 0.18170170\n",
            "iteration:  61 loss: 0.29836619\n",
            "iteration:  62 loss: 0.12477455\n",
            "iteration:  63 loss: 0.23926413\n",
            "iteration:  64 loss: 0.25917509\n",
            "iteration:  65 loss: 0.18474308\n",
            "iteration:  66 loss: 0.31956676\n",
            "iteration:  67 loss: 0.10992491\n",
            "iteration:  68 loss: 0.13816479\n",
            "iteration:  69 loss: 0.16760093\n",
            "iteration:  70 loss: 0.11506107\n",
            "iteration:  71 loss: 0.20102713\n",
            "iteration:  72 loss: 0.14815998\n",
            "iteration:  73 loss: 0.26145539\n",
            "iteration:  74 loss: 0.47580120\n",
            "iteration:  75 loss: 0.61554617\n",
            "iteration:  76 loss: 0.66199648\n",
            "iteration:  77 loss: 0.25645849\n",
            "iteration:  78 loss: 0.19490585\n",
            "iteration:  79 loss: 0.49824476\n",
            "iteration:  80 loss: 0.08940452\n",
            "iteration:  81 loss: 0.37245482\n",
            "iteration:  82 loss: 0.25540954\n",
            "iteration:  83 loss: 0.20362534\n",
            "iteration:  84 loss: 0.49351043\n",
            "iteration:  85 loss: 0.15761733\n",
            "iteration:  86 loss: 0.79618043\n",
            "iteration:  87 loss: 0.21054778\n",
            "iteration:  88 loss: 0.16638835\n",
            "iteration:  89 loss: 0.32229120\n",
            "iteration:  90 loss: 0.34061652\n",
            "iteration:  91 loss: 0.17972836\n",
            "iteration:  92 loss: 0.33490708\n",
            "iteration:  93 loss: 0.84777135\n",
            "iteration:  94 loss: 0.22623992\n",
            "iteration:  95 loss: 0.23682763\n",
            "iteration:  96 loss: 0.15537795\n",
            "iteration:  97 loss: 0.93135297\n",
            "iteration:  98 loss: 0.44816464\n",
            "iteration:  99 loss: 0.45698446\n",
            "iteration: 100 loss: 0.70516688\n",
            "iteration: 101 loss: 0.44538027\n",
            "iteration: 102 loss: 0.52384943\n",
            "iteration: 103 loss: 0.10960925\n",
            "iteration: 104 loss: 0.14439534\n",
            "iteration: 105 loss: 0.16588809\n",
            "iteration: 106 loss: 0.29779348\n",
            "iteration: 107 loss: 0.17941089\n",
            "iteration: 108 loss: 0.15928233\n",
            "iteration: 109 loss: 0.13106637\n",
            "iteration: 110 loss: 0.25863117\n",
            "iteration: 111 loss: 0.76022166\n",
            "iteration: 112 loss: 0.37418017\n",
            "iteration: 113 loss: 0.45872080\n",
            "iteration: 114 loss: 0.20339048\n",
            "iteration: 115 loss: 1.39440477\n",
            "iteration: 116 loss: 0.15736142\n",
            "iteration: 117 loss: 0.18633002\n",
            "iteration: 118 loss: 0.17281568\n",
            "iteration: 119 loss: 0.40477836\n",
            "iteration: 120 loss: 0.25262100\n",
            "iteration: 121 loss: 0.38905591\n",
            "iteration: 122 loss: 0.18868273\n",
            "iteration: 123 loss: 0.30780384\n",
            "iteration: 124 loss: 0.14227849\n",
            "iteration: 125 loss: 0.16019461\n",
            "iteration: 126 loss: 0.20002559\n",
            "iteration: 127 loss: 0.91836268\n",
            "iteration: 128 loss: 0.15510438\n",
            "iteration: 129 loss: 0.45536342\n",
            "iteration: 130 loss: 0.27014968\n",
            "iteration: 131 loss: 0.08450384\n",
            "iteration: 132 loss: 0.42081475\n",
            "iteration: 133 loss: 0.13160667\n",
            "iteration: 134 loss: 0.31728292\n",
            "iteration: 135 loss: 0.11405889\n",
            "iteration: 136 loss: 0.25530705\n",
            "iteration: 137 loss: 0.22863096\n",
            "iteration: 138 loss: 0.16561371\n",
            "iteration: 139 loss: 1.27890527\n",
            "iteration: 140 loss: 0.17235886\n",
            "iteration: 141 loss: 0.72763848\n",
            "iteration: 142 loss: 0.20646098\n",
            "iteration: 143 loss: 0.49784711\n",
            "iteration: 144 loss: 0.36464903\n",
            "iteration: 145 loss: 0.54980081\n",
            "iteration: 146 loss: 0.11227809\n",
            "iteration: 147 loss: 0.12634748\n",
            "iteration: 148 loss: 0.35371035\n",
            "iteration: 149 loss: 0.56587362\n",
            "iteration: 150 loss: 0.65629029\n",
            "iteration: 151 loss: 0.19294347\n",
            "iteration: 152 loss: 0.31059748\n",
            "iteration: 153 loss: 0.18864530\n",
            "iteration: 154 loss: 0.32561395\n",
            "iteration: 155 loss: 0.22747277\n",
            "iteration: 156 loss: 0.31848076\n",
            "iteration: 157 loss: 0.22263034\n",
            "iteration: 158 loss: 0.33422634\n",
            "iteration: 159 loss: 0.10691848\n",
            "iteration: 160 loss: 1.15410542\n",
            "iteration: 161 loss: 0.75919074\n",
            "iteration: 162 loss: 0.22373384\n",
            "iteration: 163 loss: 0.11299634\n",
            "iteration: 164 loss: 0.27363858\n",
            "iteration: 165 loss: 0.23182370\n",
            "iteration: 166 loss: 0.30249327\n",
            "iteration: 167 loss: 0.36352810\n",
            "iteration: 168 loss: 1.22602904\n",
            "iteration: 169 loss: 0.22429524\n",
            "iteration: 170 loss: 0.25834489\n",
            "iteration: 171 loss: 0.13263357\n",
            "iteration: 172 loss: 0.15202476\n",
            "iteration: 173 loss: 1.13697827\n",
            "iteration: 174 loss: 0.29822877\n",
            "iteration: 175 loss: 0.25105447\n",
            "iteration: 176 loss: 0.33705243\n",
            "iteration: 177 loss: 0.19668236\n",
            "iteration: 178 loss: 0.25701812\n",
            "iteration: 179 loss: 0.50762624\n",
            "iteration: 180 loss: 0.54309499\n",
            "iteration: 181 loss: 0.26190981\n",
            "iteration: 182 loss: 0.10466388\n",
            "iteration: 183 loss: 0.13607612\n",
            "iteration: 184 loss: 0.11691404\n",
            "iteration: 185 loss: 0.21221599\n",
            "iteration: 186 loss: 0.63030601\n",
            "iteration: 187 loss: 0.36631170\n",
            "iteration: 188 loss: 0.61957663\n",
            "iteration: 189 loss: 0.36759743\n",
            "iteration: 190 loss: 0.75197512\n",
            "iteration: 191 loss: 0.97073901\n",
            "iteration: 192 loss: 0.10274042\n",
            "iteration: 193 loss: 0.42551416\n",
            "iteration: 194 loss: 0.31218648\n",
            "iteration: 195 loss: 0.67801380\n",
            "iteration: 196 loss: 1.00061667\n",
            "iteration: 197 loss: 0.15030336\n",
            "iteration: 198 loss: 1.24757195\n",
            "iteration: 199 loss: 0.16070086\n",
            "epoch:  87 mean loss training: 0.36840928\n",
            "epoch:  87 mean loss validation: 1.28289449\n",
            "iteration:   0 loss: 0.16319859\n",
            "iteration:   1 loss: 1.33010638\n",
            "iteration:   2 loss: 0.36606309\n",
            "iteration:   3 loss: 0.24155411\n",
            "iteration:   4 loss: 0.10105763\n",
            "iteration:   5 loss: 0.39234224\n",
            "iteration:   6 loss: 0.19427803\n",
            "iteration:   7 loss: 0.41973943\n",
            "iteration:   8 loss: 1.21738410\n",
            "iteration:   9 loss: 0.19758768\n",
            "iteration:  10 loss: 0.09037226\n",
            "iteration:  11 loss: 0.38304362\n",
            "iteration:  12 loss: 0.13311419\n",
            "iteration:  13 loss: 0.17080358\n",
            "iteration:  14 loss: 0.19736554\n",
            "iteration:  15 loss: 0.89393395\n",
            "iteration:  16 loss: 1.62860882\n",
            "iteration:  17 loss: 0.14667657\n",
            "iteration:  18 loss: 0.35159528\n",
            "iteration:  19 loss: 0.55699265\n",
            "iteration:  20 loss: 0.15236433\n",
            "iteration:  21 loss: 0.76722002\n",
            "iteration:  22 loss: 0.15639691\n",
            "iteration:  23 loss: 0.09964226\n",
            "iteration:  24 loss: 0.36447281\n",
            "iteration:  25 loss: 0.34834641\n",
            "iteration:  26 loss: 0.42723706\n",
            "iteration:  27 loss: 0.48990679\n",
            "iteration:  28 loss: 0.14290759\n",
            "iteration:  29 loss: 0.42995349\n",
            "iteration:  30 loss: 0.40702271\n",
            "iteration:  31 loss: 0.80180705\n",
            "iteration:  32 loss: 1.55210090\n",
            "iteration:  33 loss: 0.12858668\n",
            "iteration:  34 loss: 0.27218404\n",
            "iteration:  35 loss: 0.38187245\n",
            "iteration:  36 loss: 0.59092891\n",
            "iteration:  37 loss: 0.48447451\n",
            "iteration:  38 loss: 0.14762406\n",
            "iteration:  39 loss: 0.27812186\n",
            "iteration:  40 loss: 0.36901635\n",
            "iteration:  41 loss: 0.16428404\n",
            "iteration:  42 loss: 0.29468876\n",
            "iteration:  43 loss: 0.15267256\n",
            "iteration:  44 loss: 0.14159857\n",
            "iteration:  45 loss: 0.79506040\n",
            "iteration:  46 loss: 0.27066243\n",
            "iteration:  47 loss: 0.70318061\n",
            "iteration:  48 loss: 0.14697924\n",
            "iteration:  49 loss: 0.47114167\n",
            "iteration:  50 loss: 1.37993169\n",
            "iteration:  51 loss: 0.51849937\n",
            "iteration:  52 loss: 0.47827625\n",
            "iteration:  53 loss: 0.60272408\n",
            "iteration:  54 loss: 0.98621804\n",
            "iteration:  55 loss: 0.38932037\n",
            "iteration:  56 loss: 0.49107015\n",
            "iteration:  57 loss: 0.68139195\n",
            "iteration:  58 loss: 0.10894267\n",
            "iteration:  59 loss: 0.16640173\n",
            "iteration:  60 loss: 0.23943481\n",
            "iteration:  61 loss: 0.49806458\n",
            "iteration:  62 loss: 0.09360025\n",
            "iteration:  63 loss: 0.12617362\n",
            "iteration:  64 loss: 0.26837155\n",
            "iteration:  65 loss: 0.08520766\n",
            "iteration:  66 loss: 0.32997084\n",
            "iteration:  67 loss: 0.20137724\n",
            "iteration:  68 loss: 0.26154670\n",
            "iteration:  69 loss: 0.17853776\n",
            "iteration:  70 loss: 0.19056308\n",
            "iteration:  71 loss: 0.10948302\n",
            "iteration:  72 loss: 0.40015715\n",
            "iteration:  73 loss: 0.23381773\n",
            "iteration:  74 loss: 0.10163991\n",
            "iteration:  75 loss: 0.40870315\n",
            "iteration:  76 loss: 0.23332438\n",
            "iteration:  77 loss: 0.13837315\n",
            "iteration:  78 loss: 0.13570547\n",
            "iteration:  79 loss: 0.46572411\n",
            "iteration:  80 loss: 0.81244957\n",
            "iteration:  81 loss: 0.09387182\n",
            "iteration:  82 loss: 0.45032334\n",
            "iteration:  83 loss: 0.34531447\n",
            "iteration:  84 loss: 0.43806902\n",
            "iteration:  85 loss: 0.61367881\n",
            "iteration:  86 loss: 0.27863941\n",
            "iteration:  87 loss: 0.09894885\n",
            "iteration:  88 loss: 0.53013825\n",
            "iteration:  89 loss: 0.49888384\n",
            "iteration:  90 loss: 0.47139919\n",
            "iteration:  91 loss: 0.14952978\n",
            "iteration:  92 loss: 0.86763287\n",
            "iteration:  93 loss: 0.16271847\n",
            "iteration:  94 loss: 0.59749395\n",
            "iteration:  95 loss: 0.24269861\n",
            "iteration:  96 loss: 0.38747814\n",
            "iteration:  97 loss: 0.67078233\n",
            "iteration:  98 loss: 0.97497284\n",
            "iteration:  99 loss: 0.12027651\n",
            "iteration: 100 loss: 0.08247816\n",
            "iteration: 101 loss: 0.38274512\n",
            "iteration: 102 loss: 0.33885330\n",
            "iteration: 103 loss: 0.48807064\n",
            "iteration: 104 loss: 0.14371155\n",
            "iteration: 105 loss: 0.23959115\n",
            "iteration: 106 loss: 0.90641993\n",
            "iteration: 107 loss: 0.09851186\n",
            "iteration: 108 loss: 0.18673597\n",
            "iteration: 109 loss: 0.67501104\n",
            "iteration: 110 loss: 0.10042034\n",
            "iteration: 111 loss: 0.13533708\n",
            "iteration: 112 loss: 0.10341612\n",
            "iteration: 113 loss: 0.31011623\n",
            "iteration: 114 loss: 0.71357816\n",
            "iteration: 115 loss: 0.31752682\n",
            "iteration: 116 loss: 0.15372106\n",
            "iteration: 117 loss: 0.69720560\n",
            "iteration: 118 loss: 0.15165743\n",
            "iteration: 119 loss: 0.25716364\n",
            "iteration: 120 loss: 0.28064609\n",
            "iteration: 121 loss: 0.73017693\n",
            "iteration: 122 loss: 0.26923454\n",
            "iteration: 123 loss: 0.65968609\n",
            "iteration: 124 loss: 0.10017499\n",
            "iteration: 125 loss: 0.24908811\n",
            "iteration: 126 loss: 0.14583775\n",
            "iteration: 127 loss: 0.42696819\n",
            "iteration: 128 loss: 0.15009487\n",
            "iteration: 129 loss: 0.44511384\n",
            "iteration: 130 loss: 0.32704926\n",
            "iteration: 131 loss: 0.11584815\n",
            "iteration: 132 loss: 0.45316917\n",
            "iteration: 133 loss: 0.26516852\n",
            "iteration: 134 loss: 0.24937180\n",
            "iteration: 135 loss: 0.71154058\n",
            "iteration: 136 loss: 0.21848069\n",
            "iteration: 137 loss: 0.74242193\n",
            "iteration: 138 loss: 0.80674469\n",
            "iteration: 139 loss: 0.21173090\n",
            "iteration: 140 loss: 0.10463578\n",
            "iteration: 141 loss: 0.53470397\n",
            "iteration: 142 loss: 0.13530129\n",
            "iteration: 143 loss: 0.23971853\n",
            "iteration: 144 loss: 0.37320250\n",
            "iteration: 145 loss: 0.15409537\n",
            "iteration: 146 loss: 0.22405602\n",
            "iteration: 147 loss: 0.32556361\n",
            "iteration: 148 loss: 0.30047989\n",
            "iteration: 149 loss: 0.94012439\n",
            "iteration: 150 loss: 0.16918334\n",
            "iteration: 151 loss: 0.15239826\n",
            "iteration: 152 loss: 0.24655128\n",
            "iteration: 153 loss: 0.14140448\n",
            "iteration: 154 loss: 0.55354828\n",
            "iteration: 155 loss: 0.40169784\n",
            "iteration: 156 loss: 0.38261977\n",
            "iteration: 157 loss: 0.29002506\n",
            "iteration: 158 loss: 0.83989805\n",
            "iteration: 159 loss: 0.10414416\n",
            "iteration: 160 loss: 0.26127800\n",
            "iteration: 161 loss: 0.33233294\n",
            "iteration: 162 loss: 0.25206977\n",
            "iteration: 163 loss: 0.23579691\n",
            "iteration: 164 loss: 0.14225870\n",
            "iteration: 165 loss: 0.39033619\n",
            "iteration: 166 loss: 0.15462819\n",
            "iteration: 167 loss: 0.30475995\n",
            "iteration: 168 loss: 0.34975833\n",
            "iteration: 169 loss: 0.11480848\n",
            "iteration: 170 loss: 0.13870224\n",
            "iteration: 171 loss: 0.25460085\n",
            "iteration: 172 loss: 0.62353206\n",
            "iteration: 173 loss: 0.62322694\n",
            "iteration: 174 loss: 0.77032375\n",
            "iteration: 175 loss: 0.39295536\n",
            "iteration: 176 loss: 0.28600389\n",
            "iteration: 177 loss: 0.10913779\n",
            "iteration: 178 loss: 0.63488948\n",
            "iteration: 179 loss: 0.09104830\n",
            "iteration: 180 loss: 0.30413187\n",
            "iteration: 181 loss: 0.27388120\n",
            "iteration: 182 loss: 0.64434099\n",
            "iteration: 183 loss: 0.07298842\n",
            "iteration: 184 loss: 0.38173854\n",
            "iteration: 185 loss: 0.37803745\n",
            "iteration: 186 loss: 0.21315315\n",
            "iteration: 187 loss: 0.17303397\n",
            "iteration: 188 loss: 0.25534740\n",
            "iteration: 189 loss: 0.23608965\n",
            "iteration: 190 loss: 0.68105227\n",
            "iteration: 191 loss: 0.35849035\n",
            "iteration: 192 loss: 0.41221863\n",
            "iteration: 193 loss: 0.12555057\n",
            "iteration: 194 loss: 0.09838326\n",
            "iteration: 195 loss: 0.10507543\n",
            "iteration: 196 loss: 0.26924619\n",
            "iteration: 197 loss: 0.11805531\n",
            "iteration: 198 loss: 0.10602358\n",
            "iteration: 199 loss: 0.15455607\n",
            "epoch:  88 mean loss training: 0.36447066\n",
            "epoch:  88 mean loss validation: 1.24388337\n",
            "iteration:   0 loss: 0.36152780\n",
            "iteration:   1 loss: 1.37756515\n",
            "iteration:   2 loss: 0.33398974\n",
            "iteration:   3 loss: 0.16052452\n",
            "iteration:   4 loss: 0.21032500\n",
            "iteration:   5 loss: 0.13326573\n",
            "iteration:   6 loss: 0.40816224\n",
            "iteration:   7 loss: 0.28941420\n",
            "iteration:   8 loss: 0.13565174\n",
            "iteration:   9 loss: 0.22546415\n",
            "iteration:  10 loss: 0.08739837\n",
            "iteration:  11 loss: 0.09826598\n",
            "iteration:  12 loss: 0.28193992\n",
            "iteration:  13 loss: 0.11030783\n",
            "iteration:  14 loss: 0.48262668\n",
            "iteration:  15 loss: 0.71169853\n",
            "iteration:  16 loss: 0.24727198\n",
            "iteration:  17 loss: 0.11551439\n",
            "iteration:  18 loss: 0.33929479\n",
            "iteration:  19 loss: 0.16236004\n",
            "iteration:  20 loss: 0.54499209\n",
            "iteration:  21 loss: 0.21904603\n",
            "iteration:  22 loss: 0.40276441\n",
            "iteration:  23 loss: 0.28087655\n",
            "iteration:  24 loss: 0.06965773\n",
            "iteration:  25 loss: 0.14314358\n",
            "iteration:  26 loss: 0.14722638\n",
            "iteration:  27 loss: 0.95791203\n",
            "iteration:  28 loss: 0.51336384\n",
            "iteration:  29 loss: 0.33698615\n",
            "iteration:  30 loss: 0.21462440\n",
            "iteration:  31 loss: 0.24095801\n",
            "iteration:  32 loss: 0.68983096\n",
            "iteration:  33 loss: 0.08387807\n",
            "iteration:  34 loss: 0.29938683\n",
            "iteration:  35 loss: 0.43973696\n",
            "iteration:  36 loss: 0.55795443\n",
            "iteration:  37 loss: 0.12310871\n",
            "iteration:  38 loss: 0.30393568\n",
            "iteration:  39 loss: 0.53455967\n",
            "iteration:  40 loss: 0.33176342\n",
            "iteration:  41 loss: 0.30097410\n",
            "iteration:  42 loss: 0.25256968\n",
            "iteration:  43 loss: 0.28410900\n",
            "iteration:  44 loss: 0.83201122\n",
            "iteration:  45 loss: 0.11028985\n",
            "iteration:  46 loss: 0.59316480\n",
            "iteration:  47 loss: 0.24720068\n",
            "iteration:  48 loss: 0.21770848\n",
            "iteration:  49 loss: 0.34713170\n",
            "iteration:  50 loss: 0.11057954\n",
            "iteration:  51 loss: 0.22855960\n",
            "iteration:  52 loss: 0.37999246\n",
            "iteration:  53 loss: 0.60378993\n",
            "iteration:  54 loss: 0.17214206\n",
            "iteration:  55 loss: 0.24184150\n",
            "iteration:  56 loss: 0.79195452\n",
            "iteration:  57 loss: 0.17959353\n",
            "iteration:  58 loss: 0.12895073\n",
            "iteration:  59 loss: 0.24479386\n",
            "iteration:  60 loss: 0.20786647\n",
            "iteration:  61 loss: 0.18585202\n",
            "iteration:  62 loss: 1.50584543\n",
            "iteration:  63 loss: 0.15062405\n",
            "iteration:  64 loss: 0.14712328\n",
            "iteration:  65 loss: 0.25216651\n",
            "iteration:  66 loss: 0.13982019\n",
            "iteration:  67 loss: 0.20106421\n",
            "iteration:  68 loss: 0.14578402\n",
            "iteration:  69 loss: 0.27034307\n",
            "iteration:  70 loss: 0.24038571\n",
            "iteration:  71 loss: 0.37226123\n",
            "iteration:  72 loss: 0.13187288\n",
            "iteration:  73 loss: 0.35340148\n",
            "iteration:  74 loss: 0.14450791\n",
            "iteration:  75 loss: 0.11416415\n",
            "iteration:  76 loss: 0.18055296\n",
            "iteration:  77 loss: 0.16873126\n",
            "iteration:  78 loss: 0.79298866\n",
            "iteration:  79 loss: 0.66332209\n",
            "iteration:  80 loss: 0.12206000\n",
            "iteration:  81 loss: 0.27617446\n",
            "iteration:  82 loss: 0.22045296\n",
            "iteration:  83 loss: 0.34450820\n",
            "iteration:  84 loss: 0.39069742\n",
            "iteration:  85 loss: 0.10696796\n",
            "iteration:  86 loss: 0.38373893\n",
            "iteration:  87 loss: 1.15007806\n",
            "iteration:  88 loss: 0.56864589\n",
            "iteration:  89 loss: 0.08356066\n",
            "iteration:  90 loss: 0.20535067\n",
            "iteration:  91 loss: 0.27556193\n",
            "iteration:  92 loss: 0.11124684\n",
            "iteration:  93 loss: 0.24959278\n",
            "iteration:  94 loss: 1.76237774\n",
            "iteration:  95 loss: 0.58181912\n",
            "iteration:  96 loss: 0.32093403\n",
            "iteration:  97 loss: 0.53702962\n",
            "iteration:  98 loss: 1.08789885\n",
            "iteration:  99 loss: 0.14288944\n",
            "iteration: 100 loss: 0.40917733\n",
            "iteration: 101 loss: 0.61096168\n",
            "iteration: 102 loss: 1.71776271\n",
            "iteration: 103 loss: 0.16737574\n",
            "iteration: 104 loss: 0.44703436\n",
            "iteration: 105 loss: 0.65980721\n",
            "iteration: 106 loss: 0.36355126\n",
            "iteration: 107 loss: 0.22867224\n",
            "iteration: 108 loss: 0.88773507\n",
            "iteration: 109 loss: 0.08118723\n",
            "iteration: 110 loss: 0.35549954\n",
            "iteration: 111 loss: 0.69872040\n",
            "iteration: 112 loss: 0.17921942\n",
            "iteration: 113 loss: 0.68636149\n",
            "iteration: 114 loss: 0.17025955\n",
            "iteration: 115 loss: 0.57436717\n",
            "iteration: 116 loss: 0.39345366\n",
            "iteration: 117 loss: 0.89271593\n",
            "iteration: 118 loss: 0.58568847\n",
            "iteration: 119 loss: 0.38788426\n",
            "iteration: 120 loss: 0.44001567\n",
            "iteration: 121 loss: 0.93583643\n",
            "iteration: 122 loss: 0.31276312\n",
            "iteration: 123 loss: 0.40088794\n",
            "iteration: 124 loss: 0.11937032\n",
            "iteration: 125 loss: 0.31277317\n",
            "iteration: 126 loss: 0.54442799\n",
            "iteration: 127 loss: 0.45443720\n",
            "iteration: 128 loss: 0.39508578\n",
            "iteration: 129 loss: 0.46311635\n",
            "iteration: 130 loss: 0.35764104\n",
            "iteration: 131 loss: 0.16996878\n",
            "iteration: 132 loss: 0.18873627\n",
            "iteration: 133 loss: 0.32506919\n",
            "iteration: 134 loss: 0.15400209\n",
            "iteration: 135 loss: 0.23936972\n",
            "iteration: 136 loss: 0.47190109\n",
            "iteration: 137 loss: 0.16166469\n",
            "iteration: 138 loss: 0.20060298\n",
            "iteration: 139 loss: 0.41072515\n",
            "iteration: 140 loss: 0.19494152\n",
            "iteration: 141 loss: 0.26400328\n",
            "iteration: 142 loss: 0.28039014\n",
            "iteration: 143 loss: 0.67815602\n",
            "iteration: 144 loss: 0.27736181\n",
            "iteration: 145 loss: 0.22226156\n",
            "iteration: 146 loss: 0.32848382\n",
            "iteration: 147 loss: 0.30529013\n",
            "iteration: 148 loss: 0.08978055\n",
            "iteration: 149 loss: 0.14500865\n",
            "iteration: 150 loss: 0.10983676\n",
            "iteration: 151 loss: 0.14835325\n",
            "iteration: 152 loss: 0.94597024\n",
            "iteration: 153 loss: 0.47992092\n",
            "iteration: 154 loss: 0.10236496\n",
            "iteration: 155 loss: 0.34030798\n",
            "iteration: 156 loss: 0.15907136\n",
            "iteration: 157 loss: 0.17826620\n",
            "iteration: 158 loss: 0.67984539\n",
            "iteration: 159 loss: 0.15203549\n",
            "iteration: 160 loss: 0.28131559\n",
            "iteration: 161 loss: 0.35321927\n",
            "iteration: 162 loss: 0.17428301\n",
            "iteration: 163 loss: 0.72907943\n",
            "iteration: 164 loss: 0.54923761\n",
            "iteration: 165 loss: 0.24215612\n",
            "iteration: 166 loss: 1.08902276\n",
            "iteration: 167 loss: 0.25929183\n",
            "iteration: 168 loss: 0.74622178\n",
            "iteration: 169 loss: 0.42470115\n",
            "iteration: 170 loss: 0.12885502\n",
            "iteration: 171 loss: 0.25364909\n",
            "iteration: 172 loss: 0.20355111\n",
            "iteration: 173 loss: 0.14322233\n",
            "iteration: 174 loss: 0.43379551\n",
            "iteration: 175 loss: 0.12887700\n",
            "iteration: 176 loss: 0.34565127\n",
            "iteration: 177 loss: 0.86302131\n",
            "iteration: 178 loss: 0.13138333\n",
            "iteration: 179 loss: 0.16685909\n",
            "iteration: 180 loss: 0.81897384\n",
            "iteration: 181 loss: 0.20789266\n",
            "iteration: 182 loss: 0.17720948\n",
            "iteration: 183 loss: 0.88414645\n",
            "iteration: 184 loss: 0.21168731\n",
            "iteration: 185 loss: 0.23106259\n",
            "iteration: 186 loss: 0.12011452\n",
            "iteration: 187 loss: 0.89071476\n",
            "iteration: 188 loss: 0.41627678\n",
            "iteration: 189 loss: 0.15793788\n",
            "iteration: 190 loss: 0.22163856\n",
            "iteration: 191 loss: 0.13366950\n",
            "iteration: 192 loss: 1.05896723\n",
            "iteration: 193 loss: 0.29297894\n",
            "iteration: 194 loss: 0.52952075\n",
            "iteration: 195 loss: 0.81236529\n",
            "iteration: 196 loss: 0.33734381\n",
            "iteration: 197 loss: 1.56510162\n",
            "iteration: 198 loss: 0.10265034\n",
            "iteration: 199 loss: 0.19151968\n",
            "epoch:  89 mean loss training: 0.37956268\n",
            "epoch:  89 mean loss validation: 1.30389500\n",
            "iteration:   0 loss: 0.21923155\n",
            "iteration:   1 loss: 0.13927568\n",
            "iteration:   2 loss: 0.42664039\n",
            "iteration:   3 loss: 0.32896969\n",
            "iteration:   4 loss: 0.64193940\n",
            "iteration:   5 loss: 0.14543295\n",
            "iteration:   6 loss: 0.15217322\n",
            "iteration:   7 loss: 0.55218053\n",
            "iteration:   8 loss: 0.83414888\n",
            "iteration:   9 loss: 0.27374434\n",
            "iteration:  10 loss: 0.49251160\n",
            "iteration:  11 loss: 0.33835906\n",
            "iteration:  12 loss: 0.18878740\n",
            "iteration:  13 loss: 0.43455026\n",
            "iteration:  14 loss: 0.12504694\n",
            "iteration:  15 loss: 0.12566499\n",
            "iteration:  16 loss: 0.21973282\n",
            "iteration:  17 loss: 0.37739632\n",
            "iteration:  18 loss: 0.96874309\n",
            "iteration:  19 loss: 0.42121452\n",
            "iteration:  20 loss: 0.30388162\n",
            "iteration:  21 loss: 0.14997405\n",
            "iteration:  22 loss: 0.39856172\n",
            "iteration:  23 loss: 0.87942231\n",
            "iteration:  24 loss: 1.35267067\n",
            "iteration:  25 loss: 0.18770652\n",
            "iteration:  26 loss: 0.53516924\n",
            "iteration:  27 loss: 0.41603938\n",
            "iteration:  28 loss: 0.10242675\n",
            "iteration:  29 loss: 0.40181988\n",
            "iteration:  30 loss: 0.30144095\n",
            "iteration:  31 loss: 0.21298876\n",
            "iteration:  32 loss: 0.68522674\n",
            "iteration:  33 loss: 0.49443582\n",
            "iteration:  34 loss: 0.57623219\n",
            "iteration:  35 loss: 0.37040916\n",
            "iteration:  36 loss: 0.16099033\n",
            "iteration:  37 loss: 0.33469287\n",
            "iteration:  38 loss: 0.13893268\n",
            "iteration:  39 loss: 0.09112644\n",
            "iteration:  40 loss: 0.68125951\n",
            "iteration:  41 loss: 0.13114600\n",
            "iteration:  42 loss: 0.10925347\n",
            "iteration:  43 loss: 0.32697418\n",
            "iteration:  44 loss: 0.16151100\n",
            "iteration:  45 loss: 0.26200855\n",
            "iteration:  46 loss: 0.81167179\n",
            "iteration:  47 loss: 0.28770301\n",
            "iteration:  48 loss: 0.09508896\n",
            "iteration:  49 loss: 0.22046362\n",
            "iteration:  50 loss: 0.37390810\n",
            "iteration:  51 loss: 0.46083316\n",
            "iteration:  52 loss: 0.28566208\n",
            "iteration:  53 loss: 0.41937578\n",
            "iteration:  54 loss: 0.55525005\n",
            "iteration:  55 loss: 1.43165505\n",
            "iteration:  56 loss: 0.25306320\n",
            "iteration:  57 loss: 0.43749836\n",
            "iteration:  58 loss: 0.70370632\n",
            "iteration:  59 loss: 0.11119834\n",
            "iteration:  60 loss: 0.48026004\n",
            "iteration:  61 loss: 0.36782447\n",
            "iteration:  62 loss: 0.19876179\n",
            "iteration:  63 loss: 0.42833811\n",
            "iteration:  64 loss: 0.29654381\n",
            "iteration:  65 loss: 0.32906231\n",
            "iteration:  66 loss: 0.56283593\n",
            "iteration:  67 loss: 0.31683195\n",
            "iteration:  68 loss: 0.45099834\n",
            "iteration:  69 loss: 0.28810194\n",
            "iteration:  70 loss: 0.22265837\n",
            "iteration:  71 loss: 0.18658626\n",
            "iteration:  72 loss: 0.18735787\n",
            "iteration:  73 loss: 0.38778025\n",
            "iteration:  74 loss: 0.10981852\n",
            "iteration:  75 loss: 0.87706286\n",
            "iteration:  76 loss: 0.08179458\n",
            "iteration:  77 loss: 0.13871309\n",
            "iteration:  78 loss: 0.20763791\n",
            "iteration:  79 loss: 0.12536532\n",
            "iteration:  80 loss: 0.46829474\n",
            "iteration:  81 loss: 1.23041952\n",
            "iteration:  82 loss: 0.78558499\n",
            "iteration:  83 loss: 0.35170808\n",
            "iteration:  84 loss: 0.41846317\n",
            "iteration:  85 loss: 0.26272088\n",
            "iteration:  86 loss: 0.39742631\n",
            "iteration:  87 loss: 0.18762858\n",
            "iteration:  88 loss: 0.10573781\n",
            "iteration:  89 loss: 0.18930027\n",
            "iteration:  90 loss: 0.10238739\n",
            "iteration:  91 loss: 0.37150100\n",
            "iteration:  92 loss: 0.11289812\n",
            "iteration:  93 loss: 0.25036278\n",
            "iteration:  94 loss: 0.18479964\n",
            "iteration:  95 loss: 0.07343204\n",
            "iteration:  96 loss: 0.11304471\n",
            "iteration:  97 loss: 0.23036572\n",
            "iteration:  98 loss: 0.20729311\n",
            "iteration:  99 loss: 0.20746650\n",
            "iteration: 100 loss: 0.12423712\n",
            "iteration: 101 loss: 0.16980401\n",
            "iteration: 102 loss: 1.20147884\n",
            "iteration: 103 loss: 0.23876438\n",
            "iteration: 104 loss: 0.16742301\n",
            "iteration: 105 loss: 0.11507378\n",
            "iteration: 106 loss: 0.11306687\n",
            "iteration: 107 loss: 0.42555854\n",
            "iteration: 108 loss: 0.32513756\n",
            "iteration: 109 loss: 0.09923901\n",
            "iteration: 110 loss: 0.27394632\n",
            "iteration: 111 loss: 0.33176270\n",
            "iteration: 112 loss: 0.26126820\n",
            "iteration: 113 loss: 0.44988036\n",
            "iteration: 114 loss: 0.85097957\n",
            "iteration: 115 loss: 0.34917125\n",
            "iteration: 116 loss: 0.37601852\n",
            "iteration: 117 loss: 0.24393034\n",
            "iteration: 118 loss: 0.17892677\n",
            "iteration: 119 loss: 0.19161534\n",
            "iteration: 120 loss: 0.30843616\n",
            "iteration: 121 loss: 0.80722910\n",
            "iteration: 122 loss: 0.08225373\n",
            "iteration: 123 loss: 0.18243533\n",
            "iteration: 124 loss: 0.33392066\n",
            "iteration: 125 loss: 0.32309705\n",
            "iteration: 126 loss: 0.32894558\n",
            "iteration: 127 loss: 0.21329191\n",
            "iteration: 128 loss: 1.04210544\n",
            "iteration: 129 loss: 0.11998817\n",
            "iteration: 130 loss: 0.32355881\n",
            "iteration: 131 loss: 0.83402926\n",
            "iteration: 132 loss: 0.35320717\n",
            "iteration: 133 loss: 0.80828822\n",
            "iteration: 134 loss: 0.35135663\n",
            "iteration: 135 loss: 0.42877474\n",
            "iteration: 136 loss: 0.42548242\n",
            "iteration: 137 loss: 0.27776739\n",
            "iteration: 138 loss: 0.83908731\n",
            "iteration: 139 loss: 0.50221759\n",
            "iteration: 140 loss: 1.15149999\n",
            "iteration: 141 loss: 0.88840669\n",
            "iteration: 142 loss: 0.14313945\n",
            "iteration: 143 loss: 1.04672444\n",
            "iteration: 144 loss: 0.50195116\n",
            "iteration: 145 loss: 0.08919340\n",
            "iteration: 146 loss: 0.15951778\n",
            "iteration: 147 loss: 0.29079390\n",
            "iteration: 148 loss: 0.62926477\n",
            "iteration: 149 loss: 0.16567573\n",
            "iteration: 150 loss: 0.38960525\n",
            "iteration: 151 loss: 0.53334743\n",
            "iteration: 152 loss: 0.34828815\n",
            "iteration: 153 loss: 0.15458556\n",
            "iteration: 154 loss: 0.09333350\n",
            "iteration: 155 loss: 0.11005668\n",
            "iteration: 156 loss: 0.31263688\n",
            "iteration: 157 loss: 0.40754580\n",
            "iteration: 158 loss: 0.24955380\n",
            "iteration: 159 loss: 0.45355639\n",
            "iteration: 160 loss: 0.34576249\n",
            "iteration: 161 loss: 0.41569102\n",
            "iteration: 162 loss: 0.27196467\n",
            "iteration: 163 loss: 0.21021187\n",
            "iteration: 164 loss: 0.93935823\n",
            "iteration: 165 loss: 0.17828520\n",
            "iteration: 166 loss: 0.16873617\n",
            "iteration: 167 loss: 0.33393753\n",
            "iteration: 168 loss: 0.73826486\n",
            "iteration: 169 loss: 0.52726841\n",
            "iteration: 170 loss: 0.16644458\n",
            "iteration: 171 loss: 0.32113731\n",
            "iteration: 172 loss: 0.19426747\n",
            "iteration: 173 loss: 0.35495993\n",
            "iteration: 174 loss: 0.10230397\n",
            "iteration: 175 loss: 0.18129617\n",
            "iteration: 176 loss: 0.22312735\n",
            "iteration: 177 loss: 0.65580153\n",
            "iteration: 178 loss: 0.52841908\n",
            "iteration: 179 loss: 0.62567627\n",
            "iteration: 180 loss: 0.26822200\n",
            "iteration: 181 loss: 0.29843113\n",
            "iteration: 182 loss: 0.17222117\n",
            "iteration: 183 loss: 0.27110124\n",
            "iteration: 184 loss: 0.43752640\n",
            "iteration: 185 loss: 0.32923210\n",
            "iteration: 186 loss: 0.10538502\n",
            "iteration: 187 loss: 0.19488907\n",
            "iteration: 188 loss: 0.29438078\n",
            "iteration: 189 loss: 0.32762200\n",
            "iteration: 190 loss: 0.18211061\n",
            "iteration: 191 loss: 0.61342686\n",
            "iteration: 192 loss: 0.28219283\n",
            "iteration: 193 loss: 0.92515814\n",
            "iteration: 194 loss: 0.35891902\n",
            "iteration: 195 loss: 0.25886816\n",
            "iteration: 196 loss: 0.23160774\n",
            "iteration: 197 loss: 0.25679541\n",
            "iteration: 198 loss: 0.14190154\n",
            "iteration: 199 loss: 0.59497762\n",
            "epoch:  90 mean loss training: 0.36816803\n",
            "epoch:  90 mean loss validation: 1.25925982\n",
            "iteration:   0 loss: 0.24019498\n",
            "iteration:   1 loss: 0.08706880\n",
            "iteration:   2 loss: 0.13694812\n",
            "iteration:   3 loss: 0.75752211\n",
            "iteration:   4 loss: 0.30530244\n",
            "iteration:   5 loss: 0.22077520\n",
            "iteration:   6 loss: 0.12425822\n",
            "iteration:   7 loss: 0.20999897\n",
            "iteration:   8 loss: 0.59605336\n",
            "iteration:   9 loss: 0.68463445\n",
            "iteration:  10 loss: 0.30659357\n",
            "iteration:  11 loss: 1.62817907\n",
            "iteration:  12 loss: 0.27891484\n",
            "iteration:  13 loss: 0.36447594\n",
            "iteration:  14 loss: 0.09502554\n",
            "iteration:  15 loss: 0.22201215\n",
            "iteration:  16 loss: 0.34106609\n",
            "iteration:  17 loss: 0.52806383\n",
            "iteration:  18 loss: 0.24409376\n",
            "iteration:  19 loss: 0.14871481\n",
            "iteration:  20 loss: 0.20896167\n",
            "iteration:  21 loss: 0.15939921\n",
            "iteration:  22 loss: 0.47568849\n",
            "iteration:  23 loss: 0.38043720\n",
            "iteration:  24 loss: 0.23698595\n",
            "iteration:  25 loss: 0.33513474\n",
            "iteration:  26 loss: 0.11595227\n",
            "iteration:  27 loss: 0.14547336\n",
            "iteration:  28 loss: 0.27448997\n",
            "iteration:  29 loss: 0.11214778\n",
            "iteration:  30 loss: 0.13767073\n",
            "iteration:  31 loss: 0.11424339\n",
            "iteration:  32 loss: 0.14767012\n",
            "iteration:  33 loss: 0.12710105\n",
            "iteration:  34 loss: 0.29561943\n",
            "iteration:  35 loss: 0.94107097\n",
            "iteration:  36 loss: 0.30135819\n",
            "iteration:  37 loss: 0.41726440\n",
            "iteration:  38 loss: 0.25425187\n",
            "iteration:  39 loss: 0.75366712\n",
            "iteration:  40 loss: 0.21834373\n",
            "iteration:  41 loss: 0.25127646\n",
            "iteration:  42 loss: 0.33233145\n",
            "iteration:  43 loss: 0.21683156\n",
            "iteration:  44 loss: 1.08092844\n",
            "iteration:  45 loss: 0.46657017\n",
            "iteration:  46 loss: 0.19339618\n",
            "iteration:  47 loss: 0.34039289\n",
            "iteration:  48 loss: 0.16361549\n",
            "iteration:  49 loss: 0.14267245\n",
            "iteration:  50 loss: 0.39803404\n",
            "iteration:  51 loss: 0.35054463\n",
            "iteration:  52 loss: 0.12129320\n",
            "iteration:  53 loss: 0.14588737\n",
            "iteration:  54 loss: 0.09989569\n",
            "iteration:  55 loss: 0.18482192\n",
            "iteration:  56 loss: 1.02655029\n",
            "iteration:  57 loss: 0.85111523\n",
            "iteration:  58 loss: 0.77194470\n",
            "iteration:  59 loss: 0.14252460\n",
            "iteration:  60 loss: 0.12245256\n",
            "iteration:  61 loss: 0.55930090\n",
            "iteration:  62 loss: 0.55923551\n",
            "iteration:  63 loss: 0.19281499\n",
            "iteration:  64 loss: 0.28589791\n",
            "iteration:  65 loss: 0.69640118\n",
            "iteration:  66 loss: 0.19360834\n",
            "iteration:  67 loss: 0.80750239\n",
            "iteration:  68 loss: 0.28628856\n",
            "iteration:  69 loss: 0.17500351\n",
            "iteration:  70 loss: 0.42995167\n",
            "iteration:  71 loss: 0.15003787\n",
            "iteration:  72 loss: 0.26491347\n",
            "iteration:  73 loss: 0.23830536\n",
            "iteration:  74 loss: 0.22543737\n",
            "iteration:  75 loss: 0.23912956\n",
            "iteration:  76 loss: 0.35609740\n",
            "iteration:  77 loss: 0.28291807\n",
            "iteration:  78 loss: 0.26259533\n",
            "iteration:  79 loss: 0.98814410\n",
            "iteration:  80 loss: 0.15501067\n",
            "iteration:  81 loss: 1.43901861\n",
            "iteration:  82 loss: 0.11007543\n",
            "iteration:  83 loss: 0.32576790\n",
            "iteration:  84 loss: 0.20609750\n",
            "iteration:  85 loss: 0.39929909\n",
            "iteration:  86 loss: 0.11899062\n",
            "iteration:  87 loss: 0.15653600\n",
            "iteration:  88 loss: 0.17689541\n",
            "iteration:  89 loss: 0.18042463\n",
            "iteration:  90 loss: 0.22289659\n",
            "iteration:  91 loss: 0.68472379\n",
            "iteration:  92 loss: 0.25635093\n",
            "iteration:  93 loss: 0.40168235\n",
            "iteration:  94 loss: 0.20683384\n",
            "iteration:  95 loss: 0.25933936\n",
            "iteration:  96 loss: 0.53765970\n",
            "iteration:  97 loss: 1.03608131\n",
            "iteration:  98 loss: 0.46719873\n",
            "iteration:  99 loss: 0.19132604\n",
            "iteration: 100 loss: 0.25132480\n",
            "iteration: 101 loss: 0.21662429\n",
            "iteration: 102 loss: 0.86423457\n",
            "iteration: 103 loss: 0.27629250\n",
            "iteration: 104 loss: 0.23950139\n",
            "iteration: 105 loss: 0.34041533\n",
            "iteration: 106 loss: 0.44854140\n",
            "iteration: 107 loss: 0.34728405\n",
            "iteration: 108 loss: 0.16398971\n",
            "iteration: 109 loss: 0.13001807\n",
            "iteration: 110 loss: 0.37127006\n",
            "iteration: 111 loss: 0.37924385\n",
            "iteration: 112 loss: 0.08890377\n",
            "iteration: 113 loss: 0.77898133\n",
            "iteration: 114 loss: 0.10226377\n",
            "iteration: 115 loss: 0.14156675\n",
            "iteration: 116 loss: 0.10798801\n",
            "iteration: 117 loss: 0.17249751\n",
            "iteration: 118 loss: 0.29707366\n",
            "iteration: 119 loss: 0.13330308\n",
            "iteration: 120 loss: 0.72386330\n",
            "iteration: 121 loss: 0.43786836\n",
            "iteration: 122 loss: 0.35691833\n",
            "iteration: 123 loss: 0.10712700\n",
            "iteration: 124 loss: 1.70430136\n",
            "iteration: 125 loss: 0.65749937\n",
            "iteration: 126 loss: 1.04145026\n",
            "iteration: 127 loss: 0.13512963\n",
            "iteration: 128 loss: 0.33445477\n",
            "iteration: 129 loss: 0.85316634\n",
            "iteration: 130 loss: 0.24158838\n",
            "iteration: 131 loss: 0.19965699\n",
            "iteration: 132 loss: 0.32382408\n",
            "iteration: 133 loss: 0.14407241\n",
            "iteration: 134 loss: 0.46784067\n",
            "iteration: 135 loss: 0.24485117\n",
            "iteration: 136 loss: 0.33375487\n",
            "iteration: 137 loss: 0.24734929\n",
            "iteration: 138 loss: 0.22044916\n",
            "iteration: 139 loss: 0.18347985\n",
            "iteration: 140 loss: 0.95514309\n",
            "iteration: 141 loss: 0.43289676\n",
            "iteration: 142 loss: 0.33103868\n",
            "iteration: 143 loss: 1.62800813\n",
            "iteration: 144 loss: 0.30981365\n",
            "iteration: 145 loss: 0.15422523\n",
            "iteration: 146 loss: 0.36549196\n",
            "iteration: 147 loss: 0.12748060\n",
            "iteration: 148 loss: 0.10633734\n",
            "iteration: 149 loss: 0.69331861\n",
            "iteration: 150 loss: 0.40216264\n",
            "iteration: 151 loss: 0.24898084\n",
            "iteration: 152 loss: 0.23497272\n",
            "iteration: 153 loss: 0.16520731\n",
            "iteration: 154 loss: 0.79426938\n",
            "iteration: 155 loss: 0.24598747\n",
            "iteration: 156 loss: 0.16710594\n",
            "iteration: 157 loss: 0.08568569\n",
            "iteration: 158 loss: 0.24368131\n",
            "iteration: 159 loss: 0.75169218\n",
            "iteration: 160 loss: 0.55974627\n",
            "iteration: 161 loss: 0.58000076\n",
            "iteration: 162 loss: 1.02164662\n",
            "iteration: 163 loss: 0.13846828\n",
            "iteration: 164 loss: 0.09395411\n",
            "iteration: 165 loss: 0.23339279\n",
            "iteration: 166 loss: 0.76789534\n",
            "iteration: 167 loss: 0.36664888\n",
            "iteration: 168 loss: 0.16331367\n",
            "iteration: 169 loss: 0.29415977\n",
            "iteration: 170 loss: 0.49441707\n",
            "iteration: 171 loss: 0.21257044\n",
            "iteration: 172 loss: 0.39573857\n",
            "iteration: 173 loss: 0.59299171\n",
            "iteration: 174 loss: 0.61579984\n",
            "iteration: 175 loss: 0.24747822\n",
            "iteration: 176 loss: 0.13569501\n",
            "iteration: 177 loss: 0.83915138\n",
            "iteration: 178 loss: 0.22911425\n",
            "iteration: 179 loss: 0.18046901\n",
            "iteration: 180 loss: 0.78680259\n",
            "iteration: 181 loss: 0.22635928\n",
            "iteration: 182 loss: 0.21191859\n",
            "iteration: 183 loss: 0.19354124\n",
            "iteration: 184 loss: 0.16387643\n",
            "iteration: 185 loss: 0.68728811\n",
            "iteration: 186 loss: 0.99865967\n",
            "iteration: 187 loss: 0.58283329\n",
            "iteration: 188 loss: 0.15259993\n",
            "iteration: 189 loss: 0.64373511\n",
            "iteration: 190 loss: 0.40260646\n",
            "iteration: 191 loss: 0.10963610\n",
            "iteration: 192 loss: 0.16000104\n",
            "iteration: 193 loss: 0.12802102\n",
            "iteration: 194 loss: 0.28130209\n",
            "iteration: 195 loss: 0.25363624\n",
            "iteration: 196 loss: 1.11079037\n",
            "iteration: 197 loss: 0.36765376\n",
            "iteration: 198 loss: 0.09313063\n",
            "iteration: 199 loss: 0.17421491\n",
            "epoch:  91 mean loss training: 0.37473214\n",
            "epoch:  91 mean loss validation: 1.22977853\n",
            "iteration:   0 loss: 0.28198013\n",
            "iteration:   1 loss: 0.59685326\n",
            "iteration:   2 loss: 0.13640517\n",
            "iteration:   3 loss: 0.50215614\n",
            "iteration:   4 loss: 0.41396791\n",
            "iteration:   5 loss: 0.10102028\n",
            "iteration:   6 loss: 0.87447876\n",
            "iteration:   7 loss: 0.34814319\n",
            "iteration:   8 loss: 0.47501355\n",
            "iteration:   9 loss: 0.24702981\n",
            "iteration:  10 loss: 0.80829167\n",
            "iteration:  11 loss: 0.29793957\n",
            "iteration:  12 loss: 0.41297486\n",
            "iteration:  13 loss: 0.24725698\n",
            "iteration:  14 loss: 0.22477692\n",
            "iteration:  15 loss: 0.26686668\n",
            "iteration:  16 loss: 0.26593626\n",
            "iteration:  17 loss: 0.76696068\n",
            "iteration:  18 loss: 0.29773486\n",
            "iteration:  19 loss: 0.19177462\n",
            "iteration:  20 loss: 0.64946842\n",
            "iteration:  21 loss: 0.30709541\n",
            "iteration:  22 loss: 0.24552874\n",
            "iteration:  23 loss: 0.44094086\n",
            "iteration:  24 loss: 0.82494587\n",
            "iteration:  25 loss: 0.81575322\n",
            "iteration:  26 loss: 0.77509433\n",
            "iteration:  27 loss: 0.13748662\n",
            "iteration:  28 loss: 0.25240049\n",
            "iteration:  29 loss: 0.86201400\n",
            "iteration:  30 loss: 0.12715544\n",
            "iteration:  31 loss: 0.25115496\n",
            "iteration:  32 loss: 0.11276028\n",
            "iteration:  33 loss: 0.59073126\n",
            "iteration:  34 loss: 1.04512715\n",
            "iteration:  35 loss: 0.48353925\n",
            "iteration:  36 loss: 0.19568035\n",
            "iteration:  37 loss: 0.11330371\n",
            "iteration:  38 loss: 0.13233829\n",
            "iteration:  39 loss: 0.46562818\n",
            "iteration:  40 loss: 0.24943784\n",
            "iteration:  41 loss: 0.29182935\n",
            "iteration:  42 loss: 0.42762598\n",
            "iteration:  43 loss: 0.19728394\n",
            "iteration:  44 loss: 0.24814580\n",
            "iteration:  45 loss: 0.49061745\n",
            "iteration:  46 loss: 0.18390027\n",
            "iteration:  47 loss: 0.10844214\n",
            "iteration:  48 loss: 0.71609616\n",
            "iteration:  49 loss: 0.11703591\n",
            "iteration:  50 loss: 0.07346588\n",
            "iteration:  51 loss: 0.29036942\n",
            "iteration:  52 loss: 0.35421017\n",
            "iteration:  53 loss: 0.24817312\n",
            "iteration:  54 loss: 0.40451086\n",
            "iteration:  55 loss: 0.70914632\n",
            "iteration:  56 loss: 0.80219126\n",
            "iteration:  57 loss: 0.17732587\n",
            "iteration:  58 loss: 0.21319683\n",
            "iteration:  59 loss: 0.25319648\n",
            "iteration:  60 loss: 0.13074397\n",
            "iteration:  61 loss: 0.22231454\n",
            "iteration:  62 loss: 0.15956266\n",
            "iteration:  63 loss: 0.40670028\n",
            "iteration:  64 loss: 0.33392468\n",
            "iteration:  65 loss: 0.17247120\n",
            "iteration:  66 loss: 0.25935441\n",
            "iteration:  67 loss: 0.30575380\n",
            "iteration:  68 loss: 0.37195852\n",
            "iteration:  69 loss: 0.18871725\n",
            "iteration:  70 loss: 0.10738917\n",
            "iteration:  71 loss: 0.59639150\n",
            "iteration:  72 loss: 0.18848597\n",
            "iteration:  73 loss: 0.32543951\n",
            "iteration:  74 loss: 0.16251637\n",
            "iteration:  75 loss: 0.09818358\n",
            "iteration:  76 loss: 0.11142176\n",
            "iteration:  77 loss: 0.64888042\n",
            "iteration:  78 loss: 0.24095505\n",
            "iteration:  79 loss: 0.46517894\n",
            "iteration:  80 loss: 0.10761244\n",
            "iteration:  81 loss: 0.93606269\n",
            "iteration:  82 loss: 0.37478489\n",
            "iteration:  83 loss: 0.52328807\n",
            "iteration:  84 loss: 0.23856759\n",
            "iteration:  85 loss: 0.07530140\n",
            "iteration:  86 loss: 0.30434287\n",
            "iteration:  87 loss: 0.39976993\n",
            "iteration:  88 loss: 0.75351834\n",
            "iteration:  89 loss: 0.40373868\n",
            "iteration:  90 loss: 0.22328827\n",
            "iteration:  91 loss: 1.44759667\n",
            "iteration:  92 loss: 0.47228974\n",
            "iteration:  93 loss: 0.32265446\n",
            "iteration:  94 loss: 0.14382356\n",
            "iteration:  95 loss: 0.60334647\n",
            "iteration:  96 loss: 0.57242882\n",
            "iteration:  97 loss: 0.42461970\n",
            "iteration:  98 loss: 0.36969998\n",
            "iteration:  99 loss: 0.20219842\n",
            "iteration: 100 loss: 0.39914307\n",
            "iteration: 101 loss: 0.24602097\n",
            "iteration: 102 loss: 0.12858045\n",
            "iteration: 103 loss: 0.44274414\n",
            "iteration: 104 loss: 0.10238242\n",
            "iteration: 105 loss: 0.44615647\n",
            "iteration: 106 loss: 0.10569532\n",
            "iteration: 107 loss: 0.91264391\n",
            "iteration: 108 loss: 0.37276244\n",
            "iteration: 109 loss: 0.24505866\n",
            "iteration: 110 loss: 0.11214901\n",
            "iteration: 111 loss: 0.48771843\n",
            "iteration: 112 loss: 0.14504990\n",
            "iteration: 113 loss: 1.13169980\n",
            "iteration: 114 loss: 0.30942151\n",
            "iteration: 115 loss: 0.29495311\n",
            "iteration: 116 loss: 0.37166840\n",
            "iteration: 117 loss: 0.19784084\n",
            "iteration: 118 loss: 0.32218224\n",
            "iteration: 119 loss: 0.18952103\n",
            "iteration: 120 loss: 0.88544178\n",
            "iteration: 121 loss: 0.25229087\n",
            "iteration: 122 loss: 0.10134077\n",
            "iteration: 123 loss: 0.58134937\n",
            "iteration: 124 loss: 0.22135639\n",
            "iteration: 125 loss: 0.70166320\n",
            "iteration: 126 loss: 0.35843003\n",
            "iteration: 127 loss: 0.58507532\n",
            "iteration: 128 loss: 0.12349746\n",
            "iteration: 129 loss: 0.22132881\n",
            "iteration: 130 loss: 0.42580426\n",
            "iteration: 131 loss: 0.20788945\n",
            "iteration: 132 loss: 0.12011677\n",
            "iteration: 133 loss: 0.25344181\n",
            "iteration: 134 loss: 0.12442829\n",
            "iteration: 135 loss: 0.17627245\n",
            "iteration: 136 loss: 0.15450534\n",
            "iteration: 137 loss: 0.68629527\n",
            "iteration: 138 loss: 0.16068998\n",
            "iteration: 139 loss: 0.35371432\n",
            "iteration: 140 loss: 0.23553467\n",
            "iteration: 141 loss: 0.65110886\n",
            "iteration: 142 loss: 0.16563025\n",
            "iteration: 143 loss: 0.21644919\n",
            "iteration: 144 loss: 0.17551573\n",
            "iteration: 145 loss: 0.85786957\n",
            "iteration: 146 loss: 0.12628436\n",
            "iteration: 147 loss: 0.10916917\n",
            "iteration: 148 loss: 0.60794073\n",
            "iteration: 149 loss: 0.24669746\n",
            "iteration: 150 loss: 0.91924632\n",
            "iteration: 151 loss: 0.45926836\n",
            "iteration: 152 loss: 0.27506113\n",
            "iteration: 153 loss: 0.11008441\n",
            "iteration: 154 loss: 0.45122689\n",
            "iteration: 155 loss: 0.21070305\n",
            "iteration: 156 loss: 0.19075753\n",
            "iteration: 157 loss: 0.28336442\n",
            "iteration: 158 loss: 0.78780353\n",
            "iteration: 159 loss: 0.15976623\n",
            "iteration: 160 loss: 0.28961840\n",
            "iteration: 161 loss: 1.36951828\n",
            "iteration: 162 loss: 0.18056259\n",
            "iteration: 163 loss: 0.12738577\n",
            "iteration: 164 loss: 0.45093635\n",
            "iteration: 165 loss: 0.10782659\n",
            "iteration: 166 loss: 0.51536661\n",
            "iteration: 167 loss: 0.47758844\n",
            "iteration: 168 loss: 0.17171931\n",
            "iteration: 169 loss: 0.36746472\n",
            "iteration: 170 loss: 0.38543230\n",
            "iteration: 171 loss: 0.33143973\n",
            "iteration: 172 loss: 0.11597908\n",
            "iteration: 173 loss: 0.51622075\n",
            "iteration: 174 loss: 0.21068591\n",
            "iteration: 175 loss: 1.27106762\n",
            "iteration: 176 loss: 0.34590974\n",
            "iteration: 177 loss: 0.14210922\n",
            "iteration: 178 loss: 0.81656390\n",
            "iteration: 179 loss: 0.15761176\n",
            "iteration: 180 loss: 0.23659304\n",
            "iteration: 181 loss: 0.42761400\n",
            "iteration: 182 loss: 0.11682896\n",
            "iteration: 183 loss: 0.56500655\n",
            "iteration: 184 loss: 0.25681257\n",
            "iteration: 185 loss: 0.35202909\n",
            "iteration: 186 loss: 0.13254684\n",
            "iteration: 187 loss: 0.24208321\n",
            "iteration: 188 loss: 0.14739394\n",
            "iteration: 189 loss: 0.63170016\n",
            "iteration: 190 loss: 0.37287015\n",
            "iteration: 191 loss: 0.15170737\n",
            "iteration: 192 loss: 0.33431068\n",
            "iteration: 193 loss: 0.22928718\n",
            "iteration: 194 loss: 0.47723240\n",
            "iteration: 195 loss: 0.85643727\n",
            "iteration: 196 loss: 0.20421913\n",
            "iteration: 197 loss: 0.19851503\n",
            "iteration: 198 loss: 0.28320128\n",
            "iteration: 199 loss: 1.10733306\n",
            "epoch:  92 mean loss training: 0.37054864\n",
            "epoch:  92 mean loss validation: 1.25717413\n",
            "iteration:   0 loss: 0.41419083\n",
            "iteration:   1 loss: 0.16690728\n",
            "iteration:   2 loss: 0.21677175\n",
            "iteration:   3 loss: 0.62883049\n",
            "iteration:   4 loss: 1.04782403\n",
            "iteration:   5 loss: 0.10278587\n",
            "iteration:   6 loss: 0.36468381\n",
            "iteration:   7 loss: 0.12523445\n",
            "iteration:   8 loss: 0.26293936\n",
            "iteration:   9 loss: 0.15959531\n",
            "iteration:  10 loss: 0.17717654\n",
            "iteration:  11 loss: 0.19176513\n",
            "iteration:  12 loss: 0.52211004\n",
            "iteration:  13 loss: 0.36878830\n",
            "iteration:  14 loss: 0.44808835\n",
            "iteration:  15 loss: 0.30426937\n",
            "iteration:  16 loss: 0.35455066\n",
            "iteration:  17 loss: 0.15405485\n",
            "iteration:  18 loss: 0.22187930\n",
            "iteration:  19 loss: 0.43989319\n",
            "iteration:  20 loss: 0.15172116\n",
            "iteration:  21 loss: 0.28718236\n",
            "iteration:  22 loss: 0.11285440\n",
            "iteration:  23 loss: 0.17435774\n",
            "iteration:  24 loss: 0.64345360\n",
            "iteration:  25 loss: 0.45771140\n",
            "iteration:  26 loss: 0.74119592\n",
            "iteration:  27 loss: 0.22282057\n",
            "iteration:  28 loss: 0.18402065\n",
            "iteration:  29 loss: 0.18200822\n",
            "iteration:  30 loss: 0.15737911\n",
            "iteration:  31 loss: 0.22356135\n",
            "iteration:  32 loss: 0.31258884\n",
            "iteration:  33 loss: 0.17378297\n",
            "iteration:  34 loss: 0.09978177\n",
            "iteration:  35 loss: 0.30476198\n",
            "iteration:  36 loss: 0.85112029\n",
            "iteration:  37 loss: 1.58613861\n",
            "iteration:  38 loss: 0.33777237\n",
            "iteration:  39 loss: 0.19238205\n",
            "iteration:  40 loss: 0.09098409\n",
            "iteration:  41 loss: 0.23100521\n",
            "iteration:  42 loss: 0.41489232\n",
            "iteration:  43 loss: 0.97054762\n",
            "iteration:  44 loss: 0.15032855\n",
            "iteration:  45 loss: 0.23759857\n",
            "iteration:  46 loss: 0.18186504\n",
            "iteration:  47 loss: 0.16603459\n",
            "iteration:  48 loss: 0.81929576\n",
            "iteration:  49 loss: 0.16975079\n",
            "iteration:  50 loss: 0.15281597\n",
            "iteration:  51 loss: 0.09870628\n",
            "iteration:  52 loss: 0.27340755\n",
            "iteration:  53 loss: 0.26107609\n",
            "iteration:  54 loss: 0.38000649\n",
            "iteration:  55 loss: 0.34959754\n",
            "iteration:  56 loss: 0.71777093\n",
            "iteration:  57 loss: 0.64333850\n",
            "iteration:  58 loss: 0.87238622\n",
            "iteration:  59 loss: 0.11374378\n",
            "iteration:  60 loss: 0.65026098\n",
            "iteration:  61 loss: 0.25636488\n",
            "iteration:  62 loss: 0.49647549\n",
            "iteration:  63 loss: 0.25087264\n",
            "iteration:  64 loss: 1.31658196\n",
            "iteration:  65 loss: 0.38714620\n",
            "iteration:  66 loss: 0.33090663\n",
            "iteration:  67 loss: 0.32102349\n",
            "iteration:  68 loss: 0.17031638\n",
            "iteration:  69 loss: 0.41258779\n",
            "iteration:  70 loss: 1.25229466\n",
            "iteration:  71 loss: 0.09577890\n",
            "iteration:  72 loss: 0.16713971\n",
            "iteration:  73 loss: 0.67299587\n",
            "iteration:  74 loss: 0.51605916\n",
            "iteration:  75 loss: 0.09624946\n",
            "iteration:  76 loss: 0.23299995\n",
            "iteration:  77 loss: 0.40416157\n",
            "iteration:  78 loss: 0.20629853\n",
            "iteration:  79 loss: 0.18061092\n",
            "iteration:  80 loss: 0.19421397\n",
            "iteration:  81 loss: 0.11041003\n",
            "iteration:  82 loss: 0.27222741\n",
            "iteration:  83 loss: 0.12920852\n",
            "iteration:  84 loss: 0.57096469\n",
            "iteration:  85 loss: 0.12553182\n",
            "iteration:  86 loss: 0.27347744\n",
            "iteration:  87 loss: 0.60242456\n",
            "iteration:  88 loss: 1.05347753\n",
            "iteration:  89 loss: 0.80758297\n",
            "iteration:  90 loss: 0.22488853\n",
            "iteration:  91 loss: 0.17617385\n",
            "iteration:  92 loss: 0.10162579\n",
            "iteration:  93 loss: 0.64556301\n",
            "iteration:  94 loss: 0.37218633\n",
            "iteration:  95 loss: 0.98579049\n",
            "iteration:  96 loss: 0.38728052\n",
            "iteration:  97 loss: 0.13940987\n",
            "iteration:  98 loss: 0.20070741\n",
            "iteration:  99 loss: 0.27085325\n",
            "iteration: 100 loss: 0.18012942\n",
            "iteration: 101 loss: 0.19412693\n",
            "iteration: 102 loss: 0.20342338\n",
            "iteration: 103 loss: 0.24023637\n",
            "iteration: 104 loss: 0.59731984\n",
            "iteration: 105 loss: 0.44794083\n",
            "iteration: 106 loss: 0.41278273\n",
            "iteration: 107 loss: 0.23534511\n",
            "iteration: 108 loss: 0.23029268\n",
            "iteration: 109 loss: 0.24777031\n",
            "iteration: 110 loss: 0.15325977\n",
            "iteration: 111 loss: 0.10473727\n",
            "iteration: 112 loss: 0.37227875\n",
            "iteration: 113 loss: 0.09769599\n",
            "iteration: 114 loss: 0.10200849\n",
            "iteration: 115 loss: 0.11129285\n",
            "iteration: 116 loss: 0.68105108\n",
            "iteration: 117 loss: 0.78919059\n",
            "iteration: 118 loss: 0.16394459\n",
            "iteration: 119 loss: 0.58287221\n",
            "iteration: 120 loss: 0.62289464\n",
            "iteration: 121 loss: 0.20974961\n",
            "iteration: 122 loss: 0.66723514\n",
            "iteration: 123 loss: 0.19377415\n",
            "iteration: 124 loss: 0.37829375\n",
            "iteration: 125 loss: 0.28617561\n",
            "iteration: 126 loss: 0.35399619\n",
            "iteration: 127 loss: 0.27531093\n",
            "iteration: 128 loss: 0.15702334\n",
            "iteration: 129 loss: 0.30138353\n",
            "iteration: 130 loss: 0.15234387\n",
            "iteration: 131 loss: 0.23732227\n",
            "iteration: 132 loss: 0.25901210\n",
            "iteration: 133 loss: 0.07956983\n",
            "iteration: 134 loss: 0.23773181\n",
            "iteration: 135 loss: 0.25509933\n",
            "iteration: 136 loss: 0.84969938\n",
            "iteration: 137 loss: 0.22755900\n",
            "iteration: 138 loss: 0.45790339\n",
            "iteration: 139 loss: 0.87094790\n",
            "iteration: 140 loss: 0.32830894\n",
            "iteration: 141 loss: 0.07868823\n",
            "iteration: 142 loss: 0.22811684\n",
            "iteration: 143 loss: 0.24592271\n",
            "iteration: 144 loss: 0.21677160\n",
            "iteration: 145 loss: 0.54087460\n",
            "iteration: 146 loss: 0.16384700\n",
            "iteration: 147 loss: 0.30247098\n",
            "iteration: 148 loss: 0.34955886\n",
            "iteration: 149 loss: 0.50484884\n",
            "iteration: 150 loss: 0.17056172\n",
            "iteration: 151 loss: 0.97266424\n",
            "iteration: 152 loss: 0.51441014\n",
            "iteration: 153 loss: 0.75587213\n",
            "iteration: 154 loss: 0.16380382\n",
            "iteration: 155 loss: 0.31745386\n",
            "iteration: 156 loss: 0.23727643\n",
            "iteration: 157 loss: 0.17209968\n",
            "iteration: 158 loss: 0.65841043\n",
            "iteration: 159 loss: 0.72308642\n",
            "iteration: 160 loss: 0.34430382\n",
            "iteration: 161 loss: 0.39489058\n",
            "iteration: 162 loss: 0.63332629\n",
            "iteration: 163 loss: 0.10658623\n",
            "iteration: 164 loss: 0.10736136\n",
            "iteration: 165 loss: 1.18457496\n",
            "iteration: 166 loss: 0.41086614\n",
            "iteration: 167 loss: 0.67146790\n",
            "iteration: 168 loss: 0.31150234\n",
            "iteration: 169 loss: 0.47086808\n",
            "iteration: 170 loss: 0.33865619\n",
            "iteration: 171 loss: 0.24531940\n",
            "iteration: 172 loss: 0.10070266\n",
            "iteration: 173 loss: 0.73356241\n",
            "iteration: 174 loss: 0.54022449\n",
            "iteration: 175 loss: 0.14607653\n",
            "iteration: 176 loss: 0.19695476\n",
            "iteration: 177 loss: 0.51665747\n",
            "iteration: 178 loss: 0.51099527\n",
            "iteration: 179 loss: 0.21279365\n",
            "iteration: 180 loss: 1.10763800\n",
            "iteration: 181 loss: 0.86438000\n",
            "iteration: 182 loss: 0.19975010\n",
            "iteration: 183 loss: 0.73623788\n",
            "iteration: 184 loss: 0.41317639\n",
            "iteration: 185 loss: 0.54987419\n",
            "iteration: 186 loss: 0.34991691\n",
            "iteration: 187 loss: 0.18034703\n",
            "iteration: 188 loss: 0.35642397\n",
            "iteration: 189 loss: 0.26623607\n",
            "iteration: 190 loss: 0.37384576\n",
            "iteration: 191 loss: 0.14148659\n",
            "iteration: 192 loss: 0.31897885\n",
            "iteration: 193 loss: 0.41734368\n",
            "iteration: 194 loss: 0.99808151\n",
            "iteration: 195 loss: 0.31623888\n",
            "iteration: 196 loss: 0.15835749\n",
            "iteration: 197 loss: 0.54670638\n",
            "iteration: 198 loss: 0.22370857\n",
            "iteration: 199 loss: 0.24410678\n",
            "epoch:  93 mean loss training: 0.37615758\n",
            "epoch:  93 mean loss validation: 1.22606885\n",
            "iteration:   0 loss: 0.13345701\n",
            "iteration:   1 loss: 0.54211694\n",
            "iteration:   2 loss: 0.79382253\n",
            "iteration:   3 loss: 0.23391518\n",
            "iteration:   4 loss: 0.43867239\n",
            "iteration:   5 loss: 1.21013165\n",
            "iteration:   6 loss: 0.25277093\n",
            "iteration:   7 loss: 0.39181340\n",
            "iteration:   8 loss: 0.34424430\n",
            "iteration:   9 loss: 0.38038245\n",
            "iteration:  10 loss: 0.18173952\n",
            "iteration:  11 loss: 0.26151258\n",
            "iteration:  12 loss: 0.87336659\n",
            "iteration:  13 loss: 0.16827038\n",
            "iteration:  14 loss: 0.12024732\n",
            "iteration:  15 loss: 0.31924617\n",
            "iteration:  16 loss: 1.03509259\n",
            "iteration:  17 loss: 0.27871940\n",
            "iteration:  18 loss: 0.12176878\n",
            "iteration:  19 loss: 0.52483314\n",
            "iteration:  20 loss: 0.62780619\n",
            "iteration:  21 loss: 0.14198256\n",
            "iteration:  22 loss: 0.63654751\n",
            "iteration:  23 loss: 0.23271294\n",
            "iteration:  24 loss: 0.11151600\n",
            "iteration:  25 loss: 0.08723703\n",
            "iteration:  26 loss: 0.17813250\n",
            "iteration:  27 loss: 0.37006629\n",
            "iteration:  28 loss: 0.15378749\n",
            "iteration:  29 loss: 0.90977836\n",
            "iteration:  30 loss: 0.13604841\n",
            "iteration:  31 loss: 0.14937282\n",
            "iteration:  32 loss: 0.14406128\n",
            "iteration:  33 loss: 0.13176228\n",
            "iteration:  34 loss: 0.24810624\n",
            "iteration:  35 loss: 0.43347085\n",
            "iteration:  36 loss: 0.45402434\n",
            "iteration:  37 loss: 0.35046524\n",
            "iteration:  38 loss: 0.23007333\n",
            "iteration:  39 loss: 0.22076973\n",
            "iteration:  40 loss: 0.22921011\n",
            "iteration:  41 loss: 0.32696524\n",
            "iteration:  42 loss: 0.47877777\n",
            "iteration:  43 loss: 0.16769668\n",
            "iteration:  44 loss: 0.45400983\n",
            "iteration:  45 loss: 0.71963155\n",
            "iteration:  46 loss: 0.13332577\n",
            "iteration:  47 loss: 0.29334301\n",
            "iteration:  48 loss: 0.23016757\n",
            "iteration:  49 loss: 0.32031035\n",
            "iteration:  50 loss: 0.33679992\n",
            "iteration:  51 loss: 0.73915219\n",
            "iteration:  52 loss: 0.17576802\n",
            "iteration:  53 loss: 0.24145472\n",
            "iteration:  54 loss: 0.15344106\n",
            "iteration:  55 loss: 0.21671097\n",
            "iteration:  56 loss: 0.28158441\n",
            "iteration:  57 loss: 0.17818367\n",
            "iteration:  58 loss: 0.31536227\n",
            "iteration:  59 loss: 0.12874922\n",
            "iteration:  60 loss: 0.13439941\n",
            "iteration:  61 loss: 0.31848222\n",
            "iteration:  62 loss: 0.66649437\n",
            "iteration:  63 loss: 0.10811986\n",
            "iteration:  64 loss: 0.23972906\n",
            "iteration:  65 loss: 0.53085542\n",
            "iteration:  66 loss: 0.26753268\n",
            "iteration:  67 loss: 0.30081812\n",
            "iteration:  68 loss: 0.19884425\n",
            "iteration:  69 loss: 0.32726875\n",
            "iteration:  70 loss: 1.13990593\n",
            "iteration:  71 loss: 0.30551478\n",
            "iteration:  72 loss: 0.31142271\n",
            "iteration:  73 loss: 0.44227025\n",
            "iteration:  74 loss: 0.28096610\n",
            "iteration:  75 loss: 0.16796485\n",
            "iteration:  76 loss: 0.19191518\n",
            "iteration:  77 loss: 0.37797704\n",
            "iteration:  78 loss: 0.30296093\n",
            "iteration:  79 loss: 0.34823340\n",
            "iteration:  80 loss: 0.22747785\n",
            "iteration:  81 loss: 0.27238637\n",
            "iteration:  82 loss: 0.22505939\n",
            "iteration:  83 loss: 0.29221597\n",
            "iteration:  84 loss: 0.24777216\n",
            "iteration:  85 loss: 0.11095797\n",
            "iteration:  86 loss: 0.93256086\n",
            "iteration:  87 loss: 0.40706131\n",
            "iteration:  88 loss: 0.20041163\n",
            "iteration:  89 loss: 0.13922003\n",
            "iteration:  90 loss: 0.18271145\n",
            "iteration:  91 loss: 0.27153420\n",
            "iteration:  92 loss: 0.18265012\n",
            "iteration:  93 loss: 0.12634145\n",
            "iteration:  94 loss: 0.13326725\n",
            "iteration:  95 loss: 0.14147294\n",
            "iteration:  96 loss: 0.28221905\n",
            "iteration:  97 loss: 0.17159665\n",
            "iteration:  98 loss: 0.20134795\n",
            "iteration:  99 loss: 0.11517935\n",
            "iteration: 100 loss: 0.49493885\n",
            "iteration: 101 loss: 0.08137742\n",
            "iteration: 102 loss: 0.61161309\n",
            "iteration: 103 loss: 0.27829844\n",
            "iteration: 104 loss: 0.13471955\n",
            "iteration: 105 loss: 0.92747766\n",
            "iteration: 106 loss: 0.56000727\n",
            "iteration: 107 loss: 0.28050122\n",
            "iteration: 108 loss: 0.30157188\n",
            "iteration: 109 loss: 0.37950650\n",
            "iteration: 110 loss: 0.28216159\n",
            "iteration: 111 loss: 0.32126009\n",
            "iteration: 112 loss: 1.11988735\n",
            "iteration: 113 loss: 0.74609137\n",
            "iteration: 114 loss: 0.11801294\n",
            "iteration: 115 loss: 0.29515076\n",
            "iteration: 116 loss: 0.86708355\n",
            "iteration: 117 loss: 0.10331213\n",
            "iteration: 118 loss: 0.13618809\n",
            "iteration: 119 loss: 0.27604041\n",
            "iteration: 120 loss: 0.42741364\n",
            "iteration: 121 loss: 0.29718801\n",
            "iteration: 122 loss: 0.11897121\n",
            "iteration: 123 loss: 0.21714026\n",
            "iteration: 124 loss: 0.42700359\n",
            "iteration: 125 loss: 0.10086586\n",
            "iteration: 126 loss: 0.44986200\n",
            "iteration: 127 loss: 0.97215927\n",
            "iteration: 128 loss: 0.15563345\n",
            "iteration: 129 loss: 0.49258131\n",
            "iteration: 130 loss: 0.16236097\n",
            "iteration: 131 loss: 0.58155733\n",
            "iteration: 132 loss: 0.35619161\n",
            "iteration: 133 loss: 0.32092309\n",
            "iteration: 134 loss: 0.51035225\n",
            "iteration: 135 loss: 0.09207769\n",
            "iteration: 136 loss: 0.20188937\n",
            "iteration: 137 loss: 0.34534770\n",
            "iteration: 138 loss: 0.37553507\n",
            "iteration: 139 loss: 0.47882736\n",
            "iteration: 140 loss: 0.15218239\n",
            "iteration: 141 loss: 0.41416857\n",
            "iteration: 142 loss: 0.29281941\n",
            "iteration: 143 loss: 0.47538325\n",
            "iteration: 144 loss: 0.14423887\n",
            "iteration: 145 loss: 0.23204494\n",
            "iteration: 146 loss: 0.59340715\n",
            "iteration: 147 loss: 0.50522423\n",
            "iteration: 148 loss: 0.17911452\n",
            "iteration: 149 loss: 1.11126184\n",
            "iteration: 150 loss: 1.26569748\n",
            "iteration: 151 loss: 0.13240239\n",
            "iteration: 152 loss: 0.10506156\n",
            "iteration: 153 loss: 0.33400208\n",
            "iteration: 154 loss: 0.10911725\n",
            "iteration: 155 loss: 0.49534667\n",
            "iteration: 156 loss: 0.21274494\n",
            "iteration: 157 loss: 0.25493035\n",
            "iteration: 158 loss: 0.35212922\n",
            "iteration: 159 loss: 0.07780990\n",
            "iteration: 160 loss: 0.13300923\n",
            "iteration: 161 loss: 0.89971185\n",
            "iteration: 162 loss: 0.43823570\n",
            "iteration: 163 loss: 0.11547974\n",
            "iteration: 164 loss: 0.30133805\n",
            "iteration: 165 loss: 1.23397791\n",
            "iteration: 166 loss: 0.44596374\n",
            "iteration: 167 loss: 0.14386395\n",
            "iteration: 168 loss: 0.08909552\n",
            "iteration: 169 loss: 0.12243904\n",
            "iteration: 170 loss: 0.31355417\n",
            "iteration: 171 loss: 0.90602380\n",
            "iteration: 172 loss: 0.23485996\n",
            "iteration: 173 loss: 0.10234658\n",
            "iteration: 174 loss: 0.89433092\n",
            "iteration: 175 loss: 1.01025450\n",
            "iteration: 176 loss: 0.10874747\n",
            "iteration: 177 loss: 0.13680035\n",
            "iteration: 178 loss: 0.35160589\n",
            "iteration: 179 loss: 0.16181870\n",
            "iteration: 180 loss: 0.31898040\n",
            "iteration: 181 loss: 0.17447384\n",
            "iteration: 182 loss: 0.21853238\n",
            "iteration: 183 loss: 0.24085414\n",
            "iteration: 184 loss: 0.42380780\n",
            "iteration: 185 loss: 0.08881044\n",
            "iteration: 186 loss: 0.33061293\n",
            "iteration: 187 loss: 0.10888070\n",
            "iteration: 188 loss: 0.21818317\n",
            "iteration: 189 loss: 0.21752718\n",
            "iteration: 190 loss: 0.37528953\n",
            "iteration: 191 loss: 0.63981348\n",
            "iteration: 192 loss: 0.34150094\n",
            "iteration: 193 loss: 0.27372044\n",
            "iteration: 194 loss: 0.82444054\n",
            "iteration: 195 loss: 0.27024332\n",
            "iteration: 196 loss: 1.32341647\n",
            "iteration: 197 loss: 0.20206967\n",
            "iteration: 198 loss: 0.51074225\n",
            "iteration: 199 loss: 0.32407808\n",
            "epoch:  94 mean loss training: 0.35453606\n",
            "epoch:  94 mean loss validation: 1.29867327\n",
            "iteration:   0 loss: 0.45363709\n",
            "iteration:   1 loss: 0.13640627\n",
            "iteration:   2 loss: 0.24773495\n",
            "iteration:   3 loss: 0.21530566\n",
            "iteration:   4 loss: 0.18571909\n",
            "iteration:   5 loss: 0.59481937\n",
            "iteration:   6 loss: 0.40602291\n",
            "iteration:   7 loss: 0.10433514\n",
            "iteration:   8 loss: 0.22524093\n",
            "iteration:   9 loss: 0.10480800\n",
            "iteration:  10 loss: 0.69388318\n",
            "iteration:  11 loss: 0.25500143\n",
            "iteration:  12 loss: 0.95526725\n",
            "iteration:  13 loss: 0.25742027\n",
            "iteration:  14 loss: 0.22266155\n",
            "iteration:  15 loss: 0.24652846\n",
            "iteration:  16 loss: 0.37347451\n",
            "iteration:  17 loss: 0.34846437\n",
            "iteration:  18 loss: 0.79311311\n",
            "iteration:  19 loss: 0.24432045\n",
            "iteration:  20 loss: 0.31942976\n",
            "iteration:  21 loss: 0.42047298\n",
            "iteration:  22 loss: 0.40505105\n",
            "iteration:  23 loss: 0.15499750\n",
            "iteration:  24 loss: 0.88339138\n",
            "iteration:  25 loss: 0.13659488\n",
            "iteration:  26 loss: 0.60511577\n",
            "iteration:  27 loss: 0.10144760\n",
            "iteration:  28 loss: 1.75847054\n",
            "iteration:  29 loss: 0.31822148\n",
            "iteration:  30 loss: 0.16386315\n",
            "iteration:  31 loss: 0.25017959\n",
            "iteration:  32 loss: 0.19043502\n",
            "iteration:  33 loss: 0.14494604\n",
            "iteration:  34 loss: 0.13512199\n",
            "iteration:  35 loss: 0.46701062\n",
            "iteration:  36 loss: 0.33773047\n",
            "iteration:  37 loss: 0.22962970\n",
            "iteration:  38 loss: 0.11994097\n",
            "iteration:  39 loss: 0.84154737\n",
            "iteration:  40 loss: 0.38360229\n",
            "iteration:  41 loss: 0.99993128\n",
            "iteration:  42 loss: 0.40354282\n",
            "iteration:  43 loss: 0.16086927\n",
            "iteration:  44 loss: 0.43447903\n",
            "iteration:  45 loss: 0.85557818\n",
            "iteration:  46 loss: 0.87779307\n",
            "iteration:  47 loss: 0.43604389\n",
            "iteration:  48 loss: 0.09442545\n",
            "iteration:  49 loss: 0.13973927\n",
            "iteration:  50 loss: 0.78760922\n",
            "iteration:  51 loss: 0.44501418\n",
            "iteration:  52 loss: 0.11631394\n",
            "iteration:  53 loss: 1.10145152\n",
            "iteration:  54 loss: 0.21676517\n",
            "iteration:  55 loss: 0.36437672\n",
            "iteration:  56 loss: 0.64249027\n",
            "iteration:  57 loss: 0.61769617\n",
            "iteration:  58 loss: 0.41987962\n",
            "iteration:  59 loss: 0.10391853\n",
            "iteration:  60 loss: 0.22736029\n",
            "iteration:  61 loss: 0.13360322\n",
            "iteration:  62 loss: 0.67396724\n",
            "iteration:  63 loss: 0.13543725\n",
            "iteration:  64 loss: 0.16012022\n",
            "iteration:  65 loss: 0.14517075\n",
            "iteration:  66 loss: 0.17731017\n",
            "iteration:  67 loss: 0.24044997\n",
            "iteration:  68 loss: 0.12385266\n",
            "iteration:  69 loss: 0.11224300\n",
            "iteration:  70 loss: 0.19067526\n",
            "iteration:  71 loss: 0.87880319\n",
            "iteration:  72 loss: 0.17366141\n",
            "iteration:  73 loss: 0.21737230\n",
            "iteration:  74 loss: 1.05269599\n",
            "iteration:  75 loss: 0.14330736\n",
            "iteration:  76 loss: 0.43699244\n",
            "iteration:  77 loss: 0.19374272\n",
            "iteration:  78 loss: 0.18899454\n",
            "iteration:  79 loss: 0.23501363\n",
            "iteration:  80 loss: 0.42210463\n",
            "iteration:  81 loss: 0.27382082\n",
            "iteration:  82 loss: 1.07357442\n",
            "iteration:  83 loss: 0.17101619\n",
            "iteration:  84 loss: 0.81534177\n",
            "iteration:  85 loss: 1.06432545\n",
            "iteration:  86 loss: 0.26455730\n",
            "iteration:  87 loss: 0.18317601\n",
            "iteration:  88 loss: 0.65959489\n",
            "iteration:  89 loss: 0.48235148\n",
            "iteration:  90 loss: 0.20297974\n",
            "iteration:  91 loss: 0.17415568\n",
            "iteration:  92 loss: 0.30031037\n",
            "iteration:  93 loss: 0.37835124\n",
            "iteration:  94 loss: 0.30226943\n",
            "iteration:  95 loss: 0.54214418\n",
            "iteration:  96 loss: 0.42263195\n",
            "iteration:  97 loss: 0.45194793\n",
            "iteration:  98 loss: 0.18187028\n",
            "iteration:  99 loss: 0.38430747\n",
            "iteration: 100 loss: 0.58950329\n",
            "iteration: 101 loss: 0.21956982\n",
            "iteration: 102 loss: 0.36465123\n",
            "iteration: 103 loss: 0.32262337\n",
            "iteration: 104 loss: 0.10639581\n",
            "iteration: 105 loss: 0.21897167\n",
            "iteration: 106 loss: 0.12642758\n",
            "iteration: 107 loss: 0.15551142\n",
            "iteration: 108 loss: 0.34428722\n",
            "iteration: 109 loss: 0.42766353\n",
            "iteration: 110 loss: 0.33697045\n",
            "iteration: 111 loss: 0.71658337\n",
            "iteration: 112 loss: 0.22297177\n",
            "iteration: 113 loss: 0.54891348\n",
            "iteration: 114 loss: 0.53034878\n",
            "iteration: 115 loss: 0.87018865\n",
            "iteration: 116 loss: 0.13756926\n",
            "iteration: 117 loss: 0.27342162\n",
            "iteration: 118 loss: 0.14327167\n",
            "iteration: 119 loss: 0.74687058\n",
            "iteration: 120 loss: 0.51169902\n",
            "iteration: 121 loss: 0.24202769\n",
            "iteration: 122 loss: 0.23650503\n",
            "iteration: 123 loss: 0.20141639\n",
            "iteration: 124 loss: 0.51490045\n",
            "iteration: 125 loss: 0.22477156\n",
            "iteration: 126 loss: 0.30841193\n",
            "iteration: 127 loss: 0.22839165\n",
            "iteration: 128 loss: 0.32002744\n",
            "iteration: 129 loss: 0.17110057\n",
            "iteration: 130 loss: 0.48127541\n",
            "iteration: 131 loss: 0.31927332\n",
            "iteration: 132 loss: 0.32205865\n",
            "iteration: 133 loss: 0.16698091\n",
            "iteration: 134 loss: 0.24719694\n",
            "iteration: 135 loss: 0.76673669\n",
            "iteration: 136 loss: 0.18830928\n",
            "iteration: 137 loss: 0.17825319\n",
            "iteration: 138 loss: 0.49622017\n",
            "iteration: 139 loss: 0.58533728\n",
            "iteration: 140 loss: 0.12960318\n",
            "iteration: 141 loss: 0.33638644\n",
            "iteration: 142 loss: 0.24765743\n",
            "iteration: 143 loss: 0.80833375\n",
            "iteration: 144 loss: 0.13348860\n",
            "iteration: 145 loss: 0.61745125\n",
            "iteration: 146 loss: 0.23565806\n",
            "iteration: 147 loss: 0.45891398\n",
            "iteration: 148 loss: 0.45527655\n",
            "iteration: 149 loss: 0.22702169\n",
            "iteration: 150 loss: 0.36689994\n",
            "iteration: 151 loss: 0.22618844\n",
            "iteration: 152 loss: 0.45884657\n",
            "iteration: 153 loss: 0.44729096\n",
            "iteration: 154 loss: 1.02524936\n",
            "iteration: 155 loss: 0.21338211\n",
            "iteration: 156 loss: 0.17703389\n",
            "iteration: 157 loss: 0.23228690\n",
            "iteration: 158 loss: 0.10178426\n",
            "iteration: 159 loss: 0.33774543\n",
            "iteration: 160 loss: 0.23178320\n",
            "iteration: 161 loss: 0.80942291\n",
            "iteration: 162 loss: 0.61312413\n",
            "iteration: 163 loss: 0.19488169\n",
            "iteration: 164 loss: 1.41382790\n",
            "iteration: 165 loss: 0.30781832\n",
            "iteration: 166 loss: 0.37010875\n",
            "iteration: 167 loss: 0.87949967\n",
            "iteration: 168 loss: 0.09087054\n",
            "iteration: 169 loss: 0.21714084\n",
            "iteration: 170 loss: 0.73571205\n",
            "iteration: 171 loss: 0.28760874\n",
            "iteration: 172 loss: 0.24996284\n",
            "iteration: 173 loss: 0.81595755\n",
            "iteration: 174 loss: 0.20160717\n",
            "iteration: 175 loss: 0.24854594\n",
            "iteration: 176 loss: 0.19676667\n",
            "iteration: 177 loss: 0.18801725\n",
            "iteration: 178 loss: 0.39776525\n",
            "iteration: 179 loss: 1.22597182\n",
            "iteration: 180 loss: 0.15481870\n",
            "iteration: 181 loss: 0.21098831\n",
            "iteration: 182 loss: 0.87737685\n",
            "iteration: 183 loss: 0.12978077\n",
            "iteration: 184 loss: 0.19209443\n",
            "iteration: 185 loss: 0.51127940\n",
            "iteration: 186 loss: 1.46430504\n",
            "iteration: 187 loss: 0.21277432\n",
            "iteration: 188 loss: 0.17091858\n",
            "iteration: 189 loss: 1.31163561\n",
            "iteration: 190 loss: 0.23007396\n",
            "iteration: 191 loss: 0.19929653\n",
            "iteration: 192 loss: 0.07790563\n",
            "iteration: 193 loss: 0.33967400\n",
            "iteration: 194 loss: 0.98793912\n",
            "iteration: 195 loss: 0.38694084\n",
            "iteration: 196 loss: 0.21277378\n",
            "iteration: 197 loss: 0.25111094\n",
            "iteration: 198 loss: 0.10069211\n",
            "iteration: 199 loss: 0.32094842\n",
            "epoch:  95 mean loss training: 0.39305362\n",
            "epoch:  95 mean loss validation: 1.26924706\n",
            "iteration:   0 loss: 1.15164006\n",
            "iteration:   1 loss: 0.35615441\n",
            "iteration:   2 loss: 0.34656551\n",
            "iteration:   3 loss: 0.11291096\n",
            "iteration:   4 loss: 0.54433620\n",
            "iteration:   5 loss: 0.09312506\n",
            "iteration:   6 loss: 0.32641351\n",
            "iteration:   7 loss: 0.11147393\n",
            "iteration:   8 loss: 0.58508813\n",
            "iteration:   9 loss: 0.19586477\n",
            "iteration:  10 loss: 0.12589785\n",
            "iteration:  11 loss: 0.11149119\n",
            "iteration:  12 loss: 0.15938869\n",
            "iteration:  13 loss: 0.19282600\n",
            "iteration:  14 loss: 0.10203004\n",
            "iteration:  15 loss: 0.12866485\n",
            "iteration:  16 loss: 0.27425426\n",
            "iteration:  17 loss: 1.14529812\n",
            "iteration:  18 loss: 0.42108366\n",
            "iteration:  19 loss: 0.29761955\n",
            "iteration:  20 loss: 0.37725365\n",
            "iteration:  21 loss: 0.09607564\n",
            "iteration:  22 loss: 0.23050696\n",
            "iteration:  23 loss: 0.51487112\n",
            "iteration:  24 loss: 0.50051820\n",
            "iteration:  25 loss: 0.14093028\n",
            "iteration:  26 loss: 0.22687501\n",
            "iteration:  27 loss: 0.39606610\n",
            "iteration:  28 loss: 0.18084969\n",
            "iteration:  29 loss: 0.26203528\n",
            "iteration:  30 loss: 0.38695419\n",
            "iteration:  31 loss: 0.15581146\n",
            "iteration:  32 loss: 0.17135617\n",
            "iteration:  33 loss: 0.09539020\n",
            "iteration:  34 loss: 0.11413185\n",
            "iteration:  35 loss: 0.50235885\n",
            "iteration:  36 loss: 0.87306839\n",
            "iteration:  37 loss: 0.47537184\n",
            "iteration:  38 loss: 0.33448747\n",
            "iteration:  39 loss: 0.54018420\n",
            "iteration:  40 loss: 0.12549809\n",
            "iteration:  41 loss: 0.92121387\n",
            "iteration:  42 loss: 0.22703552\n",
            "iteration:  43 loss: 0.33475658\n",
            "iteration:  44 loss: 0.62033916\n",
            "iteration:  45 loss: 0.49206066\n",
            "iteration:  46 loss: 0.25502014\n",
            "iteration:  47 loss: 0.43695551\n",
            "iteration:  48 loss: 0.14929824\n",
            "iteration:  49 loss: 1.62998426\n",
            "iteration:  50 loss: 0.58245075\n",
            "iteration:  51 loss: 0.19859830\n",
            "iteration:  52 loss: 0.18232414\n",
            "iteration:  53 loss: 0.19624364\n",
            "iteration:  54 loss: 0.99461472\n",
            "iteration:  55 loss: 0.19462225\n",
            "iteration:  56 loss: 0.40687808\n",
            "iteration:  57 loss: 0.16802435\n",
            "iteration:  58 loss: 0.12233582\n",
            "iteration:  59 loss: 0.24628890\n",
            "iteration:  60 loss: 0.27911076\n",
            "iteration:  61 loss: 0.21552721\n",
            "iteration:  62 loss: 0.29810432\n",
            "iteration:  63 loss: 0.22179960\n",
            "iteration:  64 loss: 0.24966551\n",
            "iteration:  65 loss: 0.34526601\n",
            "iteration:  66 loss: 0.41521791\n",
            "iteration:  67 loss: 0.42724520\n",
            "iteration:  68 loss: 0.28514367\n",
            "iteration:  69 loss: 0.14204313\n",
            "iteration:  70 loss: 0.20335831\n",
            "iteration:  71 loss: 0.78734857\n",
            "iteration:  72 loss: 0.35351539\n",
            "iteration:  73 loss: 0.40104166\n",
            "iteration:  74 loss: 0.16235530\n",
            "iteration:  75 loss: 0.10273901\n",
            "iteration:  76 loss: 0.70583516\n",
            "iteration:  77 loss: 0.20402589\n",
            "iteration:  78 loss: 0.35193777\n",
            "iteration:  79 loss: 0.53957117\n",
            "iteration:  80 loss: 0.26174235\n",
            "iteration:  81 loss: 0.10962754\n",
            "iteration:  82 loss: 0.93213397\n",
            "iteration:  83 loss: 0.13836241\n",
            "iteration:  84 loss: 0.11693116\n",
            "iteration:  85 loss: 0.41251901\n",
            "iteration:  86 loss: 0.81132025\n",
            "iteration:  87 loss: 0.93900758\n",
            "iteration:  88 loss: 1.53391528\n",
            "iteration:  89 loss: 0.48094517\n",
            "iteration:  90 loss: 0.27912804\n",
            "iteration:  91 loss: 0.28200692\n",
            "iteration:  92 loss: 0.33993739\n",
            "iteration:  93 loss: 0.08545062\n",
            "iteration:  94 loss: 0.53052187\n",
            "iteration:  95 loss: 0.08435817\n",
            "iteration:  96 loss: 0.57063389\n",
            "iteration:  97 loss: 0.32947993\n",
            "iteration:  98 loss: 0.29735529\n",
            "iteration:  99 loss: 0.14280254\n",
            "iteration: 100 loss: 0.24579158\n",
            "iteration: 101 loss: 0.12786867\n",
            "iteration: 102 loss: 0.16963059\n",
            "iteration: 103 loss: 0.79102486\n",
            "iteration: 104 loss: 0.89550215\n",
            "iteration: 105 loss: 0.12736322\n",
            "iteration: 106 loss: 0.29321054\n",
            "iteration: 107 loss: 0.47580031\n",
            "iteration: 108 loss: 0.11781397\n",
            "iteration: 109 loss: 0.41289005\n",
            "iteration: 110 loss: 0.65944541\n",
            "iteration: 111 loss: 0.51553452\n",
            "iteration: 112 loss: 0.50503731\n",
            "iteration: 113 loss: 0.07591935\n",
            "iteration: 114 loss: 0.26094332\n",
            "iteration: 115 loss: 0.75227898\n",
            "iteration: 116 loss: 1.09195745\n",
            "iteration: 117 loss: 0.27446899\n",
            "iteration: 118 loss: 0.66833305\n",
            "iteration: 119 loss: 1.16776884\n",
            "iteration: 120 loss: 0.09233500\n",
            "iteration: 121 loss: 0.27479067\n",
            "iteration: 122 loss: 0.21528018\n",
            "iteration: 123 loss: 1.53341532\n",
            "iteration: 124 loss: 0.17708345\n",
            "iteration: 125 loss: 0.35987693\n",
            "iteration: 126 loss: 0.18115711\n",
            "iteration: 127 loss: 0.36037076\n",
            "iteration: 128 loss: 0.13282625\n",
            "iteration: 129 loss: 0.43027130\n",
            "iteration: 130 loss: 0.29617292\n",
            "iteration: 131 loss: 0.39162016\n",
            "iteration: 132 loss: 0.10935870\n",
            "iteration: 133 loss: 0.21252462\n",
            "iteration: 134 loss: 0.17799407\n",
            "iteration: 135 loss: 0.50182128\n",
            "iteration: 136 loss: 0.38409773\n",
            "iteration: 137 loss: 0.43318599\n",
            "iteration: 138 loss: 0.20072326\n",
            "iteration: 139 loss: 0.07677455\n",
            "iteration: 140 loss: 1.38906515\n",
            "iteration: 141 loss: 0.26247719\n",
            "iteration: 142 loss: 0.16700210\n",
            "iteration: 143 loss: 0.59187168\n",
            "iteration: 144 loss: 0.51928681\n",
            "iteration: 145 loss: 0.16311394\n",
            "iteration: 146 loss: 0.10249846\n",
            "iteration: 147 loss: 0.11543888\n",
            "iteration: 148 loss: 0.28251457\n",
            "iteration: 149 loss: 0.08535007\n",
            "iteration: 150 loss: 0.47959438\n",
            "iteration: 151 loss: 1.03071082\n",
            "iteration: 152 loss: 0.88213110\n",
            "iteration: 153 loss: 0.16391478\n",
            "iteration: 154 loss: 0.44288504\n",
            "iteration: 155 loss: 0.48690620\n",
            "iteration: 156 loss: 0.11273143\n",
            "iteration: 157 loss: 0.11184387\n",
            "iteration: 158 loss: 0.17642829\n",
            "iteration: 159 loss: 0.61896479\n",
            "iteration: 160 loss: 0.25440398\n",
            "iteration: 161 loss: 0.36860278\n",
            "iteration: 162 loss: 0.72118312\n",
            "iteration: 163 loss: 0.25783572\n",
            "iteration: 164 loss: 0.15677719\n",
            "iteration: 165 loss: 0.50441766\n",
            "iteration: 166 loss: 0.33556536\n",
            "iteration: 167 loss: 0.24739185\n",
            "iteration: 168 loss: 0.35395330\n",
            "iteration: 169 loss: 0.45509616\n",
            "iteration: 170 loss: 0.35726765\n",
            "iteration: 171 loss: 0.44548461\n",
            "iteration: 172 loss: 0.23162812\n",
            "iteration: 173 loss: 0.36997920\n",
            "iteration: 174 loss: 0.33270213\n",
            "iteration: 175 loss: 0.19575188\n",
            "iteration: 176 loss: 0.92251235\n",
            "iteration: 177 loss: 0.57969946\n",
            "iteration: 178 loss: 0.52288312\n",
            "iteration: 179 loss: 0.33167008\n",
            "iteration: 180 loss: 0.23097518\n",
            "iteration: 181 loss: 0.26358560\n",
            "iteration: 182 loss: 0.19100738\n",
            "iteration: 183 loss: 0.09243879\n",
            "iteration: 184 loss: 0.22726341\n",
            "iteration: 185 loss: 0.83997691\n",
            "iteration: 186 loss: 0.22269112\n",
            "iteration: 187 loss: 0.52868444\n",
            "iteration: 188 loss: 0.19374974\n",
            "iteration: 189 loss: 0.15772638\n",
            "iteration: 190 loss: 0.32774645\n",
            "iteration: 191 loss: 0.28646040\n",
            "iteration: 192 loss: 0.64063096\n",
            "iteration: 193 loss: 0.37951446\n",
            "iteration: 194 loss: 0.13748367\n",
            "iteration: 195 loss: 0.40036410\n",
            "iteration: 196 loss: 0.65636480\n",
            "iteration: 197 loss: 0.31361213\n",
            "iteration: 198 loss: 0.96371579\n",
            "iteration: 199 loss: 0.16754155\n",
            "epoch:  96 mean loss training: 0.38366067\n",
            "epoch:  96 mean loss validation: 1.24478352\n",
            "iteration:   0 loss: 0.23196983\n",
            "iteration:   1 loss: 0.11582495\n",
            "iteration:   2 loss: 0.22383755\n",
            "iteration:   3 loss: 0.35185790\n",
            "iteration:   4 loss: 0.10403517\n",
            "iteration:   5 loss: 0.94462663\n",
            "iteration:   6 loss: 0.16775994\n",
            "iteration:   7 loss: 0.10346453\n",
            "iteration:   8 loss: 0.86074704\n",
            "iteration:   9 loss: 1.13655901\n",
            "iteration:  10 loss: 1.40101361\n",
            "iteration:  11 loss: 0.10913219\n",
            "iteration:  12 loss: 0.30605829\n",
            "iteration:  13 loss: 0.35204783\n",
            "iteration:  14 loss: 0.11930117\n",
            "iteration:  15 loss: 0.18260106\n",
            "iteration:  16 loss: 0.26054168\n",
            "iteration:  17 loss: 0.14286560\n",
            "iteration:  18 loss: 0.33518288\n",
            "iteration:  19 loss: 0.51253670\n",
            "iteration:  20 loss: 0.09763953\n",
            "iteration:  21 loss: 0.69264543\n",
            "iteration:  22 loss: 0.66367018\n",
            "iteration:  23 loss: 0.10304691\n",
            "iteration:  24 loss: 0.64975286\n",
            "iteration:  25 loss: 0.26651359\n",
            "iteration:  26 loss: 0.37140107\n",
            "iteration:  27 loss: 0.23105332\n",
            "iteration:  28 loss: 0.13076989\n",
            "iteration:  29 loss: 0.42195103\n",
            "iteration:  30 loss: 0.30055946\n",
            "iteration:  31 loss: 0.25201941\n",
            "iteration:  32 loss: 0.49730161\n",
            "iteration:  33 loss: 1.11991990\n",
            "iteration:  34 loss: 0.21595742\n",
            "iteration:  35 loss: 0.12272288\n",
            "iteration:  36 loss: 1.46134663\n",
            "iteration:  37 loss: 0.29678017\n",
            "iteration:  38 loss: 0.21846882\n",
            "iteration:  39 loss: 0.36998370\n",
            "iteration:  40 loss: 0.21680740\n",
            "iteration:  41 loss: 0.13582763\n",
            "iteration:  42 loss: 0.19366586\n",
            "iteration:  43 loss: 0.59999609\n",
            "iteration:  44 loss: 0.31020287\n",
            "iteration:  45 loss: 0.13584673\n",
            "iteration:  46 loss: 0.38002384\n",
            "iteration:  47 loss: 0.25077835\n",
            "iteration:  48 loss: 0.26297343\n",
            "iteration:  49 loss: 0.41514355\n",
            "iteration:  50 loss: 0.16316471\n",
            "iteration:  51 loss: 0.15582187\n",
            "iteration:  52 loss: 0.12655860\n",
            "iteration:  53 loss: 0.21478921\n",
            "iteration:  54 loss: 0.85816497\n",
            "iteration:  55 loss: 0.19702142\n",
            "iteration:  56 loss: 1.05553365\n",
            "iteration:  57 loss: 0.19225684\n",
            "iteration:  58 loss: 0.14000642\n",
            "iteration:  59 loss: 0.27640450\n",
            "iteration:  60 loss: 0.43890426\n",
            "iteration:  61 loss: 0.21061236\n",
            "iteration:  62 loss: 0.14402650\n",
            "iteration:  63 loss: 0.16198683\n",
            "iteration:  64 loss: 0.32335436\n",
            "iteration:  65 loss: 0.60921717\n",
            "iteration:  66 loss: 0.38809162\n",
            "iteration:  67 loss: 0.76243389\n",
            "iteration:  68 loss: 0.08845755\n",
            "iteration:  69 loss: 0.16998243\n",
            "iteration:  70 loss: 0.17987651\n",
            "iteration:  71 loss: 0.24541029\n",
            "iteration:  72 loss: 0.86851269\n",
            "iteration:  73 loss: 0.29980129\n",
            "iteration:  74 loss: 0.91528118\n",
            "iteration:  75 loss: 0.48930663\n",
            "iteration:  76 loss: 0.09503752\n",
            "iteration:  77 loss: 0.29359522\n",
            "iteration:  78 loss: 0.16316077\n",
            "iteration:  79 loss: 0.32126090\n",
            "iteration:  80 loss: 0.14995512\n",
            "iteration:  81 loss: 0.25833750\n",
            "iteration:  82 loss: 0.10521099\n",
            "iteration:  83 loss: 0.57676351\n",
            "iteration:  84 loss: 0.25168750\n",
            "iteration:  85 loss: 0.31001225\n",
            "iteration:  86 loss: 0.46206552\n",
            "iteration:  87 loss: 0.08614062\n",
            "iteration:  88 loss: 0.42880782\n",
            "iteration:  89 loss: 0.60655510\n",
            "iteration:  90 loss: 0.50375748\n",
            "iteration:  91 loss: 0.24715854\n",
            "iteration:  92 loss: 0.10574995\n",
            "iteration:  93 loss: 0.61190873\n",
            "iteration:  94 loss: 0.09962789\n",
            "iteration:  95 loss: 0.37689850\n",
            "iteration:  96 loss: 0.23148888\n",
            "iteration:  97 loss: 0.37700525\n",
            "iteration:  98 loss: 0.66691601\n",
            "iteration:  99 loss: 0.94683301\n",
            "iteration: 100 loss: 0.14194758\n",
            "iteration: 101 loss: 0.27819037\n",
            "iteration: 102 loss: 0.17461750\n",
            "iteration: 103 loss: 0.29777849\n",
            "iteration: 104 loss: 0.31528464\n",
            "iteration: 105 loss: 0.77608907\n",
            "iteration: 106 loss: 0.17153725\n",
            "iteration: 107 loss: 0.28359529\n",
            "iteration: 108 loss: 0.50687492\n",
            "iteration: 109 loss: 0.25343904\n",
            "iteration: 110 loss: 0.13638833\n",
            "iteration: 111 loss: 0.09868968\n",
            "iteration: 112 loss: 0.45661607\n",
            "iteration: 113 loss: 0.77150208\n",
            "iteration: 114 loss: 0.27506214\n",
            "iteration: 115 loss: 0.47023708\n",
            "iteration: 116 loss: 0.23091950\n",
            "iteration: 117 loss: 0.14590950\n",
            "iteration: 118 loss: 0.62847894\n",
            "iteration: 119 loss: 0.25254267\n",
            "iteration: 120 loss: 0.23186147\n",
            "iteration: 121 loss: 0.50506681\n",
            "iteration: 122 loss: 0.36407110\n",
            "iteration: 123 loss: 0.12091385\n",
            "iteration: 124 loss: 1.08301139\n",
            "iteration: 125 loss: 0.25721285\n",
            "iteration: 126 loss: 0.33672661\n",
            "iteration: 127 loss: 0.29548937\n",
            "iteration: 128 loss: 0.36961976\n",
            "iteration: 129 loss: 0.79266161\n",
            "iteration: 130 loss: 0.12895164\n",
            "iteration: 131 loss: 0.19328101\n",
            "iteration: 132 loss: 0.60968387\n",
            "iteration: 133 loss: 0.22303027\n",
            "iteration: 134 loss: 0.32874188\n",
            "iteration: 135 loss: 0.49408397\n",
            "iteration: 136 loss: 0.98229343\n",
            "iteration: 137 loss: 0.56878108\n",
            "iteration: 138 loss: 0.20007381\n",
            "iteration: 139 loss: 0.23844571\n",
            "iteration: 140 loss: 0.18017898\n",
            "iteration: 141 loss: 0.18274274\n",
            "iteration: 142 loss: 0.40118647\n",
            "iteration: 143 loss: 0.18387404\n",
            "iteration: 144 loss: 0.48062581\n",
            "iteration: 145 loss: 0.46491298\n",
            "iteration: 146 loss: 0.15277512\n",
            "iteration: 147 loss: 1.19495702\n",
            "iteration: 148 loss: 0.41188163\n",
            "iteration: 149 loss: 0.17754701\n",
            "iteration: 150 loss: 0.09264836\n",
            "iteration: 151 loss: 0.72805887\n",
            "iteration: 152 loss: 0.46192372\n",
            "iteration: 153 loss: 0.35363120\n",
            "iteration: 154 loss: 0.22590213\n",
            "iteration: 155 loss: 0.12416369\n",
            "iteration: 156 loss: 0.21016780\n",
            "iteration: 157 loss: 0.11360502\n",
            "iteration: 158 loss: 0.32015872\n",
            "iteration: 159 loss: 0.37323907\n",
            "iteration: 160 loss: 0.12118214\n",
            "iteration: 161 loss: 0.76682615\n",
            "iteration: 162 loss: 0.54943132\n",
            "iteration: 163 loss: 0.19197895\n",
            "iteration: 164 loss: 0.13781592\n",
            "iteration: 165 loss: 0.45801044\n",
            "iteration: 166 loss: 0.06979206\n",
            "iteration: 167 loss: 0.24670964\n",
            "iteration: 168 loss: 0.23285851\n",
            "iteration: 169 loss: 0.13084526\n",
            "iteration: 170 loss: 0.54418409\n",
            "iteration: 171 loss: 0.09751746\n",
            "iteration: 172 loss: 0.14224924\n",
            "iteration: 173 loss: 1.13054633\n",
            "iteration: 174 loss: 0.41287121\n",
            "iteration: 175 loss: 0.12855586\n",
            "iteration: 176 loss: 0.12338093\n",
            "iteration: 177 loss: 0.31144628\n",
            "iteration: 178 loss: 0.42337409\n",
            "iteration: 179 loss: 0.13165480\n",
            "iteration: 180 loss: 0.14156099\n",
            "iteration: 181 loss: 0.41505292\n",
            "iteration: 182 loss: 0.11842611\n",
            "iteration: 183 loss: 0.32488692\n",
            "iteration: 184 loss: 0.28325462\n",
            "iteration: 185 loss: 0.40069914\n",
            "iteration: 186 loss: 0.21354705\n",
            "iteration: 187 loss: 0.35435888\n",
            "iteration: 188 loss: 0.15589620\n",
            "iteration: 189 loss: 0.69073641\n",
            "iteration: 190 loss: 0.15596381\n",
            "iteration: 191 loss: 0.81579876\n",
            "iteration: 192 loss: 0.18933865\n",
            "iteration: 193 loss: 0.14149126\n",
            "iteration: 194 loss: 0.72183830\n",
            "iteration: 195 loss: 0.64591891\n",
            "iteration: 196 loss: 0.25298342\n",
            "iteration: 197 loss: 0.09053886\n",
            "iteration: 198 loss: 0.21876301\n",
            "iteration: 199 loss: 0.36959663\n",
            "epoch:  97 mean loss training: 0.36086705\n",
            "epoch:  97 mean loss validation: 1.24333322\n",
            "iteration:   0 loss: 0.10454982\n",
            "iteration:   1 loss: 0.59942710\n",
            "iteration:   2 loss: 0.59376615\n",
            "iteration:   3 loss: 0.92293864\n",
            "iteration:   4 loss: 0.21415341\n",
            "iteration:   5 loss: 0.51404703\n",
            "iteration:   6 loss: 0.24738652\n",
            "iteration:   7 loss: 0.23005599\n",
            "iteration:   8 loss: 0.32361990\n",
            "iteration:   9 loss: 0.22822325\n",
            "iteration:  10 loss: 1.08892381\n",
            "iteration:  11 loss: 0.18104592\n",
            "iteration:  12 loss: 0.52600038\n",
            "iteration:  13 loss: 0.17779902\n",
            "iteration:  14 loss: 0.40465403\n",
            "iteration:  15 loss: 0.59094256\n",
            "iteration:  16 loss: 0.11871955\n",
            "iteration:  17 loss: 0.13166878\n",
            "iteration:  18 loss: 1.25641418\n",
            "iteration:  19 loss: 0.30787697\n",
            "iteration:  20 loss: 0.36919364\n",
            "iteration:  21 loss: 0.73296303\n",
            "iteration:  22 loss: 0.09418038\n",
            "iteration:  23 loss: 0.07502407\n",
            "iteration:  24 loss: 0.09281073\n",
            "iteration:  25 loss: 0.44177091\n",
            "iteration:  26 loss: 0.40128973\n",
            "iteration:  27 loss: 0.19697756\n",
            "iteration:  28 loss: 0.72538131\n",
            "iteration:  29 loss: 0.08086713\n",
            "iteration:  30 loss: 0.14139044\n",
            "iteration:  31 loss: 0.50585157\n",
            "iteration:  32 loss: 0.57316387\n",
            "iteration:  33 loss: 0.15663125\n",
            "iteration:  34 loss: 0.11590028\n",
            "iteration:  35 loss: 1.11150730\n",
            "iteration:  36 loss: 0.14693213\n",
            "iteration:  37 loss: 0.45358908\n",
            "iteration:  38 loss: 0.40174150\n",
            "iteration:  39 loss: 0.13596669\n",
            "iteration:  40 loss: 0.31736845\n",
            "iteration:  41 loss: 0.39806786\n",
            "iteration:  42 loss: 0.30532011\n",
            "iteration:  43 loss: 0.39832097\n",
            "iteration:  44 loss: 0.32757169\n",
            "iteration:  45 loss: 0.22041714\n",
            "iteration:  46 loss: 0.59816623\n",
            "iteration:  47 loss: 0.15739009\n",
            "iteration:  48 loss: 0.36428687\n",
            "iteration:  49 loss: 0.23953360\n",
            "iteration:  50 loss: 0.45674402\n",
            "iteration:  51 loss: 0.12920989\n",
            "iteration:  52 loss: 0.26916066\n",
            "iteration:  53 loss: 0.09482152\n",
            "iteration:  54 loss: 0.35540146\n",
            "iteration:  55 loss: 0.48937553\n",
            "iteration:  56 loss: 0.22610441\n",
            "iteration:  57 loss: 0.41474023\n",
            "iteration:  58 loss: 0.16949986\n",
            "iteration:  59 loss: 1.95332491\n",
            "iteration:  60 loss: 0.36264664\n",
            "iteration:  61 loss: 0.25239190\n",
            "iteration:  62 loss: 0.16840565\n",
            "iteration:  63 loss: 0.23905736\n",
            "iteration:  64 loss: 1.03015530\n",
            "iteration:  65 loss: 1.02697372\n",
            "iteration:  66 loss: 0.10795373\n",
            "iteration:  67 loss: 0.48696250\n",
            "iteration:  68 loss: 0.25956011\n",
            "iteration:  69 loss: 0.29757851\n",
            "iteration:  70 loss: 0.40686032\n",
            "iteration:  71 loss: 0.50171727\n",
            "iteration:  72 loss: 0.15618624\n",
            "iteration:  73 loss: 0.87972718\n",
            "iteration:  74 loss: 0.83882141\n",
            "iteration:  75 loss: 0.16453840\n",
            "iteration:  76 loss: 0.14013350\n",
            "iteration:  77 loss: 0.50630468\n",
            "iteration:  78 loss: 0.16690025\n",
            "iteration:  79 loss: 0.12191810\n",
            "iteration:  80 loss: 0.12213147\n",
            "iteration:  81 loss: 0.07432319\n",
            "iteration:  82 loss: 0.22486028\n",
            "iteration:  83 loss: 0.24046111\n",
            "iteration:  84 loss: 0.79530168\n",
            "iteration:  85 loss: 0.77696592\n",
            "iteration:  86 loss: 0.10427863\n",
            "iteration:  87 loss: 0.32128036\n",
            "iteration:  88 loss: 0.44060469\n",
            "iteration:  89 loss: 0.71683943\n",
            "iteration:  90 loss: 0.13376707\n",
            "iteration:  91 loss: 0.07679304\n",
            "iteration:  92 loss: 0.23575477\n",
            "iteration:  93 loss: 0.41456631\n",
            "iteration:  94 loss: 0.71972209\n",
            "iteration:  95 loss: 0.08893256\n",
            "iteration:  96 loss: 0.11117598\n",
            "iteration:  97 loss: 0.14003983\n",
            "iteration:  98 loss: 0.41694888\n",
            "iteration:  99 loss: 0.14481236\n",
            "iteration: 100 loss: 0.62724423\n",
            "iteration: 101 loss: 0.35804862\n",
            "iteration: 102 loss: 0.46553081\n",
            "iteration: 103 loss: 0.32068822\n",
            "iteration: 104 loss: 0.90432227\n",
            "iteration: 105 loss: 0.12983152\n",
            "iteration: 106 loss: 0.34288785\n",
            "iteration: 107 loss: 0.22043288\n",
            "iteration: 108 loss: 0.86131853\n",
            "iteration: 109 loss: 0.92911446\n",
            "iteration: 110 loss: 0.21602333\n",
            "iteration: 111 loss: 0.63994360\n",
            "iteration: 112 loss: 0.43101940\n",
            "iteration: 113 loss: 0.14393349\n",
            "iteration: 114 loss: 0.57947463\n",
            "iteration: 115 loss: 0.31026125\n",
            "iteration: 116 loss: 0.24862036\n",
            "iteration: 117 loss: 0.23077875\n",
            "iteration: 118 loss: 0.35205013\n",
            "iteration: 119 loss: 1.10400915\n",
            "iteration: 120 loss: 0.20974025\n",
            "iteration: 121 loss: 1.18873966\n",
            "iteration: 122 loss: 0.24669778\n",
            "iteration: 123 loss: 1.08948708\n",
            "iteration: 124 loss: 0.20176998\n",
            "iteration: 125 loss: 0.20374900\n",
            "iteration: 126 loss: 0.33740461\n",
            "iteration: 127 loss: 0.22970422\n",
            "iteration: 128 loss: 0.14486110\n",
            "iteration: 129 loss: 0.39947149\n",
            "iteration: 130 loss: 0.25071931\n",
            "iteration: 131 loss: 0.26582718\n",
            "iteration: 132 loss: 0.14768538\n",
            "iteration: 133 loss: 0.39085251\n",
            "iteration: 134 loss: 0.09940895\n",
            "iteration: 135 loss: 0.18183172\n",
            "iteration: 136 loss: 0.23167817\n",
            "iteration: 137 loss: 0.45187253\n",
            "iteration: 138 loss: 0.12729755\n",
            "iteration: 139 loss: 0.22601597\n",
            "iteration: 140 loss: 0.27707055\n",
            "iteration: 141 loss: 0.10495097\n",
            "iteration: 142 loss: 0.30109751\n",
            "iteration: 143 loss: 0.67079765\n",
            "iteration: 144 loss: 0.37578583\n",
            "iteration: 145 loss: 0.22378278\n",
            "iteration: 146 loss: 0.30429602\n",
            "iteration: 147 loss: 0.90009350\n",
            "iteration: 148 loss: 0.18498705\n",
            "iteration: 149 loss: 0.10549558\n",
            "iteration: 150 loss: 0.32883132\n",
            "iteration: 151 loss: 0.15740712\n",
            "iteration: 152 loss: 0.37022144\n",
            "iteration: 153 loss: 0.89841223\n",
            "iteration: 154 loss: 0.25457200\n",
            "iteration: 155 loss: 1.00995862\n",
            "iteration: 156 loss: 0.29335350\n",
            "iteration: 157 loss: 0.15439247\n",
            "iteration: 158 loss: 0.28630924\n",
            "iteration: 159 loss: 0.11294727\n",
            "iteration: 160 loss: 0.16846010\n",
            "iteration: 161 loss: 0.15435252\n",
            "iteration: 162 loss: 0.31420362\n",
            "iteration: 163 loss: 0.59572434\n",
            "iteration: 164 loss: 0.08917724\n",
            "iteration: 165 loss: 0.34740138\n",
            "iteration: 166 loss: 0.18727328\n",
            "iteration: 167 loss: 0.30856544\n",
            "iteration: 168 loss: 0.11862250\n",
            "iteration: 169 loss: 0.16216643\n",
            "iteration: 170 loss: 0.15929617\n",
            "iteration: 171 loss: 0.25848478\n",
            "iteration: 172 loss: 0.24351951\n",
            "iteration: 173 loss: 0.09405623\n",
            "iteration: 174 loss: 0.24106354\n",
            "iteration: 175 loss: 0.15297993\n",
            "iteration: 176 loss: 0.47970814\n",
            "iteration: 177 loss: 0.27673453\n",
            "iteration: 178 loss: 0.36590463\n",
            "iteration: 179 loss: 0.12735595\n",
            "iteration: 180 loss: 0.48251823\n",
            "iteration: 181 loss: 0.22350180\n",
            "iteration: 182 loss: 0.57022351\n",
            "iteration: 183 loss: 0.20413753\n",
            "iteration: 184 loss: 0.52220631\n",
            "iteration: 185 loss: 0.95623392\n",
            "iteration: 186 loss: 0.15662666\n",
            "iteration: 187 loss: 0.14039719\n",
            "iteration: 188 loss: 0.37675169\n",
            "iteration: 189 loss: 0.14063452\n",
            "iteration: 190 loss: 0.85587573\n",
            "iteration: 191 loss: 0.39066288\n",
            "iteration: 192 loss: 0.66197860\n",
            "iteration: 193 loss: 0.44756240\n",
            "iteration: 194 loss: 0.16504289\n",
            "iteration: 195 loss: 0.17563334\n",
            "iteration: 196 loss: 0.47471038\n",
            "iteration: 197 loss: 0.12623093\n",
            "iteration: 198 loss: 0.13850574\n",
            "iteration: 199 loss: 0.23907900\n",
            "epoch:  98 mean loss training: 0.36831596\n",
            "epoch:  98 mean loss validation: 1.26272142\n",
            "iteration:   0 loss: 0.27096725\n",
            "iteration:   1 loss: 0.67819101\n",
            "iteration:   2 loss: 0.38131517\n",
            "iteration:   3 loss: 0.42677587\n",
            "iteration:   4 loss: 0.10781774\n",
            "iteration:   5 loss: 0.78691578\n",
            "iteration:   6 loss: 0.27038071\n",
            "iteration:   7 loss: 0.12241831\n",
            "iteration:   8 loss: 0.80978203\n",
            "iteration:   9 loss: 0.90543818\n",
            "iteration:  10 loss: 0.09289427\n",
            "iteration:  11 loss: 0.20458335\n",
            "iteration:  12 loss: 0.24172232\n",
            "iteration:  13 loss: 0.70483547\n",
            "iteration:  14 loss: 0.14323844\n",
            "iteration:  15 loss: 0.46801251\n",
            "iteration:  16 loss: 0.28648555\n",
            "iteration:  17 loss: 1.25432169\n",
            "iteration:  18 loss: 0.21043894\n",
            "iteration:  19 loss: 0.25263959\n",
            "iteration:  20 loss: 0.97083127\n",
            "iteration:  21 loss: 0.28908902\n",
            "iteration:  22 loss: 0.30673847\n",
            "iteration:  23 loss: 0.60031188\n",
            "iteration:  24 loss: 0.41575453\n",
            "iteration:  25 loss: 0.18611631\n",
            "iteration:  26 loss: 0.24293455\n",
            "iteration:  27 loss: 0.24753691\n",
            "iteration:  28 loss: 0.44216615\n",
            "iteration:  29 loss: 0.38427377\n",
            "iteration:  30 loss: 0.46516946\n",
            "iteration:  31 loss: 0.18287718\n",
            "iteration:  32 loss: 0.57704312\n",
            "iteration:  33 loss: 0.09520516\n",
            "iteration:  34 loss: 0.66133404\n",
            "iteration:  35 loss: 0.29793364\n",
            "iteration:  36 loss: 0.69284374\n",
            "iteration:  37 loss: 0.78421772\n",
            "iteration:  38 loss: 1.20846379\n",
            "iteration:  39 loss: 0.40571955\n",
            "iteration:  40 loss: 0.57829040\n",
            "iteration:  41 loss: 0.12471383\n",
            "iteration:  42 loss: 0.67841464\n",
            "iteration:  43 loss: 0.50981992\n",
            "iteration:  44 loss: 0.43731314\n",
            "iteration:  45 loss: 0.42057809\n",
            "iteration:  46 loss: 0.18059982\n",
            "iteration:  47 loss: 0.20387781\n",
            "iteration:  48 loss: 0.10781755\n",
            "iteration:  49 loss: 0.65422434\n",
            "iteration:  50 loss: 0.57737505\n",
            "iteration:  51 loss: 0.23054999\n",
            "iteration:  52 loss: 0.23772329\n",
            "iteration:  53 loss: 0.08657350\n",
            "iteration:  54 loss: 0.36044019\n",
            "iteration:  55 loss: 0.21087784\n",
            "iteration:  56 loss: 0.26695278\n",
            "iteration:  57 loss: 0.48941097\n",
            "iteration:  58 loss: 0.24460030\n",
            "iteration:  59 loss: 0.87906569\n",
            "iteration:  60 loss: 0.10619586\n",
            "iteration:  61 loss: 0.17119530\n",
            "iteration:  62 loss: 0.32537463\n",
            "iteration:  63 loss: 0.11030000\n",
            "iteration:  64 loss: 0.14980923\n",
            "iteration:  65 loss: 0.10778745\n",
            "iteration:  66 loss: 0.20105192\n",
            "iteration:  67 loss: 0.34491876\n",
            "iteration:  68 loss: 1.32217228\n",
            "iteration:  69 loss: 0.38705668\n",
            "iteration:  70 loss: 0.15825546\n",
            "iteration:  71 loss: 0.21491227\n",
            "iteration:  72 loss: 0.19312607\n",
            "iteration:  73 loss: 0.12226856\n",
            "iteration:  74 loss: 0.19318990\n",
            "iteration:  75 loss: 0.26836330\n",
            "iteration:  76 loss: 0.55028731\n",
            "iteration:  77 loss: 0.34726036\n",
            "iteration:  78 loss: 0.15297920\n",
            "iteration:  79 loss: 0.38451540\n",
            "iteration:  80 loss: 0.40871316\n",
            "iteration:  81 loss: 0.64507222\n",
            "iteration:  82 loss: 0.28544742\n",
            "iteration:  83 loss: 0.38919437\n",
            "iteration:  84 loss: 0.20311975\n",
            "iteration:  85 loss: 0.11267202\n",
            "iteration:  86 loss: 0.18227199\n",
            "iteration:  87 loss: 0.19791928\n",
            "iteration:  88 loss: 0.37494594\n",
            "iteration:  89 loss: 0.72399092\n",
            "iteration:  90 loss: 0.13580844\n",
            "iteration:  91 loss: 0.26759011\n",
            "iteration:  92 loss: 0.32120770\n",
            "iteration:  93 loss: 0.25671306\n",
            "iteration:  94 loss: 0.18887962\n",
            "iteration:  95 loss: 0.37902907\n",
            "iteration:  96 loss: 0.14777669\n",
            "iteration:  97 loss: 0.14212790\n",
            "iteration:  98 loss: 0.37205991\n",
            "iteration:  99 loss: 0.11361228\n",
            "iteration: 100 loss: 1.07376063\n",
            "iteration: 101 loss: 0.51713729\n",
            "iteration: 102 loss: 0.11052099\n",
            "iteration: 103 loss: 1.09115028\n",
            "iteration: 104 loss: 0.12821412\n",
            "iteration: 105 loss: 0.30999407\n",
            "iteration: 106 loss: 0.25385502\n",
            "iteration: 107 loss: 0.25030774\n",
            "iteration: 108 loss: 0.89015150\n",
            "iteration: 109 loss: 0.23834068\n",
            "iteration: 110 loss: 0.80906630\n",
            "iteration: 111 loss: 0.13234346\n",
            "iteration: 112 loss: 0.19919661\n",
            "iteration: 113 loss: 0.18085748\n",
            "iteration: 114 loss: 0.13428858\n",
            "iteration: 115 loss: 0.16414419\n",
            "iteration: 116 loss: 0.50393504\n",
            "iteration: 117 loss: 0.49092233\n",
            "iteration: 118 loss: 0.40070894\n",
            "iteration: 119 loss: 0.33850288\n",
            "iteration: 120 loss: 0.22942391\n",
            "iteration: 121 loss: 0.29893881\n",
            "iteration: 122 loss: 0.27199772\n",
            "iteration: 123 loss: 0.51715660\n",
            "iteration: 124 loss: 0.40268534\n",
            "iteration: 125 loss: 0.17884132\n",
            "iteration: 126 loss: 0.28904057\n",
            "iteration: 127 loss: 0.65868449\n",
            "iteration: 128 loss: 0.17558444\n",
            "iteration: 129 loss: 0.44136432\n",
            "iteration: 130 loss: 0.12565842\n",
            "iteration: 131 loss: 0.15650365\n",
            "iteration: 132 loss: 0.13837278\n",
            "iteration: 133 loss: 0.21582800\n",
            "iteration: 134 loss: 0.22782761\n",
            "iteration: 135 loss: 0.56310320\n",
            "iteration: 136 loss: 0.85585195\n",
            "iteration: 137 loss: 0.35601795\n",
            "iteration: 138 loss: 0.42030346\n",
            "iteration: 139 loss: 0.12327603\n",
            "iteration: 140 loss: 1.50580192\n",
            "iteration: 141 loss: 0.43718898\n",
            "iteration: 142 loss: 0.24680662\n",
            "iteration: 143 loss: 0.28026867\n",
            "iteration: 144 loss: 0.34943157\n",
            "iteration: 145 loss: 0.32543129\n",
            "iteration: 146 loss: 0.19419481\n",
            "iteration: 147 loss: 0.40507954\n",
            "iteration: 148 loss: 0.37081945\n",
            "iteration: 149 loss: 0.25157660\n",
            "iteration: 150 loss: 0.83763635\n",
            "iteration: 151 loss: 0.16029695\n",
            "iteration: 152 loss: 0.31103215\n",
            "iteration: 153 loss: 0.33825839\n",
            "iteration: 154 loss: 0.72198176\n",
            "iteration: 155 loss: 0.29751807\n",
            "iteration: 156 loss: 0.56771791\n",
            "iteration: 157 loss: 0.34059244\n",
            "iteration: 158 loss: 0.53138620\n",
            "iteration: 159 loss: 0.15855756\n",
            "iteration: 160 loss: 0.21879633\n",
            "iteration: 161 loss: 0.63790464\n",
            "iteration: 162 loss: 0.29898453\n",
            "iteration: 163 loss: 0.27621266\n",
            "iteration: 164 loss: 0.24716374\n",
            "iteration: 165 loss: 0.18618473\n",
            "iteration: 166 loss: 0.55338269\n",
            "iteration: 167 loss: 1.02902496\n",
            "iteration: 168 loss: 0.46184015\n",
            "iteration: 169 loss: 0.55880654\n",
            "iteration: 170 loss: 0.24512397\n",
            "iteration: 171 loss: 0.14566091\n",
            "iteration: 172 loss: 0.15187609\n",
            "iteration: 173 loss: 0.39943713\n",
            "iteration: 174 loss: 0.16304983\n",
            "iteration: 175 loss: 0.25306273\n",
            "iteration: 176 loss: 1.26541746\n",
            "iteration: 177 loss: 0.21776593\n",
            "iteration: 178 loss: 0.26874793\n",
            "iteration: 179 loss: 0.20050879\n",
            "iteration: 180 loss: 0.08053163\n",
            "iteration: 181 loss: 1.13901246\n",
            "iteration: 182 loss: 0.10556787\n",
            "iteration: 183 loss: 0.41210407\n",
            "iteration: 184 loss: 0.73292756\n",
            "iteration: 185 loss: 0.10631686\n",
            "iteration: 186 loss: 0.35575771\n",
            "iteration: 187 loss: 0.27117500\n",
            "iteration: 188 loss: 0.11838516\n",
            "iteration: 189 loss: 0.27511895\n",
            "iteration: 190 loss: 0.18950513\n",
            "iteration: 191 loss: 0.20830959\n",
            "iteration: 192 loss: 0.10242760\n",
            "iteration: 193 loss: 0.51539379\n",
            "iteration: 194 loss: 0.22018555\n",
            "iteration: 195 loss: 0.44283107\n",
            "iteration: 196 loss: 0.14082369\n",
            "iteration: 197 loss: 0.72031617\n",
            "iteration: 198 loss: 0.27039075\n",
            "iteration: 199 loss: 0.22629321\n",
            "epoch:  99 mean loss training: 0.37560943\n",
            "epoch:  99 mean loss validation: 1.24680912\n",
            "iteration:   0 loss: 0.43686026\n",
            "iteration:   1 loss: 0.36102226\n",
            "iteration:   2 loss: 0.26643988\n",
            "iteration:   3 loss: 0.61812484\n",
            "iteration:   4 loss: 0.47869575\n",
            "iteration:   5 loss: 0.27342784\n",
            "iteration:   6 loss: 0.82929218\n",
            "iteration:   7 loss: 0.08232740\n",
            "iteration:   8 loss: 0.38862097\n",
            "iteration:   9 loss: 0.33551538\n",
            "iteration:  10 loss: 0.74700034\n",
            "iteration:  11 loss: 0.26830456\n",
            "iteration:  12 loss: 0.10325660\n",
            "iteration:  13 loss: 0.27329415\n",
            "iteration:  14 loss: 0.39526206\n",
            "iteration:  15 loss: 0.14793833\n",
            "iteration:  16 loss: 0.08890646\n",
            "iteration:  17 loss: 0.43258551\n",
            "iteration:  18 loss: 0.23648968\n",
            "iteration:  19 loss: 0.16418755\n",
            "iteration:  20 loss: 0.19011262\n",
            "iteration:  21 loss: 0.10939588\n",
            "iteration:  22 loss: 0.17247464\n",
            "iteration:  23 loss: 0.18687451\n",
            "iteration:  24 loss: 0.31459612\n",
            "iteration:  25 loss: 0.84661037\n",
            "iteration:  26 loss: 0.18583617\n",
            "iteration:  27 loss: 0.44055760\n",
            "iteration:  28 loss: 0.90972513\n",
            "iteration:  29 loss: 0.20526344\n",
            "iteration:  30 loss: 0.27734661\n",
            "iteration:  31 loss: 0.66469836\n",
            "iteration:  32 loss: 0.16807602\n",
            "iteration:  33 loss: 0.81268209\n",
            "iteration:  34 loss: 0.36097997\n",
            "iteration:  35 loss: 0.21114096\n",
            "iteration:  36 loss: 0.16845313\n",
            "iteration:  37 loss: 0.28098500\n",
            "iteration:  38 loss: 0.15965644\n",
            "iteration:  39 loss: 0.50875068\n",
            "iteration:  40 loss: 0.66298640\n",
            "iteration:  41 loss: 0.91619599\n",
            "iteration:  42 loss: 0.28353071\n",
            "iteration:  43 loss: 0.07471279\n",
            "iteration:  44 loss: 0.17510913\n",
            "iteration:  45 loss: 0.31855249\n",
            "iteration:  46 loss: 0.33297583\n",
            "iteration:  47 loss: 0.32709226\n",
            "iteration:  48 loss: 0.66241741\n",
            "iteration:  49 loss: 0.12212456\n",
            "iteration:  50 loss: 0.48321679\n",
            "iteration:  51 loss: 0.59596550\n",
            "iteration:  52 loss: 0.44401956\n",
            "iteration:  53 loss: 0.26454085\n",
            "iteration:  54 loss: 0.44601586\n",
            "iteration:  55 loss: 0.20269531\n",
            "iteration:  56 loss: 0.45434210\n",
            "iteration:  57 loss: 0.16133368\n",
            "iteration:  58 loss: 0.30231902\n",
            "iteration:  59 loss: 0.39279401\n",
            "iteration:  60 loss: 0.20201470\n",
            "iteration:  61 loss: 0.24525528\n",
            "iteration:  62 loss: 0.18396229\n",
            "iteration:  63 loss: 0.21048811\n",
            "iteration:  64 loss: 0.09725413\n",
            "iteration:  65 loss: 0.35006958\n",
            "iteration:  66 loss: 1.37851584\n",
            "iteration:  67 loss: 0.36140031\n",
            "iteration:  68 loss: 0.36608309\n",
            "iteration:  69 loss: 0.23544171\n",
            "iteration:  70 loss: 0.21185192\n",
            "iteration:  71 loss: 0.08928758\n",
            "iteration:  72 loss: 0.21788979\n",
            "iteration:  73 loss: 0.19594359\n",
            "iteration:  74 loss: 0.56215513\n",
            "iteration:  75 loss: 0.73972666\n",
            "iteration:  76 loss: 0.26988775\n",
            "iteration:  77 loss: 0.23362917\n",
            "iteration:  78 loss: 0.17417121\n",
            "iteration:  79 loss: 0.41453931\n",
            "iteration:  80 loss: 1.22577763\n",
            "iteration:  81 loss: 0.25724298\n",
            "iteration:  82 loss: 0.10009173\n",
            "iteration:  83 loss: 0.24286965\n",
            "iteration:  84 loss: 0.25301334\n",
            "iteration:  85 loss: 0.40868360\n",
            "iteration:  86 loss: 0.15458792\n",
            "iteration:  87 loss: 0.80097175\n",
            "iteration:  88 loss: 0.51315427\n",
            "iteration:  89 loss: 0.42741120\n",
            "iteration:  90 loss: 1.30638385\n",
            "iteration:  91 loss: 0.20740569\n",
            "iteration:  92 loss: 0.29890427\n",
            "iteration:  93 loss: 0.15555219\n",
            "iteration:  94 loss: 0.13452682\n",
            "iteration:  95 loss: 0.20468628\n",
            "iteration:  96 loss: 0.96308368\n",
            "iteration:  97 loss: 0.28627384\n",
            "iteration:  98 loss: 0.18378459\n",
            "iteration:  99 loss: 0.25932270\n",
            "iteration: 100 loss: 0.19563371\n",
            "iteration: 101 loss: 0.53341436\n",
            "iteration: 102 loss: 0.15459183\n",
            "iteration: 103 loss: 0.31451401\n",
            "iteration: 104 loss: 0.11341290\n",
            "iteration: 105 loss: 0.20615722\n",
            "iteration: 106 loss: 0.25530756\n",
            "iteration: 107 loss: 0.32012251\n",
            "iteration: 108 loss: 0.45771277\n",
            "iteration: 109 loss: 0.68782580\n",
            "iteration: 110 loss: 0.62550795\n",
            "iteration: 111 loss: 0.15568440\n",
            "iteration: 112 loss: 0.15065235\n",
            "iteration: 113 loss: 0.46267918\n",
            "iteration: 114 loss: 0.22244385\n",
            "iteration: 115 loss: 0.48168764\n",
            "iteration: 116 loss: 1.01838517\n",
            "iteration: 117 loss: 0.87966651\n",
            "iteration: 118 loss: 0.79657459\n",
            "iteration: 119 loss: 0.21854889\n",
            "iteration: 120 loss: 0.45201558\n",
            "iteration: 121 loss: 0.21553995\n",
            "iteration: 122 loss: 1.31775045\n",
            "iteration: 123 loss: 0.32099044\n",
            "iteration: 124 loss: 0.44741547\n",
            "iteration: 125 loss: 0.28464347\n",
            "iteration: 126 loss: 0.49518603\n",
            "iteration: 127 loss: 0.07962460\n",
            "iteration: 128 loss: 0.11085124\n",
            "iteration: 129 loss: 0.30973452\n",
            "iteration: 130 loss: 1.12041903\n",
            "iteration: 131 loss: 0.12810750\n",
            "iteration: 132 loss: 0.51288867\n",
            "iteration: 133 loss: 0.35846034\n",
            "iteration: 134 loss: 0.79973340\n",
            "iteration: 135 loss: 0.54841775\n",
            "iteration: 136 loss: 0.70063496\n",
            "iteration: 137 loss: 0.69565463\n",
            "iteration: 138 loss: 0.58477163\n",
            "iteration: 139 loss: 0.65478218\n",
            "iteration: 140 loss: 1.31792879\n",
            "iteration: 141 loss: 0.12459521\n",
            "iteration: 142 loss: 1.14539051\n",
            "iteration: 143 loss: 0.08904120\n",
            "iteration: 144 loss: 0.14271504\n",
            "iteration: 145 loss: 0.46806246\n",
            "iteration: 146 loss: 0.16056114\n",
            "iteration: 147 loss: 0.26957175\n",
            "iteration: 148 loss: 0.09229147\n",
            "iteration: 149 loss: 0.32829008\n",
            "iteration: 150 loss: 0.24389188\n",
            "iteration: 151 loss: 0.19337659\n",
            "iteration: 152 loss: 0.10664049\n",
            "iteration: 153 loss: 0.44969067\n",
            "iteration: 154 loss: 0.21158403\n",
            "iteration: 155 loss: 0.21633326\n",
            "iteration: 156 loss: 0.29775545\n",
            "iteration: 157 loss: 0.58692741\n",
            "iteration: 158 loss: 0.29071414\n",
            "iteration: 159 loss: 0.87611592\n",
            "iteration: 160 loss: 0.15910110\n",
            "iteration: 161 loss: 0.28413638\n",
            "iteration: 162 loss: 0.37147200\n",
            "iteration: 163 loss: 0.13307855\n",
            "iteration: 164 loss: 0.21384168\n",
            "iteration: 165 loss: 0.49814966\n",
            "iteration: 166 loss: 0.11903697\n",
            "iteration: 167 loss: 0.38073835\n",
            "iteration: 168 loss: 0.46251851\n",
            "iteration: 169 loss: 0.17820519\n",
            "iteration: 170 loss: 0.80293983\n",
            "iteration: 171 loss: 0.33984339\n",
            "iteration: 172 loss: 0.49297804\n",
            "iteration: 173 loss: 0.13207123\n",
            "iteration: 174 loss: 0.53250360\n",
            "iteration: 175 loss: 0.59661508\n",
            "iteration: 176 loss: 0.29519865\n",
            "iteration: 177 loss: 0.41560411\n",
            "iteration: 178 loss: 0.11834513\n",
            "iteration: 179 loss: 0.77193159\n",
            "iteration: 180 loss: 0.10334860\n",
            "iteration: 181 loss: 0.15492716\n",
            "iteration: 182 loss: 0.58691019\n",
            "iteration: 183 loss: 0.21378994\n",
            "iteration: 184 loss: 0.21831295\n",
            "iteration: 185 loss: 0.07994344\n",
            "iteration: 186 loss: 0.39726034\n",
            "iteration: 187 loss: 0.88707513\n",
            "iteration: 188 loss: 0.36146945\n",
            "iteration: 189 loss: 0.10306114\n",
            "iteration: 190 loss: 0.36247832\n",
            "iteration: 191 loss: 0.55913317\n",
            "iteration: 192 loss: 0.39237005\n",
            "iteration: 193 loss: 0.20989700\n",
            "iteration: 194 loss: 0.47772330\n",
            "iteration: 195 loss: 0.09131674\n",
            "iteration: 196 loss: 0.36815923\n",
            "iteration: 197 loss: 0.32913595\n",
            "iteration: 198 loss: 0.33443886\n",
            "iteration: 199 loss: 0.67346120\n",
            "epoch: 100 mean loss training: 0.38525075\n",
            "epoch: 100 mean loss validation: 1.24318039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceKFzS_PhFPT",
        "outputId": "90089de4-ee6a-470a-8a8f-d3035b381440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Load training model\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCL&SSL.pth\".format(churn_percentage)))\n",
        "lmcl_loss.load_state_dict(torch.load(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/LMCLOSS&SSL.pth\".format(churn_percentage)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INWvsnGGhFPU",
        "outputId": "91fd397f-1fd0-4af2-cfda-20ae622cc62f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.06810510\n"
          ]
        }
      ],
      "source": [
        "#Creating predictions\n",
        "with torch.no_grad():\n",
        "    valid, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, valid_embed = model(numerical_valid_data, categorical_valid_data)\n",
        "    logits, mlogits = lmcl_loss(valid_embed, valid_outputs)\n",
        "    classification_loss_valid = loss_function(mlogits, valid_outputs)\n",
        "    total_valid_loss = classification_loss_valid\n",
        "print(f'Loss: {total_valid_loss:.8f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryLNu5EYhFPU",
        "outputId": "dcf9f881-a72a-4a77-a68b-c590a5b38c8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[508  56]\n",
            " [ 18  84]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.93       564\n",
            "           1       0.60      0.82      0.69       102\n",
            "\n",
            "    accuracy                           0.89       666\n",
            "   macro avg       0.78      0.86      0.81       666\n",
            "weighted avg       0.91      0.89      0.90       666\n",
            "\n",
            "Accuracy:  0.8888888888888888\n",
            "F1 Score:  0.8131624838880885\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================================\n",
        "#the main result seen is the F1 Score, because\n",
        "#the misleading accuracy metric is used for imbalance data\n",
        "# =============================================================================================\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "valid_val = np.argmax(logits.data, axis=1)\n",
        "print(confusion_matrix(valid_outputs, valid_val))\n",
        "print(classification_report(valid_outputs, valid_val))\n",
        "print(\"Accuracy: \", accuracy_score(valid_outputs, valid_val))\n",
        "print(\"F1 Score: \", f1_score(valid_outputs, valid_val, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function for smoothing the loss plot by using exponential moving average\n",
        "#https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar\n",
        "def smooth(scalars, weight):\n",
        "    last = scalars[0]  # First value in the plot (first timestep)\n",
        "    smoothed = list()\n",
        "    for point in scalars:\n",
        "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
        "        smoothed.append(smoothed_val)                        # Save it\n",
        "        last = smoothed_val                                  # Anchor the last smoothed value\n",
        "\n",
        "    return smoothed"
      ],
      "metadata": {
        "id": "ghwl4CbNvEuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "me_losses_train = []\n",
        "for l in mean_losses_train:\n",
        "  me_losses_train.append(l.detach().numpy())\n",
        "\n",
        "print (me_losses_train)\n",
        "print (mean_losses_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj6Gmgc6vOC5",
        "outputId": "4b1e94e7-b56d-4b77-e7f4-2900fee11df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(1.2247869, dtype=float32), array(0.86653256, dtype=float32), array(0.7635045, dtype=float32), array(0.6880164, dtype=float32), array(0.63423896, dtype=float32), array(0.62234455, dtype=float32), array(0.5648858, dtype=float32), array(0.54311854, dtype=float32), array(0.54198897, dtype=float32), array(0.49771476, dtype=float32), array(0.4961456, dtype=float32), array(0.47742745, dtype=float32), array(0.50268734, dtype=float32), array(0.46210247, dtype=float32), array(0.46624324, dtype=float32), array(0.45977634, dtype=float32), array(0.44989696, dtype=float32), array(0.43736085, dtype=float32), array(0.4178189, dtype=float32), array(0.42675182, dtype=float32), array(0.41973522, dtype=float32), array(0.42069846, dtype=float32), array(0.40668392, dtype=float32), array(0.39619792, dtype=float32), array(0.4085117, dtype=float32), array(0.40267777, dtype=float32), array(0.39815968, dtype=float32), array(0.3958592, dtype=float32), array(0.40324298, dtype=float32), array(0.39711505, dtype=float32), array(0.38855875, dtype=float32), array(0.3865734, dtype=float32), array(0.37133044, dtype=float32), array(0.37271556, dtype=float32), array(0.37664634, dtype=float32), array(0.37914166, dtype=float32), array(0.38906142, dtype=float32), array(0.39081845, dtype=float32), array(0.38126504, dtype=float32), array(0.36993665, dtype=float32), array(0.37939754, dtype=float32), array(0.39099663, dtype=float32), array(0.37872994, dtype=float32), array(0.3661252, dtype=float32), array(0.37566265, dtype=float32), array(0.38759127, dtype=float32), array(0.36814037, dtype=float32), array(0.35899255, dtype=float32), array(0.3790855, dtype=float32), array(0.38481468, dtype=float32), array(0.38844994, dtype=float32), array(0.36315164, dtype=float32), array(0.37869063, dtype=float32), array(0.36474115, dtype=float32), array(0.3733852, dtype=float32), array(0.3769849, dtype=float32), array(0.37212735, dtype=float32), array(0.3720728, dtype=float32), array(0.36551803, dtype=float32), array(0.36441648, dtype=float32), array(0.37614882, dtype=float32), array(0.38114357, dtype=float32), array(0.3830309, dtype=float32), array(0.3712068, dtype=float32), array(0.35749617, dtype=float32), array(0.37794724, dtype=float32), array(0.37716064, dtype=float32), array(0.3632806, dtype=float32), array(0.38878986, dtype=float32), array(0.3808227, dtype=float32), array(0.37157056, dtype=float32), array(0.3654243, dtype=float32), array(0.38068065, dtype=float32), array(0.3631174, dtype=float32), array(0.37957576, dtype=float32), array(0.36717525, dtype=float32), array(0.36823264, dtype=float32), array(0.37320018, dtype=float32), array(0.3748948, dtype=float32), array(0.3621671, dtype=float32), array(0.3948179, dtype=float32), array(0.37409264, dtype=float32), array(0.3909066, dtype=float32), array(0.35294932, dtype=float32), array(0.3652375, dtype=float32), array(0.36308122, dtype=float32), array(0.36840928, dtype=float32), array(0.36447066, dtype=float32), array(0.37956268, dtype=float32), array(0.36816803, dtype=float32), array(0.37473214, dtype=float32), array(0.37054864, dtype=float32), array(0.37615758, dtype=float32), array(0.35453606, dtype=float32), array(0.39305362, dtype=float32), array(0.38366067, dtype=float32), array(0.36086705, dtype=float32), array(0.36831596, dtype=float32), array(0.37560943, dtype=float32), array(0.38525075, dtype=float32)]\n",
            "[tensor(0.9969), tensor(0.9849), tensor(1.0000), tensor(0.9851), tensor(1.0037), tensor(1.0354), tensor(1.0343), tensor(1.0603), tensor(1.0644), tensor(1.0996), tensor(1.1195), tensor(1.1169), tensor(1.1689), tensor(1.1281), tensor(1.1579), tensor(1.1659), tensor(1.1710), tensor(1.1758), tensor(1.1619), tensor(1.2220), tensor(1.1977), tensor(1.1866), tensor(1.2079), tensor(1.2162), tensor(1.2114), tensor(1.2090), tensor(1.2362), tensor(1.2305), tensor(1.1959), tensor(1.2037), tensor(1.2797), tensor(1.2318), tensor(1.2279), tensor(1.2199), tensor(1.2163), tensor(1.2199), tensor(1.2227), tensor(1.2442), tensor(1.2502), tensor(1.2557), tensor(1.2705), tensor(1.2383), tensor(1.2364), tensor(1.2545), tensor(1.2685), tensor(1.2877), tensor(1.2754), tensor(1.2290), tensor(1.2457), tensor(1.2372), tensor(1.2672), tensor(1.2574), tensor(1.2162), tensor(1.2815), tensor(1.2583), tensor(1.2364), tensor(1.2354), tensor(1.2404), tensor(1.2560), tensor(1.2520), tensor(1.2689), tensor(1.2533), tensor(1.2688), tensor(1.2322), tensor(1.2647), tensor(1.2569), tensor(1.2612), tensor(1.2428), tensor(1.2589), tensor(1.2291), tensor(1.2576), tensor(1.2475), tensor(1.2807), tensor(1.2515), tensor(1.2777), tensor(1.2408), tensor(1.2262), tensor(1.2444), tensor(1.2584), tensor(1.2468), tensor(1.2623), tensor(1.2407), tensor(1.2388), tensor(1.2701), tensor(1.2394), tensor(1.2879), tensor(1.2829), tensor(1.2439), tensor(1.3039), tensor(1.2593), tensor(1.2298), tensor(1.2572), tensor(1.2261), tensor(1.2987), tensor(1.2692), tensor(1.2448), tensor(1.2433), tensor(1.2627), tensor(1.2468), tensor(1.2432)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5j7ZbFxhFPV",
        "outputId": "6022d6be-5518-4bc8-8c5d-d09b0a2ff846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "epochs_to_show = 100\n",
        "smoothing = 0.5\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(me_losses_train[0:epochs_to_show],smoothing), label = 'train')\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(mean_losses_valid[0:epochs_to_show],smoothing), label = 'validation')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('loss graph')\n",
        "plt.show()\n",
        "print(best_epoch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "LMCL&SSL _1st_Run_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}