{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DinneRatj/Vector-Embedding-Model/blob/main/SSL_1st_Run_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY3wanO-yfBz",
        "outputId": "fd9ae615-b198-44c9-bebb-469e806da7a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f98339cf950>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Imporing libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Added so that the random numbers are always the same when the program is run, so the results are always the same\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TrMkxpd0B9w",
        "outputId": "018bce2d-cd25-4c85-9177-19351e6e69b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting to Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "Tk1DAippyfB6",
        "outputId": "166d9bee-a256-4a60-a34c-b62ddb19e368",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     state  account length  area code phone number international plan  \\\n",
              "0       KS             128        415     382-4657                 no   \n",
              "1       OH             107        415     371-7191                 no   \n",
              "2       NJ             137        415     358-1921                 no   \n",
              "3       OH              84        408     375-9999                yes   \n",
              "4       OK              75        415     330-6626                yes   \n",
              "...    ...             ...        ...          ...                ...   \n",
              "3328    AZ             192        415     414-4276                 no   \n",
              "3329    WV              68        415     370-3271                 no   \n",
              "3330    RI              28        510     328-8230                 no   \n",
              "3331    CT             184        510     364-6381                yes   \n",
              "3332    TN              74        415     400-4344                 no   \n",
              "\n",
              "     voice mail plan  number vmail messages  total day minutes  \\\n",
              "0                yes                     25              265.1   \n",
              "1                yes                     26              161.6   \n",
              "2                 no                      0              243.4   \n",
              "3                 no                      0              299.4   \n",
              "4                 no                      0              166.7   \n",
              "...              ...                    ...                ...   \n",
              "3328             yes                     36              156.2   \n",
              "3329              no                      0              231.1   \n",
              "3330              no                      0              180.8   \n",
              "3331              no                      0              213.8   \n",
              "3332             yes                     25              234.4   \n",
              "\n",
              "      total day calls  total day charge  total eve minutes  total eve calls  \\\n",
              "0                 110             45.07              197.4               99   \n",
              "1                 123             27.47              195.5              103   \n",
              "2                 114             41.38              121.2              110   \n",
              "3                  71             50.90               61.9               88   \n",
              "4                 113             28.34              148.3              122   \n",
              "...               ...               ...                ...              ...   \n",
              "3328               77             26.55              215.5              126   \n",
              "3329               57             39.29              153.4               55   \n",
              "3330              109             30.74              288.8               58   \n",
              "3331              105             36.35              159.6               84   \n",
              "3332              113             39.85              265.9               82   \n",
              "\n",
              "      total eve charge  total night minutes  total night calls  \\\n",
              "0                16.78                244.7                 91   \n",
              "1                16.62                254.4                103   \n",
              "2                10.30                162.6                104   \n",
              "3                 5.26                196.9                 89   \n",
              "4                12.61                186.9                121   \n",
              "...                ...                  ...                ...   \n",
              "3328             18.32                279.1                 83   \n",
              "3329             13.04                191.3                123   \n",
              "3330             24.55                191.9                 91   \n",
              "3331             13.57                139.2                137   \n",
              "3332             22.60                241.4                 77   \n",
              "\n",
              "      total night charge  total intl minutes  total intl calls  \\\n",
              "0                  11.01                10.0                 3   \n",
              "1                  11.45                13.7                 3   \n",
              "2                   7.32                12.2                 5   \n",
              "3                   8.86                 6.6                 7   \n",
              "4                   8.41                10.1                 3   \n",
              "...                  ...                 ...               ...   \n",
              "3328               12.56                 9.9                 6   \n",
              "3329                8.61                 9.6                 4   \n",
              "3330                8.64                14.1                 6   \n",
              "3331                6.26                 5.0                10   \n",
              "3332               10.86                13.7                 4   \n",
              "\n",
              "      total intl charge  customer service calls  churn  \n",
              "0                  2.70                       1  False  \n",
              "1                  3.70                       1  False  \n",
              "2                  3.29                       0  False  \n",
              "3                  1.78                       2  False  \n",
              "4                  2.73                       3  False  \n",
              "...                 ...                     ...    ...  \n",
              "3328               2.67                       2  False  \n",
              "3329               2.59                       3  False  \n",
              "3330               3.81                       2  False  \n",
              "3331               1.35                       2  False  \n",
              "3332               3.70                       0  False  \n",
              "\n",
              "[3333 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-766ba367-7f15-4cd2-babe-8d63a0c81234\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>account length</th>\n",
              "      <th>area code</th>\n",
              "      <th>phone number</th>\n",
              "      <th>international plan</th>\n",
              "      <th>voice mail plan</th>\n",
              "      <th>number vmail messages</th>\n",
              "      <th>total day minutes</th>\n",
              "      <th>total day calls</th>\n",
              "      <th>total day charge</th>\n",
              "      <th>total eve minutes</th>\n",
              "      <th>total eve calls</th>\n",
              "      <th>total eve charge</th>\n",
              "      <th>total night minutes</th>\n",
              "      <th>total night calls</th>\n",
              "      <th>total night charge</th>\n",
              "      <th>total intl minutes</th>\n",
              "      <th>total intl calls</th>\n",
              "      <th>total intl charge</th>\n",
              "      <th>customer service calls</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KS</td>\n",
              "      <td>128</td>\n",
              "      <td>415</td>\n",
              "      <td>382-4657</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>265.1</td>\n",
              "      <td>110</td>\n",
              "      <td>45.07</td>\n",
              "      <td>197.4</td>\n",
              "      <td>99</td>\n",
              "      <td>16.78</td>\n",
              "      <td>244.7</td>\n",
              "      <td>91</td>\n",
              "      <td>11.01</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OH</td>\n",
              "      <td>107</td>\n",
              "      <td>415</td>\n",
              "      <td>371-7191</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>26</td>\n",
              "      <td>161.6</td>\n",
              "      <td>123</td>\n",
              "      <td>27.47</td>\n",
              "      <td>195.5</td>\n",
              "      <td>103</td>\n",
              "      <td>16.62</td>\n",
              "      <td>254.4</td>\n",
              "      <td>103</td>\n",
              "      <td>11.45</td>\n",
              "      <td>13.7</td>\n",
              "      <td>3</td>\n",
              "      <td>3.70</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NJ</td>\n",
              "      <td>137</td>\n",
              "      <td>415</td>\n",
              "      <td>358-1921</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>243.4</td>\n",
              "      <td>114</td>\n",
              "      <td>41.38</td>\n",
              "      <td>121.2</td>\n",
              "      <td>110</td>\n",
              "      <td>10.30</td>\n",
              "      <td>162.6</td>\n",
              "      <td>104</td>\n",
              "      <td>7.32</td>\n",
              "      <td>12.2</td>\n",
              "      <td>5</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OH</td>\n",
              "      <td>84</td>\n",
              "      <td>408</td>\n",
              "      <td>375-9999</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>299.4</td>\n",
              "      <td>71</td>\n",
              "      <td>50.90</td>\n",
              "      <td>61.9</td>\n",
              "      <td>88</td>\n",
              "      <td>5.26</td>\n",
              "      <td>196.9</td>\n",
              "      <td>89</td>\n",
              "      <td>8.86</td>\n",
              "      <td>6.6</td>\n",
              "      <td>7</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OK</td>\n",
              "      <td>75</td>\n",
              "      <td>415</td>\n",
              "      <td>330-6626</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>166.7</td>\n",
              "      <td>113</td>\n",
              "      <td>28.34</td>\n",
              "      <td>148.3</td>\n",
              "      <td>122</td>\n",
              "      <td>12.61</td>\n",
              "      <td>186.9</td>\n",
              "      <td>121</td>\n",
              "      <td>8.41</td>\n",
              "      <td>10.1</td>\n",
              "      <td>3</td>\n",
              "      <td>2.73</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>AZ</td>\n",
              "      <td>192</td>\n",
              "      <td>415</td>\n",
              "      <td>414-4276</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>36</td>\n",
              "      <td>156.2</td>\n",
              "      <td>77</td>\n",
              "      <td>26.55</td>\n",
              "      <td>215.5</td>\n",
              "      <td>126</td>\n",
              "      <td>18.32</td>\n",
              "      <td>279.1</td>\n",
              "      <td>83</td>\n",
              "      <td>12.56</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "      <td>2.67</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3329</th>\n",
              "      <td>WV</td>\n",
              "      <td>68</td>\n",
              "      <td>415</td>\n",
              "      <td>370-3271</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>231.1</td>\n",
              "      <td>57</td>\n",
              "      <td>39.29</td>\n",
              "      <td>153.4</td>\n",
              "      <td>55</td>\n",
              "      <td>13.04</td>\n",
              "      <td>191.3</td>\n",
              "      <td>123</td>\n",
              "      <td>8.61</td>\n",
              "      <td>9.6</td>\n",
              "      <td>4</td>\n",
              "      <td>2.59</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>RI</td>\n",
              "      <td>28</td>\n",
              "      <td>510</td>\n",
              "      <td>328-8230</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>180.8</td>\n",
              "      <td>109</td>\n",
              "      <td>30.74</td>\n",
              "      <td>288.8</td>\n",
              "      <td>58</td>\n",
              "      <td>24.55</td>\n",
              "      <td>191.9</td>\n",
              "      <td>91</td>\n",
              "      <td>8.64</td>\n",
              "      <td>14.1</td>\n",
              "      <td>6</td>\n",
              "      <td>3.81</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3331</th>\n",
              "      <td>CT</td>\n",
              "      <td>184</td>\n",
              "      <td>510</td>\n",
              "      <td>364-6381</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>0</td>\n",
              "      <td>213.8</td>\n",
              "      <td>105</td>\n",
              "      <td>36.35</td>\n",
              "      <td>159.6</td>\n",
              "      <td>84</td>\n",
              "      <td>13.57</td>\n",
              "      <td>139.2</td>\n",
              "      <td>137</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3332</th>\n",
              "      <td>TN</td>\n",
              "      <td>74</td>\n",
              "      <td>415</td>\n",
              "      <td>400-4344</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>25</td>\n",
              "      <td>234.4</td>\n",
              "      <td>113</td>\n",
              "      <td>39.85</td>\n",
              "      <td>265.9</td>\n",
              "      <td>82</td>\n",
              "      <td>22.60</td>\n",
              "      <td>241.4</td>\n",
              "      <td>77</td>\n",
              "      <td>10.86</td>\n",
              "      <td>13.7</td>\n",
              "      <td>4</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3333 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-766ba367-7f15-4cd2-babe-8d63a0c81234')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-766ba367-7f15-4cd2-babe-8d63a0c81234 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-766ba367-7f15-4cd2-babe-8d63a0c81234');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Reading data\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/bigml_59c28831336c6604c800002a.csv\")\n",
        "pd.options.display.max_columns = None\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_23UROsqyfB7"
      },
      "outputs": [],
      "source": [
        "#Defining columns\n",
        "numerical_columns = ['number vmail messages', 'total day minutes', 'total day calls',\n",
        "                     'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge', 'total night minutes',\n",
        "                     'total night calls', 'total night charge', 'total intl minutes', 'total intl calls',\n",
        "                     'total intl charge', 'customer service calls']\n",
        "categorical_columns = ['state', 'international plan', 'voice mail plan','area code']\n",
        "outputs = ['churn']\n",
        "\n",
        "#Input >14 Numerical coloums and 4 Categorical coloums\n",
        "#Output > 1 Categorical coloum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCxjjLqlyfB8"
      },
      "outputs": [],
      "source": [
        "churn_data = dataset[dataset['churn'] == 'True']\n",
        "notchurn_data = dataset[dataset['churn'] == 'False']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aItDmQWsyfB8",
        "outputId": "f052d1bd-966f-455a-e3d3-386bccb1ee32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2349,  1.5668,  0.4766,  ..., -0.6012, -0.0857, -0.4279],\n",
            "        [ 1.3079, -0.3337,  1.1245,  ..., -0.6012,  1.2412, -0.4279],\n",
            "        [-0.5918,  1.1683,  0.6760,  ...,  0.2115,  0.6972, -1.1882],\n",
            "        ...,\n",
            "        [-0.5918,  0.0188,  0.4268,  ...,  0.6179,  1.3871,  0.3324],\n",
            "        [-0.5918,  0.6248,  0.2275,  ...,  2.2434, -1.8770,  0.3324],\n",
            "        [ 1.2349,  1.0030,  0.6261,  ..., -0.1948,  1.2412, -1.1882]])\n",
            "torch.float32\n",
            "torch.Size([3333, 14])\n",
            "___________________________________________________________________________\n",
            "tensor([[16,  0,  1,  1],\n",
            "        [35,  0,  1,  1],\n",
            "        [31,  0,  0,  1],\n",
            "        ...,\n",
            "        [39,  0,  0,  2],\n",
            "        [ 6,  1,  0,  2],\n",
            "        [42,  0,  1,  1]])\n",
            "torch.int64\n",
            "torch.Size([3333, 4])\n",
            "___________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Processing columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Numerical\n",
        "#Convert our numerical columns to tensors\n",
        "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
        "\n",
        "#Fixed how to use scaler\n",
        "numerical_data = scaler.fit_transform(numerical_data)\n",
        "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
        "\n",
        "#Categorical\n",
        "#Convert the types for categorical columns to category\n",
        "for category in categorical_columns:\n",
        "    dataset[category] = dataset[category].astype('category')\n",
        "\n",
        "#Convert data in the four categorical columns into numpy arrays and then stack all the columns horizontally\n",
        "st = dataset['state'].cat.codes.values\n",
        "ip = dataset['international plan'].cat.codes.values\n",
        "vm = dataset['voice mail plan'].cat.codes.values\n",
        "ac = dataset['area code'].cat.codes.values\n",
        "\n",
        "categorical_data = np.stack([st, ip, vm, ac], 1)\n",
        "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
        "\n",
        "#Outputs\n",
        "#Convert the output numpy array into a tensor object\n",
        "dataset[outputs] = dataset[outputs].astype(int)\n",
        "outputs = torch.tensor(dataset[outputs].values).flatten()\n",
        "outputs = outputs.long()\n",
        "\n",
        "#Print Outputs\n",
        "print(numerical_data)\n",
        "print(numerical_data.dtype)\n",
        "print(numerical_data.shape)\n",
        "print('_' * 75)\n",
        "\n",
        "print(categorical_data)\n",
        "print(categorical_data.dtype)\n",
        "print(categorical_data.shape)\n",
        "print('_' * 75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh2UHgRgyfB9",
        "outputId": "9845e934-2982-4cdf-d366-afc75e9c25c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1999\n",
            "666\n",
            "666\n"
          ]
        }
      ],
      "source": [
        "#Dividing dataset into Train, Valid and Test\n",
        "total_records = 3333\n",
        "\n",
        "train_records = int(total_records * .6)\n",
        "valid_records = int(total_records * .2)\n",
        "test_records = int(total_records * .2)\n",
        "\n",
        "numerical_train_data = numerical_data[:train_records]\n",
        "numerical_valid_data = numerical_data[train_records:train_records+valid_records]\n",
        "numerical_test_data = numerical_data[train_records+valid_records:total_records]\n",
        "\n",
        "categorical_train_data = categorical_data[:train_records]\n",
        "categorical_valid_data = categorical_data[train_records:train_records+valid_records]\n",
        "categorical_test_data = categorical_data[train_records+valid_records:total_records]\n",
        "\n",
        "train_outputs = outputs[:train_records]\n",
        "valid_outputs = outputs[train_records:train_records+valid_records]\n",
        "test_outputs = outputs[train_records+valid_records:total_records]\n",
        "\n",
        "#Print divide dataset\n",
        "print(train_records)\n",
        "print(valid_records)\n",
        "print(test_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fB8lN0byfB-"
      },
      "outputs": [],
      "source": [
        "#Define a class named Model, which will be used to train the model\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "#Creating the Neural Network\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(14, 100) #Numerical\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.BatchNorm1d(100)\n",
        "        self.weights1 = Parameter(torch.Tensor(1, 120))\n",
        "        init.kaiming_uniform_(self.weights1, a=math.sqrt(5))\n",
        "        \n",
        "        self.weights2 = Parameter(torch.Tensor(1, 120))\n",
        "        init.kaiming_uniform_(self.weights2, a=math.sqrt(5))\n",
        "        \n",
        "        self.bias1 = Parameter(torch.Tensor(1))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weights1)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias1, -bound, bound)\n",
        "        \n",
        "        self.bias2 = Parameter(torch.Tensor(1))\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weights2)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias2, -bound, bound)\n",
        "        \n",
        "        #Categorical\n",
        "        self.layer1_1 = nn.Embedding(51, 5) #51 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_1 = nn.BatchNorm1d(5)\n",
        "        self.layer1_2 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_2 = nn.BatchNorm1d(5)\n",
        "        self.layer1_3 = nn.Embedding(2, 5) #2 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_3 = nn.BatchNorm1d(5)\n",
        "        self.layer1_4 = nn.Embedding(3, 5) #3 unique data cat state coloum as input and 5 dimensi/vector size as output\n",
        "        self.bn1_4 = nn.BatchNorm1d(5)\n",
        "        \n",
        "        self.layer2 = nn.Linear(120, 120)\n",
        "        self.bn2 = nn.BatchNorm1d(120)\n",
        "        \n",
        "        #Decoder\n",
        "        self.decoder_categorical_1 = nn.Linear(120,51)\n",
        "        self.decoder_categorical_2 = nn.Linear(120,2)\n",
        "        self.decoder_categorical_3 = nn.Linear(120,2)\n",
        "        self.decoder_categorical_4 = nn.Linear(120,3)\n",
        "                \n",
        "        self.decoder_numerical = nn.Linear(120,14)\n",
        "        \n",
        "    def forward(self, x_numerical, x_categorical):\n",
        "        x1 = self.layer1(x_numerical)\n",
        "        x1 = self.relu(x1)\n",
        "        x1 = self.bn1(x1)\n",
        "        \n",
        "        #Decoder\n",
        "        x1_embedding = self.layer1_1(x_categorical[:,0])\n",
        "        x1_embedding = self.relu(x1_embedding)\n",
        "        x1_embedding = self.bn1_1(x1_embedding)\n",
        "        \n",
        "        x2_embedding = self.layer1_2(x_categorical[:,1])\n",
        "        x2_embedding = self.relu(x2_embedding)\n",
        "        x2_embedding = self.bn1_2(x2_embedding)\n",
        "        \n",
        "        x3_embedding = self.layer1_3(x_categorical[:,2])\n",
        "        x3_embedding = self.relu(x3_embedding)\n",
        "        x3_embedding = self.bn1_3(x3_embedding)\n",
        "        \n",
        "        x4_embedding = self.layer1_4(x_categorical[:,3])\n",
        "        x4_embedding = self.relu(x4_embedding)\n",
        "        x4_embedding = self.bn1_4(x4_embedding)\n",
        "        \n",
        "        x_embedding = torch.cat([x1_embedding,x2_embedding,x3_embedding,x4_embedding], 1)\n",
        "\n",
        "        x1 = torch.cat([x1, x_embedding], 1)                \n",
        "        \n",
        "        #Decoder\n",
        "        x2 = self.layer2(x1)\n",
        "        emb = self.relu(x2)\n",
        "        x2 = self.bn2(emb)\n",
        "                \n",
        "        x2_weights1 = torch.mm(x2, self.weights1.t()) + self.bias1\n",
        "        x2_weights2 = torch.mm(x2, self.weights2.t()) + self.bias2\n",
        "        \n",
        "        x3 = torch.cat([x2_weights1, x2_weights2], 1)\n",
        "        \n",
        "        categorical_1_decoded = self.decoder_categorical_1(x2)\n",
        "        categorical_2_decoded = self.decoder_categorical_2(x2)\n",
        "        categorical_3_decoded = self.decoder_categorical_3(x2)\n",
        "        categorical_4_decoded = self.decoder_categorical_4(x2)\n",
        "            \n",
        "        numerical_decoded = self.decoder_numerical(x2)\n",
        "                \n",
        "        return emb, self.weights1, self.weights2, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmZkIaVFyfCA",
        "outputId": "6e8a8506-c1bf-412e-81dd-e2ae52cf1f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=120, out_features=14, bias=True)\n"
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "print(model.decoder_numerical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs-sbCF6yfCB"
      },
      "outputs": [],
      "source": [
        "#===============================================================================================\n",
        "#Defining churn:loyal weight ratio. churn_percentage=0.7 means churn:loyal weight ratio of 7:3.\n",
        "#===============================================================================================\n",
        "churn_percentage = 0.7\n",
        "\n",
        "#Defining loss function\n",
        "loss_function = nn.CrossEntropyLoss(weight=torch.Tensor([1-churn_percentage, churn_percentage]))\n",
        "loss_function_autoencoder = nn.CrossEntropyLoss() #Classification - Categorical\n",
        "loss_function_mse = nn.MSELoss() #Regression - Numerical\n",
        "\n",
        "#Defining optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Added learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC6Ns-8ByfCC",
        "scrolled": true,
        "outputId": "521350e2-be71-440a-bce3-7faff5a7970a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration:  50 loss: 0.05643968\n",
            "iteration:  51 loss: 0.13086797\n",
            "iteration:  52 loss: 0.07577938\n",
            "iteration:  53 loss: 0.16875018\n",
            "iteration:  54 loss: 0.08903444\n",
            "iteration:  55 loss: 0.08287454\n",
            "iteration:  56 loss: 0.20509577\n",
            "iteration:  57 loss: 0.13769862\n",
            "iteration:  58 loss: 0.08431005\n",
            "iteration:  59 loss: 0.10630603\n",
            "iteration:  60 loss: 0.06796440\n",
            "iteration:  61 loss: 0.08182432\n",
            "iteration:  62 loss: 0.09119770\n",
            "iteration:  63 loss: 0.09760933\n",
            "iteration:  64 loss: 0.13276878\n",
            "iteration:  65 loss: 0.16014901\n",
            "iteration:  66 loss: 0.11505982\n",
            "iteration:  67 loss: 0.26405528\n",
            "iteration:  68 loss: 0.07710174\n",
            "iteration:  69 loss: 0.15749097\n",
            "iteration:  70 loss: 0.14058833\n",
            "iteration:  71 loss: 0.15856576\n",
            "iteration:  72 loss: 0.10021763\n",
            "iteration:  73 loss: 0.06757753\n",
            "iteration:  74 loss: 0.08328173\n",
            "iteration:  75 loss: 0.39198589\n",
            "iteration:  76 loss: 0.20797610\n",
            "iteration:  77 loss: 0.15365638\n",
            "iteration:  78 loss: 0.07151240\n",
            "iteration:  79 loss: 0.08334254\n",
            "iteration:  80 loss: 0.12080689\n",
            "iteration:  81 loss: 0.07589207\n",
            "iteration:  82 loss: 0.16197717\n",
            "iteration:  83 loss: 0.13903445\n",
            "iteration:  84 loss: 0.09601082\n",
            "iteration:  85 loss: 0.17613807\n",
            "iteration:  86 loss: 0.13918468\n",
            "iteration:  87 loss: 0.08207055\n",
            "iteration:  88 loss: 0.11595780\n",
            "iteration:  89 loss: 0.09390339\n",
            "iteration:  90 loss: 0.11306807\n",
            "iteration:  91 loss: 0.09254666\n",
            "iteration:  92 loss: 0.18851647\n",
            "iteration:  93 loss: 0.11851823\n",
            "iteration:  94 loss: 0.08022868\n",
            "iteration:  95 loss: 0.26392549\n",
            "iteration:  96 loss: 0.58349770\n",
            "iteration:  97 loss: 0.06755678\n",
            "iteration:  98 loss: 0.07695526\n",
            "iteration:  99 loss: 0.08323253\n",
            "iteration: 100 loss: 0.24431306\n",
            "iteration: 101 loss: 0.07210010\n",
            "iteration: 102 loss: 0.17007555\n",
            "iteration: 103 loss: 0.06719992\n",
            "iteration: 104 loss: 0.23226814\n",
            "iteration: 105 loss: 0.06029646\n",
            "iteration: 106 loss: 0.05176067\n",
            "iteration: 107 loss: 0.09848019\n",
            "iteration: 108 loss: 0.09188952\n",
            "iteration: 109 loss: 0.06641284\n",
            "iteration: 110 loss: 0.04050460\n",
            "iteration: 111 loss: 0.09738484\n",
            "iteration: 112 loss: 0.06190650\n",
            "iteration: 113 loss: 0.09958987\n",
            "iteration: 114 loss: 0.08110923\n",
            "iteration: 115 loss: 0.10461228\n",
            "iteration: 116 loss: 0.05732434\n",
            "iteration: 117 loss: 0.09830511\n",
            "iteration: 118 loss: 0.07508281\n",
            "iteration: 119 loss: 0.15960340\n",
            "iteration: 120 loss: 0.05390078\n",
            "iteration: 121 loss: 0.08390704\n",
            "iteration: 122 loss: 0.13268164\n",
            "iteration: 123 loss: 0.09325989\n",
            "iteration: 124 loss: 0.07612124\n",
            "iteration: 125 loss: 0.07502060\n",
            "iteration: 126 loss: 0.12726745\n",
            "iteration: 127 loss: 0.14711347\n",
            "iteration: 128 loss: 0.04899809\n",
            "iteration: 129 loss: 0.07797348\n",
            "iteration: 130 loss: 0.07751536\n",
            "iteration: 131 loss: 0.10930584\n",
            "iteration: 132 loss: 0.07121946\n",
            "iteration: 133 loss: 0.06256931\n",
            "iteration: 134 loss: 0.12183292\n",
            "iteration: 135 loss: 0.07486660\n",
            "iteration: 136 loss: 0.06543396\n",
            "iteration: 137 loss: 0.10369643\n",
            "iteration: 138 loss: 0.08308952\n",
            "iteration: 139 loss: 0.06613708\n",
            "iteration: 140 loss: 0.06770850\n",
            "iteration: 141 loss: 0.06645097\n",
            "iteration: 142 loss: 0.07157537\n",
            "iteration: 143 loss: 0.12406069\n",
            "iteration: 144 loss: 0.07132535\n",
            "iteration: 145 loss: 0.12750669\n",
            "iteration: 146 loss: 0.07685478\n",
            "iteration: 147 loss: 0.11060940\n",
            "iteration: 148 loss: 0.09202085\n",
            "iteration: 149 loss: 0.11178175\n",
            "iteration: 150 loss: 0.06948470\n",
            "iteration: 151 loss: 0.39300409\n",
            "iteration: 152 loss: 0.15643091\n",
            "iteration: 153 loss: 0.15546283\n",
            "iteration: 154 loss: 0.11431774\n",
            "iteration: 155 loss: 0.29138035\n",
            "iteration: 156 loss: 0.10010663\n",
            "iteration: 157 loss: 0.09407681\n",
            "iteration: 158 loss: 0.05794970\n",
            "iteration: 159 loss: 0.07528904\n",
            "iteration: 160 loss: 0.07606496\n",
            "iteration: 161 loss: 0.05349230\n",
            "iteration: 162 loss: 0.05011706\n",
            "iteration: 163 loss: 0.12828088\n",
            "iteration: 164 loss: 0.09054039\n",
            "iteration: 165 loss: 0.10781974\n",
            "iteration: 166 loss: 0.10991459\n",
            "iteration: 167 loss: 0.09834540\n",
            "iteration: 168 loss: 0.05669443\n",
            "iteration: 169 loss: 0.13791475\n",
            "iteration: 170 loss: 0.10721944\n",
            "iteration: 171 loss: 0.19732562\n",
            "iteration: 172 loss: 0.25401312\n",
            "iteration: 173 loss: 0.06438533\n",
            "iteration: 174 loss: 0.11375353\n",
            "iteration: 175 loss: 0.06292305\n",
            "iteration: 176 loss: 0.15829626\n",
            "iteration: 177 loss: 0.07619801\n",
            "iteration: 178 loss: 0.10088705\n",
            "iteration: 179 loss: 0.22428352\n",
            "iteration: 180 loss: 0.10717905\n",
            "iteration: 181 loss: 0.10908325\n",
            "iteration: 182 loss: 0.08239384\n",
            "iteration: 183 loss: 0.13712171\n",
            "iteration: 184 loss: 0.18562305\n",
            "iteration: 185 loss: 0.08995265\n",
            "iteration: 186 loss: 0.09390827\n",
            "iteration: 187 loss: 0.07923648\n",
            "iteration: 188 loss: 0.12196068\n",
            "iteration: 189 loss: 0.14444444\n",
            "iteration: 190 loss: 0.06355704\n",
            "iteration: 191 loss: 0.08503710\n",
            "iteration: 192 loss: 0.04879741\n",
            "iteration: 193 loss: 0.08019584\n",
            "iteration: 194 loss: 0.09903292\n",
            "iteration: 195 loss: 0.07440252\n",
            "iteration: 196 loss: 0.11895485\n",
            "iteration: 197 loss: 0.12645558\n",
            "iteration: 198 loss: 0.05371412\n",
            "iteration: 199 loss: 0.06682844\n",
            "epoch:  76 mean loss training: 0.11990666\n",
            "epoch:  76 mean loss validation: 1.14554226\n",
            "iteration:   0 loss: 0.16167343\n",
            "iteration:   1 loss: 0.06436157\n",
            "iteration:   2 loss: 0.06720031\n",
            "iteration:   3 loss: 0.10495281\n",
            "iteration:   4 loss: 0.22020057\n",
            "iteration:   5 loss: 0.11435664\n",
            "iteration:   6 loss: 0.07143830\n",
            "iteration:   7 loss: 0.21476690\n",
            "iteration:   8 loss: 0.09667017\n",
            "iteration:   9 loss: 0.11425950\n",
            "iteration:  10 loss: 0.06784967\n",
            "iteration:  11 loss: 0.09524518\n",
            "iteration:  12 loss: 0.13973388\n",
            "iteration:  13 loss: 0.06498788\n",
            "iteration:  14 loss: 0.08602512\n",
            "iteration:  15 loss: 0.11907455\n",
            "iteration:  16 loss: 0.09204721\n",
            "iteration:  17 loss: 0.07203010\n",
            "iteration:  18 loss: 0.10121542\n",
            "iteration:  19 loss: 0.12109069\n",
            "iteration:  20 loss: 0.06697445\n",
            "iteration:  21 loss: 0.10640810\n",
            "iteration:  22 loss: 0.11159839\n",
            "iteration:  23 loss: 0.17744236\n",
            "iteration:  24 loss: 0.09143391\n",
            "iteration:  25 loss: 0.06787316\n",
            "iteration:  26 loss: 0.06813699\n",
            "iteration:  27 loss: 0.09175071\n",
            "iteration:  28 loss: 0.11556704\n",
            "iteration:  29 loss: 0.08134224\n",
            "iteration:  30 loss: 0.05694102\n",
            "iteration:  31 loss: 0.08352564\n",
            "iteration:  32 loss: 0.08750454\n",
            "iteration:  33 loss: 0.14038044\n",
            "iteration:  34 loss: 0.14771491\n",
            "iteration:  35 loss: 0.14752798\n",
            "iteration:  36 loss: 0.07785109\n",
            "iteration:  37 loss: 0.12276356\n",
            "iteration:  38 loss: 0.10716502\n",
            "iteration:  39 loss: 0.16914718\n",
            "iteration:  40 loss: 0.15579179\n",
            "iteration:  41 loss: 0.06858121\n",
            "iteration:  42 loss: 0.10821933\n",
            "iteration:  43 loss: 0.10781555\n",
            "iteration:  44 loss: 0.06658133\n",
            "iteration:  45 loss: 0.18167661\n",
            "iteration:  46 loss: 0.14693806\n",
            "iteration:  47 loss: 0.08460488\n",
            "iteration:  48 loss: 0.11049861\n",
            "iteration:  49 loss: 0.05448905\n",
            "iteration:  50 loss: 0.07700521\n",
            "iteration:  51 loss: 0.07699168\n",
            "iteration:  52 loss: 0.11771540\n",
            "iteration:  53 loss: 0.09229040\n",
            "iteration:  54 loss: 0.06441196\n",
            "iteration:  55 loss: 0.05172009\n",
            "iteration:  56 loss: 0.10364382\n",
            "iteration:  57 loss: 0.10291710\n",
            "iteration:  58 loss: 0.12824820\n",
            "iteration:  59 loss: 0.10113844\n",
            "iteration:  60 loss: 0.09610275\n",
            "iteration:  61 loss: 0.11529781\n",
            "iteration:  62 loss: 0.11149269\n",
            "iteration:  63 loss: 0.07114998\n",
            "iteration:  64 loss: 0.08394131\n",
            "iteration:  65 loss: 0.20292482\n",
            "iteration:  66 loss: 0.05806652\n",
            "iteration:  67 loss: 0.10690191\n",
            "iteration:  68 loss: 0.06246599\n",
            "iteration:  69 loss: 0.07343663\n",
            "iteration:  70 loss: 0.07750227\n",
            "iteration:  71 loss: 0.12988898\n",
            "iteration:  72 loss: 0.08940080\n",
            "iteration:  73 loss: 0.05676335\n",
            "iteration:  74 loss: 0.11645927\n",
            "iteration:  75 loss: 0.34993675\n",
            "iteration:  76 loss: 0.09488208\n",
            "iteration:  77 loss: 0.07583275\n",
            "iteration:  78 loss: 0.18866539\n",
            "iteration:  79 loss: 0.06141002\n",
            "iteration:  80 loss: 0.08050680\n",
            "iteration:  81 loss: 0.06851208\n",
            "iteration:  82 loss: 0.14043763\n",
            "iteration:  83 loss: 0.14420637\n",
            "iteration:  84 loss: 0.06811538\n",
            "iteration:  85 loss: 0.07646554\n",
            "iteration:  86 loss: 0.14398479\n",
            "iteration:  87 loss: 0.20318502\n",
            "iteration:  88 loss: 0.14393790\n",
            "iteration:  89 loss: 0.08329982\n",
            "iteration:  90 loss: 0.08362332\n",
            "iteration:  91 loss: 0.09784016\n",
            "iteration:  92 loss: 0.10521969\n",
            "iteration:  93 loss: 0.06536765\n",
            "iteration:  94 loss: 0.19619572\n",
            "iteration:  95 loss: 0.09542008\n",
            "iteration:  96 loss: 0.05745742\n",
            "iteration:  97 loss: 0.23132041\n",
            "iteration:  98 loss: 0.11202171\n",
            "iteration:  99 loss: 0.06439302\n",
            "iteration: 100 loss: 0.09117554\n",
            "iteration: 101 loss: 0.06518365\n",
            "iteration: 102 loss: 0.07861403\n",
            "iteration: 103 loss: 0.11336892\n",
            "iteration: 104 loss: 0.13651748\n",
            "iteration: 105 loss: 0.25934660\n",
            "iteration: 106 loss: 0.08854937\n",
            "iteration: 107 loss: 0.15637246\n",
            "iteration: 108 loss: 0.09783724\n",
            "iteration: 109 loss: 0.18459035\n",
            "iteration: 110 loss: 0.19900805\n",
            "iteration: 111 loss: 0.35258925\n",
            "iteration: 112 loss: 0.07296452\n",
            "iteration: 113 loss: 0.04299929\n",
            "iteration: 114 loss: 0.10887578\n",
            "iteration: 115 loss: 0.28117031\n",
            "iteration: 116 loss: 0.11821600\n",
            "iteration: 117 loss: 0.10683392\n",
            "iteration: 118 loss: 0.07910925\n",
            "iteration: 119 loss: 0.08724810\n",
            "iteration: 120 loss: 0.06963273\n",
            "iteration: 121 loss: 0.07884268\n",
            "iteration: 122 loss: 0.13597967\n",
            "iteration: 123 loss: 0.62059975\n",
            "iteration: 124 loss: 0.05820330\n",
            "iteration: 125 loss: 0.13126875\n",
            "iteration: 126 loss: 0.13792644\n",
            "iteration: 127 loss: 0.12962000\n",
            "iteration: 128 loss: 0.13652344\n",
            "iteration: 129 loss: 0.15818632\n",
            "iteration: 130 loss: 0.06249660\n",
            "iteration: 131 loss: 0.10899869\n",
            "iteration: 132 loss: 0.05656416\n",
            "iteration: 133 loss: 0.07125562\n",
            "iteration: 134 loss: 0.09923792\n",
            "iteration: 135 loss: 0.13322261\n",
            "iteration: 136 loss: 0.31901687\n",
            "iteration: 137 loss: 0.10054430\n",
            "iteration: 138 loss: 0.13344707\n",
            "iteration: 139 loss: 0.18332061\n",
            "iteration: 140 loss: 0.22933917\n",
            "iteration: 141 loss: 0.09821649\n",
            "iteration: 142 loss: 0.12417354\n",
            "iteration: 143 loss: 0.07254770\n",
            "iteration: 144 loss: 0.10055911\n",
            "iteration: 145 loss: 0.09283060\n",
            "iteration: 146 loss: 0.15174747\n",
            "iteration: 147 loss: 0.24746193\n",
            "iteration: 148 loss: 0.06793512\n",
            "iteration: 149 loss: 0.14573798\n",
            "iteration: 150 loss: 0.10703763\n",
            "iteration: 151 loss: 0.04655477\n",
            "iteration: 152 loss: 0.22284691\n",
            "iteration: 153 loss: 0.10049313\n",
            "iteration: 154 loss: 0.07607532\n",
            "iteration: 155 loss: 0.13913304\n",
            "iteration: 156 loss: 0.10996858\n",
            "iteration: 157 loss: 0.13507336\n",
            "iteration: 158 loss: 0.16779348\n",
            "iteration: 159 loss: 0.15143746\n",
            "iteration: 160 loss: 0.09246075\n",
            "iteration: 161 loss: 0.17648774\n",
            "iteration: 162 loss: 0.15349145\n",
            "iteration: 163 loss: 0.08762731\n",
            "iteration: 164 loss: 0.11296376\n",
            "iteration: 165 loss: 0.09518452\n",
            "iteration: 166 loss: 0.09998845\n",
            "iteration: 167 loss: 0.07922402\n",
            "iteration: 168 loss: 0.06509798\n",
            "iteration: 169 loss: 0.13838795\n",
            "iteration: 170 loss: 0.09455109\n",
            "iteration: 171 loss: 0.14002353\n",
            "iteration: 172 loss: 0.34467033\n",
            "iteration: 173 loss: 0.16182753\n",
            "iteration: 174 loss: 0.12599850\n",
            "iteration: 175 loss: 0.09916432\n",
            "iteration: 176 loss: 0.08863946\n",
            "iteration: 177 loss: 0.10328898\n",
            "iteration: 178 loss: 0.06378741\n",
            "iteration: 179 loss: 0.09304740\n",
            "iteration: 180 loss: 0.17205156\n",
            "iteration: 181 loss: 0.13034202\n",
            "iteration: 182 loss: 0.09606384\n",
            "iteration: 183 loss: 0.08480953\n",
            "iteration: 184 loss: 0.05349531\n",
            "iteration: 185 loss: 0.19208583\n",
            "iteration: 186 loss: 0.06692506\n",
            "iteration: 187 loss: 0.09143175\n",
            "iteration: 188 loss: 0.15030989\n",
            "iteration: 189 loss: 0.16504064\n",
            "iteration: 190 loss: 0.09396805\n",
            "iteration: 191 loss: 0.06258330\n",
            "iteration: 192 loss: 0.06510755\n",
            "iteration: 193 loss: 0.07982669\n",
            "iteration: 194 loss: 0.08829476\n",
            "iteration: 195 loss: 0.15161811\n",
            "iteration: 196 loss: 0.13082853\n",
            "iteration: 197 loss: 0.06976980\n",
            "iteration: 198 loss: 0.08728255\n",
            "iteration: 199 loss: 0.12280052\n",
            "epoch:  77 mean loss training: 0.11798253\n",
            "epoch:  77 mean loss validation: 1.14284098\n",
            "iteration:   0 loss: 0.08268376\n",
            "iteration:   1 loss: 0.10749600\n",
            "iteration:   2 loss: 0.08269703\n",
            "iteration:   3 loss: 0.20695041\n",
            "iteration:   4 loss: 0.16626593\n",
            "iteration:   5 loss: 0.10659730\n",
            "iteration:   6 loss: 0.34177044\n",
            "iteration:   7 loss: 0.09523842\n",
            "iteration:   8 loss: 0.09417278\n",
            "iteration:   9 loss: 0.05480733\n",
            "iteration:  10 loss: 0.10950862\n",
            "iteration:  11 loss: 0.11157298\n",
            "iteration:  12 loss: 0.09167590\n",
            "iteration:  13 loss: 0.11904703\n",
            "iteration:  14 loss: 0.08747114\n",
            "iteration:  15 loss: 0.29706407\n",
            "iteration:  16 loss: 0.10409045\n",
            "iteration:  17 loss: 0.12093501\n",
            "iteration:  18 loss: 0.06560051\n",
            "iteration:  19 loss: 0.07136317\n",
            "iteration:  20 loss: 0.07903862\n",
            "iteration:  21 loss: 0.06894916\n",
            "iteration:  22 loss: 0.08648242\n",
            "iteration:  23 loss: 0.14112154\n",
            "iteration:  24 loss: 0.13502204\n",
            "iteration:  25 loss: 0.07381261\n",
            "iteration:  26 loss: 0.12350324\n",
            "iteration:  27 loss: 0.05347567\n",
            "iteration:  28 loss: 0.09307399\n",
            "iteration:  29 loss: 0.11263198\n",
            "iteration:  30 loss: 0.07729417\n",
            "iteration:  31 loss: 0.09229502\n",
            "iteration:  32 loss: 0.08020439\n",
            "iteration:  33 loss: 0.11572794\n",
            "iteration:  34 loss: 0.10406378\n",
            "iteration:  35 loss: 0.41541287\n",
            "iteration:  36 loss: 0.07110622\n",
            "iteration:  37 loss: 0.16968724\n",
            "iteration:  38 loss: 0.18288565\n",
            "iteration:  39 loss: 0.08653860\n",
            "iteration:  40 loss: 0.08023580\n",
            "iteration:  41 loss: 0.16746232\n",
            "iteration:  42 loss: 0.13000800\n",
            "iteration:  43 loss: 0.09282324\n",
            "iteration:  44 loss: 0.15534934\n",
            "iteration:  45 loss: 0.10201115\n",
            "iteration:  46 loss: 0.06244286\n",
            "iteration:  47 loss: 0.12027246\n",
            "iteration:  48 loss: 0.08668500\n",
            "iteration:  49 loss: 0.09791310\n",
            "iteration:  50 loss: 0.09551587\n",
            "iteration:  51 loss: 0.06602411\n",
            "iteration:  52 loss: 0.09086400\n",
            "iteration:  53 loss: 0.10923029\n",
            "iteration:  54 loss: 0.07811145\n",
            "iteration:  55 loss: 0.11029756\n",
            "iteration:  56 loss: 0.17800374\n",
            "iteration:  57 loss: 0.06973682\n",
            "iteration:  58 loss: 0.08362029\n",
            "iteration:  59 loss: 0.13400492\n",
            "iteration:  60 loss: 0.06496455\n",
            "iteration:  61 loss: 0.07017761\n",
            "iteration:  62 loss: 0.08722800\n",
            "iteration:  63 loss: 0.08486436\n",
            "iteration:  64 loss: 0.29446286\n",
            "iteration:  65 loss: 0.05967338\n",
            "iteration:  66 loss: 0.44564778\n",
            "iteration:  67 loss: 0.12958302\n",
            "iteration:  68 loss: 0.13838258\n",
            "iteration:  69 loss: 0.10113900\n",
            "iteration:  70 loss: 0.25394815\n",
            "iteration:  71 loss: 0.11253896\n",
            "iteration:  72 loss: 0.20043364\n",
            "iteration:  73 loss: 0.09148866\n",
            "iteration:  74 loss: 0.06714309\n",
            "iteration:  75 loss: 0.16459069\n",
            "iteration:  76 loss: 0.04513928\n",
            "iteration:  77 loss: 0.33561629\n",
            "iteration:  78 loss: 0.07763840\n",
            "iteration:  79 loss: 0.18968898\n",
            "iteration:  80 loss: 0.23793025\n",
            "iteration:  81 loss: 0.10083237\n",
            "iteration:  82 loss: 0.07657604\n",
            "iteration:  83 loss: 0.05163180\n",
            "iteration:  84 loss: 0.08310278\n",
            "iteration:  85 loss: 0.16231152\n",
            "iteration:  86 loss: 0.10533488\n",
            "iteration:  87 loss: 0.07705291\n",
            "iteration:  88 loss: 0.11326341\n",
            "iteration:  89 loss: 0.09790039\n",
            "iteration:  90 loss: 0.15872706\n",
            "iteration:  91 loss: 0.10552416\n",
            "iteration:  92 loss: 0.08343642\n",
            "iteration:  93 loss: 0.07122910\n",
            "iteration:  94 loss: 0.09425402\n",
            "iteration:  95 loss: 0.11202774\n",
            "iteration:  96 loss: 0.08760425\n",
            "iteration:  97 loss: 0.07158098\n",
            "iteration:  98 loss: 0.13537967\n",
            "iteration:  99 loss: 0.09610136\n",
            "iteration: 100 loss: 0.15016128\n",
            "iteration: 101 loss: 0.19205904\n",
            "iteration: 102 loss: 0.29540244\n",
            "iteration: 103 loss: 0.14003500\n",
            "iteration: 104 loss: 0.16689110\n",
            "iteration: 105 loss: 0.04794990\n",
            "iteration: 106 loss: 0.23394594\n",
            "iteration: 107 loss: 0.07984371\n",
            "iteration: 108 loss: 0.14266951\n",
            "iteration: 109 loss: 0.04526605\n",
            "iteration: 110 loss: 0.21560012\n",
            "iteration: 111 loss: 0.08859952\n",
            "iteration: 112 loss: 0.06727471\n",
            "iteration: 113 loss: 0.11335850\n",
            "iteration: 114 loss: 0.09623368\n",
            "iteration: 115 loss: 0.10493604\n",
            "iteration: 116 loss: 0.12941918\n",
            "iteration: 117 loss: 0.07468766\n",
            "iteration: 118 loss: 0.04847557\n",
            "iteration: 119 loss: 0.07121637\n",
            "iteration: 120 loss: 0.06893936\n",
            "iteration: 121 loss: 0.06118570\n",
            "iteration: 122 loss: 0.07359947\n",
            "iteration: 123 loss: 0.28325886\n",
            "iteration: 124 loss: 0.06893754\n",
            "iteration: 125 loss: 0.11304411\n",
            "iteration: 126 loss: 0.10097718\n",
            "iteration: 127 loss: 0.09006957\n",
            "iteration: 128 loss: 0.12269292\n",
            "iteration: 129 loss: 0.16858998\n",
            "iteration: 130 loss: 0.12797707\n",
            "iteration: 131 loss: 0.19155666\n",
            "iteration: 132 loss: 0.14039129\n",
            "iteration: 133 loss: 0.09558956\n",
            "iteration: 134 loss: 0.12443344\n",
            "iteration: 135 loss: 0.07219394\n",
            "iteration: 136 loss: 0.09297715\n",
            "iteration: 137 loss: 0.08483133\n",
            "iteration: 138 loss: 0.16490562\n",
            "iteration: 139 loss: 0.05497849\n",
            "iteration: 140 loss: 0.11086835\n",
            "iteration: 141 loss: 0.13296190\n",
            "iteration: 142 loss: 0.19953471\n",
            "iteration: 143 loss: 0.14601168\n",
            "iteration: 144 loss: 0.14463788\n",
            "iteration: 145 loss: 0.07674015\n",
            "iteration: 146 loss: 0.07232923\n",
            "iteration: 147 loss: 0.11280421\n",
            "iteration: 148 loss: 0.10192004\n",
            "iteration: 149 loss: 0.15224391\n",
            "iteration: 150 loss: 0.07092118\n",
            "iteration: 151 loss: 0.15722376\n",
            "iteration: 152 loss: 0.08710039\n",
            "iteration: 153 loss: 0.04459000\n",
            "iteration: 154 loss: 0.73219824\n",
            "iteration: 155 loss: 0.09765439\n",
            "iteration: 156 loss: 0.06733794\n",
            "iteration: 157 loss: 0.10279727\n",
            "iteration: 158 loss: 0.09301031\n",
            "iteration: 159 loss: 0.14078730\n",
            "iteration: 160 loss: 0.13544375\n",
            "iteration: 161 loss: 0.12526768\n",
            "iteration: 162 loss: 0.19289507\n",
            "iteration: 163 loss: 0.08911388\n",
            "iteration: 164 loss: 0.12554848\n",
            "iteration: 165 loss: 0.06353503\n",
            "iteration: 166 loss: 0.16742310\n",
            "iteration: 167 loss: 0.24383256\n",
            "iteration: 168 loss: 0.13652699\n",
            "iteration: 169 loss: 0.08247432\n",
            "iteration: 170 loss: 0.05866754\n",
            "iteration: 171 loss: 0.09753048\n",
            "iteration: 172 loss: 0.29499847\n",
            "iteration: 173 loss: 0.08538351\n",
            "iteration: 174 loss: 0.10028408\n",
            "iteration: 175 loss: 0.09913667\n",
            "iteration: 176 loss: 0.06390137\n",
            "iteration: 177 loss: 0.10651857\n",
            "iteration: 178 loss: 0.15248084\n",
            "iteration: 179 loss: 0.36923841\n",
            "iteration: 180 loss: 0.06567541\n",
            "iteration: 181 loss: 0.10842675\n",
            "iteration: 182 loss: 0.07268840\n",
            "iteration: 183 loss: 0.08218790\n",
            "iteration: 184 loss: 0.10502697\n",
            "iteration: 185 loss: 0.06828662\n",
            "iteration: 186 loss: 0.08835912\n",
            "iteration: 187 loss: 0.09918629\n",
            "iteration: 188 loss: 0.14119929\n",
            "iteration: 189 loss: 0.11503716\n",
            "iteration: 190 loss: 0.06180075\n",
            "iteration: 191 loss: 0.07078286\n",
            "iteration: 192 loss: 0.10052000\n",
            "iteration: 193 loss: 0.09860344\n",
            "iteration: 194 loss: 0.07131805\n",
            "iteration: 195 loss: 0.09990968\n",
            "iteration: 196 loss: 0.08978677\n",
            "iteration: 197 loss: 0.11359685\n",
            "iteration: 198 loss: 0.09699978\n",
            "iteration: 199 loss: 0.10443362\n",
            "epoch:  78 mean loss training: 0.12209477\n",
            "epoch:  78 mean loss validation: 1.14984822\n",
            "iteration:   0 loss: 0.05112423\n",
            "iteration:   1 loss: 0.09828406\n",
            "iteration:   2 loss: 0.20670882\n",
            "iteration:   3 loss: 0.06194730\n",
            "iteration:   4 loss: 0.07399556\n",
            "iteration:   5 loss: 0.15075839\n",
            "iteration:   6 loss: 0.06618968\n",
            "iteration:   7 loss: 0.11225550\n",
            "iteration:   8 loss: 0.08796665\n",
            "iteration:   9 loss: 0.08463281\n",
            "iteration:  10 loss: 0.08463996\n",
            "iteration:  11 loss: 0.08784759\n",
            "iteration:  12 loss: 0.04825802\n",
            "iteration:  13 loss: 0.09098101\n",
            "iteration:  14 loss: 0.05817644\n",
            "iteration:  15 loss: 0.14552674\n",
            "iteration:  16 loss: 0.18778346\n",
            "iteration:  17 loss: 0.06901782\n",
            "iteration:  18 loss: 0.06255065\n",
            "iteration:  19 loss: 0.10848904\n",
            "iteration:  20 loss: 0.17802434\n",
            "iteration:  21 loss: 0.09739947\n",
            "iteration:  22 loss: 0.14194335\n",
            "iteration:  23 loss: 0.52122700\n",
            "iteration:  24 loss: 0.11470212\n",
            "iteration:  25 loss: 0.11400632\n",
            "iteration:  26 loss: 0.10314982\n",
            "iteration:  27 loss: 0.13185920\n",
            "iteration:  28 loss: 0.05464088\n",
            "iteration:  29 loss: 0.12663735\n",
            "iteration:  30 loss: 0.06992288\n",
            "iteration:  31 loss: 0.25383964\n",
            "iteration:  32 loss: 0.10319804\n",
            "iteration:  33 loss: 0.12871151\n",
            "iteration:  34 loss: 0.06884467\n",
            "iteration:  35 loss: 0.07726361\n",
            "iteration:  36 loss: 0.07015153\n",
            "iteration:  37 loss: 0.05892874\n",
            "iteration:  38 loss: 0.07792627\n",
            "iteration:  39 loss: 0.17787953\n",
            "iteration:  40 loss: 0.15085360\n",
            "iteration:  41 loss: 0.07370910\n",
            "iteration:  42 loss: 0.10717995\n",
            "iteration:  43 loss: 0.05334980\n",
            "iteration:  44 loss: 0.07440744\n",
            "iteration:  45 loss: 0.06332114\n",
            "iteration:  46 loss: 0.12250851\n",
            "iteration:  47 loss: 0.09347746\n",
            "iteration:  48 loss: 0.08941521\n",
            "iteration:  49 loss: 0.07629080\n",
            "iteration:  50 loss: 0.12302059\n",
            "iteration:  51 loss: 0.08564316\n",
            "iteration:  52 loss: 0.15345782\n",
            "iteration:  53 loss: 0.05730336\n",
            "iteration:  54 loss: 0.05908056\n",
            "iteration:  55 loss: 0.16989475\n",
            "iteration:  56 loss: 0.09743954\n",
            "iteration:  57 loss: 0.25105664\n",
            "iteration:  58 loss: 0.13221158\n",
            "iteration:  59 loss: 0.39283866\n",
            "iteration:  60 loss: 0.17034361\n",
            "iteration:  61 loss: 0.05360718\n",
            "iteration:  62 loss: 0.12628749\n",
            "iteration:  63 loss: 0.12865914\n",
            "iteration:  64 loss: 0.10706896\n",
            "iteration:  65 loss: 0.07538231\n",
            "iteration:  66 loss: 0.10339276\n",
            "iteration:  67 loss: 0.12003405\n",
            "iteration:  68 loss: 0.16371596\n",
            "iteration:  69 loss: 0.08761404\n",
            "iteration:  70 loss: 0.10555197\n",
            "iteration:  71 loss: 0.12879735\n",
            "iteration:  72 loss: 0.20496707\n",
            "iteration:  73 loss: 0.13207950\n",
            "iteration:  74 loss: 0.07546786\n",
            "iteration:  75 loss: 0.17370674\n",
            "iteration:  76 loss: 0.15104480\n",
            "iteration:  77 loss: 0.10936795\n",
            "iteration:  78 loss: 0.17826366\n",
            "iteration:  79 loss: 0.13838936\n",
            "iteration:  80 loss: 0.10447139\n",
            "iteration:  81 loss: 0.09987767\n",
            "iteration:  82 loss: 0.09627479\n",
            "iteration:  83 loss: 0.07530113\n",
            "iteration:  84 loss: 0.16328393\n",
            "iteration:  85 loss: 0.06503946\n",
            "iteration:  86 loss: 0.11620596\n",
            "iteration:  87 loss: 0.21191223\n",
            "iteration:  88 loss: 0.07572692\n",
            "iteration:  89 loss: 0.05978556\n",
            "iteration:  90 loss: 0.07389427\n",
            "iteration:  91 loss: 0.08811404\n",
            "iteration:  92 loss: 0.30018955\n",
            "iteration:  93 loss: 0.21227619\n",
            "iteration:  94 loss: 0.07615422\n",
            "iteration:  95 loss: 0.08789831\n",
            "iteration:  96 loss: 0.07653263\n",
            "iteration:  97 loss: 0.12832549\n",
            "iteration:  98 loss: 0.12653147\n",
            "iteration:  99 loss: 0.18663865\n",
            "iteration: 100 loss: 0.13790137\n",
            "iteration: 101 loss: 0.11081793\n",
            "iteration: 102 loss: 0.07615037\n",
            "iteration: 103 loss: 0.13476740\n",
            "iteration: 104 loss: 0.28846553\n",
            "iteration: 105 loss: 0.13349858\n",
            "iteration: 106 loss: 0.06579824\n",
            "iteration: 107 loss: 0.06397828\n",
            "iteration: 108 loss: 0.10656837\n",
            "iteration: 109 loss: 0.08421764\n",
            "iteration: 110 loss: 0.08294795\n",
            "iteration: 111 loss: 0.06867131\n",
            "iteration: 112 loss: 0.08357860\n",
            "iteration: 113 loss: 0.09455281\n",
            "iteration: 114 loss: 0.32215053\n",
            "iteration: 115 loss: 0.06041607\n",
            "iteration: 116 loss: 0.10326240\n",
            "iteration: 117 loss: 0.15157767\n",
            "iteration: 118 loss: 0.07631890\n",
            "iteration: 119 loss: 0.09398858\n",
            "iteration: 120 loss: 0.12680241\n",
            "iteration: 121 loss: 0.16986001\n",
            "iteration: 122 loss: 0.07884216\n",
            "iteration: 123 loss: 0.07923619\n",
            "iteration: 124 loss: 0.05367819\n",
            "iteration: 125 loss: 0.13439807\n",
            "iteration: 126 loss: 0.18282069\n",
            "iteration: 127 loss: 0.13507442\n",
            "iteration: 128 loss: 0.06177771\n",
            "iteration: 129 loss: 0.09517750\n",
            "iteration: 130 loss: 0.07279581\n",
            "iteration: 131 loss: 0.04584657\n",
            "iteration: 132 loss: 0.15219533\n",
            "iteration: 133 loss: 0.08482430\n",
            "iteration: 134 loss: 0.11720765\n",
            "iteration: 135 loss: 0.13291585\n",
            "iteration: 136 loss: 0.10291368\n",
            "iteration: 137 loss: 0.42231685\n",
            "iteration: 138 loss: 0.09166548\n",
            "iteration: 139 loss: 0.12292841\n",
            "iteration: 140 loss: 0.12003794\n",
            "iteration: 141 loss: 0.13053994\n",
            "iteration: 142 loss: 0.11262943\n",
            "iteration: 143 loss: 0.14884365\n",
            "iteration: 144 loss: 0.14555126\n",
            "iteration: 145 loss: 0.05801835\n",
            "iteration: 146 loss: 0.18691519\n",
            "iteration: 147 loss: 0.05847225\n",
            "iteration: 148 loss: 0.09521189\n",
            "iteration: 149 loss: 0.08799337\n",
            "iteration: 150 loss: 0.06764653\n",
            "iteration: 151 loss: 0.09672803\n",
            "iteration: 152 loss: 0.13006066\n",
            "iteration: 153 loss: 0.32328886\n",
            "iteration: 154 loss: 0.08021428\n",
            "iteration: 155 loss: 0.14713913\n",
            "iteration: 156 loss: 0.17896506\n",
            "iteration: 157 loss: 0.05588394\n",
            "iteration: 158 loss: 0.07763851\n",
            "iteration: 159 loss: 0.12774409\n",
            "iteration: 160 loss: 0.10785493\n",
            "iteration: 161 loss: 0.09702049\n",
            "iteration: 162 loss: 0.09458999\n",
            "iteration: 163 loss: 0.08514720\n",
            "iteration: 164 loss: 0.53765839\n",
            "iteration: 165 loss: 0.07594419\n",
            "iteration: 166 loss: 0.09727465\n",
            "iteration: 167 loss: 0.12431535\n",
            "iteration: 168 loss: 0.11109533\n",
            "iteration: 169 loss: 0.11423571\n",
            "iteration: 170 loss: 0.08139093\n",
            "iteration: 171 loss: 0.07427274\n",
            "iteration: 172 loss: 0.12243856\n",
            "iteration: 173 loss: 0.10103312\n",
            "iteration: 174 loss: 0.14631529\n",
            "iteration: 175 loss: 0.07931229\n",
            "iteration: 176 loss: 0.06336946\n",
            "iteration: 177 loss: 0.18763191\n",
            "iteration: 178 loss: 0.09038366\n",
            "iteration: 179 loss: 0.06059023\n",
            "iteration: 180 loss: 0.08483290\n",
            "iteration: 181 loss: 0.16119003\n",
            "iteration: 182 loss: 0.05365192\n",
            "iteration: 183 loss: 0.18070748\n",
            "iteration: 184 loss: 0.07587148\n",
            "iteration: 185 loss: 0.12784110\n",
            "iteration: 186 loss: 0.08338611\n",
            "iteration: 187 loss: 0.14469646\n",
            "iteration: 188 loss: 0.13021606\n",
            "iteration: 189 loss: 0.16415218\n",
            "iteration: 190 loss: 0.11391380\n",
            "iteration: 191 loss: 0.14487255\n",
            "iteration: 192 loss: 0.07446050\n",
            "iteration: 193 loss: 0.05923340\n",
            "iteration: 194 loss: 0.06988139\n",
            "iteration: 195 loss: 0.09081267\n",
            "iteration: 196 loss: 0.08362108\n",
            "iteration: 197 loss: 0.13643183\n",
            "iteration: 198 loss: 0.08308801\n",
            "iteration: 199 loss: 0.08643964\n",
            "epoch:  79 mean loss training: 0.11912815\n",
            "epoch:  79 mean loss validation: 1.18128371\n",
            "iteration:   0 loss: 0.10705553\n",
            "iteration:   1 loss: 0.12225610\n",
            "iteration:   2 loss: 0.15016600\n",
            "iteration:   3 loss: 0.07900551\n",
            "iteration:   4 loss: 0.05355482\n",
            "iteration:   5 loss: 0.08481191\n",
            "iteration:   6 loss: 0.09482638\n",
            "iteration:   7 loss: 0.13077866\n",
            "iteration:   8 loss: 0.10273952\n",
            "iteration:   9 loss: 0.18099867\n",
            "iteration:  10 loss: 0.11159396\n",
            "iteration:  11 loss: 0.07489401\n",
            "iteration:  12 loss: 0.09481808\n",
            "iteration:  13 loss: 0.08872470\n",
            "iteration:  14 loss: 0.09284541\n",
            "iteration:  15 loss: 0.06461816\n",
            "iteration:  16 loss: 0.14565679\n",
            "iteration:  17 loss: 0.09609792\n",
            "iteration:  18 loss: 0.09649323\n",
            "iteration:  19 loss: 0.13034806\n",
            "iteration:  20 loss: 0.09518145\n",
            "iteration:  21 loss: 0.05970250\n",
            "iteration:  22 loss: 0.11788745\n",
            "iteration:  23 loss: 0.06030751\n",
            "iteration:  24 loss: 0.11870009\n",
            "iteration:  25 loss: 0.09128983\n",
            "iteration:  26 loss: 0.07705956\n",
            "iteration:  27 loss: 0.11284728\n",
            "iteration:  28 loss: 0.14222279\n",
            "iteration:  29 loss: 0.11739840\n",
            "iteration:  30 loss: 0.16627848\n",
            "iteration:  31 loss: 0.08615714\n",
            "iteration:  32 loss: 0.08126093\n",
            "iteration:  33 loss: 0.19162564\n",
            "iteration:  34 loss: 0.21597007\n",
            "iteration:  35 loss: 0.10743609\n",
            "iteration:  36 loss: 0.13204665\n",
            "iteration:  37 loss: 0.11726631\n",
            "iteration:  38 loss: 0.10474747\n",
            "iteration:  39 loss: 0.08938074\n",
            "iteration:  40 loss: 0.14494194\n",
            "iteration:  41 loss: 0.11834221\n",
            "iteration:  42 loss: 0.31381956\n",
            "iteration:  43 loss: 0.09623316\n",
            "iteration:  44 loss: 0.19583602\n",
            "iteration:  45 loss: 0.07600921\n",
            "iteration:  46 loss: 0.08104672\n",
            "iteration:  47 loss: 0.13169299\n",
            "iteration:  48 loss: 0.20743084\n",
            "iteration:  49 loss: 0.08479244\n",
            "iteration:  50 loss: 0.09055644\n",
            "iteration:  51 loss: 0.17654109\n",
            "iteration:  52 loss: 0.06896052\n",
            "iteration:  53 loss: 0.07763066\n",
            "iteration:  54 loss: 0.09650993\n",
            "iteration:  55 loss: 0.08334595\n",
            "iteration:  56 loss: 0.09159076\n",
            "iteration:  57 loss: 0.24470818\n",
            "iteration:  58 loss: 0.19265555\n",
            "iteration:  59 loss: 0.08468504\n",
            "iteration:  60 loss: 0.09820609\n",
            "iteration:  61 loss: 0.10172620\n",
            "iteration:  62 loss: 0.18114361\n",
            "iteration:  63 loss: 0.06169015\n",
            "iteration:  64 loss: 0.29816684\n",
            "iteration:  65 loss: 0.28168356\n",
            "iteration:  66 loss: 0.07350178\n",
            "iteration:  67 loss: 0.12957615\n",
            "iteration:  68 loss: 0.11924823\n",
            "iteration:  69 loss: 0.10163930\n",
            "iteration:  70 loss: 0.07517372\n",
            "iteration:  71 loss: 0.07358045\n",
            "iteration:  72 loss: 0.11772661\n",
            "iteration:  73 loss: 0.11457808\n",
            "iteration:  74 loss: 0.07850639\n",
            "iteration:  75 loss: 0.06401129\n",
            "iteration:  76 loss: 0.13620980\n",
            "iteration:  77 loss: 0.07275373\n",
            "iteration:  78 loss: 0.13598673\n",
            "iteration:  79 loss: 0.05761139\n",
            "iteration:  80 loss: 0.11003912\n",
            "iteration:  81 loss: 0.04902746\n",
            "iteration:  82 loss: 0.12662661\n",
            "iteration:  83 loss: 0.07053691\n",
            "iteration:  84 loss: 0.13715765\n",
            "iteration:  85 loss: 0.05553097\n",
            "iteration:  86 loss: 0.10295247\n",
            "iteration:  87 loss: 0.06948984\n",
            "iteration:  88 loss: 0.06732865\n",
            "iteration:  89 loss: 0.08612634\n",
            "iteration:  90 loss: 0.11061731\n",
            "iteration:  91 loss: 0.06412339\n",
            "iteration:  92 loss: 0.08475962\n",
            "iteration:  93 loss: 0.10980563\n",
            "iteration:  94 loss: 0.08933619\n",
            "iteration:  95 loss: 0.21844879\n",
            "iteration:  96 loss: 0.10886198\n",
            "iteration:  97 loss: 0.06867300\n",
            "iteration:  98 loss: 0.05297889\n",
            "iteration:  99 loss: 0.14008448\n",
            "iteration: 100 loss: 0.09319867\n",
            "iteration: 101 loss: 0.18753025\n",
            "iteration: 102 loss: 0.06565779\n",
            "iteration: 103 loss: 0.05929526\n",
            "iteration: 104 loss: 0.11166631\n",
            "iteration: 105 loss: 0.06257870\n",
            "iteration: 106 loss: 0.11849399\n",
            "iteration: 107 loss: 0.56756163\n",
            "iteration: 108 loss: 0.08943796\n",
            "iteration: 109 loss: 0.05795870\n",
            "iteration: 110 loss: 0.18067248\n",
            "iteration: 111 loss: 0.09171999\n",
            "iteration: 112 loss: 0.06505141\n",
            "iteration: 113 loss: 0.07057444\n",
            "iteration: 114 loss: 0.06604842\n",
            "iteration: 115 loss: 0.12139365\n",
            "iteration: 116 loss: 0.04981469\n",
            "iteration: 117 loss: 0.07618793\n",
            "iteration: 118 loss: 0.13842286\n",
            "iteration: 119 loss: 0.14760759\n",
            "iteration: 120 loss: 0.21327963\n",
            "iteration: 121 loss: 0.06998929\n",
            "iteration: 122 loss: 0.26104355\n",
            "iteration: 123 loss: 0.28729609\n",
            "iteration: 124 loss: 0.08467452\n",
            "iteration: 125 loss: 0.07617892\n",
            "iteration: 126 loss: 0.07582726\n",
            "iteration: 127 loss: 0.05756889\n",
            "iteration: 128 loss: 0.08418754\n",
            "iteration: 129 loss: 0.07586536\n",
            "iteration: 130 loss: 0.17361465\n",
            "iteration: 131 loss: 0.09032980\n",
            "iteration: 132 loss: 0.12357114\n",
            "iteration: 133 loss: 0.19260688\n",
            "iteration: 134 loss: 0.17248650\n",
            "iteration: 135 loss: 0.28370154\n",
            "iteration: 136 loss: 0.10592805\n",
            "iteration: 137 loss: 0.09997401\n",
            "iteration: 138 loss: 0.08446048\n",
            "iteration: 139 loss: 0.08037274\n",
            "iteration: 140 loss: 0.09968007\n",
            "iteration: 141 loss: 0.14305288\n",
            "iteration: 142 loss: 0.19055650\n",
            "iteration: 143 loss: 0.07593072\n",
            "iteration: 144 loss: 0.16484064\n",
            "iteration: 145 loss: 0.05264314\n",
            "iteration: 146 loss: 0.14174888\n",
            "iteration: 147 loss: 0.07950748\n",
            "iteration: 148 loss: 0.09588766\n",
            "iteration: 149 loss: 0.12466164\n",
            "iteration: 150 loss: 0.10613037\n",
            "iteration: 151 loss: 0.11456372\n",
            "iteration: 152 loss: 0.08863685\n",
            "iteration: 153 loss: 0.08431184\n",
            "iteration: 154 loss: 0.05481609\n",
            "iteration: 155 loss: 0.13802212\n",
            "iteration: 156 loss: 0.11576369\n",
            "iteration: 157 loss: 0.08056557\n",
            "iteration: 158 loss: 0.07212093\n",
            "iteration: 159 loss: 0.11453342\n",
            "iteration: 160 loss: 0.08243829\n",
            "iteration: 161 loss: 0.08145055\n",
            "iteration: 162 loss: 0.18494219\n",
            "iteration: 163 loss: 0.10901585\n",
            "iteration: 164 loss: 0.05256622\n",
            "iteration: 165 loss: 0.20417976\n",
            "iteration: 166 loss: 0.04618873\n",
            "iteration: 167 loss: 0.13128044\n",
            "iteration: 168 loss: 0.06244132\n",
            "iteration: 169 loss: 0.08431560\n",
            "iteration: 170 loss: 0.06483684\n",
            "iteration: 171 loss: 0.22802645\n",
            "iteration: 172 loss: 0.12263672\n",
            "iteration: 173 loss: 0.11455405\n",
            "iteration: 174 loss: 0.07844915\n",
            "iteration: 175 loss: 0.11796947\n",
            "iteration: 176 loss: 0.12649933\n",
            "iteration: 177 loss: 0.06137071\n",
            "iteration: 178 loss: 0.14246494\n",
            "iteration: 179 loss: 0.15436175\n",
            "iteration: 180 loss: 0.08158208\n",
            "iteration: 181 loss: 0.34979442\n",
            "iteration: 182 loss: 0.19828609\n",
            "iteration: 183 loss: 0.06887335\n",
            "iteration: 184 loss: 0.45621607\n",
            "iteration: 185 loss: 0.05635684\n",
            "iteration: 186 loss: 0.06595487\n",
            "iteration: 187 loss: 0.15190001\n",
            "iteration: 188 loss: 0.07789740\n",
            "iteration: 189 loss: 0.06607370\n",
            "iteration: 190 loss: 0.14253333\n",
            "iteration: 191 loss: 0.09394754\n",
            "iteration: 192 loss: 0.11029408\n",
            "iteration: 193 loss: 0.21675885\n",
            "iteration: 194 loss: 0.17141284\n",
            "iteration: 195 loss: 0.25206685\n",
            "iteration: 196 loss: 0.14283881\n",
            "iteration: 197 loss: 0.10273661\n",
            "iteration: 198 loss: 0.06809848\n",
            "iteration: 199 loss: 0.17115346\n",
            "epoch:  80 mean loss training: 0.11943085\n",
            "epoch:  80 mean loss validation: 1.16335738\n",
            "iteration:   0 loss: 0.12791470\n",
            "iteration:   1 loss: 0.11289219\n",
            "iteration:   2 loss: 0.08410162\n",
            "iteration:   3 loss: 0.09584063\n",
            "iteration:   4 loss: 0.15094337\n",
            "iteration:   5 loss: 0.35270527\n",
            "iteration:   6 loss: 0.07906158\n",
            "iteration:   7 loss: 0.08254549\n",
            "iteration:   8 loss: 0.12236428\n",
            "iteration:   9 loss: 0.11989632\n",
            "iteration:  10 loss: 0.08625902\n",
            "iteration:  11 loss: 0.08569512\n",
            "iteration:  12 loss: 0.05778929\n",
            "iteration:  13 loss: 0.10258724\n",
            "iteration:  14 loss: 0.08907348\n",
            "iteration:  15 loss: 0.17241928\n",
            "iteration:  16 loss: 0.41702265\n",
            "iteration:  17 loss: 0.07162126\n",
            "iteration:  18 loss: 0.05770718\n",
            "iteration:  19 loss: 0.08924067\n",
            "iteration:  20 loss: 0.12718959\n",
            "iteration:  21 loss: 0.09423469\n",
            "iteration:  22 loss: 0.50441587\n",
            "iteration:  23 loss: 0.07915930\n",
            "iteration:  24 loss: 0.29860595\n",
            "iteration:  25 loss: 0.11027358\n",
            "iteration:  26 loss: 0.09909679\n",
            "iteration:  27 loss: 0.09528413\n",
            "iteration:  28 loss: 0.10560258\n",
            "iteration:  29 loss: 0.13637173\n",
            "iteration:  30 loss: 0.29626653\n",
            "iteration:  31 loss: 0.21678248\n",
            "iteration:  32 loss: 0.13293511\n",
            "iteration:  33 loss: 0.51251274\n",
            "iteration:  34 loss: 0.09496415\n",
            "iteration:  35 loss: 0.10525233\n",
            "iteration:  36 loss: 0.07623589\n",
            "iteration:  37 loss: 0.10378309\n",
            "iteration:  38 loss: 0.07325508\n",
            "iteration:  39 loss: 0.07257624\n",
            "iteration:  40 loss: 0.12481893\n",
            "iteration:  41 loss: 0.05608846\n",
            "iteration:  42 loss: 0.15576982\n",
            "iteration:  43 loss: 0.06051699\n",
            "iteration:  44 loss: 0.17449327\n",
            "iteration:  45 loss: 0.25006428\n",
            "iteration:  46 loss: 0.08087167\n",
            "iteration:  47 loss: 0.06214292\n",
            "iteration:  48 loss: 0.14943562\n",
            "iteration:  49 loss: 0.05899413\n",
            "iteration:  50 loss: 0.08943440\n",
            "iteration:  51 loss: 0.06459136\n",
            "iteration:  52 loss: 0.14049491\n",
            "iteration:  53 loss: 0.16968347\n",
            "iteration:  54 loss: 0.09028575\n",
            "iteration:  55 loss: 0.11344915\n",
            "iteration:  56 loss: 0.14216882\n",
            "iteration:  57 loss: 0.15867096\n",
            "iteration:  58 loss: 0.19059865\n",
            "iteration:  59 loss: 0.06257576\n",
            "iteration:  60 loss: 0.07487476\n",
            "iteration:  61 loss: 0.07914525\n",
            "iteration:  62 loss: 0.11081757\n",
            "iteration:  63 loss: 0.18882531\n",
            "iteration:  64 loss: 0.06201167\n",
            "iteration:  65 loss: 0.06289791\n",
            "iteration:  66 loss: 0.06992371\n",
            "iteration:  67 loss: 0.06383019\n",
            "iteration:  68 loss: 0.09602942\n",
            "iteration:  69 loss: 0.18037148\n",
            "iteration:  70 loss: 0.08883359\n",
            "iteration:  71 loss: 0.14895754\n",
            "iteration:  72 loss: 0.33820650\n",
            "iteration:  73 loss: 0.22245014\n",
            "iteration:  74 loss: 0.08140559\n",
            "iteration:  75 loss: 0.14776993\n",
            "iteration:  76 loss: 0.07168175\n",
            "iteration:  77 loss: 0.30439493\n",
            "iteration:  78 loss: 0.06003388\n",
            "iteration:  79 loss: 0.11701375\n",
            "iteration:  80 loss: 0.13458902\n",
            "iteration:  81 loss: 0.14616984\n",
            "iteration:  82 loss: 0.07959518\n",
            "iteration:  83 loss: 0.05939700\n",
            "iteration:  84 loss: 0.16344759\n",
            "iteration:  85 loss: 0.10201025\n",
            "iteration:  86 loss: 0.14452630\n",
            "iteration:  87 loss: 0.06839431\n",
            "iteration:  88 loss: 0.07948749\n",
            "iteration:  89 loss: 0.11323644\n",
            "iteration:  90 loss: 0.05677572\n",
            "iteration:  91 loss: 0.10158409\n",
            "iteration:  92 loss: 0.07697053\n",
            "iteration:  93 loss: 0.14863691\n",
            "iteration:  94 loss: 0.21610215\n",
            "iteration:  95 loss: 0.13952075\n",
            "iteration:  96 loss: 0.05460024\n",
            "iteration:  97 loss: 0.06879587\n",
            "iteration:  98 loss: 0.09341973\n",
            "iteration:  99 loss: 0.08828905\n",
            "iteration: 100 loss: 0.10875101\n",
            "iteration: 101 loss: 0.09959765\n",
            "iteration: 102 loss: 0.07090802\n",
            "iteration: 103 loss: 0.09039259\n",
            "iteration: 104 loss: 0.12803845\n",
            "iteration: 105 loss: 0.14460155\n",
            "iteration: 106 loss: 0.14373125\n",
            "iteration: 107 loss: 0.10237775\n",
            "iteration: 108 loss: 0.19303426\n",
            "iteration: 109 loss: 0.12189441\n",
            "iteration: 110 loss: 0.12621696\n",
            "iteration: 111 loss: 0.08209435\n",
            "iteration: 112 loss: 0.09562930\n",
            "iteration: 113 loss: 0.09398572\n",
            "iteration: 114 loss: 0.06087924\n",
            "iteration: 115 loss: 0.07014301\n",
            "iteration: 116 loss: 0.13488433\n",
            "iteration: 117 loss: 0.23457760\n",
            "iteration: 118 loss: 0.11780900\n",
            "iteration: 119 loss: 0.06181065\n",
            "iteration: 120 loss: 0.08673099\n",
            "iteration: 121 loss: 0.10714287\n",
            "iteration: 122 loss: 0.19019631\n",
            "iteration: 123 loss: 0.17541578\n",
            "iteration: 124 loss: 0.06628261\n",
            "iteration: 125 loss: 0.08733057\n",
            "iteration: 126 loss: 0.07031960\n",
            "iteration: 127 loss: 0.26169953\n",
            "iteration: 128 loss: 0.08520867\n",
            "iteration: 129 loss: 0.07850397\n",
            "iteration: 130 loss: 0.32628635\n",
            "iteration: 131 loss: 0.07596935\n",
            "iteration: 132 loss: 0.06238701\n",
            "iteration: 133 loss: 0.11109348\n",
            "iteration: 134 loss: 0.09849508\n",
            "iteration: 135 loss: 0.22896925\n",
            "iteration: 136 loss: 0.05849903\n",
            "iteration: 137 loss: 0.11180890\n",
            "iteration: 138 loss: 0.11718501\n",
            "iteration: 139 loss: 0.12871481\n",
            "iteration: 140 loss: 0.10519227\n",
            "iteration: 141 loss: 0.08619714\n",
            "iteration: 142 loss: 0.25193956\n",
            "iteration: 143 loss: 0.06073677\n",
            "iteration: 144 loss: 0.06867056\n",
            "iteration: 145 loss: 0.07123238\n",
            "iteration: 146 loss: 0.08649878\n",
            "iteration: 147 loss: 0.09174195\n",
            "iteration: 148 loss: 0.08254971\n",
            "iteration: 149 loss: 0.12742674\n",
            "iteration: 150 loss: 0.13302678\n",
            "iteration: 151 loss: 0.16031401\n",
            "iteration: 152 loss: 0.16081890\n",
            "iteration: 153 loss: 0.18899420\n",
            "iteration: 154 loss: 0.09215263\n",
            "iteration: 155 loss: 0.09701882\n",
            "iteration: 156 loss: 0.09230577\n",
            "iteration: 157 loss: 0.09148514\n",
            "iteration: 158 loss: 0.17090558\n",
            "iteration: 159 loss: 0.11264198\n",
            "iteration: 160 loss: 0.09002795\n",
            "iteration: 161 loss: 0.03918351\n",
            "iteration: 162 loss: 0.18918489\n",
            "iteration: 163 loss: 0.28011414\n",
            "iteration: 164 loss: 0.15720147\n",
            "iteration: 165 loss: 0.10771058\n",
            "iteration: 166 loss: 0.27793473\n",
            "iteration: 167 loss: 0.10224237\n",
            "iteration: 168 loss: 0.11066326\n",
            "iteration: 169 loss: 0.10506515\n",
            "iteration: 170 loss: 0.07049423\n",
            "iteration: 171 loss: 0.30382803\n",
            "iteration: 172 loss: 0.05074672\n",
            "iteration: 173 loss: 0.16292509\n",
            "iteration: 174 loss: 0.11233059\n",
            "iteration: 175 loss: 0.13113408\n",
            "iteration: 176 loss: 0.09549477\n",
            "iteration: 177 loss: 0.18271837\n",
            "iteration: 178 loss: 0.07962988\n",
            "iteration: 179 loss: 0.07127345\n",
            "iteration: 180 loss: 0.09318362\n",
            "iteration: 181 loss: 0.10088556\n",
            "iteration: 182 loss: 0.13990282\n",
            "iteration: 183 loss: 0.07428084\n",
            "iteration: 184 loss: 0.10898428\n",
            "iteration: 185 loss: 0.11681920\n",
            "iteration: 186 loss: 0.08389795\n",
            "iteration: 187 loss: 0.09193352\n",
            "iteration: 188 loss: 0.06496163\n",
            "iteration: 189 loss: 0.07063317\n",
            "iteration: 190 loss: 0.11818586\n",
            "iteration: 191 loss: 0.12131679\n",
            "iteration: 192 loss: 0.05271195\n",
            "iteration: 193 loss: 0.10680443\n",
            "iteration: 194 loss: 0.09635404\n",
            "iteration: 195 loss: 0.18705663\n",
            "iteration: 196 loss: 0.13523334\n",
            "iteration: 197 loss: 0.11581618\n",
            "iteration: 198 loss: 0.06295880\n",
            "iteration: 199 loss: 0.15062863\n",
            "epoch:  81 mean loss training: 0.12473276\n",
            "epoch:  81 mean loss validation: 1.16677105\n",
            "iteration:   0 loss: 0.07094889\n",
            "iteration:   1 loss: 0.06112336\n",
            "iteration:   2 loss: 0.18614188\n",
            "iteration:   3 loss: 0.09565167\n",
            "iteration:   4 loss: 0.07631472\n",
            "iteration:   5 loss: 0.08223564\n",
            "iteration:   6 loss: 0.32620519\n",
            "iteration:   7 loss: 0.04496417\n",
            "iteration:   8 loss: 0.17029031\n",
            "iteration:   9 loss: 0.07704245\n",
            "iteration:  10 loss: 0.06172985\n",
            "iteration:  11 loss: 0.08058348\n",
            "iteration:  12 loss: 0.09431231\n",
            "iteration:  13 loss: 0.05785335\n",
            "iteration:  14 loss: 0.11164778\n",
            "iteration:  15 loss: 0.09350886\n",
            "iteration:  16 loss: 0.04395283\n",
            "iteration:  17 loss: 0.10086560\n",
            "iteration:  18 loss: 0.13162014\n",
            "iteration:  19 loss: 0.05343071\n",
            "iteration:  20 loss: 0.07550055\n",
            "iteration:  21 loss: 0.05477703\n",
            "iteration:  22 loss: 0.07906638\n",
            "iteration:  23 loss: 0.07119054\n",
            "iteration:  24 loss: 0.10401996\n",
            "iteration:  25 loss: 0.10852794\n",
            "iteration:  26 loss: 0.13406307\n",
            "iteration:  27 loss: 0.11956920\n",
            "iteration:  28 loss: 0.22515289\n",
            "iteration:  29 loss: 0.07049565\n",
            "iteration:  30 loss: 0.06949651\n",
            "iteration:  31 loss: 0.18177877\n",
            "iteration:  32 loss: 0.05693182\n",
            "iteration:  33 loss: 0.08414952\n",
            "iteration:  34 loss: 0.46376929\n",
            "iteration:  35 loss: 0.07864871\n",
            "iteration:  36 loss: 0.35255483\n",
            "iteration:  37 loss: 0.10169438\n",
            "iteration:  38 loss: 0.19461669\n",
            "iteration:  39 loss: 0.09655607\n",
            "iteration:  40 loss: 0.05053507\n",
            "iteration:  41 loss: 0.11642149\n",
            "iteration:  42 loss: 0.08487585\n",
            "iteration:  43 loss: 0.12092579\n",
            "iteration:  44 loss: 0.20695099\n",
            "iteration:  45 loss: 0.09654693\n",
            "iteration:  46 loss: 0.16976476\n",
            "iteration:  47 loss: 0.17269610\n",
            "iteration:  48 loss: 0.11377589\n",
            "iteration:  49 loss: 0.19830987\n",
            "iteration:  50 loss: 0.07176282\n",
            "iteration:  51 loss: 0.30886894\n",
            "iteration:  52 loss: 0.12829509\n",
            "iteration:  53 loss: 0.08395173\n",
            "iteration:  54 loss: 0.04944352\n",
            "iteration:  55 loss: 0.11882909\n",
            "iteration:  56 loss: 0.17813632\n",
            "iteration:  57 loss: 0.04996265\n",
            "iteration:  58 loss: 0.07849957\n",
            "iteration:  59 loss: 0.19042170\n",
            "iteration:  60 loss: 0.48893386\n",
            "iteration:  61 loss: 0.05450101\n",
            "iteration:  62 loss: 0.12513815\n",
            "iteration:  63 loss: 0.17093185\n",
            "iteration:  64 loss: 0.30526590\n",
            "iteration:  65 loss: 0.08094265\n",
            "iteration:  66 loss: 0.14098480\n",
            "iteration:  67 loss: 0.27327752\n",
            "iteration:  68 loss: 0.08051173\n",
            "iteration:  69 loss: 0.03923935\n",
            "iteration:  70 loss: 0.13199349\n",
            "iteration:  71 loss: 0.09632926\n",
            "iteration:  72 loss: 0.10503603\n",
            "iteration:  73 loss: 0.10174672\n",
            "iteration:  74 loss: 0.38982150\n",
            "iteration:  75 loss: 0.17790210\n",
            "iteration:  76 loss: 0.30148304\n",
            "iteration:  77 loss: 0.08076219\n",
            "iteration:  78 loss: 0.06004330\n",
            "iteration:  79 loss: 0.12163235\n",
            "iteration:  80 loss: 0.14154766\n",
            "iteration:  81 loss: 0.05329289\n",
            "iteration:  82 loss: 0.14107352\n",
            "iteration:  83 loss: 0.08032548\n",
            "iteration:  84 loss: 0.07234477\n",
            "iteration:  85 loss: 0.06332999\n",
            "iteration:  86 loss: 0.07950867\n",
            "iteration:  87 loss: 0.15279230\n",
            "iteration:  88 loss: 0.26831138\n",
            "iteration:  89 loss: 0.17815344\n",
            "iteration:  90 loss: 0.08557743\n",
            "iteration:  91 loss: 0.06615941\n",
            "iteration:  92 loss: 0.14681566\n",
            "iteration:  93 loss: 0.10939046\n",
            "iteration:  94 loss: 0.06835400\n",
            "iteration:  95 loss: 0.06516290\n",
            "iteration:  96 loss: 0.07169640\n",
            "iteration:  97 loss: 0.05081726\n",
            "iteration:  98 loss: 0.14013958\n",
            "iteration:  99 loss: 0.07391854\n",
            "iteration: 100 loss: 0.28978950\n",
            "iteration: 101 loss: 0.12472850\n",
            "iteration: 102 loss: 0.12742400\n",
            "iteration: 103 loss: 0.15506649\n",
            "iteration: 104 loss: 0.08062395\n",
            "iteration: 105 loss: 0.11818226\n",
            "iteration: 106 loss: 0.05791783\n",
            "iteration: 107 loss: 0.09522986\n",
            "iteration: 108 loss: 0.15218318\n",
            "iteration: 109 loss: 0.21890166\n",
            "iteration: 110 loss: 0.08152493\n",
            "iteration: 111 loss: 0.05632662\n",
            "iteration: 112 loss: 0.11488093\n",
            "iteration: 113 loss: 0.07527086\n",
            "iteration: 114 loss: 0.06520342\n",
            "iteration: 115 loss: 0.10432890\n",
            "iteration: 116 loss: 0.10343452\n",
            "iteration: 117 loss: 0.08641833\n",
            "iteration: 118 loss: 0.08621228\n",
            "iteration: 119 loss: 0.11931606\n",
            "iteration: 120 loss: 0.14539370\n",
            "iteration: 121 loss: 0.22517759\n",
            "iteration: 122 loss: 0.06663570\n",
            "iteration: 123 loss: 0.07135425\n",
            "iteration: 124 loss: 0.29576877\n",
            "iteration: 125 loss: 0.14027195\n",
            "iteration: 126 loss: 0.10784006\n",
            "iteration: 127 loss: 0.14115351\n",
            "iteration: 128 loss: 0.06593305\n",
            "iteration: 129 loss: 0.26286736\n",
            "iteration: 130 loss: 0.08551338\n",
            "iteration: 131 loss: 0.17038241\n",
            "iteration: 132 loss: 0.08250411\n",
            "iteration: 133 loss: 0.08818506\n",
            "iteration: 134 loss: 0.11104818\n",
            "iteration: 135 loss: 0.14145677\n",
            "iteration: 136 loss: 0.09466338\n",
            "iteration: 137 loss: 0.15608096\n",
            "iteration: 138 loss: 0.08076443\n",
            "iteration: 139 loss: 0.07485389\n",
            "iteration: 140 loss: 0.07175364\n",
            "iteration: 141 loss: 0.08640887\n",
            "iteration: 142 loss: 0.13132693\n",
            "iteration: 143 loss: 0.13477141\n",
            "iteration: 144 loss: 0.12449966\n",
            "iteration: 145 loss: 0.11278179\n",
            "iteration: 146 loss: 0.23563939\n",
            "iteration: 147 loss: 0.10752501\n",
            "iteration: 148 loss: 0.11196674\n",
            "iteration: 149 loss: 0.13801983\n",
            "iteration: 150 loss: 0.18059203\n",
            "iteration: 151 loss: 0.08547528\n",
            "iteration: 152 loss: 0.05878549\n",
            "iteration: 153 loss: 0.05585449\n",
            "iteration: 154 loss: 0.11108646\n",
            "iteration: 155 loss: 0.08594373\n",
            "iteration: 156 loss: 0.12469402\n",
            "iteration: 157 loss: 0.08886452\n",
            "iteration: 158 loss: 0.22195381\n",
            "iteration: 159 loss: 0.10464603\n",
            "iteration: 160 loss: 0.11459047\n",
            "iteration: 161 loss: 0.07581912\n",
            "iteration: 162 loss: 0.11618131\n",
            "iteration: 163 loss: 0.09208426\n",
            "iteration: 164 loss: 0.07218641\n",
            "iteration: 165 loss: 0.10845789\n",
            "iteration: 166 loss: 0.20448585\n",
            "iteration: 167 loss: 0.09647037\n",
            "iteration: 168 loss: 0.09161344\n",
            "iteration: 169 loss: 0.10545243\n",
            "iteration: 170 loss: 0.09640397\n",
            "iteration: 171 loss: 0.06026392\n",
            "iteration: 172 loss: 0.10376889\n",
            "iteration: 173 loss: 0.06591027\n",
            "iteration: 174 loss: 0.13196051\n",
            "iteration: 175 loss: 0.07582593\n",
            "iteration: 176 loss: 0.06006472\n",
            "iteration: 177 loss: 0.06949512\n",
            "iteration: 178 loss: 0.17298992\n",
            "iteration: 179 loss: 0.18942915\n",
            "iteration: 180 loss: 0.21376042\n",
            "iteration: 181 loss: 0.09029321\n",
            "iteration: 182 loss: 0.08599832\n",
            "iteration: 183 loss: 0.06052342\n",
            "iteration: 184 loss: 0.11020621\n",
            "iteration: 185 loss: 0.13981372\n",
            "iteration: 186 loss: 0.10594212\n",
            "iteration: 187 loss: 0.08516646\n",
            "iteration: 188 loss: 0.12925522\n",
            "iteration: 189 loss: 0.08172297\n",
            "iteration: 190 loss: 0.08130731\n",
            "iteration: 191 loss: 0.13154423\n",
            "iteration: 192 loss: 0.07497284\n",
            "iteration: 193 loss: 0.17426845\n",
            "iteration: 194 loss: 0.12832834\n",
            "iteration: 195 loss: 0.09042694\n",
            "iteration: 196 loss: 0.07690240\n",
            "iteration: 197 loss: 0.09405510\n",
            "iteration: 198 loss: 0.09271365\n",
            "iteration: 199 loss: 0.14204594\n",
            "epoch:  82 mean loss training: 0.12176624\n",
            "epoch:  82 mean loss validation: 1.17766702\n",
            "iteration:   0 loss: 0.09934020\n",
            "iteration:   1 loss: 0.11062115\n",
            "iteration:   2 loss: 0.08330339\n",
            "iteration:   3 loss: 0.07736985\n",
            "iteration:   4 loss: 0.26088321\n",
            "iteration:   5 loss: 0.08129549\n",
            "iteration:   6 loss: 0.09031440\n",
            "iteration:   7 loss: 0.09849401\n",
            "iteration:   8 loss: 0.22031090\n",
            "iteration:   9 loss: 0.09728105\n",
            "iteration:  10 loss: 0.04324269\n",
            "iteration:  11 loss: 0.26970443\n",
            "iteration:  12 loss: 0.12348019\n",
            "iteration:  13 loss: 0.27210698\n",
            "iteration:  14 loss: 0.08620210\n",
            "iteration:  15 loss: 0.15174037\n",
            "iteration:  16 loss: 0.04650339\n",
            "iteration:  17 loss: 0.10853948\n",
            "iteration:  18 loss: 0.10105962\n",
            "iteration:  19 loss: 0.12702197\n",
            "iteration:  20 loss: 0.05352261\n",
            "iteration:  21 loss: 0.15466493\n",
            "iteration:  22 loss: 0.12222957\n",
            "iteration:  23 loss: 0.16639283\n",
            "iteration:  24 loss: 0.11449796\n",
            "iteration:  25 loss: 0.14929083\n",
            "iteration:  26 loss: 0.09858521\n",
            "iteration:  27 loss: 0.04331881\n",
            "iteration:  28 loss: 0.07396469\n",
            "iteration:  29 loss: 0.08175944\n",
            "iteration:  30 loss: 0.05862259\n",
            "iteration:  31 loss: 0.11814269\n",
            "iteration:  32 loss: 0.06852780\n",
            "iteration:  33 loss: 0.08133250\n",
            "iteration:  34 loss: 0.06245586\n",
            "iteration:  35 loss: 0.11377422\n",
            "iteration:  36 loss: 0.06794901\n",
            "iteration:  37 loss: 0.18857014\n",
            "iteration:  38 loss: 0.07772937\n",
            "iteration:  39 loss: 0.21623111\n",
            "iteration:  40 loss: 0.06044824\n",
            "iteration:  41 loss: 0.08347274\n",
            "iteration:  42 loss: 0.07975507\n",
            "iteration:  43 loss: 0.10647156\n",
            "iteration:  44 loss: 0.14397597\n",
            "iteration:  45 loss: 0.17834261\n",
            "iteration:  46 loss: 0.16484027\n",
            "iteration:  47 loss: 0.09941658\n",
            "iteration:  48 loss: 0.13367338\n",
            "iteration:  49 loss: 0.03917054\n",
            "iteration:  50 loss: 0.07253777\n",
            "iteration:  51 loss: 0.16107720\n",
            "iteration:  52 loss: 0.10224814\n",
            "iteration:  53 loss: 0.13470238\n",
            "iteration:  54 loss: 0.10491481\n",
            "iteration:  55 loss: 0.17763451\n",
            "iteration:  56 loss: 0.07740775\n",
            "iteration:  57 loss: 0.08417460\n",
            "iteration:  58 loss: 0.11016543\n",
            "iteration:  59 loss: 0.06952437\n",
            "iteration:  60 loss: 0.08914241\n",
            "iteration:  61 loss: 0.11190530\n",
            "iteration:  62 loss: 0.07333719\n",
            "iteration:  63 loss: 0.13688008\n",
            "iteration:  64 loss: 0.12378612\n",
            "iteration:  65 loss: 0.22698027\n",
            "iteration:  66 loss: 0.13828300\n",
            "iteration:  67 loss: 0.13734107\n",
            "iteration:  68 loss: 0.13058476\n",
            "iteration:  69 loss: 0.06698974\n",
            "iteration:  70 loss: 0.13438791\n",
            "iteration:  71 loss: 0.08249994\n",
            "iteration:  72 loss: 0.10703979\n",
            "iteration:  73 loss: 0.07225071\n",
            "iteration:  74 loss: 0.06681604\n",
            "iteration:  75 loss: 0.05215231\n",
            "iteration:  76 loss: 0.10273021\n",
            "iteration:  77 loss: 0.10584280\n",
            "iteration:  78 loss: 0.08119014\n",
            "iteration:  79 loss: 0.07942702\n",
            "iteration:  80 loss: 0.16646492\n",
            "iteration:  81 loss: 0.13542630\n",
            "iteration:  82 loss: 0.21357021\n",
            "iteration:  83 loss: 0.09660783\n",
            "iteration:  84 loss: 0.09866147\n",
            "iteration:  85 loss: 0.12848490\n",
            "iteration:  86 loss: 0.11611360\n",
            "iteration:  87 loss: 0.09567896\n",
            "iteration:  88 loss: 0.14703077\n",
            "iteration:  89 loss: 0.08170721\n",
            "iteration:  90 loss: 0.10480832\n",
            "iteration:  91 loss: 0.09633702\n",
            "iteration:  92 loss: 0.07748827\n",
            "iteration:  93 loss: 0.11969948\n",
            "iteration:  94 loss: 0.06948012\n",
            "iteration:  95 loss: 0.04659836\n",
            "iteration:  96 loss: 0.13292651\n",
            "iteration:  97 loss: 0.08444068\n",
            "iteration:  98 loss: 0.39939824\n",
            "iteration:  99 loss: 0.08914429\n",
            "iteration: 100 loss: 0.24833767\n",
            "iteration: 101 loss: 0.07218185\n",
            "iteration: 102 loss: 0.09920763\n",
            "iteration: 103 loss: 0.13787800\n",
            "iteration: 104 loss: 0.08085982\n",
            "iteration: 105 loss: 0.13285126\n",
            "iteration: 106 loss: 0.22723967\n",
            "iteration: 107 loss: 0.07929135\n",
            "iteration: 108 loss: 0.22265153\n",
            "iteration: 109 loss: 0.32569924\n",
            "iteration: 110 loss: 0.17980669\n",
            "iteration: 111 loss: 0.08120281\n",
            "iteration: 112 loss: 0.11117660\n",
            "iteration: 113 loss: 0.07072888\n",
            "iteration: 114 loss: 0.05658184\n",
            "iteration: 115 loss: 0.08785830\n",
            "iteration: 116 loss: 0.07776839\n",
            "iteration: 117 loss: 0.27700341\n",
            "iteration: 118 loss: 0.10487020\n",
            "iteration: 119 loss: 0.11468348\n",
            "iteration: 120 loss: 0.13334958\n",
            "iteration: 121 loss: 0.08436485\n",
            "iteration: 122 loss: 0.12213638\n",
            "iteration: 123 loss: 0.14430667\n",
            "iteration: 124 loss: 0.13217890\n",
            "iteration: 125 loss: 0.07154080\n",
            "iteration: 126 loss: 0.05406515\n",
            "iteration: 127 loss: 0.16476387\n",
            "iteration: 128 loss: 0.07046067\n",
            "iteration: 129 loss: 0.07255095\n",
            "iteration: 130 loss: 0.32394063\n",
            "iteration: 131 loss: 0.10125910\n",
            "iteration: 132 loss: 0.15344812\n",
            "iteration: 133 loss: 0.10744348\n",
            "iteration: 134 loss: 0.16365403\n",
            "iteration: 135 loss: 0.23482919\n",
            "iteration: 136 loss: 0.07840211\n",
            "iteration: 137 loss: 0.06460135\n",
            "iteration: 138 loss: 0.09070460\n",
            "iteration: 139 loss: 0.12466998\n",
            "iteration: 140 loss: 0.24994069\n",
            "iteration: 141 loss: 0.06497269\n",
            "iteration: 142 loss: 0.19777480\n",
            "iteration: 143 loss: 0.06661476\n",
            "iteration: 144 loss: 0.14318094\n",
            "iteration: 145 loss: 0.16338578\n",
            "iteration: 146 loss: 0.09036487\n",
            "iteration: 147 loss: 0.11767067\n",
            "iteration: 148 loss: 0.08833919\n",
            "iteration: 149 loss: 0.06141576\n",
            "iteration: 150 loss: 0.17933272\n",
            "iteration: 151 loss: 0.13356334\n",
            "iteration: 152 loss: 0.29446235\n",
            "iteration: 153 loss: 0.25844088\n",
            "iteration: 154 loss: 0.11171104\n",
            "iteration: 155 loss: 0.07188039\n",
            "iteration: 156 loss: 0.19145042\n",
            "iteration: 157 loss: 0.11050915\n",
            "iteration: 158 loss: 0.24201444\n",
            "iteration: 159 loss: 0.16633831\n",
            "iteration: 160 loss: 0.06699600\n",
            "iteration: 161 loss: 0.11128409\n",
            "iteration: 162 loss: 0.06942502\n",
            "iteration: 163 loss: 0.13445169\n",
            "iteration: 164 loss: 0.07329281\n",
            "iteration: 165 loss: 0.12992261\n",
            "iteration: 166 loss: 0.18799201\n",
            "iteration: 167 loss: 0.10872382\n",
            "iteration: 168 loss: 0.09721925\n",
            "iteration: 169 loss: 0.12222740\n",
            "iteration: 170 loss: 0.08830309\n",
            "iteration: 171 loss: 0.08169283\n",
            "iteration: 172 loss: 0.09116413\n",
            "iteration: 173 loss: 0.07128982\n",
            "iteration: 174 loss: 0.14769734\n",
            "iteration: 175 loss: 0.15311718\n",
            "iteration: 176 loss: 0.14615028\n",
            "iteration: 177 loss: 0.05989698\n",
            "iteration: 178 loss: 0.17007093\n",
            "iteration: 179 loss: 0.09507182\n",
            "iteration: 180 loss: 0.30625647\n",
            "iteration: 181 loss: 0.06974943\n",
            "iteration: 182 loss: 0.36018005\n",
            "iteration: 183 loss: 0.12917328\n",
            "iteration: 184 loss: 0.03718976\n",
            "iteration: 185 loss: 0.07237518\n",
            "iteration: 186 loss: 0.08993576\n",
            "iteration: 187 loss: 0.07977419\n",
            "iteration: 188 loss: 0.07903884\n",
            "iteration: 189 loss: 0.09192772\n",
            "iteration: 190 loss: 0.07348496\n",
            "iteration: 191 loss: 0.12690389\n",
            "iteration: 192 loss: 0.22068933\n",
            "iteration: 193 loss: 0.08518269\n",
            "iteration: 194 loss: 0.07868079\n",
            "iteration: 195 loss: 0.15932226\n",
            "iteration: 196 loss: 0.06420507\n",
            "iteration: 197 loss: 0.11381373\n",
            "iteration: 198 loss: 0.06552043\n",
            "iteration: 199 loss: 0.05867196\n",
            "epoch:  83 mean loss training: 0.12157971\n",
            "epoch:  83 mean loss validation: 1.16623485\n",
            "iteration:   0 loss: 0.10561676\n",
            "iteration:   1 loss: 0.14292021\n",
            "iteration:   2 loss: 0.12622508\n",
            "iteration:   3 loss: 0.10107388\n",
            "iteration:   4 loss: 0.09099241\n",
            "iteration:   5 loss: 0.06568703\n",
            "iteration:   6 loss: 0.10202889\n",
            "iteration:   7 loss: 0.10765232\n",
            "iteration:   8 loss: 0.08287872\n",
            "iteration:   9 loss: 0.09882788\n",
            "iteration:  10 loss: 0.12538689\n",
            "iteration:  11 loss: 0.11445744\n",
            "iteration:  12 loss: 0.05521079\n",
            "iteration:  13 loss: 0.10782458\n",
            "iteration:  14 loss: 0.06230360\n",
            "iteration:  15 loss: 0.15745604\n",
            "iteration:  16 loss: 0.09043490\n",
            "iteration:  17 loss: 0.11410190\n",
            "iteration:  18 loss: 0.10473353\n",
            "iteration:  19 loss: 0.07470809\n",
            "iteration:  20 loss: 0.35862154\n",
            "iteration:  21 loss: 0.19090506\n",
            "iteration:  22 loss: 0.18861020\n",
            "iteration:  23 loss: 0.13815339\n",
            "iteration:  24 loss: 0.09488884\n",
            "iteration:  25 loss: 0.28937829\n",
            "iteration:  26 loss: 0.06992866\n",
            "iteration:  27 loss: 0.08668201\n",
            "iteration:  28 loss: 0.07344528\n",
            "iteration:  29 loss: 0.07468867\n",
            "iteration:  30 loss: 0.06570926\n",
            "iteration:  31 loss: 0.06331336\n",
            "iteration:  32 loss: 0.08737016\n",
            "iteration:  33 loss: 0.17437433\n",
            "iteration:  34 loss: 0.08717109\n",
            "iteration:  35 loss: 0.07637563\n",
            "iteration:  36 loss: 0.16898435\n",
            "iteration:  37 loss: 0.10247424\n",
            "iteration:  38 loss: 0.05908779\n",
            "iteration:  39 loss: 0.06086855\n",
            "iteration:  40 loss: 0.12479156\n",
            "iteration:  41 loss: 0.07187907\n",
            "iteration:  42 loss: 0.30902508\n",
            "iteration:  43 loss: 0.09974109\n",
            "iteration:  44 loss: 0.07334953\n",
            "iteration:  45 loss: 0.10853145\n",
            "iteration:  46 loss: 0.10780075\n",
            "iteration:  47 loss: 0.11575125\n",
            "iteration:  48 loss: 0.07764488\n",
            "iteration:  49 loss: 0.15016170\n",
            "iteration:  50 loss: 0.13545106\n",
            "iteration:  51 loss: 0.04000751\n",
            "iteration:  52 loss: 0.08572838\n",
            "iteration:  53 loss: 0.18174182\n",
            "iteration:  54 loss: 0.07722460\n",
            "iteration:  55 loss: 0.18890704\n",
            "iteration:  56 loss: 0.08935405\n",
            "iteration:  57 loss: 0.15018696\n",
            "iteration:  58 loss: 0.07952926\n",
            "iteration:  59 loss: 0.07695476\n",
            "iteration:  60 loss: 0.21977797\n",
            "iteration:  61 loss: 0.23702995\n",
            "iteration:  62 loss: 0.06601520\n",
            "iteration:  63 loss: 0.32590064\n",
            "iteration:  64 loss: 0.12341978\n",
            "iteration:  65 loss: 0.10840529\n",
            "iteration:  66 loss: 0.10520388\n",
            "iteration:  67 loss: 0.06289126\n",
            "iteration:  68 loss: 0.09700543\n",
            "iteration:  69 loss: 0.09299725\n",
            "iteration:  70 loss: 0.06644628\n",
            "iteration:  71 loss: 0.17639118\n",
            "iteration:  72 loss: 0.13967422\n",
            "iteration:  73 loss: 0.06263598\n",
            "iteration:  74 loss: 0.08158028\n",
            "iteration:  75 loss: 0.11148310\n",
            "iteration:  76 loss: 0.08999687\n",
            "iteration:  77 loss: 0.04899701\n",
            "iteration:  78 loss: 0.04836642\n",
            "iteration:  79 loss: 0.18679230\n",
            "iteration:  80 loss: 0.10887206\n",
            "iteration:  81 loss: 0.11327681\n",
            "iteration:  82 loss: 0.09085792\n",
            "iteration:  83 loss: 0.25182447\n",
            "iteration:  84 loss: 0.09874438\n",
            "iteration:  85 loss: 0.10894386\n",
            "iteration:  86 loss: 0.06979010\n",
            "iteration:  87 loss: 0.12060110\n",
            "iteration:  88 loss: 0.04767254\n",
            "iteration:  89 loss: 0.14542374\n",
            "iteration:  90 loss: 0.05554087\n",
            "iteration:  91 loss: 0.19755608\n",
            "iteration:  92 loss: 0.12778497\n",
            "iteration:  93 loss: 0.10685670\n",
            "iteration:  94 loss: 0.21829732\n",
            "iteration:  95 loss: 0.09497075\n",
            "iteration:  96 loss: 0.07618281\n",
            "iteration:  97 loss: 0.21173251\n",
            "iteration:  98 loss: 0.16168867\n",
            "iteration:  99 loss: 0.21335085\n",
            "iteration: 100 loss: 0.06862775\n",
            "iteration: 101 loss: 0.09838179\n",
            "iteration: 102 loss: 0.07032631\n",
            "iteration: 103 loss: 0.08053384\n",
            "iteration: 104 loss: 0.07912734\n",
            "iteration: 105 loss: 0.21071707\n",
            "iteration: 106 loss: 0.06321871\n",
            "iteration: 107 loss: 0.10525909\n",
            "iteration: 108 loss: 0.07990743\n",
            "iteration: 109 loss: 0.07747312\n",
            "iteration: 110 loss: 0.20298713\n",
            "iteration: 111 loss: 0.06172306\n",
            "iteration: 112 loss: 0.11317357\n",
            "iteration: 113 loss: 0.08718766\n",
            "iteration: 114 loss: 0.08628161\n",
            "iteration: 115 loss: 0.09553304\n",
            "iteration: 116 loss: 0.26903057\n",
            "iteration: 117 loss: 0.09978069\n",
            "iteration: 118 loss: 0.09627248\n",
            "iteration: 119 loss: 0.12721676\n",
            "iteration: 120 loss: 0.13148382\n",
            "iteration: 121 loss: 0.20255959\n",
            "iteration: 122 loss: 0.13832113\n",
            "iteration: 123 loss: 0.13493250\n",
            "iteration: 124 loss: 0.07157494\n",
            "iteration: 125 loss: 0.10733181\n",
            "iteration: 126 loss: 0.05187244\n",
            "iteration: 127 loss: 0.09157815\n",
            "iteration: 128 loss: 0.06755236\n",
            "iteration: 129 loss: 0.12934278\n",
            "iteration: 130 loss: 0.12878112\n",
            "iteration: 131 loss: 0.08056408\n",
            "iteration: 132 loss: 0.09307585\n",
            "iteration: 133 loss: 0.11298343\n",
            "iteration: 134 loss: 0.07658184\n",
            "iteration: 135 loss: 0.06846410\n",
            "iteration: 136 loss: 0.09324331\n",
            "iteration: 137 loss: 0.15634397\n",
            "iteration: 138 loss: 0.12337796\n",
            "iteration: 139 loss: 0.07789184\n",
            "iteration: 140 loss: 0.10740671\n",
            "iteration: 141 loss: 0.09816944\n",
            "iteration: 142 loss: 0.06533769\n",
            "iteration: 143 loss: 0.08574557\n",
            "iteration: 144 loss: 0.12024561\n",
            "iteration: 145 loss: 0.26425499\n",
            "iteration: 146 loss: 0.06491943\n",
            "iteration: 147 loss: 0.13084486\n",
            "iteration: 148 loss: 0.25917688\n",
            "iteration: 149 loss: 0.07357207\n",
            "iteration: 150 loss: 0.09757444\n",
            "iteration: 151 loss: 0.31035835\n",
            "iteration: 152 loss: 0.05783976\n",
            "iteration: 153 loss: 0.05814072\n",
            "iteration: 154 loss: 0.08207656\n",
            "iteration: 155 loss: 0.11021324\n",
            "iteration: 156 loss: 0.15004015\n",
            "iteration: 157 loss: 0.06216041\n",
            "iteration: 158 loss: 0.06149343\n",
            "iteration: 159 loss: 0.10470647\n",
            "iteration: 160 loss: 0.07781386\n",
            "iteration: 161 loss: 0.08827684\n",
            "iteration: 162 loss: 0.06818061\n",
            "iteration: 163 loss: 0.10115674\n",
            "iteration: 164 loss: 0.17108636\n",
            "iteration: 165 loss: 0.06794290\n",
            "iteration: 166 loss: 0.08342373\n",
            "iteration: 167 loss: 0.09919922\n",
            "iteration: 168 loss: 0.12340477\n",
            "iteration: 169 loss: 0.07557636\n",
            "iteration: 170 loss: 0.17009792\n",
            "iteration: 171 loss: 0.09498934\n",
            "iteration: 172 loss: 0.36651477\n",
            "iteration: 173 loss: 0.13841170\n",
            "iteration: 174 loss: 0.18133265\n",
            "iteration: 175 loss: 0.03790875\n",
            "iteration: 176 loss: 0.04605985\n",
            "iteration: 177 loss: 0.06673110\n",
            "iteration: 178 loss: 0.15671429\n",
            "iteration: 179 loss: 0.08704129\n",
            "iteration: 180 loss: 0.08422608\n",
            "iteration: 181 loss: 0.09403878\n",
            "iteration: 182 loss: 0.08898691\n",
            "iteration: 183 loss: 0.06140583\n",
            "iteration: 184 loss: 0.18909955\n",
            "iteration: 185 loss: 0.09947982\n",
            "iteration: 186 loss: 0.14150003\n",
            "iteration: 187 loss: 0.09982628\n",
            "iteration: 188 loss: 0.08705640\n",
            "iteration: 189 loss: 0.09638911\n",
            "iteration: 190 loss: 0.11540955\n",
            "iteration: 191 loss: 0.09696266\n",
            "iteration: 192 loss: 0.15572411\n",
            "iteration: 193 loss: 0.21103796\n",
            "iteration: 194 loss: 0.07044694\n",
            "iteration: 195 loss: 0.15315276\n",
            "iteration: 196 loss: 0.05081709\n",
            "iteration: 197 loss: 0.07493421\n",
            "iteration: 198 loss: 0.05649034\n",
            "iteration: 199 loss: 0.09290590\n",
            "epoch:  84 mean loss training: 0.11526468\n",
            "epoch:  84 mean loss validation: 1.18436956\n",
            "iteration:   0 loss: 0.09124433\n",
            "iteration:   1 loss: 0.28838652\n",
            "iteration:   2 loss: 0.17513449\n",
            "iteration:   3 loss: 0.06461471\n",
            "iteration:   4 loss: 0.06002795\n",
            "iteration:   5 loss: 0.06440342\n",
            "iteration:   6 loss: 0.07394944\n",
            "iteration:   7 loss: 0.59044850\n",
            "iteration:   8 loss: 0.11616470\n",
            "iteration:   9 loss: 0.15920919\n",
            "iteration:  10 loss: 0.18789563\n",
            "iteration:  11 loss: 0.19396979\n",
            "iteration:  12 loss: 0.17457567\n",
            "iteration:  13 loss: 0.18632677\n",
            "iteration:  14 loss: 0.08357750\n",
            "iteration:  15 loss: 0.04828589\n",
            "iteration:  16 loss: 0.09384474\n",
            "iteration:  17 loss: 0.09394255\n",
            "iteration:  18 loss: 0.39847875\n",
            "iteration:  19 loss: 0.10485888\n",
            "iteration:  20 loss: 0.15742832\n",
            "iteration:  21 loss: 0.18468648\n",
            "iteration:  22 loss: 0.10642204\n",
            "iteration:  23 loss: 0.17117451\n",
            "iteration:  24 loss: 0.12179609\n",
            "iteration:  25 loss: 0.11342975\n",
            "iteration:  26 loss: 0.09196776\n",
            "iteration:  27 loss: 0.08082353\n",
            "iteration:  28 loss: 0.15867294\n",
            "iteration:  29 loss: 0.07186067\n",
            "iteration:  30 loss: 0.06687668\n",
            "iteration:  31 loss: 0.27180079\n",
            "iteration:  32 loss: 0.46148020\n",
            "iteration:  33 loss: 0.10603209\n",
            "iteration:  34 loss: 0.07700058\n",
            "iteration:  35 loss: 0.05646189\n",
            "iteration:  36 loss: 0.05643472\n",
            "iteration:  37 loss: 0.07298897\n",
            "iteration:  38 loss: 0.08369258\n",
            "iteration:  39 loss: 0.13403311\n",
            "iteration:  40 loss: 0.13434628\n",
            "iteration:  41 loss: 0.06084026\n",
            "iteration:  42 loss: 0.05925087\n",
            "iteration:  43 loss: 0.11604153\n",
            "iteration:  44 loss: 0.09417808\n",
            "iteration:  45 loss: 0.09321598\n",
            "iteration:  46 loss: 0.08748864\n",
            "iteration:  47 loss: 0.11975530\n",
            "iteration:  48 loss: 0.17449254\n",
            "iteration:  49 loss: 0.13154146\n",
            "iteration:  50 loss: 0.07378623\n",
            "iteration:  51 loss: 0.14295062\n",
            "iteration:  52 loss: 0.26502505\n",
            "iteration:  53 loss: 0.09765058\n",
            "iteration:  54 loss: 0.24534480\n",
            "iteration:  55 loss: 0.25679696\n",
            "iteration:  56 loss: 0.08652037\n",
            "iteration:  57 loss: 0.16010170\n",
            "iteration:  58 loss: 0.08557393\n",
            "iteration:  59 loss: 0.18398306\n",
            "iteration:  60 loss: 0.08221485\n",
            "iteration:  61 loss: 0.04931722\n",
            "iteration:  62 loss: 0.15406547\n",
            "iteration:  63 loss: 0.09224814\n",
            "iteration:  64 loss: 0.06898199\n",
            "iteration:  65 loss: 0.07662275\n",
            "iteration:  66 loss: 0.05884347\n",
            "iteration:  67 loss: 0.10469445\n",
            "iteration:  68 loss: 0.13581795\n",
            "iteration:  69 loss: 0.07688361\n",
            "iteration:  70 loss: 0.07487541\n",
            "iteration:  71 loss: 0.05130539\n",
            "iteration:  72 loss: 0.08210470\n",
            "iteration:  73 loss: 0.12897131\n",
            "iteration:  74 loss: 0.04760684\n",
            "iteration:  75 loss: 0.06946290\n",
            "iteration:  76 loss: 0.08920997\n",
            "iteration:  77 loss: 0.12073158\n",
            "iteration:  78 loss: 0.30455777\n",
            "iteration:  79 loss: 0.05702241\n",
            "iteration:  80 loss: 0.07301036\n",
            "iteration:  81 loss: 0.10367590\n",
            "iteration:  82 loss: 0.09870835\n",
            "iteration:  83 loss: 0.06370449\n",
            "iteration:  84 loss: 0.09521505\n",
            "iteration:  85 loss: 0.08161376\n",
            "iteration:  86 loss: 0.07615592\n",
            "iteration:  87 loss: 0.07333440\n",
            "iteration:  88 loss: 0.05784158\n",
            "iteration:  89 loss: 0.09970795\n",
            "iteration:  90 loss: 0.06734866\n",
            "iteration:  91 loss: 0.27787930\n",
            "iteration:  92 loss: 0.10228704\n",
            "iteration:  93 loss: 0.08104780\n",
            "iteration:  94 loss: 0.09368463\n",
            "iteration:  95 loss: 0.07664400\n",
            "iteration:  96 loss: 0.10241958\n",
            "iteration:  97 loss: 0.17571089\n",
            "iteration:  98 loss: 0.17585909\n",
            "iteration:  99 loss: 0.11758256\n",
            "iteration: 100 loss: 0.10182069\n",
            "iteration: 101 loss: 0.07179742\n",
            "iteration: 102 loss: 0.10481557\n",
            "iteration: 103 loss: 0.05350097\n",
            "iteration: 104 loss: 0.13329226\n",
            "iteration: 105 loss: 0.19394632\n",
            "iteration: 106 loss: 0.30030224\n",
            "iteration: 107 loss: 0.08145248\n",
            "iteration: 108 loss: 0.28252095\n",
            "iteration: 109 loss: 0.04928211\n",
            "iteration: 110 loss: 0.07702441\n",
            "iteration: 111 loss: 0.18973650\n",
            "iteration: 112 loss: 0.05874562\n",
            "iteration: 113 loss: 0.17102946\n",
            "iteration: 114 loss: 0.08447947\n",
            "iteration: 115 loss: 0.10688707\n",
            "iteration: 116 loss: 0.10015581\n",
            "iteration: 117 loss: 0.06668955\n",
            "iteration: 118 loss: 0.19787703\n",
            "iteration: 119 loss: 0.15616396\n",
            "iteration: 120 loss: 0.15457460\n",
            "iteration: 121 loss: 0.11219853\n",
            "iteration: 122 loss: 0.11442356\n",
            "iteration: 123 loss: 0.12049204\n",
            "iteration: 124 loss: 0.15098667\n",
            "iteration: 125 loss: 0.31579846\n",
            "iteration: 126 loss: 0.08320330\n",
            "iteration: 127 loss: 0.07808450\n",
            "iteration: 128 loss: 0.08226242\n",
            "iteration: 129 loss: 0.09382796\n",
            "iteration: 130 loss: 0.08334062\n",
            "iteration: 131 loss: 0.07094400\n",
            "iteration: 132 loss: 0.21476917\n",
            "iteration: 133 loss: 0.14533956\n",
            "iteration: 134 loss: 0.10860863\n",
            "iteration: 135 loss: 0.09838687\n",
            "iteration: 136 loss: 0.09080096\n",
            "iteration: 137 loss: 0.13338295\n",
            "iteration: 138 loss: 0.08931436\n",
            "iteration: 139 loss: 0.10769510\n",
            "iteration: 140 loss: 0.07813204\n",
            "iteration: 141 loss: 0.10910283\n",
            "iteration: 142 loss: 0.17332971\n",
            "iteration: 143 loss: 0.13775630\n",
            "iteration: 144 loss: 0.05892855\n",
            "iteration: 145 loss: 0.12934881\n",
            "iteration: 146 loss: 0.18375310\n",
            "iteration: 147 loss: 0.07172612\n",
            "iteration: 148 loss: 0.05456294\n",
            "iteration: 149 loss: 0.08701307\n",
            "iteration: 150 loss: 0.13177887\n",
            "iteration: 151 loss: 0.42797080\n",
            "iteration: 152 loss: 0.06706411\n",
            "iteration: 153 loss: 0.07445475\n",
            "iteration: 154 loss: 0.12093168\n",
            "iteration: 155 loss: 0.11440172\n",
            "iteration: 156 loss: 0.09448425\n",
            "iteration: 157 loss: 0.11888359\n",
            "iteration: 158 loss: 0.09339870\n",
            "iteration: 159 loss: 0.09114428\n",
            "iteration: 160 loss: 0.14895016\n",
            "iteration: 161 loss: 0.07381065\n",
            "iteration: 162 loss: 0.08093569\n",
            "iteration: 163 loss: 0.04693151\n",
            "iteration: 164 loss: 0.04993821\n",
            "iteration: 165 loss: 0.05592910\n",
            "iteration: 166 loss: 0.26074481\n",
            "iteration: 167 loss: 0.18070076\n",
            "iteration: 168 loss: 0.09243001\n",
            "iteration: 169 loss: 0.14254235\n",
            "iteration: 170 loss: 0.14680073\n",
            "iteration: 171 loss: 0.05688946\n",
            "iteration: 172 loss: 0.07759678\n",
            "iteration: 173 loss: 0.21561748\n",
            "iteration: 174 loss: 0.07720225\n",
            "iteration: 175 loss: 0.27829528\n",
            "iteration: 176 loss: 0.07934501\n",
            "iteration: 177 loss: 0.14114633\n",
            "iteration: 178 loss: 0.14513731\n",
            "iteration: 179 loss: 0.09717435\n",
            "iteration: 180 loss: 0.14398551\n",
            "iteration: 181 loss: 0.03905149\n",
            "iteration: 182 loss: 0.20794323\n",
            "iteration: 183 loss: 0.07902312\n",
            "iteration: 184 loss: 0.07226123\n",
            "iteration: 185 loss: 0.08440121\n",
            "iteration: 186 loss: 0.18362400\n",
            "iteration: 187 loss: 0.09837912\n",
            "iteration: 188 loss: 0.06706712\n",
            "iteration: 189 loss: 0.07858630\n",
            "iteration: 190 loss: 0.06933332\n",
            "iteration: 191 loss: 0.09282085\n",
            "iteration: 192 loss: 0.07215665\n",
            "iteration: 193 loss: 0.05973631\n",
            "iteration: 194 loss: 0.13253918\n",
            "iteration: 195 loss: 0.09795019\n",
            "iteration: 196 loss: 0.17032523\n",
            "iteration: 197 loss: 0.10349969\n",
            "iteration: 198 loss: 0.13531283\n",
            "iteration: 199 loss: 0.07404453\n",
            "epoch:  85 mean loss training: 0.12270269\n",
            "epoch:  85 mean loss validation: 1.17978740\n",
            "iteration:   0 loss: 0.06849841\n",
            "iteration:   1 loss: 0.19364251\n",
            "iteration:   2 loss: 0.10800109\n",
            "iteration:   3 loss: 0.06211911\n",
            "iteration:   4 loss: 0.10853210\n",
            "iteration:   5 loss: 0.06001972\n",
            "iteration:   6 loss: 0.14570332\n",
            "iteration:   7 loss: 0.07384448\n",
            "iteration:   8 loss: 0.16407856\n",
            "iteration:   9 loss: 0.27117044\n",
            "iteration:  10 loss: 0.08923942\n",
            "iteration:  11 loss: 0.06187554\n",
            "iteration:  12 loss: 0.14618543\n",
            "iteration:  13 loss: 0.19504827\n",
            "iteration:  14 loss: 0.05560469\n",
            "iteration:  15 loss: 0.10971715\n",
            "iteration:  16 loss: 0.13070770\n",
            "iteration:  17 loss: 0.09523945\n",
            "iteration:  18 loss: 0.12218919\n",
            "iteration:  19 loss: 0.11544879\n",
            "iteration:  20 loss: 0.19166526\n",
            "iteration:  21 loss: 0.24927981\n",
            "iteration:  22 loss: 0.06156055\n",
            "iteration:  23 loss: 0.20585012\n",
            "iteration:  24 loss: 0.05269746\n",
            "iteration:  25 loss: 0.04938061\n",
            "iteration:  26 loss: 0.26108310\n",
            "iteration:  27 loss: 0.17827375\n",
            "iteration:  28 loss: 0.05778153\n",
            "iteration:  29 loss: 0.10523949\n",
            "iteration:  30 loss: 0.09841805\n",
            "iteration:  31 loss: 0.08352620\n",
            "iteration:  32 loss: 0.10307878\n",
            "iteration:  33 loss: 0.08823715\n",
            "iteration:  34 loss: 0.08398506\n",
            "iteration:  35 loss: 0.07152133\n",
            "iteration:  36 loss: 0.14542338\n",
            "iteration:  37 loss: 0.08146448\n",
            "iteration:  38 loss: 0.08414318\n",
            "iteration:  39 loss: 0.09627204\n",
            "iteration:  40 loss: 0.14953235\n",
            "iteration:  41 loss: 0.04239198\n",
            "iteration:  42 loss: 0.11265175\n",
            "iteration:  43 loss: 0.15626115\n",
            "iteration:  44 loss: 0.09543641\n",
            "iteration:  45 loss: 0.10276833\n",
            "iteration:  46 loss: 0.04945808\n",
            "iteration:  47 loss: 0.08522254\n",
            "iteration:  48 loss: 0.10725613\n",
            "iteration:  49 loss: 0.06964793\n",
            "iteration:  50 loss: 0.08515786\n",
            "iteration:  51 loss: 0.13626328\n",
            "iteration:  52 loss: 0.15281659\n",
            "iteration:  53 loss: 0.06364416\n",
            "iteration:  54 loss: 0.08688581\n",
            "iteration:  55 loss: 0.04342714\n",
            "iteration:  56 loss: 0.11229201\n",
            "iteration:  57 loss: 0.12259765\n",
            "iteration:  58 loss: 0.09121309\n",
            "iteration:  59 loss: 0.18990518\n",
            "iteration:  60 loss: 0.28417683\n",
            "iteration:  61 loss: 0.21332292\n",
            "iteration:  62 loss: 0.10285355\n",
            "iteration:  63 loss: 0.10745500\n",
            "iteration:  64 loss: 0.08723535\n",
            "iteration:  65 loss: 0.15477577\n",
            "iteration:  66 loss: 0.05300219\n",
            "iteration:  67 loss: 0.28542781\n",
            "iteration:  68 loss: 0.20151153\n",
            "iteration:  69 loss: 0.11043258\n",
            "iteration:  70 loss: 0.07589088\n",
            "iteration:  71 loss: 0.09903815\n",
            "iteration:  72 loss: 0.13883279\n",
            "iteration:  73 loss: 0.08482020\n",
            "iteration:  74 loss: 0.09235694\n",
            "iteration:  75 loss: 0.09141155\n",
            "iteration:  76 loss: 0.07278947\n",
            "iteration:  77 loss: 0.10478843\n",
            "iteration:  78 loss: 0.15243956\n",
            "iteration:  79 loss: 0.08763885\n",
            "iteration:  80 loss: 0.06343161\n",
            "iteration:  81 loss: 0.14950866\n",
            "iteration:  82 loss: 0.14615282\n",
            "iteration:  83 loss: 0.14497598\n",
            "iteration:  84 loss: 0.14478862\n",
            "iteration:  85 loss: 0.10567719\n",
            "iteration:  86 loss: 0.08556080\n",
            "iteration:  87 loss: 0.11102328\n",
            "iteration:  88 loss: 0.08776151\n",
            "iteration:  89 loss: 0.16484563\n",
            "iteration:  90 loss: 0.05642285\n",
            "iteration:  91 loss: 0.10356140\n",
            "iteration:  92 loss: 0.11195251\n",
            "iteration:  93 loss: 0.08562753\n",
            "iteration:  94 loss: 0.07948460\n",
            "iteration:  95 loss: 0.06138460\n",
            "iteration:  96 loss: 0.29866517\n",
            "iteration:  97 loss: 0.10843737\n",
            "iteration:  98 loss: 0.15972489\n",
            "iteration:  99 loss: 0.13317807\n",
            "iteration: 100 loss: 0.14136617\n",
            "iteration: 101 loss: 0.08043849\n",
            "iteration: 102 loss: 0.33815897\n",
            "iteration: 103 loss: 0.06136945\n",
            "iteration: 104 loss: 0.15465137\n",
            "iteration: 105 loss: 0.07144719\n",
            "iteration: 106 loss: 0.10372455\n",
            "iteration: 107 loss: 0.14303754\n",
            "iteration: 108 loss: 0.09509248\n",
            "iteration: 109 loss: 0.09346214\n",
            "iteration: 110 loss: 0.06938107\n",
            "iteration: 111 loss: 0.31607151\n",
            "iteration: 112 loss: 0.08862403\n",
            "iteration: 113 loss: 0.10063310\n",
            "iteration: 114 loss: 0.11879420\n",
            "iteration: 115 loss: 0.11538946\n",
            "iteration: 116 loss: 0.06815706\n",
            "iteration: 117 loss: 0.18296616\n",
            "iteration: 118 loss: 0.06403787\n",
            "iteration: 119 loss: 0.09684923\n",
            "iteration: 120 loss: 0.07100277\n",
            "iteration: 121 loss: 0.12831675\n",
            "iteration: 122 loss: 0.10546707\n",
            "iteration: 123 loss: 0.07324169\n",
            "iteration: 124 loss: 0.04795091\n",
            "iteration: 125 loss: 0.08034144\n",
            "iteration: 126 loss: 0.07816355\n",
            "iteration: 127 loss: 0.10390558\n",
            "iteration: 128 loss: 0.05116957\n",
            "iteration: 129 loss: 0.27276424\n",
            "iteration: 130 loss: 0.07615245\n",
            "iteration: 131 loss: 0.09040099\n",
            "iteration: 132 loss: 0.11272320\n",
            "iteration: 133 loss: 0.08766340\n",
            "iteration: 134 loss: 0.05702317\n",
            "iteration: 135 loss: 0.14190435\n",
            "iteration: 136 loss: 0.08312207\n",
            "iteration: 137 loss: 0.10440829\n",
            "iteration: 138 loss: 0.06227470\n",
            "iteration: 139 loss: 0.08398770\n",
            "iteration: 140 loss: 0.12787578\n",
            "iteration: 141 loss: 0.08309288\n",
            "iteration: 142 loss: 0.19263002\n",
            "iteration: 143 loss: 0.13107967\n",
            "iteration: 144 loss: 0.06789055\n",
            "iteration: 145 loss: 0.26420119\n",
            "iteration: 146 loss: 0.07747048\n",
            "iteration: 147 loss: 0.11657475\n",
            "iteration: 148 loss: 0.37728119\n",
            "iteration: 149 loss: 0.09493247\n",
            "iteration: 150 loss: 0.11898781\n",
            "iteration: 151 loss: 0.06411735\n",
            "iteration: 152 loss: 0.28050333\n",
            "iteration: 153 loss: 0.13001028\n",
            "iteration: 154 loss: 0.19701782\n",
            "iteration: 155 loss: 0.12116594\n",
            "iteration: 156 loss: 0.06890972\n",
            "iteration: 157 loss: 0.13130017\n",
            "iteration: 158 loss: 0.08151895\n",
            "iteration: 159 loss: 0.06549324\n",
            "iteration: 160 loss: 0.08576657\n",
            "iteration: 161 loss: 0.09062473\n",
            "iteration: 162 loss: 0.09929651\n",
            "iteration: 163 loss: 0.13913415\n",
            "iteration: 164 loss: 0.10480289\n",
            "iteration: 165 loss: 0.07354762\n",
            "iteration: 166 loss: 0.08070379\n",
            "iteration: 167 loss: 0.18420896\n",
            "iteration: 168 loss: 0.04883783\n",
            "iteration: 169 loss: 0.09608746\n",
            "iteration: 170 loss: 0.11333939\n",
            "iteration: 171 loss: 0.12452495\n",
            "iteration: 172 loss: 0.06524014\n",
            "iteration: 173 loss: 0.08805303\n",
            "iteration: 174 loss: 0.14696912\n",
            "iteration: 175 loss: 0.10474846\n",
            "iteration: 176 loss: 0.07570268\n",
            "iteration: 177 loss: 0.81697702\n",
            "iteration: 178 loss: 0.15012652\n",
            "iteration: 179 loss: 0.07229677\n",
            "iteration: 180 loss: 0.08472297\n",
            "iteration: 181 loss: 0.08733639\n",
            "iteration: 182 loss: 0.13566856\n",
            "iteration: 183 loss: 0.11664566\n",
            "iteration: 184 loss: 0.07457866\n",
            "iteration: 185 loss: 0.44949013\n",
            "iteration: 186 loss: 0.41864192\n",
            "iteration: 187 loss: 0.09365862\n",
            "iteration: 188 loss: 0.05487036\n",
            "iteration: 189 loss: 0.08721274\n",
            "iteration: 190 loss: 0.12435140\n",
            "iteration: 191 loss: 0.07134543\n",
            "iteration: 192 loss: 0.08549632\n",
            "iteration: 193 loss: 0.12071823\n",
            "iteration: 194 loss: 0.25672111\n",
            "iteration: 195 loss: 0.16517378\n",
            "iteration: 196 loss: 0.09928430\n",
            "iteration: 197 loss: 0.17256023\n",
            "iteration: 198 loss: 0.10082680\n",
            "iteration: 199 loss: 0.06133549\n",
            "epoch:  86 mean loss training: 0.12306131\n",
            "epoch:  86 mean loss validation: 1.19321716\n",
            "iteration:   0 loss: 0.22375846\n",
            "iteration:   1 loss: 0.05202118\n",
            "iteration:   2 loss: 0.10286750\n",
            "iteration:   3 loss: 0.08663520\n",
            "iteration:   4 loss: 0.07221142\n",
            "iteration:   5 loss: 0.07254119\n",
            "iteration:   6 loss: 0.05313474\n",
            "iteration:   7 loss: 0.20255049\n",
            "iteration:   8 loss: 0.09043975\n",
            "iteration:   9 loss: 0.14465635\n",
            "iteration:  10 loss: 0.22192711\n",
            "iteration:  11 loss: 0.14494750\n",
            "iteration:  12 loss: 0.13048962\n",
            "iteration:  13 loss: 0.28399438\n",
            "iteration:  14 loss: 0.08342081\n",
            "iteration:  15 loss: 0.23489667\n",
            "iteration:  16 loss: 0.04889142\n",
            "iteration:  17 loss: 0.14957252\n",
            "iteration:  18 loss: 0.09752426\n",
            "iteration:  19 loss: 0.21082661\n",
            "iteration:  20 loss: 0.09822162\n",
            "iteration:  21 loss: 0.15761814\n",
            "iteration:  22 loss: 0.07745928\n",
            "iteration:  23 loss: 0.10138290\n",
            "iteration:  24 loss: 0.08929008\n",
            "iteration:  25 loss: 0.06019688\n",
            "iteration:  26 loss: 0.08924244\n",
            "iteration:  27 loss: 0.06623314\n",
            "iteration:  28 loss: 0.07256910\n",
            "iteration:  29 loss: 0.07441869\n",
            "iteration:  30 loss: 0.05912636\n",
            "iteration:  31 loss: 0.09226893\n",
            "iteration:  32 loss: 0.15713030\n",
            "iteration:  33 loss: 0.09773826\n",
            "iteration:  34 loss: 0.12823074\n",
            "iteration:  35 loss: 0.08812650\n",
            "iteration:  36 loss: 0.06584302\n",
            "iteration:  37 loss: 0.08478845\n",
            "iteration:  38 loss: 0.08852698\n",
            "iteration:  39 loss: 0.26564950\n",
            "iteration:  40 loss: 0.11210074\n",
            "iteration:  41 loss: 0.19513838\n",
            "iteration:  42 loss: 0.08732964\n",
            "iteration:  43 loss: 0.08610727\n",
            "iteration:  44 loss: 0.08287241\n",
            "iteration:  45 loss: 0.09170162\n",
            "iteration:  46 loss: 0.06218939\n",
            "iteration:  47 loss: 0.14589691\n",
            "iteration:  48 loss: 0.07013165\n",
            "iteration:  49 loss: 0.09378603\n",
            "iteration:  50 loss: 0.06884290\n",
            "iteration:  51 loss: 0.09146254\n",
            "iteration:  52 loss: 0.08459742\n",
            "iteration:  53 loss: 0.07073308\n",
            "iteration:  54 loss: 0.06776398\n",
            "iteration:  55 loss: 0.07162199\n",
            "iteration:  56 loss: 0.26316553\n",
            "iteration:  57 loss: 0.10981078\n",
            "iteration:  58 loss: 0.13088596\n",
            "iteration:  59 loss: 0.24325493\n",
            "iteration:  60 loss: 0.05208387\n",
            "iteration:  61 loss: 0.13493571\n",
            "iteration:  62 loss: 0.09465079\n",
            "iteration:  63 loss: 0.10242236\n",
            "iteration:  64 loss: 0.11548410\n",
            "iteration:  65 loss: 0.10198405\n",
            "iteration:  66 loss: 0.05309919\n",
            "iteration:  67 loss: 0.06945288\n",
            "iteration:  68 loss: 0.07518277\n",
            "iteration:  69 loss: 0.08003145\n",
            "iteration:  70 loss: 0.07309506\n",
            "iteration:  71 loss: 0.09566687\n",
            "iteration:  72 loss: 0.05602656\n",
            "iteration:  73 loss: 0.09075937\n",
            "iteration:  74 loss: 0.16111811\n",
            "iteration:  75 loss: 0.15592989\n",
            "iteration:  76 loss: 0.06105053\n",
            "iteration:  77 loss: 0.15783864\n",
            "iteration:  78 loss: 0.14049505\n",
            "iteration:  79 loss: 0.15589593\n",
            "iteration:  80 loss: 0.06988733\n",
            "iteration:  81 loss: 0.11176507\n",
            "iteration:  82 loss: 0.07300815\n",
            "iteration:  83 loss: 0.11717298\n",
            "iteration:  84 loss: 0.11320938\n",
            "iteration:  85 loss: 0.05586107\n",
            "iteration:  86 loss: 0.24062943\n",
            "iteration:  87 loss: 0.07872150\n",
            "iteration:  88 loss: 0.08985330\n",
            "iteration:  89 loss: 0.08840352\n",
            "iteration:  90 loss: 0.08749115\n",
            "iteration:  91 loss: 0.05528347\n",
            "iteration:  92 loss: 0.25713617\n",
            "iteration:  93 loss: 0.24201819\n",
            "iteration:  94 loss: 0.09381223\n",
            "iteration:  95 loss: 0.05973019\n",
            "iteration:  96 loss: 0.07764965\n",
            "iteration:  97 loss: 0.18644597\n",
            "iteration:  98 loss: 0.10401134\n",
            "iteration:  99 loss: 0.14744079\n",
            "iteration: 100 loss: 0.19870582\n",
            "iteration: 101 loss: 0.10063949\n",
            "iteration: 102 loss: 0.07678390\n",
            "iteration: 103 loss: 0.10910745\n",
            "iteration: 104 loss: 0.09731893\n",
            "iteration: 105 loss: 0.08059593\n",
            "iteration: 106 loss: 0.12417135\n",
            "iteration: 107 loss: 0.06799854\n",
            "iteration: 108 loss: 0.05351952\n",
            "iteration: 109 loss: 0.06808157\n",
            "iteration: 110 loss: 0.09029199\n",
            "iteration: 111 loss: 0.27550113\n",
            "iteration: 112 loss: 0.08812463\n",
            "iteration: 113 loss: 0.09992372\n",
            "iteration: 114 loss: 0.14132327\n",
            "iteration: 115 loss: 0.47505158\n",
            "iteration: 116 loss: 0.05471800\n",
            "iteration: 117 loss: 0.08298916\n",
            "iteration: 118 loss: 0.08545076\n",
            "iteration: 119 loss: 0.17536251\n",
            "iteration: 120 loss: 0.08956960\n",
            "iteration: 121 loss: 0.15268672\n",
            "iteration: 122 loss: 0.08760411\n",
            "iteration: 123 loss: 0.06530468\n",
            "iteration: 124 loss: 0.06382491\n",
            "iteration: 125 loss: 0.06470449\n",
            "iteration: 126 loss: 0.10767433\n",
            "iteration: 127 loss: 0.10172048\n",
            "iteration: 128 loss: 0.10359994\n",
            "iteration: 129 loss: 0.07121851\n",
            "iteration: 130 loss: 0.11967054\n",
            "iteration: 131 loss: 0.05463798\n",
            "iteration: 132 loss: 0.09251112\n",
            "iteration: 133 loss: 0.05389290\n",
            "iteration: 134 loss: 0.10235068\n",
            "iteration: 135 loss: 0.08408094\n",
            "iteration: 136 loss: 0.22488396\n",
            "iteration: 137 loss: 0.06468613\n",
            "iteration: 138 loss: 0.10395620\n",
            "iteration: 139 loss: 0.28937337\n",
            "iteration: 140 loss: 0.11749223\n",
            "iteration: 141 loss: 0.13441388\n",
            "iteration: 142 loss: 0.07524379\n",
            "iteration: 143 loss: 0.19854817\n",
            "iteration: 144 loss: 0.09474938\n",
            "iteration: 145 loss: 0.21451035\n",
            "iteration: 146 loss: 0.06337828\n",
            "iteration: 147 loss: 0.09537301\n",
            "iteration: 148 loss: 0.10309547\n",
            "iteration: 149 loss: 0.06845599\n",
            "iteration: 150 loss: 0.08340748\n",
            "iteration: 151 loss: 0.10476767\n",
            "iteration: 152 loss: 0.11381797\n",
            "iteration: 153 loss: 0.11909933\n",
            "iteration: 154 loss: 0.15856542\n",
            "iteration: 155 loss: 0.14056483\n",
            "iteration: 156 loss: 0.16459131\n",
            "iteration: 157 loss: 0.16135436\n",
            "iteration: 158 loss: 0.16308424\n",
            "iteration: 159 loss: 0.05954078\n",
            "iteration: 160 loss: 0.28852621\n",
            "iteration: 161 loss: 0.26251939\n",
            "iteration: 162 loss: 0.24534878\n",
            "iteration: 163 loss: 0.06348612\n",
            "iteration: 164 loss: 0.12779287\n",
            "iteration: 165 loss: 0.09312928\n",
            "iteration: 166 loss: 0.10203063\n",
            "iteration: 167 loss: 0.09338370\n",
            "iteration: 168 loss: 0.52486235\n",
            "iteration: 169 loss: 0.07602142\n",
            "iteration: 170 loss: 0.06854682\n",
            "iteration: 171 loss: 0.06716569\n",
            "iteration: 172 loss: 0.07178748\n",
            "iteration: 173 loss: 0.37733015\n",
            "iteration: 174 loss: 0.06750154\n",
            "iteration: 175 loss: 0.06605421\n",
            "iteration: 176 loss: 0.09673294\n",
            "iteration: 177 loss: 0.10451789\n",
            "iteration: 178 loss: 0.07263565\n",
            "iteration: 179 loss: 0.11634202\n",
            "iteration: 180 loss: 0.10526069\n",
            "iteration: 181 loss: 0.16206969\n",
            "iteration: 182 loss: 0.06774837\n",
            "iteration: 183 loss: 0.07655147\n",
            "iteration: 184 loss: 0.08147087\n",
            "iteration: 185 loss: 0.06043257\n",
            "iteration: 186 loss: 0.13324983\n",
            "iteration: 187 loss: 0.06874534\n",
            "iteration: 188 loss: 0.09621745\n",
            "iteration: 189 loss: 0.16647109\n",
            "iteration: 190 loss: 0.15472549\n",
            "iteration: 191 loss: 0.18111220\n",
            "iteration: 192 loss: 0.06391427\n",
            "iteration: 193 loss: 0.12024905\n",
            "iteration: 194 loss: 0.07257186\n",
            "iteration: 195 loss: 0.15448728\n",
            "iteration: 196 loss: 0.45294163\n",
            "iteration: 197 loss: 0.06670111\n",
            "iteration: 198 loss: 0.33240241\n",
            "iteration: 199 loss: 0.07454756\n",
            "epoch:  87 mean loss training: 0.12041372\n",
            "epoch:  87 mean loss validation: 1.20796037\n",
            "iteration:   0 loss: 0.09070832\n",
            "iteration:   1 loss: 0.26061904\n",
            "iteration:   2 loss: 0.16528530\n",
            "iteration:   3 loss: 0.09707855\n",
            "iteration:   4 loss: 0.11154320\n",
            "iteration:   5 loss: 0.07159525\n",
            "iteration:   6 loss: 0.05477823\n",
            "iteration:   7 loss: 0.05867720\n",
            "iteration:   8 loss: 0.22330424\n",
            "iteration:   9 loss: 0.07728526\n",
            "iteration:  10 loss: 0.04903418\n",
            "iteration:  11 loss: 0.10543461\n",
            "iteration:  12 loss: 0.09555281\n",
            "iteration:  13 loss: 0.12642217\n",
            "iteration:  14 loss: 0.07752848\n",
            "iteration:  15 loss: 0.28591508\n",
            "iteration:  16 loss: 0.32216558\n",
            "iteration:  17 loss: 0.11809111\n",
            "iteration:  18 loss: 0.12558010\n",
            "iteration:  19 loss: 0.23264934\n",
            "iteration:  20 loss: 0.05824192\n",
            "iteration:  21 loss: 0.10595017\n",
            "iteration:  22 loss: 0.07791104\n",
            "iteration:  23 loss: 0.09193991\n",
            "iteration:  24 loss: 0.17978230\n",
            "iteration:  25 loss: 0.10623255\n",
            "iteration:  26 loss: 0.16350098\n",
            "iteration:  27 loss: 0.09135623\n",
            "iteration:  28 loss: 0.08400011\n",
            "iteration:  29 loss: 0.15423393\n",
            "iteration:  30 loss: 0.11902037\n",
            "iteration:  31 loss: 0.24326345\n",
            "iteration:  32 loss: 0.26526442\n",
            "iteration:  33 loss: 0.14075232\n",
            "iteration:  34 loss: 0.07405394\n",
            "iteration:  35 loss: 0.16443515\n",
            "iteration:  36 loss: 0.18145762\n",
            "iteration:  37 loss: 0.12536724\n",
            "iteration:  38 loss: 0.08936203\n",
            "iteration:  39 loss: 0.04816308\n",
            "iteration:  40 loss: 0.12986706\n",
            "iteration:  41 loss: 0.10128948\n",
            "iteration:  42 loss: 0.21770865\n",
            "iteration:  43 loss: 0.07133877\n",
            "iteration:  44 loss: 0.13779110\n",
            "iteration:  45 loss: 0.15177834\n",
            "iteration:  46 loss: 0.07772019\n",
            "iteration:  47 loss: 0.20012839\n",
            "iteration:  48 loss: 0.05255910\n",
            "iteration:  49 loss: 0.10065804\n",
            "iteration:  50 loss: 0.32973284\n",
            "iteration:  51 loss: 0.07167049\n",
            "iteration:  52 loss: 0.17007357\n",
            "iteration:  53 loss: 0.11489859\n",
            "iteration:  54 loss: 0.27764806\n",
            "iteration:  55 loss: 0.19040456\n",
            "iteration:  56 loss: 0.10410502\n",
            "iteration:  57 loss: 0.26594931\n",
            "iteration:  58 loss: 0.08778497\n",
            "iteration:  59 loss: 0.07239216\n",
            "iteration:  60 loss: 0.16911870\n",
            "iteration:  61 loss: 0.09102733\n",
            "iteration:  62 loss: 0.06501625\n",
            "iteration:  63 loss: 0.11225163\n",
            "iteration:  64 loss: 0.08061254\n",
            "iteration:  65 loss: 0.06780258\n",
            "iteration:  66 loss: 0.11250456\n",
            "iteration:  67 loss: 0.06279541\n",
            "iteration:  68 loss: 0.05093668\n",
            "iteration:  69 loss: 0.08836038\n",
            "iteration:  70 loss: 0.06550235\n",
            "iteration:  71 loss: 0.05530528\n",
            "iteration:  72 loss: 0.15956381\n",
            "iteration:  73 loss: 0.10143064\n",
            "iteration:  74 loss: 0.07954776\n",
            "iteration:  75 loss: 0.09700505\n",
            "iteration:  76 loss: 0.08998789\n",
            "iteration:  77 loss: 0.07824127\n",
            "iteration:  78 loss: 0.05351496\n",
            "iteration:  79 loss: 0.08608699\n",
            "iteration:  80 loss: 0.12259935\n",
            "iteration:  81 loss: 0.06149636\n",
            "iteration:  82 loss: 0.10357315\n",
            "iteration:  83 loss: 0.10854582\n",
            "iteration:  84 loss: 0.07949337\n",
            "iteration:  85 loss: 0.06135546\n",
            "iteration:  86 loss: 0.13095972\n",
            "iteration:  87 loss: 0.07784998\n",
            "iteration:  88 loss: 0.14683405\n",
            "iteration:  89 loss: 0.09201372\n",
            "iteration:  90 loss: 0.06860186\n",
            "iteration:  91 loss: 0.06244465\n",
            "iteration:  92 loss: 0.14815336\n",
            "iteration:  93 loss: 0.06249624\n",
            "iteration:  94 loss: 0.08581869\n",
            "iteration:  95 loss: 0.10325706\n",
            "iteration:  96 loss: 0.08921544\n",
            "iteration:  97 loss: 0.16321334\n",
            "iteration:  98 loss: 0.31991228\n",
            "iteration:  99 loss: 0.08267418\n",
            "iteration: 100 loss: 0.05264959\n",
            "iteration: 101 loss: 0.11429410\n",
            "iteration: 102 loss: 0.12397596\n",
            "iteration: 103 loss: 0.07615994\n",
            "iteration: 104 loss: 0.07098348\n",
            "iteration: 105 loss: 0.15878543\n",
            "iteration: 106 loss: 0.19015455\n",
            "iteration: 107 loss: 0.06630703\n",
            "iteration: 108 loss: 0.07385827\n",
            "iteration: 109 loss: 0.32419634\n",
            "iteration: 110 loss: 0.04310920\n",
            "iteration: 111 loss: 0.09354083\n",
            "iteration: 112 loss: 0.05487106\n",
            "iteration: 113 loss: 0.05194619\n",
            "iteration: 114 loss: 0.06505841\n",
            "iteration: 115 loss: 0.06091023\n",
            "iteration: 116 loss: 0.06322572\n",
            "iteration: 117 loss: 0.15140706\n",
            "iteration: 118 loss: 0.09679928\n",
            "iteration: 119 loss: 0.15719059\n",
            "iteration: 120 loss: 0.14022699\n",
            "iteration: 121 loss: 0.20621532\n",
            "iteration: 122 loss: 0.10744992\n",
            "iteration: 123 loss: 0.29373017\n",
            "iteration: 124 loss: 0.07441802\n",
            "iteration: 125 loss: 0.17998928\n",
            "iteration: 126 loss: 0.06695965\n",
            "iteration: 127 loss: 0.07779522\n",
            "iteration: 128 loss: 0.09279304\n",
            "iteration: 129 loss: 0.12718841\n",
            "iteration: 130 loss: 0.06303362\n",
            "iteration: 131 loss: 0.09475593\n",
            "iteration: 132 loss: 0.15582231\n",
            "iteration: 133 loss: 0.08527879\n",
            "iteration: 134 loss: 0.13154314\n",
            "iteration: 135 loss: 0.10199805\n",
            "iteration: 136 loss: 0.10618053\n",
            "iteration: 137 loss: 0.15614626\n",
            "iteration: 138 loss: 0.09187232\n",
            "iteration: 139 loss: 0.07300803\n",
            "iteration: 140 loss: 0.06208303\n",
            "iteration: 141 loss: 0.25864574\n",
            "iteration: 142 loss: 0.06925254\n",
            "iteration: 143 loss: 0.11562149\n",
            "iteration: 144 loss: 0.15234649\n",
            "iteration: 145 loss: 0.11268969\n",
            "iteration: 146 loss: 0.09400721\n",
            "iteration: 147 loss: 0.08084209\n",
            "iteration: 148 loss: 0.08551636\n",
            "iteration: 149 loss: 0.20764707\n",
            "iteration: 150 loss: 0.06631684\n",
            "iteration: 151 loss: 0.09034090\n",
            "iteration: 152 loss: 0.11967340\n",
            "iteration: 153 loss: 0.06915421\n",
            "iteration: 154 loss: 0.16641447\n",
            "iteration: 155 loss: 0.10877612\n",
            "iteration: 156 loss: 0.14129904\n",
            "iteration: 157 loss: 0.06844802\n",
            "iteration: 158 loss: 0.21708229\n",
            "iteration: 159 loss: 0.06589191\n",
            "iteration: 160 loss: 0.17076622\n",
            "iteration: 161 loss: 0.12435436\n",
            "iteration: 162 loss: 0.10042408\n",
            "iteration: 163 loss: 0.07475519\n",
            "iteration: 164 loss: 0.05630072\n",
            "iteration: 165 loss: 0.22074866\n",
            "iteration: 166 loss: 0.10141027\n",
            "iteration: 167 loss: 0.09448772\n",
            "iteration: 168 loss: 0.16095674\n",
            "iteration: 169 loss: 0.05685639\n",
            "iteration: 170 loss: 0.05963334\n",
            "iteration: 171 loss: 0.08133725\n",
            "iteration: 172 loss: 0.18186955\n",
            "iteration: 173 loss: 0.21784872\n",
            "iteration: 174 loss: 0.20360239\n",
            "iteration: 175 loss: 0.17041008\n",
            "iteration: 176 loss: 0.09165747\n",
            "iteration: 177 loss: 0.04404770\n",
            "iteration: 178 loss: 0.21124989\n",
            "iteration: 179 loss: 0.06286746\n",
            "iteration: 180 loss: 0.10589252\n",
            "iteration: 181 loss: 0.10610814\n",
            "iteration: 182 loss: 0.16490477\n",
            "iteration: 183 loss: 0.05990165\n",
            "iteration: 184 loss: 0.07805052\n",
            "iteration: 185 loss: 0.10697454\n",
            "iteration: 186 loss: 0.08099635\n",
            "iteration: 187 loss: 0.06945273\n",
            "iteration: 188 loss: 0.15010384\n",
            "iteration: 189 loss: 0.09927955\n",
            "iteration: 190 loss: 0.07569239\n",
            "iteration: 191 loss: 0.09403531\n",
            "iteration: 192 loss: 0.20793916\n",
            "iteration: 193 loss: 0.05624556\n",
            "iteration: 194 loss: 0.04801230\n",
            "iteration: 195 loss: 0.05465098\n",
            "iteration: 196 loss: 0.09770575\n",
            "iteration: 197 loss: 0.06840178\n",
            "iteration: 198 loss: 0.04105292\n",
            "iteration: 199 loss: 0.09110862\n",
            "epoch:  88 mean loss training: 0.11693063\n",
            "epoch:  88 mean loss validation: 1.16908097\n",
            "iteration:   0 loss: 0.09486549\n",
            "iteration:   1 loss: 0.29669613\n",
            "iteration:   2 loss: 0.13659540\n",
            "iteration:   3 loss: 0.03960950\n",
            "iteration:   4 loss: 0.15781143\n",
            "iteration:   5 loss: 0.05971636\n",
            "iteration:   6 loss: 0.14594331\n",
            "iteration:   7 loss: 0.13606110\n",
            "iteration:   8 loss: 0.06904408\n",
            "iteration:   9 loss: 0.08840969\n",
            "iteration:  10 loss: 0.05358541\n",
            "iteration:  11 loss: 0.08408643\n",
            "iteration:  12 loss: 0.12858599\n",
            "iteration:  13 loss: 0.10883215\n",
            "iteration:  14 loss: 0.12292664\n",
            "iteration:  15 loss: 0.09361008\n",
            "iteration:  16 loss: 0.09621412\n",
            "iteration:  17 loss: 0.06861904\n",
            "iteration:  18 loss: 0.12395872\n",
            "iteration:  19 loss: 0.09198488\n",
            "iteration:  20 loss: 0.11403136\n",
            "iteration:  21 loss: 0.13274188\n",
            "iteration:  22 loss: 0.10137764\n",
            "iteration:  23 loss: 0.10226972\n",
            "iteration:  24 loss: 0.05642349\n",
            "iteration:  25 loss: 0.06635597\n",
            "iteration:  26 loss: 0.08716875\n",
            "iteration:  27 loss: 0.29876739\n",
            "iteration:  28 loss: 0.09464185\n",
            "iteration:  29 loss: 0.13083968\n",
            "iteration:  30 loss: 0.09234831\n",
            "iteration:  31 loss: 0.08672040\n",
            "iteration:  32 loss: 0.10166085\n",
            "iteration:  33 loss: 0.04221246\n",
            "iteration:  34 loss: 0.09288577\n",
            "iteration:  35 loss: 0.25322002\n",
            "iteration:  36 loss: 0.11092020\n",
            "iteration:  37 loss: 0.10084435\n",
            "iteration:  38 loss: 0.10679958\n",
            "iteration:  39 loss: 0.14944524\n",
            "iteration:  40 loss: 0.10278942\n",
            "iteration:  41 loss: 0.08419616\n",
            "iteration:  42 loss: 0.14084280\n",
            "iteration:  43 loss: 0.14421059\n",
            "iteration:  44 loss: 0.12076199\n",
            "iteration:  45 loss: 0.08414403\n",
            "iteration:  46 loss: 0.18510291\n",
            "iteration:  47 loss: 0.06355853\n",
            "iteration:  48 loss: 0.09792984\n",
            "iteration:  49 loss: 0.09902599\n",
            "iteration:  50 loss: 0.08066002\n",
            "iteration:  51 loss: 0.10343106\n",
            "iteration:  52 loss: 0.09586664\n",
            "iteration:  53 loss: 0.13919325\n",
            "iteration:  54 loss: 0.04311856\n",
            "iteration:  55 loss: 0.09709470\n",
            "iteration:  56 loss: 0.12879232\n",
            "iteration:  57 loss: 0.12519771\n",
            "iteration:  58 loss: 0.05124787\n",
            "iteration:  59 loss: 0.06305226\n",
            "iteration:  60 loss: 0.05421339\n",
            "iteration:  61 loss: 0.09212427\n",
            "iteration:  62 loss: 0.23155361\n",
            "iteration:  63 loss: 0.08564524\n",
            "iteration:  64 loss: 0.05304778\n",
            "iteration:  65 loss: 0.15564799\n",
            "iteration:  66 loss: 0.04428542\n",
            "iteration:  67 loss: 0.06837206\n",
            "iteration:  68 loss: 0.07278116\n",
            "iteration:  69 loss: 0.11424384\n",
            "iteration:  70 loss: 0.12740016\n",
            "iteration:  71 loss: 0.09704185\n",
            "iteration:  72 loss: 0.08401988\n",
            "iteration:  73 loss: 0.07858226\n",
            "iteration:  74 loss: 0.08458430\n",
            "iteration:  75 loss: 0.05195954\n",
            "iteration:  76 loss: 0.07755286\n",
            "iteration:  77 loss: 0.05375582\n",
            "iteration:  78 loss: 0.14994968\n",
            "iteration:  79 loss: 0.11891092\n",
            "iteration:  80 loss: 0.09500201\n",
            "iteration:  81 loss: 0.07325864\n",
            "iteration:  82 loss: 0.07553102\n",
            "iteration:  83 loss: 0.14238338\n",
            "iteration:  84 loss: 0.06507991\n",
            "iteration:  85 loss: 0.05686200\n",
            "iteration:  86 loss: 0.08842000\n",
            "iteration:  87 loss: 0.38765657\n",
            "iteration:  88 loss: 0.12291512\n",
            "iteration:  89 loss: 0.04855067\n",
            "iteration:  90 loss: 0.10956752\n",
            "iteration:  91 loss: 0.06313843\n",
            "iteration:  92 loss: 0.07181513\n",
            "iteration:  93 loss: 0.07439626\n",
            "iteration:  94 loss: 0.37537915\n",
            "iteration:  95 loss: 0.08739324\n",
            "iteration:  96 loss: 0.08125329\n",
            "iteration:  97 loss: 0.18310304\n",
            "iteration:  98 loss: 0.39106289\n",
            "iteration:  99 loss: 0.07365029\n",
            "iteration: 100 loss: 0.10632725\n",
            "iteration: 101 loss: 0.08339871\n",
            "iteration: 102 loss: 0.43092713\n",
            "iteration: 103 loss: 0.13874269\n",
            "iteration: 104 loss: 0.09675429\n",
            "iteration: 105 loss: 0.11551680\n",
            "iteration: 106 loss: 0.12277399\n",
            "iteration: 107 loss: 0.05404784\n",
            "iteration: 108 loss: 0.12609586\n",
            "iteration: 109 loss: 0.06144107\n",
            "iteration: 110 loss: 0.10151570\n",
            "iteration: 111 loss: 0.10574244\n",
            "iteration: 112 loss: 0.07593475\n",
            "iteration: 113 loss: 0.30711830\n",
            "iteration: 114 loss: 0.12187043\n",
            "iteration: 115 loss: 0.09896351\n",
            "iteration: 116 loss: 0.14098419\n",
            "iteration: 117 loss: 0.20642668\n",
            "iteration: 118 loss: 0.08133161\n",
            "iteration: 119 loss: 0.09923001\n",
            "iteration: 120 loss: 0.12570986\n",
            "iteration: 121 loss: 0.12259501\n",
            "iteration: 122 loss: 0.10198992\n",
            "iteration: 123 loss: 0.10765712\n",
            "iteration: 124 loss: 0.05778612\n",
            "iteration: 125 loss: 0.10069710\n",
            "iteration: 126 loss: 0.20757821\n",
            "iteration: 127 loss: 0.09329041\n",
            "iteration: 128 loss: 0.14148030\n",
            "iteration: 129 loss: 0.11120576\n",
            "iteration: 130 loss: 0.14431073\n",
            "iteration: 131 loss: 0.09071919\n",
            "iteration: 132 loss: 0.17022163\n",
            "iteration: 133 loss: 0.11021176\n",
            "iteration: 134 loss: 0.08889045\n",
            "iteration: 135 loss: 0.08409722\n",
            "iteration: 136 loss: 0.12120186\n",
            "iteration: 137 loss: 0.07216648\n",
            "iteration: 138 loss: 0.09089547\n",
            "iteration: 139 loss: 0.11679104\n",
            "iteration: 140 loss: 0.05187628\n",
            "iteration: 141 loss: 0.07704805\n",
            "iteration: 142 loss: 0.06161633\n",
            "iteration: 143 loss: 0.31379023\n",
            "iteration: 144 loss: 0.08220074\n",
            "iteration: 145 loss: 0.10003862\n",
            "iteration: 146 loss: 0.11031701\n",
            "iteration: 147 loss: 0.13352835\n",
            "iteration: 148 loss: 0.06329715\n",
            "iteration: 149 loss: 0.07245435\n",
            "iteration: 150 loss: 0.05739744\n",
            "iteration: 151 loss: 0.06969405\n",
            "iteration: 152 loss: 0.08452824\n",
            "iteration: 153 loss: 0.12639025\n",
            "iteration: 154 loss: 0.05906727\n",
            "iteration: 155 loss: 0.16320172\n",
            "iteration: 156 loss: 0.05873793\n",
            "iteration: 157 loss: 0.08307470\n",
            "iteration: 158 loss: 0.29833302\n",
            "iteration: 159 loss: 0.10902602\n",
            "iteration: 160 loss: 0.05964844\n",
            "iteration: 161 loss: 0.10135317\n",
            "iteration: 162 loss: 0.06538957\n",
            "iteration: 163 loss: 0.13390163\n",
            "iteration: 164 loss: 0.22688223\n",
            "iteration: 165 loss: 0.08732104\n",
            "iteration: 166 loss: 0.37273487\n",
            "iteration: 167 loss: 0.09180645\n",
            "iteration: 168 loss: 0.19488716\n",
            "iteration: 169 loss: 0.15712093\n",
            "iteration: 170 loss: 0.06046223\n",
            "iteration: 171 loss: 0.09283468\n",
            "iteration: 172 loss: 0.07908647\n",
            "iteration: 173 loss: 0.09816486\n",
            "iteration: 174 loss: 0.17551179\n",
            "iteration: 175 loss: 0.07161961\n",
            "iteration: 176 loss: 0.10430112\n",
            "iteration: 177 loss: 0.11122570\n",
            "iteration: 178 loss: 0.05577292\n",
            "iteration: 179 loss: 0.06871957\n",
            "iteration: 180 loss: 0.09549962\n",
            "iteration: 181 loss: 0.10355308\n",
            "iteration: 182 loss: 0.14847338\n",
            "iteration: 183 loss: 0.27008379\n",
            "iteration: 184 loss: 0.06012306\n",
            "iteration: 185 loss: 0.14926133\n",
            "iteration: 186 loss: 0.06860590\n",
            "iteration: 187 loss: 0.27160504\n",
            "iteration: 188 loss: 0.20221442\n",
            "iteration: 189 loss: 0.10391836\n",
            "iteration: 190 loss: 0.07771036\n",
            "iteration: 191 loss: 0.09734355\n",
            "iteration: 192 loss: 0.44606352\n",
            "iteration: 193 loss: 0.09839208\n",
            "iteration: 194 loss: 0.11729030\n",
            "iteration: 195 loss: 0.11862648\n",
            "iteration: 196 loss: 0.09493911\n",
            "iteration: 197 loss: 0.51625246\n",
            "iteration: 198 loss: 0.14597011\n",
            "iteration: 199 loss: 0.08813025\n",
            "epoch:  89 mean loss training: 0.11962098\n",
            "epoch:  89 mean loss validation: 1.19966304\n",
            "iteration:   0 loss: 0.10591660\n",
            "iteration:   1 loss: 0.08846761\n",
            "iteration:   2 loss: 0.22204742\n",
            "iteration:   3 loss: 0.06684609\n",
            "iteration:   4 loss: 0.10383108\n",
            "iteration:   5 loss: 0.10444260\n",
            "iteration:   6 loss: 0.12126353\n",
            "iteration:   7 loss: 0.12513123\n",
            "iteration:   8 loss: 0.18607429\n",
            "iteration:   9 loss: 0.10790803\n",
            "iteration:  10 loss: 0.21283889\n",
            "iteration:  11 loss: 0.06828003\n",
            "iteration:  12 loss: 0.05310284\n",
            "iteration:  13 loss: 0.14977472\n",
            "iteration:  14 loss: 0.08215558\n",
            "iteration:  15 loss: 0.06845217\n",
            "iteration:  16 loss: 0.12499699\n",
            "iteration:  17 loss: 0.10173498\n",
            "iteration:  18 loss: 0.27035624\n",
            "iteration:  19 loss: 0.10239437\n",
            "iteration:  20 loss: 0.09124289\n",
            "iteration:  21 loss: 0.04043665\n",
            "iteration:  22 loss: 0.12949917\n",
            "iteration:  23 loss: 0.30719557\n",
            "iteration:  24 loss: 0.42255986\n",
            "iteration:  25 loss: 0.11944367\n",
            "iteration:  26 loss: 0.09653810\n",
            "iteration:  27 loss: 0.07820330\n",
            "iteration:  28 loss: 0.06182939\n",
            "iteration:  29 loss: 0.10303129\n",
            "iteration:  30 loss: 0.17764257\n",
            "iteration:  31 loss: 0.13129465\n",
            "iteration:  32 loss: 0.07114171\n",
            "iteration:  33 loss: 0.19827801\n",
            "iteration:  34 loss: 0.24964312\n",
            "iteration:  35 loss: 0.07921795\n",
            "iteration:  36 loss: 0.07676346\n",
            "iteration:  37 loss: 0.09115513\n",
            "iteration:  38 loss: 0.08025854\n",
            "iteration:  39 loss: 0.08328186\n",
            "iteration:  40 loss: 0.22542751\n",
            "iteration:  41 loss: 0.07632121\n",
            "iteration:  42 loss: 0.07289384\n",
            "iteration:  43 loss: 0.10512617\n",
            "iteration:  44 loss: 0.10202745\n",
            "iteration:  45 loss: 0.05997704\n",
            "iteration:  46 loss: 0.11101566\n",
            "iteration:  47 loss: 0.18582182\n",
            "iteration:  48 loss: 0.06570297\n",
            "iteration:  49 loss: 0.12338314\n",
            "iteration:  50 loss: 0.16612734\n",
            "iteration:  51 loss: 0.11386055\n",
            "iteration:  52 loss: 0.11352526\n",
            "iteration:  53 loss: 0.13465755\n",
            "iteration:  54 loss: 0.20713499\n",
            "iteration:  55 loss: 0.55274457\n",
            "iteration:  56 loss: 0.10489769\n",
            "iteration:  57 loss: 0.14371888\n",
            "iteration:  58 loss: 0.12306689\n",
            "iteration:  59 loss: 0.07856902\n",
            "iteration:  60 loss: 0.15703079\n",
            "iteration:  61 loss: 0.10143815\n",
            "iteration:  62 loss: 0.08522979\n",
            "iteration:  63 loss: 0.10565646\n",
            "iteration:  64 loss: 0.07090528\n",
            "iteration:  65 loss: 0.08743054\n",
            "iteration:  66 loss: 0.17022733\n",
            "iteration:  67 loss: 0.08680608\n",
            "iteration:  68 loss: 0.13343246\n",
            "iteration:  69 loss: 0.08381435\n",
            "iteration:  70 loss: 0.10184487\n",
            "iteration:  71 loss: 0.06982574\n",
            "iteration:  72 loss: 0.08780868\n",
            "iteration:  73 loss: 0.21987796\n",
            "iteration:  74 loss: 0.05754581\n",
            "iteration:  75 loss: 0.18536092\n",
            "iteration:  76 loss: 0.07446034\n",
            "iteration:  77 loss: 0.06130014\n",
            "iteration:  78 loss: 0.09578842\n",
            "iteration:  79 loss: 0.04946042\n",
            "iteration:  80 loss: 0.14145377\n",
            "iteration:  81 loss: 0.40746078\n",
            "iteration:  82 loss: 0.18371558\n",
            "iteration:  83 loss: 0.18180408\n",
            "iteration:  84 loss: 0.11764078\n",
            "iteration:  85 loss: 0.11793326\n",
            "iteration:  86 loss: 0.30897483\n",
            "iteration:  87 loss: 0.07094872\n",
            "iteration:  88 loss: 0.04845887\n",
            "iteration:  89 loss: 0.07678883\n",
            "iteration:  90 loss: 0.04239900\n",
            "iteration:  91 loss: 0.09792861\n",
            "iteration:  92 loss: 0.06083958\n",
            "iteration:  93 loss: 0.13782179\n",
            "iteration:  94 loss: 0.08637387\n",
            "iteration:  95 loss: 0.03682145\n",
            "iteration:  96 loss: 0.06621630\n",
            "iteration:  97 loss: 0.10578130\n",
            "iteration:  98 loss: 0.07756674\n",
            "iteration:  99 loss: 0.12665434\n",
            "iteration: 100 loss: 0.07022144\n",
            "iteration: 101 loss: 0.10293516\n",
            "iteration: 102 loss: 0.33561271\n",
            "iteration: 103 loss: 0.04708521\n",
            "iteration: 104 loss: 0.09617639\n",
            "iteration: 105 loss: 0.07301241\n",
            "iteration: 106 loss: 0.04030232\n",
            "iteration: 107 loss: 0.10486311\n",
            "iteration: 108 loss: 0.11158600\n",
            "iteration: 109 loss: 0.07735549\n",
            "iteration: 110 loss: 0.11649482\n",
            "iteration: 111 loss: 0.14294255\n",
            "iteration: 112 loss: 0.11147138\n",
            "iteration: 113 loss: 0.18851590\n",
            "iteration: 114 loss: 0.34130108\n",
            "iteration: 115 loss: 0.07776900\n",
            "iteration: 116 loss: 0.09409832\n",
            "iteration: 117 loss: 0.07218637\n",
            "iteration: 118 loss: 0.11026022\n",
            "iteration: 119 loss: 0.08781332\n",
            "iteration: 120 loss: 0.18890262\n",
            "iteration: 121 loss: 0.10880204\n",
            "iteration: 122 loss: 0.06032593\n",
            "iteration: 123 loss: 0.08559199\n",
            "iteration: 124 loss: 0.09444910\n",
            "iteration: 125 loss: 0.08279696\n",
            "iteration: 126 loss: 0.10321307\n",
            "iteration: 127 loss: 0.08054058\n",
            "iteration: 128 loss: 0.34817857\n",
            "iteration: 129 loss: 0.06541368\n",
            "iteration: 130 loss: 0.08721022\n",
            "iteration: 131 loss: 0.19480196\n",
            "iteration: 132 loss: 0.11045220\n",
            "iteration: 133 loss: 0.11902499\n",
            "iteration: 134 loss: 0.13366900\n",
            "iteration: 135 loss: 0.13999170\n",
            "iteration: 136 loss: 0.13028929\n",
            "iteration: 137 loss: 0.07010964\n",
            "iteration: 138 loss: 0.12947553\n",
            "iteration: 139 loss: 0.14469375\n",
            "iteration: 140 loss: 0.40269977\n",
            "iteration: 141 loss: 0.16972488\n",
            "iteration: 142 loss: 0.11390346\n",
            "iteration: 143 loss: 0.46641707\n",
            "iteration: 144 loss: 0.12255964\n",
            "iteration: 145 loss: 0.08467640\n",
            "iteration: 146 loss: 0.09577110\n",
            "iteration: 147 loss: 0.08372493\n",
            "iteration: 148 loss: 0.08449732\n",
            "iteration: 149 loss: 0.05093034\n",
            "iteration: 150 loss: 0.12419654\n",
            "iteration: 151 loss: 0.09452090\n",
            "iteration: 152 loss: 0.11774065\n",
            "iteration: 153 loss: 0.08874236\n",
            "iteration: 154 loss: 0.08082688\n",
            "iteration: 155 loss: 0.06954837\n",
            "iteration: 156 loss: 0.11952947\n",
            "iteration: 157 loss: 0.10216053\n",
            "iteration: 158 loss: 0.15188178\n",
            "iteration: 159 loss: 0.10342012\n",
            "iteration: 160 loss: 0.09806877\n",
            "iteration: 161 loss: 0.09840550\n",
            "iteration: 162 loss: 0.07960700\n",
            "iteration: 163 loss: 0.09903990\n",
            "iteration: 164 loss: 0.12089700\n",
            "iteration: 165 loss: 0.05202369\n",
            "iteration: 166 loss: 0.08820927\n",
            "iteration: 167 loss: 0.19078143\n",
            "iteration: 168 loss: 0.21200290\n",
            "iteration: 169 loss: 0.25277898\n",
            "iteration: 170 loss: 0.08881159\n",
            "iteration: 171 loss: 0.12273200\n",
            "iteration: 172 loss: 0.07533802\n",
            "iteration: 173 loss: 0.07644180\n",
            "iteration: 174 loss: 0.06053316\n",
            "iteration: 175 loss: 0.06045971\n",
            "iteration: 176 loss: 0.08301954\n",
            "iteration: 177 loss: 0.09783330\n",
            "iteration: 178 loss: 0.18259361\n",
            "iteration: 179 loss: 0.16508648\n",
            "iteration: 180 loss: 0.10421018\n",
            "iteration: 181 loss: 0.15674433\n",
            "iteration: 182 loss: 0.11754827\n",
            "iteration: 183 loss: 0.08413560\n",
            "iteration: 184 loss: 0.10124335\n",
            "iteration: 185 loss: 0.22130309\n",
            "iteration: 186 loss: 0.07928867\n",
            "iteration: 187 loss: 0.10093719\n",
            "iteration: 188 loss: 0.09860067\n",
            "iteration: 189 loss: 0.10738295\n",
            "iteration: 190 loss: 0.05309799\n",
            "iteration: 191 loss: 0.11352405\n",
            "iteration: 192 loss: 0.07743242\n",
            "iteration: 193 loss: 0.14017999\n",
            "iteration: 194 loss: 0.07617287\n",
            "iteration: 195 loss: 0.04095004\n",
            "iteration: 196 loss: 0.09509404\n",
            "iteration: 197 loss: 0.11781560\n",
            "iteration: 198 loss: 0.12641294\n",
            "iteration: 199 loss: 0.08143888\n",
            "epoch:  90 mean loss training: 0.12302542\n",
            "epoch:  90 mean loss validation: 1.18912792\n",
            "iteration:   0 loss: 0.08025898\n",
            "iteration:   1 loss: 0.06886264\n",
            "iteration:   2 loss: 0.13801171\n",
            "iteration:   3 loss: 0.16122358\n",
            "iteration:   4 loss: 0.07649089\n",
            "iteration:   5 loss: 0.11178728\n",
            "iteration:   6 loss: 0.06218422\n",
            "iteration:   7 loss: 0.08423194\n",
            "iteration:   8 loss: 0.12956069\n",
            "iteration:   9 loss: 0.07789866\n",
            "iteration:  10 loss: 0.12037270\n",
            "iteration:  11 loss: 0.75529236\n",
            "iteration:  12 loss: 0.13098137\n",
            "iteration:  13 loss: 0.09464414\n",
            "iteration:  14 loss: 0.05806252\n",
            "iteration:  15 loss: 0.09793188\n",
            "iteration:  16 loss: 0.10916786\n",
            "iteration:  17 loss: 0.17661944\n",
            "iteration:  18 loss: 0.08950292\n",
            "iteration:  19 loss: 0.09421495\n",
            "iteration:  20 loss: 0.06379148\n",
            "iteration:  21 loss: 0.07331590\n",
            "iteration:  22 loss: 0.06482054\n",
            "iteration:  23 loss: 0.10885040\n",
            "iteration:  24 loss: 0.12611666\n",
            "iteration:  25 loss: 0.09729850\n",
            "iteration:  26 loss: 0.06162838\n",
            "iteration:  27 loss: 0.06825511\n",
            "iteration:  28 loss: 0.05824297\n",
            "iteration:  29 loss: 0.05732932\n",
            "iteration:  30 loss: 0.10776538\n",
            "iteration:  31 loss: 0.07520391\n",
            "iteration:  32 loss: 0.04691150\n",
            "iteration:  33 loss: 0.08978503\n",
            "iteration:  34 loss: 0.05731688\n",
            "iteration:  35 loss: 0.11368242\n",
            "iteration:  36 loss: 0.12493426\n",
            "iteration:  37 loss: 0.08405700\n",
            "iteration:  38 loss: 0.17026196\n",
            "iteration:  39 loss: 0.15743873\n",
            "iteration:  40 loss: 0.07952907\n",
            "iteration:  41 loss: 0.11100848\n",
            "iteration:  42 loss: 0.08797382\n",
            "iteration:  43 loss: 0.06450482\n",
            "iteration:  44 loss: 0.21434882\n",
            "iteration:  45 loss: 0.21572976\n",
            "iteration:  46 loss: 0.07619156\n",
            "iteration:  47 loss: 0.16694704\n",
            "iteration:  48 loss: 0.06869285\n",
            "iteration:  49 loss: 0.07673191\n",
            "iteration:  50 loss: 0.10751590\n",
            "iteration:  51 loss: 0.09883419\n",
            "iteration:  52 loss: 0.06470030\n",
            "iteration:  53 loss: 0.10971111\n",
            "iteration:  54 loss: 0.08124354\n",
            "iteration:  55 loss: 0.09463038\n",
            "iteration:  56 loss: 0.15929593\n",
            "iteration:  57 loss: 0.32289413\n",
            "iteration:  58 loss: 0.17108639\n",
            "iteration:  59 loss: 0.05387581\n",
            "iteration:  60 loss: 0.07321152\n",
            "iteration:  61 loss: 0.10646216\n",
            "iteration:  62 loss: 0.17612445\n",
            "iteration:  63 loss: 0.07023333\n",
            "iteration:  64 loss: 0.11155130\n",
            "iteration:  65 loss: 0.08318229\n",
            "iteration:  66 loss: 0.05436774\n",
            "iteration:  67 loss: 0.23116405\n",
            "iteration:  68 loss: 0.12965389\n",
            "iteration:  69 loss: 0.10411259\n",
            "iteration:  70 loss: 0.07029416\n",
            "iteration:  71 loss: 0.06906911\n",
            "iteration:  72 loss: 0.14786325\n",
            "iteration:  73 loss: 0.09205419\n",
            "iteration:  74 loss: 0.11764640\n",
            "iteration:  75 loss: 0.08302014\n",
            "iteration:  76 loss: 0.13105559\n",
            "iteration:  77 loss: 0.06547977\n",
            "iteration:  78 loss: 0.05127795\n",
            "iteration:  79 loss: 0.13026050\n",
            "iteration:  80 loss: 0.08657068\n",
            "iteration:  81 loss: 0.40095326\n",
            "iteration:  82 loss: 0.05266294\n",
            "iteration:  83 loss: 0.12957639\n",
            "iteration:  84 loss: 0.04247182\n",
            "iteration:  85 loss: 0.11600509\n",
            "iteration:  86 loss: 0.07847489\n",
            "iteration:  87 loss: 0.08696301\n",
            "iteration:  88 loss: 0.09960491\n",
            "iteration:  89 loss: 0.08178374\n",
            "iteration:  90 loss: 0.09333694\n",
            "iteration:  91 loss: 0.13546233\n",
            "iteration:  92 loss: 0.07070027\n",
            "iteration:  93 loss: 0.07225085\n",
            "iteration:  94 loss: 0.07480685\n",
            "iteration:  95 loss: 0.08739434\n",
            "iteration:  96 loss: 0.10078964\n",
            "iteration:  97 loss: 0.19052911\n",
            "iteration:  98 loss: 0.10789254\n",
            "iteration:  99 loss: 0.09849459\n",
            "iteration: 100 loss: 0.11541125\n",
            "iteration: 101 loss: 0.13070937\n",
            "iteration: 102 loss: 0.14778751\n",
            "iteration: 103 loss: 0.15937281\n",
            "iteration: 104 loss: 0.09942831\n",
            "iteration: 105 loss: 0.06871169\n",
            "iteration: 106 loss: 0.07219514\n",
            "iteration: 107 loss: 0.10593572\n",
            "iteration: 108 loss: 0.11974908\n",
            "iteration: 109 loss: 0.05793127\n",
            "iteration: 110 loss: 0.07475621\n",
            "iteration: 111 loss: 0.10233766\n",
            "iteration: 112 loss: 0.05559473\n",
            "iteration: 113 loss: 0.21558441\n",
            "iteration: 114 loss: 0.07649211\n",
            "iteration: 115 loss: 0.11265001\n",
            "iteration: 116 loss: 0.06568727\n",
            "iteration: 117 loss: 0.06586616\n",
            "iteration: 118 loss: 0.12207042\n",
            "iteration: 119 loss: 0.10642492\n",
            "iteration: 120 loss: 0.21370298\n",
            "iteration: 121 loss: 0.07669584\n",
            "iteration: 122 loss: 0.07190064\n",
            "iteration: 123 loss: 0.07471348\n",
            "iteration: 124 loss: 0.51404673\n",
            "iteration: 125 loss: 0.20817329\n",
            "iteration: 126 loss: 0.14277649\n",
            "iteration: 127 loss: 0.08974306\n",
            "iteration: 128 loss: 0.13378114\n",
            "iteration: 129 loss: 0.18720731\n",
            "iteration: 130 loss: 0.06673820\n",
            "iteration: 131 loss: 0.12744325\n",
            "iteration: 132 loss: 0.11830574\n",
            "iteration: 133 loss: 0.06370487\n",
            "iteration: 134 loss: 0.12187811\n",
            "iteration: 135 loss: 0.07990422\n",
            "iteration: 136 loss: 0.14491585\n",
            "iteration: 137 loss: 0.09269500\n",
            "iteration: 138 loss: 0.15742332\n",
            "iteration: 139 loss: 0.06419580\n",
            "iteration: 140 loss: 0.25246373\n",
            "iteration: 141 loss: 0.07641263\n",
            "iteration: 142 loss: 0.05982903\n",
            "iteration: 143 loss: 0.18870699\n",
            "iteration: 144 loss: 0.12693745\n",
            "iteration: 145 loss: 0.08586454\n",
            "iteration: 146 loss: 0.10059007\n",
            "iteration: 147 loss: 0.07508679\n",
            "iteration: 148 loss: 0.05823614\n",
            "iteration: 149 loss: 0.12241017\n",
            "iteration: 150 loss: 0.12027179\n",
            "iteration: 151 loss: 0.09338025\n",
            "iteration: 152 loss: 0.11242497\n",
            "iteration: 153 loss: 0.04542578\n",
            "iteration: 154 loss: 0.18260585\n",
            "iteration: 155 loss: 0.17520119\n",
            "iteration: 156 loss: 0.09538464\n",
            "iteration: 157 loss: 0.08218503\n",
            "iteration: 158 loss: 0.08553091\n",
            "iteration: 159 loss: 0.09216078\n",
            "iteration: 160 loss: 0.18789168\n",
            "iteration: 161 loss: 0.28147858\n",
            "iteration: 162 loss: 0.56963032\n",
            "iteration: 163 loss: 0.07100629\n",
            "iteration: 164 loss: 0.05930712\n",
            "iteration: 165 loss: 0.11879537\n",
            "iteration: 166 loss: 0.23393355\n",
            "iteration: 167 loss: 0.13202251\n",
            "iteration: 168 loss: 0.05714347\n",
            "iteration: 169 loss: 0.11341769\n",
            "iteration: 170 loss: 0.13911095\n",
            "iteration: 171 loss: 0.05218162\n",
            "iteration: 172 loss: 0.09642105\n",
            "iteration: 173 loss: 0.14661992\n",
            "iteration: 174 loss: 0.17755905\n",
            "iteration: 175 loss: 0.12188052\n",
            "iteration: 176 loss: 0.07208635\n",
            "iteration: 177 loss: 0.18271652\n",
            "iteration: 178 loss: 0.08657283\n",
            "iteration: 179 loss: 0.15107784\n",
            "iteration: 180 loss: 0.17142242\n",
            "iteration: 181 loss: 0.10608783\n",
            "iteration: 182 loss: 0.07976127\n",
            "iteration: 183 loss: 0.22539978\n",
            "iteration: 184 loss: 0.12315363\n",
            "iteration: 185 loss: 0.09558700\n",
            "iteration: 186 loss: 0.27268243\n",
            "iteration: 187 loss: 0.11644156\n",
            "iteration: 188 loss: 0.06836027\n",
            "iteration: 189 loss: 0.19373171\n",
            "iteration: 190 loss: 0.09895581\n",
            "iteration: 191 loss: 0.08114044\n",
            "iteration: 192 loss: 0.05012430\n",
            "iteration: 193 loss: 0.08803722\n",
            "iteration: 194 loss: 0.10474816\n",
            "iteration: 195 loss: 0.15356167\n",
            "iteration: 196 loss: 0.26656646\n",
            "iteration: 197 loss: 0.16309024\n",
            "iteration: 198 loss: 0.07192876\n",
            "iteration: 199 loss: 0.07756782\n",
            "epoch:  91 mean loss training: 0.11984757\n",
            "epoch:  91 mean loss validation: 1.15553653\n",
            "iteration:   0 loss: 0.10419008\n",
            "iteration:   1 loss: 0.17037882\n",
            "iteration:   2 loss: 0.07741266\n",
            "iteration:   3 loss: 0.10429260\n",
            "iteration:   4 loss: 0.15484804\n",
            "iteration:   5 loss: 0.05246278\n",
            "iteration:   6 loss: 0.11362768\n",
            "iteration:   7 loss: 0.08538339\n",
            "iteration:   8 loss: 0.07745786\n",
            "iteration:   9 loss: 0.09176039\n",
            "iteration:  10 loss: 0.24361259\n",
            "iteration:  11 loss: 0.14502768\n",
            "iteration:  12 loss: 0.13933119\n",
            "iteration:  13 loss: 0.09603431\n",
            "iteration:  14 loss: 0.09415668\n",
            "iteration:  15 loss: 0.09606045\n",
            "iteration:  16 loss: 0.18926588\n",
            "iteration:  17 loss: 0.10945660\n",
            "iteration:  18 loss: 0.09491619\n",
            "iteration:  19 loss: 0.09576100\n",
            "iteration:  20 loss: 0.13811816\n",
            "iteration:  21 loss: 0.11830933\n",
            "iteration:  22 loss: 0.12550721\n",
            "iteration:  23 loss: 0.08947749\n",
            "iteration:  24 loss: 0.13573769\n",
            "iteration:  25 loss: 0.22400488\n",
            "iteration:  26 loss: 0.13219507\n",
            "iteration:  27 loss: 0.08836471\n",
            "iteration:  28 loss: 0.09837085\n",
            "iteration:  29 loss: 0.14275032\n",
            "iteration:  30 loss: 0.05469954\n",
            "iteration:  31 loss: 0.13643113\n",
            "iteration:  32 loss: 0.05865869\n",
            "iteration:  33 loss: 0.12931378\n",
            "iteration:  34 loss: 0.44875672\n",
            "iteration:  35 loss: 0.12073095\n",
            "iteration:  36 loss: 0.07863375\n",
            "iteration:  37 loss: 0.04860584\n",
            "iteration:  38 loss: 0.06469810\n",
            "iteration:  39 loss: 0.09624799\n",
            "iteration:  40 loss: 0.09271047\n",
            "iteration:  41 loss: 0.11388698\n",
            "iteration:  42 loss: 0.05509304\n",
            "iteration:  43 loss: 0.06527057\n",
            "iteration:  44 loss: 0.09419380\n",
            "iteration:  45 loss: 0.16006361\n",
            "iteration:  46 loss: 0.09977205\n",
            "iteration:  47 loss: 0.07538325\n",
            "iteration:  48 loss: 0.11182582\n",
            "iteration:  49 loss: 0.07753178\n",
            "iteration:  50 loss: 0.06368119\n",
            "iteration:  51 loss: 0.08212789\n",
            "iteration:  52 loss: 0.09531327\n",
            "iteration:  53 loss: 0.11849326\n",
            "iteration:  54 loss: 0.05472256\n",
            "iteration:  55 loss: 0.11030668\n",
            "iteration:  56 loss: 0.16755177\n",
            "iteration:  57 loss: 0.07816244\n",
            "iteration:  58 loss: 0.09210132\n",
            "iteration:  59 loss: 0.09574032\n",
            "iteration:  60 loss: 0.08685786\n",
            "iteration:  61 loss: 0.09275421\n",
            "iteration:  62 loss: 0.07350284\n",
            "iteration:  63 loss: 0.08234441\n",
            "iteration:  64 loss: 0.13123032\n",
            "iteration:  65 loss: 0.06471048\n",
            "iteration:  66 loss: 0.08278238\n",
            "iteration:  67 loss: 0.08879708\n",
            "iteration:  68 loss: 0.11792406\n",
            "iteration:  69 loss: 0.09760220\n",
            "iteration:  70 loss: 0.07668392\n",
            "iteration:  71 loss: 0.08222517\n",
            "iteration:  72 loss: 0.07854453\n",
            "iteration:  73 loss: 0.12998939\n",
            "iteration:  74 loss: 0.09654099\n",
            "iteration:  75 loss: 0.05216843\n",
            "iteration:  76 loss: 0.08271423\n",
            "iteration:  77 loss: 0.17008352\n",
            "iteration:  78 loss: 0.05753359\n",
            "iteration:  79 loss: 0.09251438\n",
            "iteration:  80 loss: 0.06983088\n",
            "iteration:  81 loss: 0.17228921\n",
            "iteration:  82 loss: 0.10160755\n",
            "iteration:  83 loss: 0.08876982\n",
            "iteration:  84 loss: 0.16043082\n",
            "iteration:  85 loss: 0.12788738\n",
            "iteration:  86 loss: 0.09024927\n",
            "iteration:  87 loss: 0.17433843\n",
            "iteration:  88 loss: 0.10880879\n",
            "iteration:  89 loss: 0.20338677\n",
            "iteration:  90 loss: 0.09310503\n",
            "iteration:  91 loss: 0.46158320\n",
            "iteration:  92 loss: 0.26189443\n",
            "iteration:  93 loss: 0.16005692\n",
            "iteration:  94 loss: 0.09559858\n",
            "iteration:  95 loss: 0.18559715\n",
            "iteration:  96 loss: 0.13873890\n",
            "iteration:  97 loss: 0.13681933\n",
            "iteration:  98 loss: 0.08316356\n",
            "iteration:  99 loss: 0.06000063\n",
            "iteration: 100 loss: 0.13334244\n",
            "iteration: 101 loss: 0.13806549\n",
            "iteration: 102 loss: 0.20114672\n",
            "iteration: 103 loss: 0.06649385\n",
            "iteration: 104 loss: 0.04621597\n",
            "iteration: 105 loss: 0.12241808\n",
            "iteration: 106 loss: 0.05193619\n",
            "iteration: 107 loss: 0.21184306\n",
            "iteration: 108 loss: 0.11779046\n",
            "iteration: 109 loss: 0.04945098\n",
            "iteration: 110 loss: 0.06235768\n",
            "iteration: 111 loss: 0.08181931\n",
            "iteration: 112 loss: 0.08165275\n",
            "iteration: 113 loss: 0.19370648\n",
            "iteration: 114 loss: 0.14262393\n",
            "iteration: 115 loss: 0.14210474\n",
            "iteration: 116 loss: 0.05772835\n",
            "iteration: 117 loss: 0.09020218\n",
            "iteration: 118 loss: 0.08787944\n",
            "iteration: 119 loss: 0.09095635\n",
            "iteration: 120 loss: 0.31745589\n",
            "iteration: 121 loss: 0.08393027\n",
            "iteration: 122 loss: 0.07993072\n",
            "iteration: 123 loss: 0.28574225\n",
            "iteration: 124 loss: 0.08463365\n",
            "iteration: 125 loss: 0.12215458\n",
            "iteration: 126 loss: 0.13233061\n",
            "iteration: 127 loss: 0.22827333\n",
            "iteration: 128 loss: 0.07314372\n",
            "iteration: 129 loss: 0.06317428\n",
            "iteration: 130 loss: 0.10876467\n",
            "iteration: 131 loss: 0.09917822\n",
            "iteration: 132 loss: 0.08364202\n",
            "iteration: 133 loss: 0.18193898\n",
            "iteration: 134 loss: 0.05584222\n",
            "iteration: 135 loss: 0.08694574\n",
            "iteration: 136 loss: 0.05224412\n",
            "iteration: 137 loss: 0.26911628\n",
            "iteration: 138 loss: 0.13727485\n",
            "iteration: 139 loss: 0.11805586\n",
            "iteration: 140 loss: 0.05904102\n",
            "iteration: 141 loss: 0.24002185\n",
            "iteration: 142 loss: 0.06540412\n",
            "iteration: 143 loss: 0.11716400\n",
            "iteration: 144 loss: 0.08126360\n",
            "iteration: 145 loss: 0.21863258\n",
            "iteration: 146 loss: 0.06000949\n",
            "iteration: 147 loss: 0.04943176\n",
            "iteration: 148 loss: 0.10561357\n",
            "iteration: 149 loss: 0.08833627\n",
            "iteration: 150 loss: 0.08020173\n",
            "iteration: 151 loss: 0.06294956\n",
            "iteration: 152 loss: 0.08275937\n",
            "iteration: 153 loss: 0.05702669\n",
            "iteration: 154 loss: 0.13418898\n",
            "iteration: 155 loss: 0.12018997\n",
            "iteration: 156 loss: 0.11129328\n",
            "iteration: 157 loss: 0.10871164\n",
            "iteration: 158 loss: 0.12438241\n",
            "iteration: 159 loss: 0.08832830\n",
            "iteration: 160 loss: 0.10011388\n",
            "iteration: 161 loss: 0.18283403\n",
            "iteration: 162 loss: 0.08899036\n",
            "iteration: 163 loss: 0.05687600\n",
            "iteration: 164 loss: 0.18664683\n",
            "iteration: 165 loss: 0.10185240\n",
            "iteration: 166 loss: 0.11710192\n",
            "iteration: 167 loss: 0.19504738\n",
            "iteration: 168 loss: 0.06755313\n",
            "iteration: 169 loss: 0.13624844\n",
            "iteration: 170 loss: 0.16347125\n",
            "iteration: 171 loss: 0.12180880\n",
            "iteration: 172 loss: 0.06856403\n",
            "iteration: 173 loss: 0.08501148\n",
            "iteration: 174 loss: 0.07872257\n",
            "iteration: 175 loss: 0.48771378\n",
            "iteration: 176 loss: 0.13058203\n",
            "iteration: 177 loss: 0.05429054\n",
            "iteration: 178 loss: 0.17379215\n",
            "iteration: 179 loss: 0.10582949\n",
            "iteration: 180 loss: 0.09347080\n",
            "iteration: 181 loss: 0.15610249\n",
            "iteration: 182 loss: 0.07703420\n",
            "iteration: 183 loss: 0.12931740\n",
            "iteration: 184 loss: 0.09778742\n",
            "iteration: 185 loss: 0.10703787\n",
            "iteration: 186 loss: 0.05889995\n",
            "iteration: 187 loss: 0.10064799\n",
            "iteration: 188 loss: 0.07125147\n",
            "iteration: 189 loss: 0.16007316\n",
            "iteration: 190 loss: 0.10861337\n",
            "iteration: 191 loss: 0.07822342\n",
            "iteration: 192 loss: 0.12153533\n",
            "iteration: 193 loss: 0.06230452\n",
            "iteration: 194 loss: 0.25376055\n",
            "iteration: 195 loss: 0.24102652\n",
            "iteration: 196 loss: 0.17009142\n",
            "iteration: 197 loss: 0.05703657\n",
            "iteration: 198 loss: 0.08596409\n",
            "iteration: 199 loss: 0.29626837\n",
            "epoch:  92 mean loss training: 0.11849298\n",
            "epoch:  92 mean loss validation: 1.18125916\n",
            "iteration:   0 loss: 0.10803541\n",
            "iteration:   1 loss: 0.10021272\n",
            "iteration:   2 loss: 0.07490452\n",
            "iteration:   3 loss: 0.14384773\n",
            "iteration:   4 loss: 0.33917999\n",
            "iteration:   5 loss: 0.06617364\n",
            "iteration:   6 loss: 0.09108105\n",
            "iteration:   7 loss: 0.07247708\n",
            "iteration:   8 loss: 0.09191223\n",
            "iteration:   9 loss: 0.08223228\n",
            "iteration:  10 loss: 0.14662063\n",
            "iteration:  11 loss: 0.07904208\n",
            "iteration:  12 loss: 0.11889581\n",
            "iteration:  13 loss: 0.10652634\n",
            "iteration:  14 loss: 0.14970732\n",
            "iteration:  15 loss: 0.12254408\n",
            "iteration:  16 loss: 0.16540778\n",
            "iteration:  17 loss: 0.06229597\n",
            "iteration:  18 loss: 0.09093896\n",
            "iteration:  19 loss: 0.13818088\n",
            "iteration:  20 loss: 0.06439888\n",
            "iteration:  21 loss: 0.10020043\n",
            "iteration:  22 loss: 0.04636476\n",
            "iteration:  23 loss: 0.12566638\n",
            "iteration:  24 loss: 0.11162212\n",
            "iteration:  25 loss: 0.08948611\n",
            "iteration:  26 loss: 0.38546911\n",
            "iteration:  27 loss: 0.08572136\n",
            "iteration:  28 loss: 0.13474959\n",
            "iteration:  29 loss: 0.07523423\n",
            "iteration:  30 loss: 0.09596241\n",
            "iteration:  31 loss: 0.06471074\n",
            "iteration:  32 loss: 0.10486147\n",
            "iteration:  33 loss: 0.07912554\n",
            "iteration:  34 loss: 0.05164363\n",
            "iteration:  35 loss: 0.12893078\n",
            "iteration:  36 loss: 0.25600982\n",
            "iteration:  37 loss: 0.61072654\n",
            "iteration:  38 loss: 0.12389110\n",
            "iteration:  39 loss: 0.06054708\n",
            "iteration:  40 loss: 0.04460290\n",
            "iteration:  41 loss: 0.10995069\n",
            "iteration:  42 loss: 0.14105082\n",
            "iteration:  43 loss: 0.15265769\n",
            "iteration:  44 loss: 0.08786365\n",
            "iteration:  45 loss: 0.07437652\n",
            "iteration:  46 loss: 0.07354708\n",
            "iteration:  47 loss: 0.07704799\n",
            "iteration:  48 loss: 0.21687265\n",
            "iteration:  49 loss: 0.07549379\n",
            "iteration:  50 loss: 0.05804831\n",
            "iteration:  51 loss: 0.05816509\n",
            "iteration:  52 loss: 0.05668832\n",
            "iteration:  53 loss: 0.09590344\n",
            "iteration:  54 loss: 0.11128967\n",
            "iteration:  55 loss: 0.14586224\n",
            "iteration:  56 loss: 0.18477510\n",
            "iteration:  57 loss: 0.12948771\n",
            "iteration:  58 loss: 0.22021633\n",
            "iteration:  59 loss: 0.07448408\n",
            "iteration:  60 loss: 0.13366796\n",
            "iteration:  61 loss: 0.07311745\n",
            "iteration:  62 loss: 0.07405896\n",
            "iteration:  63 loss: 0.09369175\n",
            "iteration:  64 loss: 0.35664889\n",
            "iteration:  65 loss: 0.12066448\n",
            "iteration:  66 loss: 0.09281722\n",
            "iteration:  67 loss: 0.12339574\n",
            "iteration:  68 loss: 0.08390050\n",
            "iteration:  69 loss: 0.13572751\n",
            "iteration:  70 loss: 0.30108926\n",
            "iteration:  71 loss: 0.07016844\n",
            "iteration:  72 loss: 0.07611296\n",
            "iteration:  73 loss: 0.28536457\n",
            "iteration:  74 loss: 0.17882484\n",
            "iteration:  75 loss: 0.05440105\n",
            "iteration:  76 loss: 0.11414187\n",
            "iteration:  77 loss: 0.06581144\n",
            "iteration:  78 loss: 0.06872281\n",
            "iteration:  79 loss: 0.06594896\n",
            "iteration:  80 loss: 0.10504425\n",
            "iteration:  81 loss: 0.07872400\n",
            "iteration:  82 loss: 0.08571807\n",
            "iteration:  83 loss: 0.06617205\n",
            "iteration:  84 loss: 0.20911646\n",
            "iteration:  85 loss: 0.10289490\n",
            "iteration:  86 loss: 0.08788352\n",
            "iteration:  87 loss: 0.07496154\n",
            "iteration:  88 loss: 0.23233986\n",
            "iteration:  89 loss: 0.22122505\n",
            "iteration:  90 loss: 0.11917669\n",
            "iteration:  91 loss: 0.08324591\n",
            "iteration:  92 loss: 0.08078346\n",
            "iteration:  93 loss: 0.09040096\n",
            "iteration:  94 loss: 0.14629480\n",
            "iteration:  95 loss: 0.18950289\n",
            "iteration:  96 loss: 0.09122255\n",
            "iteration:  97 loss: 0.06388544\n",
            "iteration:  98 loss: 0.11848687\n",
            "iteration:  99 loss: 0.08153749\n",
            "iteration: 100 loss: 0.04858175\n",
            "iteration: 101 loss: 0.12715988\n",
            "iteration: 102 loss: 0.08422224\n",
            "iteration: 103 loss: 0.11455650\n",
            "iteration: 104 loss: 0.08426723\n",
            "iteration: 105 loss: 0.11377294\n",
            "iteration: 106 loss: 0.09561191\n",
            "iteration: 107 loss: 0.12655684\n",
            "iteration: 108 loss: 0.10098578\n",
            "iteration: 109 loss: 0.14707388\n",
            "iteration: 110 loss: 0.05578555\n",
            "iteration: 111 loss: 0.04706839\n",
            "iteration: 112 loss: 0.25917500\n",
            "iteration: 113 loss: 0.05845752\n",
            "iteration: 114 loss: 0.05303499\n",
            "iteration: 115 loss: 0.08450833\n",
            "iteration: 116 loss: 0.15519567\n",
            "iteration: 117 loss: 0.18652593\n",
            "iteration: 118 loss: 0.06810976\n",
            "iteration: 119 loss: 0.13198437\n",
            "iteration: 120 loss: 0.17937931\n",
            "iteration: 121 loss: 0.09609569\n",
            "iteration: 122 loss: 0.26152349\n",
            "iteration: 123 loss: 0.15345478\n",
            "iteration: 124 loss: 0.12291276\n",
            "iteration: 125 loss: 0.09509729\n",
            "iteration: 126 loss: 0.09921718\n",
            "iteration: 127 loss: 0.14448120\n",
            "iteration: 128 loss: 0.06728503\n",
            "iteration: 129 loss: 0.09818979\n",
            "iteration: 130 loss: 0.06338010\n",
            "iteration: 131 loss: 0.05567275\n",
            "iteration: 132 loss: 0.10669273\n",
            "iteration: 133 loss: 0.07852049\n",
            "iteration: 134 loss: 0.09689040\n",
            "iteration: 135 loss: 0.13708413\n",
            "iteration: 136 loss: 0.17176573\n",
            "iteration: 137 loss: 0.12398085\n",
            "iteration: 138 loss: 0.23162156\n",
            "iteration: 139 loss: 0.32872012\n",
            "iteration: 140 loss: 0.10420403\n",
            "iteration: 141 loss: 0.04032740\n",
            "iteration: 142 loss: 0.10810225\n",
            "iteration: 143 loss: 0.04397181\n",
            "iteration: 144 loss: 0.06012311\n",
            "iteration: 145 loss: 0.16716790\n",
            "iteration: 146 loss: 0.05395062\n",
            "iteration: 147 loss: 0.11195256\n",
            "iteration: 148 loss: 0.11353754\n",
            "iteration: 149 loss: 0.15401988\n",
            "iteration: 150 loss: 0.07991372\n",
            "iteration: 151 loss: 0.34618890\n",
            "iteration: 152 loss: 0.13652945\n",
            "iteration: 153 loss: 0.09633724\n",
            "iteration: 154 loss: 0.06872057\n",
            "iteration: 155 loss: 0.16783743\n",
            "iteration: 156 loss: 0.06588691\n",
            "iteration: 157 loss: 0.05480698\n",
            "iteration: 158 loss: 0.11078297\n",
            "iteration: 159 loss: 0.22830121\n",
            "iteration: 160 loss: 0.13375314\n",
            "iteration: 161 loss: 0.05754858\n",
            "iteration: 162 loss: 0.23913693\n",
            "iteration: 163 loss: 0.05812513\n",
            "iteration: 164 loss: 0.12703329\n",
            "iteration: 165 loss: 0.41712636\n",
            "iteration: 166 loss: 0.12031521\n",
            "iteration: 167 loss: 0.20575576\n",
            "iteration: 168 loss: 0.09093270\n",
            "iteration: 169 loss: 0.06706887\n",
            "iteration: 170 loss: 0.08856863\n",
            "iteration: 171 loss: 0.11121431\n",
            "iteration: 172 loss: 0.05085461\n",
            "iteration: 173 loss: 0.15724358\n",
            "iteration: 174 loss: 0.21611066\n",
            "iteration: 175 loss: 0.10154185\n",
            "iteration: 176 loss: 0.15052482\n",
            "iteration: 177 loss: 0.16139507\n",
            "iteration: 178 loss: 0.17238963\n",
            "iteration: 179 loss: 0.08350807\n",
            "iteration: 180 loss: 0.24611267\n",
            "iteration: 181 loss: 0.08608880\n",
            "iteration: 182 loss: 0.10157001\n",
            "iteration: 183 loss: 0.10486492\n",
            "iteration: 184 loss: 0.07405613\n",
            "iteration: 185 loss: 0.21338890\n",
            "iteration: 186 loss: 0.09194846\n",
            "iteration: 187 loss: 0.12125309\n",
            "iteration: 188 loss: 0.12262277\n",
            "iteration: 189 loss: 0.11061928\n",
            "iteration: 190 loss: 0.19484618\n",
            "iteration: 191 loss: 0.08648503\n",
            "iteration: 192 loss: 0.23305365\n",
            "iteration: 193 loss: 0.15937886\n",
            "iteration: 194 loss: 0.32334417\n",
            "iteration: 195 loss: 0.07890321\n",
            "iteration: 196 loss: 0.05805592\n",
            "iteration: 197 loss: 0.16646370\n",
            "iteration: 198 loss: 0.07813030\n",
            "iteration: 199 loss: 0.10099965\n",
            "epoch:  93 mean loss training: 0.12480205\n",
            "epoch:  93 mean loss validation: 1.15431345\n",
            "iteration:   0 loss: 0.09055838\n",
            "iteration:   1 loss: 0.14661989\n",
            "iteration:   2 loss: 0.24596715\n",
            "iteration:   3 loss: 0.10929735\n",
            "iteration:   4 loss: 0.13543585\n",
            "iteration:   5 loss: 0.22580375\n",
            "iteration:   6 loss: 0.08844644\n",
            "iteration:   7 loss: 0.15721400\n",
            "iteration:   8 loss: 0.17167804\n",
            "iteration:   9 loss: 0.11254188\n",
            "iteration:  10 loss: 0.04712798\n",
            "iteration:  11 loss: 0.11155879\n",
            "iteration:  12 loss: 0.27184549\n",
            "iteration:  13 loss: 0.07529052\n",
            "iteration:  14 loss: 0.06753919\n",
            "iteration:  15 loss: 0.09369224\n",
            "iteration:  16 loss: 0.16114791\n",
            "iteration:  17 loss: 0.18105337\n",
            "iteration:  18 loss: 0.06643081\n",
            "iteration:  19 loss: 0.13267063\n",
            "iteration:  20 loss: 0.10558023\n",
            "iteration:  21 loss: 0.10061254\n",
            "iteration:  22 loss: 0.13869201\n",
            "iteration:  23 loss: 0.12048015\n",
            "iteration:  24 loss: 0.13540468\n",
            "iteration:  25 loss: 0.05909589\n",
            "iteration:  26 loss: 0.09485925\n",
            "iteration:  27 loss: 0.10024937\n",
            "iteration:  28 loss: 0.10593945\n",
            "iteration:  29 loss: 0.26587138\n",
            "iteration:  30 loss: 0.08693017\n",
            "iteration:  31 loss: 0.06663172\n",
            "iteration:  32 loss: 0.07255004\n",
            "iteration:  33 loss: 0.08920936\n",
            "iteration:  34 loss: 0.09234827\n",
            "iteration:  35 loss: 0.16212484\n",
            "iteration:  36 loss: 0.11783102\n",
            "iteration:  37 loss: 0.10605808\n",
            "iteration:  38 loss: 0.08031967\n",
            "iteration:  39 loss: 0.08627864\n",
            "iteration:  40 loss: 0.07917587\n",
            "iteration:  41 loss: 0.09249369\n",
            "iteration:  42 loss: 0.14090618\n",
            "iteration:  43 loss: 0.06139182\n",
            "iteration:  44 loss: 0.25485635\n",
            "iteration:  45 loss: 0.08858420\n",
            "iteration:  46 loss: 0.12161227\n",
            "iteration:  47 loss: 0.13127108\n",
            "iteration:  48 loss: 0.07431916\n",
            "iteration:  49 loss: 0.06821785\n",
            "iteration:  50 loss: 0.08112068\n",
            "iteration:  51 loss: 0.20904404\n",
            "iteration:  52 loss: 0.07451729\n",
            "iteration:  53 loss: 0.18215966\n",
            "iteration:  54 loss: 0.06314513\n",
            "iteration:  55 loss: 0.10129772\n",
            "iteration:  56 loss: 0.09362827\n",
            "iteration:  57 loss: 0.10587688\n",
            "iteration:  58 loss: 0.17039827\n",
            "iteration:  59 loss: 0.07429224\n",
            "iteration:  60 loss: 0.06169307\n",
            "iteration:  61 loss: 0.17452267\n",
            "iteration:  62 loss: 0.06511748\n",
            "iteration:  63 loss: 0.07266877\n",
            "iteration:  64 loss: 0.07834746\n",
            "iteration:  65 loss: 0.16413282\n",
            "iteration:  66 loss: 0.10623330\n",
            "iteration:  67 loss: 0.12524579\n",
            "iteration:  68 loss: 0.10269234\n",
            "iteration:  69 loss: 0.13759011\n",
            "iteration:  70 loss: 0.52483243\n",
            "iteration:  71 loss: 0.12172084\n",
            "iteration:  72 loss: 0.10720634\n",
            "iteration:  73 loss: 0.14923379\n",
            "iteration:  74 loss: 0.09422051\n",
            "iteration:  75 loss: 0.06798556\n",
            "iteration:  76 loss: 0.07737473\n",
            "iteration:  77 loss: 0.10356195\n",
            "iteration:  78 loss: 0.05772345\n",
            "iteration:  79 loss: 0.09095167\n",
            "iteration:  80 loss: 0.10005574\n",
            "iteration:  81 loss: 0.10679218\n",
            "iteration:  82 loss: 0.12736014\n",
            "iteration:  83 loss: 0.08671574\n",
            "iteration:  84 loss: 0.10707509\n",
            "iteration:  85 loss: 0.07045959\n",
            "iteration:  86 loss: 0.22958472\n",
            "iteration:  87 loss: 0.08730298\n",
            "iteration:  88 loss: 0.07519951\n",
            "iteration:  89 loss: 0.10210378\n",
            "iteration:  90 loss: 0.08404374\n",
            "iteration:  91 loss: 0.06822038\n",
            "iteration:  92 loss: 0.06240168\n",
            "iteration:  93 loss: 0.08497329\n",
            "iteration:  94 loss: 0.07723236\n",
            "iteration:  95 loss: 0.09635381\n",
            "iteration:  96 loss: 0.13004640\n",
            "iteration:  97 loss: 0.11141734\n",
            "iteration:  98 loss: 0.05773616\n",
            "iteration:  99 loss: 0.05150621\n",
            "iteration: 100 loss: 0.19233140\n",
            "iteration: 101 loss: 0.06514967\n",
            "iteration: 102 loss: 0.06059141\n",
            "iteration: 103 loss: 0.10569230\n",
            "iteration: 104 loss: 0.06640480\n",
            "iteration: 105 loss: 0.12646042\n",
            "iteration: 106 loss: 0.12574856\n",
            "iteration: 107 loss: 0.08438960\n",
            "iteration: 108 loss: 0.07709656\n",
            "iteration: 109 loss: 0.11514032\n",
            "iteration: 110 loss: 0.10784088\n",
            "iteration: 111 loss: 0.12580915\n",
            "iteration: 112 loss: 0.23220186\n",
            "iteration: 113 loss: 0.15689686\n",
            "iteration: 114 loss: 0.06318726\n",
            "iteration: 115 loss: 0.08444937\n",
            "iteration: 116 loss: 0.26221654\n",
            "iteration: 117 loss: 0.08123899\n",
            "iteration: 118 loss: 0.07637331\n",
            "iteration: 119 loss: 0.11085040\n",
            "iteration: 120 loss: 0.12015761\n",
            "iteration: 121 loss: 0.06833911\n",
            "iteration: 122 loss: 0.09133968\n",
            "iteration: 123 loss: 0.06639976\n",
            "iteration: 124 loss: 0.08483587\n",
            "iteration: 125 loss: 0.05250518\n",
            "iteration: 126 loss: 0.07997194\n",
            "iteration: 127 loss: 0.26813459\n",
            "iteration: 128 loss: 0.08350666\n",
            "iteration: 129 loss: 0.09094040\n",
            "iteration: 130 loss: 0.07064465\n",
            "iteration: 131 loss: 0.15975757\n",
            "iteration: 132 loss: 0.08124644\n",
            "iteration: 133 loss: 0.06592804\n",
            "iteration: 134 loss: 0.15854686\n",
            "iteration: 135 loss: 0.05195130\n",
            "iteration: 136 loss: 0.08493057\n",
            "iteration: 137 loss: 0.13646032\n",
            "iteration: 138 loss: 0.13175076\n",
            "iteration: 139 loss: 0.32123700\n",
            "iteration: 140 loss: 0.13707474\n",
            "iteration: 141 loss: 0.13286646\n",
            "iteration: 142 loss: 0.09112865\n",
            "iteration: 143 loss: 0.11426611\n",
            "iteration: 144 loss: 0.05817333\n",
            "iteration: 145 loss: 0.08639839\n",
            "iteration: 146 loss: 0.09586512\n",
            "iteration: 147 loss: 0.16722028\n",
            "iteration: 148 loss: 0.08203965\n",
            "iteration: 149 loss: 0.22497454\n",
            "iteration: 150 loss: 0.21486188\n",
            "iteration: 151 loss: 0.08946095\n",
            "iteration: 152 loss: 0.05062246\n",
            "iteration: 153 loss: 0.14727642\n",
            "iteration: 154 loss: 0.05362604\n",
            "iteration: 155 loss: 0.11485262\n",
            "iteration: 156 loss: 0.13060471\n",
            "iteration: 157 loss: 0.15739486\n",
            "iteration: 158 loss: 0.09366995\n",
            "iteration: 159 loss: 0.05855102\n",
            "iteration: 160 loss: 0.06486174\n",
            "iteration: 161 loss: 0.18174857\n",
            "iteration: 162 loss: 0.14141583\n",
            "iteration: 163 loss: 0.06696257\n",
            "iteration: 164 loss: 0.09705603\n",
            "iteration: 165 loss: 0.41544038\n",
            "iteration: 166 loss: 0.13826670\n",
            "iteration: 167 loss: 0.06390014\n",
            "iteration: 168 loss: 0.06148143\n",
            "iteration: 169 loss: 0.14041239\n",
            "iteration: 170 loss: 0.08071278\n",
            "iteration: 171 loss: 0.28433475\n",
            "iteration: 172 loss: 0.06947862\n",
            "iteration: 173 loss: 0.05864201\n",
            "iteration: 174 loss: 0.14248973\n",
            "iteration: 175 loss: 0.42712605\n",
            "iteration: 176 loss: 0.05433386\n",
            "iteration: 177 loss: 0.07963720\n",
            "iteration: 178 loss: 0.11147055\n",
            "iteration: 179 loss: 0.07249685\n",
            "iteration: 180 loss: 0.11155644\n",
            "iteration: 181 loss: 0.07607201\n",
            "iteration: 182 loss: 0.13410504\n",
            "iteration: 183 loss: 0.13674647\n",
            "iteration: 184 loss: 0.10128457\n",
            "iteration: 185 loss: 0.05188320\n",
            "iteration: 186 loss: 0.14929342\n",
            "iteration: 187 loss: 0.08420304\n",
            "iteration: 188 loss: 0.14160815\n",
            "iteration: 189 loss: 0.13257858\n",
            "iteration: 190 loss: 0.11763999\n",
            "iteration: 191 loss: 0.14323740\n",
            "iteration: 192 loss: 0.12445438\n",
            "iteration: 193 loss: 0.10524370\n",
            "iteration: 194 loss: 0.23569477\n",
            "iteration: 195 loss: 0.25792950\n",
            "iteration: 196 loss: 0.33552030\n",
            "iteration: 197 loss: 0.07597252\n",
            "iteration: 198 loss: 0.17585340\n",
            "iteration: 199 loss: 0.09711121\n",
            "epoch:  94 mean loss training: 0.11988384\n",
            "epoch:  94 mean loss validation: 1.19034564\n",
            "iteration:   0 loss: 0.17312512\n",
            "iteration:   1 loss: 0.07047480\n",
            "iteration:   2 loss: 0.10106145\n",
            "iteration:   3 loss: 0.07602561\n",
            "iteration:   4 loss: 0.08256578\n",
            "iteration:   5 loss: 0.07181095\n",
            "iteration:   6 loss: 0.08148590\n",
            "iteration:   7 loss: 0.14092618\n",
            "iteration:   8 loss: 0.13535601\n",
            "iteration:   9 loss: 0.03623960\n",
            "iteration:  10 loss: 0.10733110\n",
            "iteration:  11 loss: 0.08799310\n",
            "iteration:  12 loss: 0.16745837\n",
            "iteration:  13 loss: 0.08888173\n",
            "iteration:  14 loss: 0.08674955\n",
            "iteration:  15 loss: 0.08656074\n",
            "iteration:  16 loss: 0.11068486\n",
            "iteration:  17 loss: 0.16995883\n",
            "iteration:  18 loss: 0.04105012\n",
            "iteration:  19 loss: 0.09287107\n",
            "iteration:  20 loss: 0.15471923\n",
            "iteration:  21 loss: 0.10613513\n",
            "iteration:  22 loss: 0.12193005\n",
            "iteration:  23 loss: 0.07473655\n",
            "iteration:  24 loss: 0.39060724\n",
            "iteration:  25 loss: 0.07369255\n",
            "iteration:  26 loss: 0.10575066\n",
            "iteration:  27 loss: 0.08610869\n",
            "iteration:  28 loss: 0.65901864\n",
            "iteration:  29 loss: 0.08000203\n",
            "iteration:  30 loss: 0.07598958\n",
            "iteration:  31 loss: 0.17677553\n",
            "iteration:  32 loss: 0.04185822\n",
            "iteration:  33 loss: 0.07436315\n",
            "iteration:  34 loss: 0.08496322\n",
            "iteration:  35 loss: 0.11520658\n",
            "iteration:  36 loss: 0.17179832\n",
            "iteration:  37 loss: 0.11165760\n",
            "iteration:  38 loss: 0.07209903\n",
            "iteration:  39 loss: 0.16092321\n",
            "iteration:  40 loss: 0.08738464\n",
            "iteration:  41 loss: 0.20371540\n",
            "iteration:  42 loss: 0.07340722\n",
            "iteration:  43 loss: 0.07829364\n",
            "iteration:  44 loss: 0.19650748\n",
            "iteration:  45 loss: 0.18109037\n",
            "iteration:  46 loss: 0.24549718\n",
            "iteration:  47 loss: 0.08975043\n",
            "iteration:  48 loss: 0.09737327\n",
            "iteration:  49 loss: 0.06284200\n",
            "iteration:  50 loss: 0.15505704\n",
            "iteration:  51 loss: 0.17038099\n",
            "iteration:  52 loss: 0.10720731\n",
            "iteration:  53 loss: 0.32071459\n",
            "iteration:  54 loss: 0.07551058\n",
            "iteration:  55 loss: 0.15790138\n",
            "iteration:  56 loss: 0.17267311\n",
            "iteration:  57 loss: 0.16520865\n",
            "iteration:  58 loss: 0.24579999\n",
            "iteration:  59 loss: 0.05480934\n",
            "iteration:  60 loss: 0.11739742\n",
            "iteration:  61 loss: 0.10834485\n",
            "iteration:  62 loss: 0.29506120\n",
            "iteration:  63 loss: 0.07657346\n",
            "iteration:  64 loss: 0.07151061\n",
            "iteration:  65 loss: 0.07001396\n",
            "iteration:  66 loss: 0.06003546\n",
            "iteration:  67 loss: 0.06372391\n",
            "iteration:  68 loss: 0.06347957\n",
            "iteration:  69 loss: 0.06392913\n",
            "iteration:  70 loss: 0.15761748\n",
            "iteration:  71 loss: 0.17379278\n",
            "iteration:  72 loss: 0.06238949\n",
            "iteration:  73 loss: 0.06133280\n",
            "iteration:  74 loss: 0.11307654\n",
            "iteration:  75 loss: 0.06603113\n",
            "iteration:  76 loss: 0.09181505\n",
            "iteration:  77 loss: 0.09211975\n",
            "iteration:  78 loss: 0.09895224\n",
            "iteration:  79 loss: 0.09617519\n",
            "iteration:  80 loss: 0.05698294\n",
            "iteration:  81 loss: 0.09519604\n",
            "iteration:  82 loss: 0.37770292\n",
            "iteration:  83 loss: 0.12111437\n",
            "iteration:  84 loss: 0.11656648\n",
            "iteration:  85 loss: 0.18728095\n",
            "iteration:  86 loss: 0.09811325\n",
            "iteration:  87 loss: 0.05625202\n",
            "iteration:  88 loss: 0.15812773\n",
            "iteration:  89 loss: 0.07734922\n",
            "iteration:  90 loss: 0.11597599\n",
            "iteration:  91 loss: 0.08682919\n",
            "iteration:  92 loss: 0.06142796\n",
            "iteration:  93 loss: 0.14341532\n",
            "iteration:  94 loss: 0.14465468\n",
            "iteration:  95 loss: 0.15481259\n",
            "iteration:  96 loss: 0.15889643\n",
            "iteration:  97 loss: 0.17721699\n",
            "iteration:  98 loss: 0.10986650\n",
            "iteration:  99 loss: 0.08571596\n",
            "iteration: 100 loss: 0.11864632\n",
            "iteration: 101 loss: 0.13035218\n",
            "iteration: 102 loss: 0.11973374\n",
            "iteration: 103 loss: 0.11317627\n",
            "iteration: 104 loss: 0.06205594\n",
            "iteration: 105 loss: 0.14225517\n",
            "iteration: 106 loss: 0.07169743\n",
            "iteration: 107 loss: 0.06178747\n",
            "iteration: 108 loss: 0.08624922\n",
            "iteration: 109 loss: 0.10961947\n",
            "iteration: 110 loss: 0.10281443\n",
            "iteration: 111 loss: 0.11408917\n",
            "iteration: 112 loss: 0.07713626\n",
            "iteration: 113 loss: 0.12597038\n",
            "iteration: 114 loss: 0.07158245\n",
            "iteration: 115 loss: 0.16822743\n",
            "iteration: 116 loss: 0.07786015\n",
            "iteration: 117 loss: 0.09776916\n",
            "iteration: 118 loss: 0.06355192\n",
            "iteration: 119 loss: 0.35880277\n",
            "iteration: 120 loss: 0.16836372\n",
            "iteration: 121 loss: 0.08865449\n",
            "iteration: 122 loss: 0.10022906\n",
            "iteration: 123 loss: 0.09945093\n",
            "iteration: 124 loss: 0.12513109\n",
            "iteration: 125 loss: 0.07223950\n",
            "iteration: 126 loss: 0.15251118\n",
            "iteration: 127 loss: 0.09986956\n",
            "iteration: 128 loss: 0.13104936\n",
            "iteration: 129 loss: 0.07734501\n",
            "iteration: 130 loss: 0.09307176\n",
            "iteration: 131 loss: 0.13631265\n",
            "iteration: 132 loss: 0.13500261\n",
            "iteration: 133 loss: 0.08091275\n",
            "iteration: 134 loss: 0.08676644\n",
            "iteration: 135 loss: 0.12232709\n",
            "iteration: 136 loss: 0.10344449\n",
            "iteration: 137 loss: 0.11249349\n",
            "iteration: 138 loss: 0.23308712\n",
            "iteration: 139 loss: 0.09708892\n",
            "iteration: 140 loss: 0.07674526\n",
            "iteration: 141 loss: 0.06498367\n",
            "iteration: 142 loss: 0.09823744\n",
            "iteration: 143 loss: 0.12473583\n",
            "iteration: 144 loss: 0.06577686\n",
            "iteration: 145 loss: 0.07629479\n",
            "iteration: 146 loss: 0.06901122\n",
            "iteration: 147 loss: 0.22663906\n",
            "iteration: 148 loss: 0.30509141\n",
            "iteration: 149 loss: 0.09865825\n",
            "iteration: 150 loss: 0.13576740\n",
            "iteration: 151 loss: 0.09029776\n",
            "iteration: 152 loss: 0.18095998\n",
            "iteration: 153 loss: 0.14720978\n",
            "iteration: 154 loss: 0.14036366\n",
            "iteration: 155 loss: 0.14148897\n",
            "iteration: 156 loss: 0.06955614\n",
            "iteration: 157 loss: 0.15243636\n",
            "iteration: 158 loss: 0.08307087\n",
            "iteration: 159 loss: 0.18228960\n",
            "iteration: 160 loss: 0.05336224\n",
            "iteration: 161 loss: 0.27515736\n",
            "iteration: 162 loss: 0.06434118\n",
            "iteration: 163 loss: 0.06925820\n",
            "iteration: 164 loss: 0.35122487\n",
            "iteration: 165 loss: 0.13014570\n",
            "iteration: 166 loss: 0.07957388\n",
            "iteration: 167 loss: 0.20025830\n",
            "iteration: 168 loss: 0.04435634\n",
            "iteration: 169 loss: 0.07277413\n",
            "iteration: 170 loss: 0.26149252\n",
            "iteration: 171 loss: 0.07928929\n",
            "iteration: 172 loss: 0.07946624\n",
            "iteration: 173 loss: 0.37412181\n",
            "iteration: 174 loss: 0.04717164\n",
            "iteration: 175 loss: 0.08950997\n",
            "iteration: 176 loss: 0.07857503\n",
            "iteration: 177 loss: 0.09689137\n",
            "iteration: 178 loss: 0.08860169\n",
            "iteration: 179 loss: 0.47992694\n",
            "iteration: 180 loss: 0.06568050\n",
            "iteration: 181 loss: 0.11879905\n",
            "iteration: 182 loss: 0.28727198\n",
            "iteration: 183 loss: 0.07419755\n",
            "iteration: 184 loss: 0.12984544\n",
            "iteration: 185 loss: 0.12624912\n",
            "iteration: 186 loss: 0.52902669\n",
            "iteration: 187 loss: 0.04447851\n",
            "iteration: 188 loss: 0.09706254\n",
            "iteration: 189 loss: 0.38384911\n",
            "iteration: 190 loss: 0.12577096\n",
            "iteration: 191 loss: 0.05713064\n",
            "iteration: 192 loss: 0.04548306\n",
            "iteration: 193 loss: 0.06178352\n",
            "iteration: 194 loss: 0.35212207\n",
            "iteration: 195 loss: 0.10730937\n",
            "iteration: 196 loss: 0.06823861\n",
            "iteration: 197 loss: 0.07408063\n",
            "iteration: 198 loss: 0.07997088\n",
            "iteration: 199 loss: 0.15479928\n",
            "epoch:  95 mean loss training: 0.12795965\n",
            "epoch:  95 mean loss validation: 1.18509889\n",
            "iteration:   0 loss: 0.22754700\n",
            "iteration:   1 loss: 0.09130783\n",
            "iteration:   2 loss: 0.13935483\n",
            "iteration:   3 loss: 0.05936492\n",
            "iteration:   4 loss: 0.14345895\n",
            "iteration:   5 loss: 0.06413491\n",
            "iteration:   6 loss: 0.07525387\n",
            "iteration:   7 loss: 0.07493903\n",
            "iteration:   8 loss: 0.10378952\n",
            "iteration:   9 loss: 0.10646422\n",
            "iteration:  10 loss: 0.07695910\n",
            "iteration:  11 loss: 0.06865473\n",
            "iteration:  12 loss: 0.06795719\n",
            "iteration:  13 loss: 0.10295514\n",
            "iteration:  14 loss: 0.05371409\n",
            "iteration:  15 loss: 0.06496012\n",
            "iteration:  16 loss: 0.07436028\n",
            "iteration:  17 loss: 0.22968729\n",
            "iteration:  18 loss: 0.21602476\n",
            "iteration:  19 loss: 0.10244383\n",
            "iteration:  20 loss: 0.18625581\n",
            "iteration:  21 loss: 0.07356106\n",
            "iteration:  22 loss: 0.10611063\n",
            "iteration:  23 loss: 0.07606254\n",
            "iteration:  24 loss: 0.16075420\n",
            "iteration:  25 loss: 0.15450066\n",
            "iteration:  26 loss: 0.17976996\n",
            "iteration:  27 loss: 0.12762256\n",
            "iteration:  28 loss: 0.08154575\n",
            "iteration:  29 loss: 0.10763120\n",
            "iteration:  30 loss: 0.13989311\n",
            "iteration:  31 loss: 0.09021103\n",
            "iteration:  32 loss: 0.05737866\n",
            "iteration:  33 loss: 0.06526507\n",
            "iteration:  34 loss: 0.07931843\n",
            "iteration:  35 loss: 0.12640752\n",
            "iteration:  36 loss: 0.18950604\n",
            "iteration:  37 loss: 0.16796140\n",
            "iteration:  38 loss: 0.11142115\n",
            "iteration:  39 loss: 0.05401165\n",
            "iteration:  40 loss: 0.09579046\n",
            "iteration:  41 loss: 0.35367975\n",
            "iteration:  42 loss: 0.12972340\n",
            "iteration:  43 loss: 0.07990114\n",
            "iteration:  44 loss: 0.21750030\n",
            "iteration:  45 loss: 0.16220936\n",
            "iteration:  46 loss: 0.13638832\n",
            "iteration:  47 loss: 0.10342836\n",
            "iteration:  48 loss: 0.11156993\n",
            "iteration:  49 loss: 0.20804331\n",
            "iteration:  50 loss: 0.09626220\n",
            "iteration:  51 loss: 0.06696670\n",
            "iteration:  52 loss: 0.10714692\n",
            "iteration:  53 loss: 0.08136386\n",
            "iteration:  54 loss: 0.32894775\n",
            "iteration:  55 loss: 0.09461614\n",
            "iteration:  56 loss: 0.12144410\n",
            "iteration:  57 loss: 0.14113735\n",
            "iteration:  58 loss: 0.07046679\n",
            "iteration:  59 loss: 0.10411268\n",
            "iteration:  60 loss: 0.07569273\n",
            "iteration:  61 loss: 0.06573955\n",
            "iteration:  62 loss: 0.10727049\n",
            "iteration:  63 loss: 0.09148683\n",
            "iteration:  64 loss: 0.06128439\n",
            "iteration:  65 loss: 0.08465625\n",
            "iteration:  66 loss: 0.21831669\n",
            "iteration:  67 loss: 0.12173482\n",
            "iteration:  68 loss: 0.10244492\n",
            "iteration:  69 loss: 0.10162944\n",
            "iteration:  70 loss: 0.11604366\n",
            "iteration:  71 loss: 0.16980463\n",
            "iteration:  72 loss: 0.05331925\n",
            "iteration:  73 loss: 0.11508121\n",
            "iteration:  74 loss: 0.11076844\n",
            "iteration:  75 loss: 0.06954293\n",
            "iteration:  76 loss: 0.14345767\n",
            "iteration:  77 loss: 0.08839968\n",
            "iteration:  78 loss: 0.06611679\n",
            "iteration:  79 loss: 0.07225795\n",
            "iteration:  80 loss: 0.14630914\n",
            "iteration:  81 loss: 0.06012838\n",
            "iteration:  82 loss: 0.18603580\n",
            "iteration:  83 loss: 0.05669638\n",
            "iteration:  84 loss: 0.07883190\n",
            "iteration:  85 loss: 0.06346193\n",
            "iteration:  86 loss: 0.12449433\n",
            "iteration:  87 loss: 0.21481463\n",
            "iteration:  88 loss: 0.71615571\n",
            "iteration:  89 loss: 0.07708552\n",
            "iteration:  90 loss: 0.08199067\n",
            "iteration:  91 loss: 0.09241930\n",
            "iteration:  92 loss: 0.07574719\n",
            "iteration:  93 loss: 0.04089892\n",
            "iteration:  94 loss: 0.13636094\n",
            "iteration:  95 loss: 0.08729698\n",
            "iteration:  96 loss: 0.14926353\n",
            "iteration:  97 loss: 0.12506570\n",
            "iteration:  98 loss: 0.06305704\n",
            "iteration:  99 loss: 0.13621607\n",
            "iteration: 100 loss: 0.17232329\n",
            "iteration: 101 loss: 0.12991300\n",
            "iteration: 102 loss: 0.06846820\n",
            "iteration: 103 loss: 0.14157104\n",
            "iteration: 104 loss: 0.18821797\n",
            "iteration: 105 loss: 0.06129421\n",
            "iteration: 106 loss: 0.07637543\n",
            "iteration: 107 loss: 0.09522188\n",
            "iteration: 108 loss: 0.07062390\n",
            "iteration: 109 loss: 0.11639412\n",
            "iteration: 110 loss: 0.13445067\n",
            "iteration: 111 loss: 0.15043804\n",
            "iteration: 112 loss: 0.22526151\n",
            "iteration: 113 loss: 0.06383236\n",
            "iteration: 114 loss: 0.06857378\n",
            "iteration: 115 loss: 0.15142532\n",
            "iteration: 116 loss: 0.22575702\n",
            "iteration: 117 loss: 0.12254114\n",
            "iteration: 118 loss: 0.09242700\n",
            "iteration: 119 loss: 0.29879284\n",
            "iteration: 120 loss: 0.08001710\n",
            "iteration: 121 loss: 0.09006205\n",
            "iteration: 122 loss: 0.07439760\n",
            "iteration: 123 loss: 0.47881818\n",
            "iteration: 124 loss: 0.09498741\n",
            "iteration: 125 loss: 0.13587317\n",
            "iteration: 126 loss: 0.07871979\n",
            "iteration: 127 loss: 0.05740469\n",
            "iteration: 128 loss: 0.08943924\n",
            "iteration: 129 loss: 0.09156656\n",
            "iteration: 130 loss: 0.14201510\n",
            "iteration: 131 loss: 0.13014250\n",
            "iteration: 132 loss: 0.04540956\n",
            "iteration: 133 loss: 0.06285983\n",
            "iteration: 134 loss: 0.12122355\n",
            "iteration: 135 loss: 0.18072747\n",
            "iteration: 136 loss: 0.10210114\n",
            "iteration: 137 loss: 0.07733323\n",
            "iteration: 138 loss: 0.08974846\n",
            "iteration: 139 loss: 0.05149359\n",
            "iteration: 140 loss: 0.28942859\n",
            "iteration: 141 loss: 0.11142752\n",
            "iteration: 142 loss: 0.14388122\n",
            "iteration: 143 loss: 0.26592198\n",
            "iteration: 144 loss: 0.22270539\n",
            "iteration: 145 loss: 0.05894830\n",
            "iteration: 146 loss: 0.06277590\n",
            "iteration: 147 loss: 0.07891651\n",
            "iteration: 148 loss: 0.06687030\n",
            "iteration: 149 loss: 0.09134002\n",
            "iteration: 150 loss: 0.10433660\n",
            "iteration: 151 loss: 0.15435266\n",
            "iteration: 152 loss: 0.10039364\n",
            "iteration: 153 loss: 0.08000118\n",
            "iteration: 154 loss: 0.19307555\n",
            "iteration: 155 loss: 0.06567338\n",
            "iteration: 156 loss: 0.05811211\n",
            "iteration: 157 loss: 0.07988910\n",
            "iteration: 158 loss: 0.12450942\n",
            "iteration: 159 loss: 0.12006946\n",
            "iteration: 160 loss: 0.11426365\n",
            "iteration: 161 loss: 0.13133080\n",
            "iteration: 162 loss: 0.13989100\n",
            "iteration: 163 loss: 0.06102040\n",
            "iteration: 164 loss: 0.08618434\n",
            "iteration: 165 loss: 0.08805592\n",
            "iteration: 166 loss: 0.15965879\n",
            "iteration: 167 loss: 0.08883499\n",
            "iteration: 168 loss: 0.15562558\n",
            "iteration: 169 loss: 0.09941584\n",
            "iteration: 170 loss: 0.08504613\n",
            "iteration: 171 loss: 0.33311692\n",
            "iteration: 172 loss: 0.17829062\n",
            "iteration: 173 loss: 0.07474057\n",
            "iteration: 174 loss: 0.10570407\n",
            "iteration: 175 loss: 0.10180038\n",
            "iteration: 176 loss: 0.17167899\n",
            "iteration: 177 loss: 0.20820452\n",
            "iteration: 178 loss: 0.17662163\n",
            "iteration: 179 loss: 0.09362362\n",
            "iteration: 180 loss: 0.09669108\n",
            "iteration: 181 loss: 0.12736492\n",
            "iteration: 182 loss: 0.10264228\n",
            "iteration: 183 loss: 0.06534333\n",
            "iteration: 184 loss: 0.12788752\n",
            "iteration: 185 loss: 0.27307236\n",
            "iteration: 186 loss: 0.09765261\n",
            "iteration: 187 loss: 0.15621418\n",
            "iteration: 188 loss: 0.08442091\n",
            "iteration: 189 loss: 0.05500661\n",
            "iteration: 190 loss: 0.13809380\n",
            "iteration: 191 loss: 0.08986228\n",
            "iteration: 192 loss: 0.22501123\n",
            "iteration: 193 loss: 0.10229011\n",
            "iteration: 194 loss: 0.08590727\n",
            "iteration: 195 loss: 0.12861283\n",
            "iteration: 196 loss: 0.12146644\n",
            "iteration: 197 loss: 0.14896664\n",
            "iteration: 198 loss: 0.30566496\n",
            "iteration: 199 loss: 0.08256733\n",
            "epoch:  96 mean loss training: 0.12352658\n",
            "epoch:  96 mean loss validation: 1.15596032\n",
            "iteration:   0 loss: 0.14795986\n",
            "iteration:   1 loss: 0.06608003\n",
            "iteration:   2 loss: 0.07417220\n",
            "iteration:   3 loss: 0.11637801\n",
            "iteration:   4 loss: 0.06258705\n",
            "iteration:   5 loss: 0.42365965\n",
            "iteration:   6 loss: 0.07443905\n",
            "iteration:   7 loss: 0.07134585\n",
            "iteration:   8 loss: 0.21257155\n",
            "iteration:   9 loss: 0.45708504\n",
            "iteration:  10 loss: 0.37228256\n",
            "iteration:  11 loss: 0.07249978\n",
            "iteration:  12 loss: 0.12580045\n",
            "iteration:  13 loss: 0.06486425\n",
            "iteration:  14 loss: 0.06394634\n",
            "iteration:  15 loss: 0.09661973\n",
            "iteration:  16 loss: 0.12412855\n",
            "iteration:  17 loss: 0.08122144\n",
            "iteration:  18 loss: 0.11595173\n",
            "iteration:  19 loss: 0.12076691\n",
            "iteration:  20 loss: 0.07197677\n",
            "iteration:  21 loss: 0.21113916\n",
            "iteration:  22 loss: 0.18290085\n",
            "iteration:  23 loss: 0.06943089\n",
            "iteration:  24 loss: 0.11109307\n",
            "iteration:  25 loss: 0.09045859\n",
            "iteration:  26 loss: 0.07260108\n",
            "iteration:  27 loss: 0.09323533\n",
            "iteration:  28 loss: 0.07063961\n",
            "iteration:  29 loss: 0.06150881\n",
            "iteration:  30 loss: 0.11960776\n",
            "iteration:  31 loss: 0.07716976\n",
            "iteration:  32 loss: 0.16179368\n",
            "iteration:  33 loss: 0.24491152\n",
            "iteration:  34 loss: 0.07975657\n",
            "iteration:  35 loss: 0.07890983\n",
            "iteration:  36 loss: 0.44176355\n",
            "iteration:  37 loss: 0.10353704\n",
            "iteration:  38 loss: 0.06326051\n",
            "iteration:  39 loss: 0.13616651\n",
            "iteration:  40 loss: 0.13894781\n",
            "iteration:  41 loss: 0.08057274\n",
            "iteration:  42 loss: 0.06008030\n",
            "iteration:  43 loss: 0.14714444\n",
            "iteration:  44 loss: 0.14058265\n",
            "iteration:  45 loss: 0.07506214\n",
            "iteration:  46 loss: 0.15079734\n",
            "iteration:  47 loss: 0.08186348\n",
            "iteration:  48 loss: 0.09558724\n",
            "iteration:  49 loss: 0.13835835\n",
            "iteration:  50 loss: 0.05437570\n",
            "iteration:  51 loss: 0.07019952\n",
            "iteration:  52 loss: 0.06686387\n",
            "iteration:  53 loss: 0.05358536\n",
            "iteration:  54 loss: 0.12756796\n",
            "iteration:  55 loss: 0.09431502\n",
            "iteration:  56 loss: 0.42539763\n",
            "iteration:  57 loss: 0.10764319\n",
            "iteration:  58 loss: 0.06793594\n",
            "iteration:  59 loss: 0.08183126\n",
            "iteration:  60 loss: 0.18580902\n",
            "iteration:  61 loss: 0.14656705\n",
            "iteration:  62 loss: 0.07036621\n",
            "iteration:  63 loss: 0.07618604\n",
            "iteration:  64 loss: 0.18073851\n",
            "iteration:  65 loss: 0.12165099\n",
            "iteration:  66 loss: 0.11892937\n",
            "iteration:  67 loss: 0.17599672\n",
            "iteration:  68 loss: 0.05693340\n",
            "iteration:  69 loss: 0.07503581\n",
            "iteration:  70 loss: 0.09719472\n",
            "iteration:  71 loss: 0.10212687\n",
            "iteration:  72 loss: 0.27569792\n",
            "iteration:  73 loss: 0.05193797\n",
            "iteration:  74 loss: 0.11791654\n",
            "iteration:  75 loss: 0.10648613\n",
            "iteration:  76 loss: 0.07833426\n",
            "iteration:  77 loss: 0.13538942\n",
            "iteration:  78 loss: 0.11050987\n",
            "iteration:  79 loss: 0.09074356\n",
            "iteration:  80 loss: 0.07478331\n",
            "iteration:  81 loss: 0.07159647\n",
            "iteration:  82 loss: 0.05164716\n",
            "iteration:  83 loss: 0.13335854\n",
            "iteration:  84 loss: 0.07439001\n",
            "iteration:  85 loss: 0.10081799\n",
            "iteration:  86 loss: 0.08293606\n",
            "iteration:  87 loss: 0.05253623\n",
            "iteration:  88 loss: 0.08678429\n",
            "iteration:  89 loss: 0.08603837\n",
            "iteration:  90 loss: 0.09347195\n",
            "iteration:  91 loss: 0.10290667\n",
            "iteration:  92 loss: 0.04901376\n",
            "iteration:  93 loss: 0.11160442\n",
            "iteration:  94 loss: 0.07902245\n",
            "iteration:  95 loss: 0.11495515\n",
            "iteration:  96 loss: 0.10133401\n",
            "iteration:  97 loss: 0.15120837\n",
            "iteration:  98 loss: 0.16417223\n",
            "iteration:  99 loss: 0.24229014\n",
            "iteration: 100 loss: 0.05862335\n",
            "iteration: 101 loss: 0.23033039\n",
            "iteration: 102 loss: 0.08629993\n",
            "iteration: 103 loss: 0.11005933\n",
            "iteration: 104 loss: 0.06361233\n",
            "iteration: 105 loss: 0.22729786\n",
            "iteration: 106 loss: 0.06330381\n",
            "iteration: 107 loss: 0.15955904\n",
            "iteration: 108 loss: 0.10134269\n",
            "iteration: 109 loss: 0.14092840\n",
            "iteration: 110 loss: 0.06656379\n",
            "iteration: 111 loss: 0.06670199\n",
            "iteration: 112 loss: 0.20601347\n",
            "iteration: 113 loss: 0.06421895\n",
            "iteration: 114 loss: 0.11891311\n",
            "iteration: 115 loss: 0.14280804\n",
            "iteration: 116 loss: 0.11292908\n",
            "iteration: 117 loss: 0.07151178\n",
            "iteration: 118 loss: 0.21743248\n",
            "iteration: 119 loss: 0.12788579\n",
            "iteration: 120 loss: 0.09632216\n",
            "iteration: 121 loss: 0.16916111\n",
            "iteration: 122 loss: 0.13152179\n",
            "iteration: 123 loss: 0.07321516\n",
            "iteration: 124 loss: 0.18887293\n",
            "iteration: 125 loss: 0.08784723\n",
            "iteration: 126 loss: 0.09277916\n",
            "iteration: 127 loss: 0.15717340\n",
            "iteration: 128 loss: 0.08631803\n",
            "iteration: 129 loss: 0.21769302\n",
            "iteration: 130 loss: 0.07669110\n",
            "iteration: 131 loss: 0.10908432\n",
            "iteration: 132 loss: 0.27267325\n",
            "iteration: 133 loss: 0.07039958\n",
            "iteration: 134 loss: 0.05755639\n",
            "iteration: 135 loss: 0.08209573\n",
            "iteration: 136 loss: 0.16145992\n",
            "iteration: 137 loss: 0.13632509\n",
            "iteration: 138 loss: 0.09244110\n",
            "iteration: 139 loss: 0.11810108\n",
            "iteration: 140 loss: 0.11226518\n",
            "iteration: 141 loss: 0.08223265\n",
            "iteration: 142 loss: 0.11663206\n",
            "iteration: 143 loss: 0.06604982\n",
            "iteration: 144 loss: 0.13686378\n",
            "iteration: 145 loss: 0.06897150\n",
            "iteration: 146 loss: 0.06030846\n",
            "iteration: 147 loss: 0.21269022\n",
            "iteration: 148 loss: 0.08311668\n",
            "iteration: 149 loss: 0.08987639\n",
            "iteration: 150 loss: 0.05902642\n",
            "iteration: 151 loss: 0.27660465\n",
            "iteration: 152 loss: 0.07165360\n",
            "iteration: 153 loss: 0.08530974\n",
            "iteration: 154 loss: 0.09544582\n",
            "iteration: 155 loss: 0.09160160\n",
            "iteration: 156 loss: 0.07718850\n",
            "iteration: 157 loss: 0.06063981\n",
            "iteration: 158 loss: 0.12497814\n",
            "iteration: 159 loss: 0.19644049\n",
            "iteration: 160 loss: 0.06172350\n",
            "iteration: 161 loss: 0.10525956\n",
            "iteration: 162 loss: 0.22072890\n",
            "iteration: 163 loss: 0.05399366\n",
            "iteration: 164 loss: 0.10060754\n",
            "iteration: 165 loss: 0.06314290\n",
            "iteration: 166 loss: 0.06073696\n",
            "iteration: 167 loss: 0.10677034\n",
            "iteration: 168 loss: 0.07263069\n",
            "iteration: 169 loss: 0.11853245\n",
            "iteration: 170 loss: 0.14337820\n",
            "iteration: 171 loss: 0.06509711\n",
            "iteration: 172 loss: 0.10820789\n",
            "iteration: 173 loss: 0.23872271\n",
            "iteration: 174 loss: 0.08515340\n",
            "iteration: 175 loss: 0.07555297\n",
            "iteration: 176 loss: 0.04738617\n",
            "iteration: 177 loss: 0.09397861\n",
            "iteration: 178 loss: 0.11671986\n",
            "iteration: 179 loss: 0.08258089\n",
            "iteration: 180 loss: 0.05984646\n",
            "iteration: 181 loss: 0.12377626\n",
            "iteration: 182 loss: 0.06021958\n",
            "iteration: 183 loss: 0.12953258\n",
            "iteration: 184 loss: 0.14104685\n",
            "iteration: 185 loss: 0.07529633\n",
            "iteration: 186 loss: 0.06119374\n",
            "iteration: 187 loss: 0.09787983\n",
            "iteration: 188 loss: 0.06497097\n",
            "iteration: 189 loss: 0.15602136\n",
            "iteration: 190 loss: 0.07732255\n",
            "iteration: 191 loss: 0.20231271\n",
            "iteration: 192 loss: 0.06869830\n",
            "iteration: 193 loss: 0.08664375\n",
            "iteration: 194 loss: 0.23713270\n",
            "iteration: 195 loss: 0.19078854\n",
            "iteration: 196 loss: 0.07707563\n",
            "iteration: 197 loss: 0.04451466\n",
            "iteration: 198 loss: 0.07380579\n",
            "iteration: 199 loss: 0.09452894\n",
            "epoch:  97 mean loss training: 0.11684914\n",
            "epoch:  97 mean loss validation: 1.17600143\n",
            "iteration:   0 loss: 0.05108563\n",
            "iteration:   1 loss: 0.12128754\n",
            "iteration:   2 loss: 0.21612537\n",
            "iteration:   3 loss: 0.15409356\n",
            "iteration:   4 loss: 0.08529633\n",
            "iteration:   5 loss: 0.19391675\n",
            "iteration:   6 loss: 0.07559407\n",
            "iteration:   7 loss: 0.13139549\n",
            "iteration:   8 loss: 0.09099132\n",
            "iteration:   9 loss: 0.07730609\n",
            "iteration:  10 loss: 0.31386325\n",
            "iteration:  11 loss: 0.07929584\n",
            "iteration:  12 loss: 0.14939645\n",
            "iteration:  13 loss: 0.09528365\n",
            "iteration:  14 loss: 0.06667964\n",
            "iteration:  15 loss: 0.15146409\n",
            "iteration:  16 loss: 0.05541246\n",
            "iteration:  17 loss: 0.07980412\n",
            "iteration:  18 loss: 0.28159451\n",
            "iteration:  19 loss: 0.06858923\n",
            "iteration:  20 loss: 0.22826672\n",
            "iteration:  21 loss: 0.06674063\n",
            "iteration:  22 loss: 0.04519096\n",
            "iteration:  23 loss: 0.04031451\n",
            "iteration:  24 loss: 0.06162962\n",
            "iteration:  25 loss: 0.15092997\n",
            "iteration:  26 loss: 0.09017535\n",
            "iteration:  27 loss: 0.10840367\n",
            "iteration:  28 loss: 0.22448507\n",
            "iteration:  29 loss: 0.08113917\n",
            "iteration:  30 loss: 0.09463817\n",
            "iteration:  31 loss: 0.16589235\n",
            "iteration:  32 loss: 0.15921755\n",
            "iteration:  33 loss: 0.08595078\n",
            "iteration:  34 loss: 0.05620382\n",
            "iteration:  35 loss: 0.19293204\n",
            "iteration:  36 loss: 0.06245603\n",
            "iteration:  37 loss: 0.15637130\n",
            "iteration:  38 loss: 0.14929198\n",
            "iteration:  39 loss: 0.10201320\n",
            "iteration:  40 loss: 0.08595626\n",
            "iteration:  41 loss: 0.12158381\n",
            "iteration:  42 loss: 0.13298227\n",
            "iteration:  43 loss: 0.12055404\n",
            "iteration:  44 loss: 0.07017443\n",
            "iteration:  45 loss: 0.08062555\n",
            "iteration:  46 loss: 0.29045442\n",
            "iteration:  47 loss: 0.08739452\n",
            "iteration:  48 loss: 0.11791915\n",
            "iteration:  49 loss: 0.12435432\n",
            "iteration:  50 loss: 0.13205297\n",
            "iteration:  51 loss: 0.08621799\n",
            "iteration:  52 loss: 0.15934625\n",
            "iteration:  53 loss: 0.05780343\n",
            "iteration:  54 loss: 0.17590417\n",
            "iteration:  55 loss: 0.13203676\n",
            "iteration:  56 loss: 0.12119839\n",
            "iteration:  57 loss: 0.15268785\n",
            "iteration:  58 loss: 0.07377369\n",
            "iteration:  59 loss: 0.81370389\n",
            "iteration:  60 loss: 0.07944792\n",
            "iteration:  61 loss: 0.06406918\n",
            "iteration:  62 loss: 0.06575335\n",
            "iteration:  63 loss: 0.07600170\n",
            "iteration:  64 loss: 0.27278781\n",
            "iteration:  65 loss: 0.37444893\n",
            "iteration:  66 loss: 0.06829752\n",
            "iteration:  67 loss: 0.15916614\n",
            "iteration:  68 loss: 0.06833129\n",
            "iteration:  69 loss: 0.12602252\n",
            "iteration:  70 loss: 0.26957479\n",
            "iteration:  71 loss: 0.09399442\n",
            "iteration:  72 loss: 0.07438843\n",
            "iteration:  73 loss: 0.33158943\n",
            "iteration:  74 loss: 0.16645429\n",
            "iteration:  75 loss: 0.14545941\n",
            "iteration:  76 loss: 0.05919977\n",
            "iteration:  77 loss: 0.15221301\n",
            "iteration:  78 loss: 0.10607232\n",
            "iteration:  79 loss: 0.06809894\n",
            "iteration:  80 loss: 0.07265754\n",
            "iteration:  81 loss: 0.06350871\n",
            "iteration:  82 loss: 0.06685599\n",
            "iteration:  83 loss: 0.10690223\n",
            "iteration:  84 loss: 0.21952140\n",
            "iteration:  85 loss: 0.19279292\n",
            "iteration:  86 loss: 0.10239649\n",
            "iteration:  87 loss: 0.19273916\n",
            "iteration:  88 loss: 0.16343181\n",
            "iteration:  89 loss: 0.24211499\n",
            "iteration:  90 loss: 0.06884495\n",
            "iteration:  91 loss: 0.07297707\n",
            "iteration:  92 loss: 0.09543420\n",
            "iteration:  93 loss: 0.08773302\n",
            "iteration:  94 loss: 0.31118315\n",
            "iteration:  95 loss: 0.06610619\n",
            "iteration:  96 loss: 0.08772466\n",
            "iteration:  97 loss: 0.12224837\n",
            "iteration:  98 loss: 0.15541418\n",
            "iteration:  99 loss: 0.10597485\n",
            "iteration: 100 loss: 0.07302043\n",
            "iteration: 101 loss: 0.13828911\n",
            "iteration: 102 loss: 0.15127861\n",
            "iteration: 103 loss: 0.07025090\n",
            "iteration: 104 loss: 0.11946413\n",
            "iteration: 105 loss: 0.06204896\n",
            "iteration: 106 loss: 0.10578354\n",
            "iteration: 107 loss: 0.10052741\n",
            "iteration: 108 loss: 0.22608882\n",
            "iteration: 109 loss: 0.30450511\n",
            "iteration: 110 loss: 0.12098062\n",
            "iteration: 111 loss: 0.13582519\n",
            "iteration: 112 loss: 0.06608862\n",
            "iteration: 113 loss: 0.05978941\n",
            "iteration: 114 loss: 0.22627613\n",
            "iteration: 115 loss: 0.15158811\n",
            "iteration: 116 loss: 0.09100744\n",
            "iteration: 117 loss: 0.10211091\n",
            "iteration: 118 loss: 0.08783314\n",
            "iteration: 119 loss: 0.27535155\n",
            "iteration: 120 loss: 0.09608512\n",
            "iteration: 121 loss: 0.15361841\n",
            "iteration: 122 loss: 0.09765471\n",
            "iteration: 123 loss: 0.20001349\n",
            "iteration: 124 loss: 0.04129555\n",
            "iteration: 125 loss: 0.08630379\n",
            "iteration: 126 loss: 0.09065184\n",
            "iteration: 127 loss: 0.07213585\n",
            "iteration: 128 loss: 0.08604889\n",
            "iteration: 129 loss: 0.12173030\n",
            "iteration: 130 loss: 0.11258407\n",
            "iteration: 131 loss: 0.10723671\n",
            "iteration: 132 loss: 0.08511014\n",
            "iteration: 133 loss: 0.13765045\n",
            "iteration: 134 loss: 0.06922103\n",
            "iteration: 135 loss: 0.08183017\n",
            "iteration: 136 loss: 0.09999046\n",
            "iteration: 137 loss: 0.14584523\n",
            "iteration: 138 loss: 0.06635796\n",
            "iteration: 139 loss: 0.12149635\n",
            "iteration: 140 loss: 0.09674821\n",
            "iteration: 141 loss: 0.05828171\n",
            "iteration: 142 loss: 0.13349237\n",
            "iteration: 143 loss: 0.15911993\n",
            "iteration: 144 loss: 0.14299248\n",
            "iteration: 145 loss: 0.13970096\n",
            "iteration: 146 loss: 0.09874208\n",
            "iteration: 147 loss: 0.07240552\n",
            "iteration: 148 loss: 0.11461214\n",
            "iteration: 149 loss: 0.08896536\n",
            "iteration: 150 loss: 0.05481964\n",
            "iteration: 151 loss: 0.08107366\n",
            "iteration: 152 loss: 0.10887372\n",
            "iteration: 153 loss: 0.23656237\n",
            "iteration: 154 loss: 0.07222811\n",
            "iteration: 155 loss: 0.29476765\n",
            "iteration: 156 loss: 0.13852902\n",
            "iteration: 157 loss: 0.05090377\n",
            "iteration: 158 loss: 0.21935287\n",
            "iteration: 159 loss: 0.08226226\n",
            "iteration: 160 loss: 0.13705254\n",
            "iteration: 161 loss: 0.10556111\n",
            "iteration: 162 loss: 0.14399679\n",
            "iteration: 163 loss: 0.11246933\n",
            "iteration: 164 loss: 0.05803236\n",
            "iteration: 165 loss: 0.12884341\n",
            "iteration: 166 loss: 0.06716140\n",
            "iteration: 167 loss: 0.11652530\n",
            "iteration: 168 loss: 0.06086123\n",
            "iteration: 169 loss: 0.10225976\n",
            "iteration: 170 loss: 0.09488086\n",
            "iteration: 171 loss: 0.04780020\n",
            "iteration: 172 loss: 0.10186493\n",
            "iteration: 173 loss: 0.08315769\n",
            "iteration: 174 loss: 0.05793090\n",
            "iteration: 175 loss: 0.06596662\n",
            "iteration: 176 loss: 0.11966513\n",
            "iteration: 177 loss: 0.08996724\n",
            "iteration: 178 loss: 0.06272236\n",
            "iteration: 179 loss: 0.08297359\n",
            "iteration: 180 loss: 0.08329767\n",
            "iteration: 181 loss: 0.11910485\n",
            "iteration: 182 loss: 0.09297825\n",
            "iteration: 183 loss: 0.08390185\n",
            "iteration: 184 loss: 0.13616255\n",
            "iteration: 185 loss: 0.42727810\n",
            "iteration: 186 loss: 0.11114286\n",
            "iteration: 187 loss: 0.07771453\n",
            "iteration: 188 loss: 0.06452628\n",
            "iteration: 189 loss: 0.10613395\n",
            "iteration: 190 loss: 0.11211568\n",
            "iteration: 191 loss: 0.11767214\n",
            "iteration: 192 loss: 0.11933625\n",
            "iteration: 193 loss: 0.11686654\n",
            "iteration: 194 loss: 0.08132400\n",
            "iteration: 195 loss: 0.08380966\n",
            "iteration: 196 loss: 0.10007634\n",
            "iteration: 197 loss: 0.07537991\n",
            "iteration: 198 loss: 0.06614122\n",
            "iteration: 199 loss: 0.06601772\n",
            "epoch:  98 mean loss training: 0.12340725\n",
            "epoch:  98 mean loss validation: 1.18952107\n",
            "iteration:   0 loss: 0.10317641\n",
            "iteration:   1 loss: 0.11491795\n",
            "iteration:   2 loss: 0.05736385\n",
            "iteration:   3 loss: 0.12324325\n",
            "iteration:   4 loss: 0.08235579\n",
            "iteration:   5 loss: 0.15902443\n",
            "iteration:   6 loss: 0.09788200\n",
            "iteration:   7 loss: 0.07510597\n",
            "iteration:   8 loss: 0.15656157\n",
            "iteration:   9 loss: 0.31212395\n",
            "iteration:  10 loss: 0.06015312\n",
            "iteration:  11 loss: 0.11945041\n",
            "iteration:  12 loss: 0.11912499\n",
            "iteration:  13 loss: 0.33865622\n",
            "iteration:  14 loss: 0.07256036\n",
            "iteration:  15 loss: 0.12072074\n",
            "iteration:  16 loss: 0.11243673\n",
            "iteration:  17 loss: 0.33303279\n",
            "iteration:  18 loss: 0.13780598\n",
            "iteration:  19 loss: 0.09561127\n",
            "iteration:  20 loss: 0.18716402\n",
            "iteration:  21 loss: 0.09760331\n",
            "iteration:  22 loss: 0.11787982\n",
            "iteration:  23 loss: 0.14063752\n",
            "iteration:  24 loss: 0.16224699\n",
            "iteration:  25 loss: 0.07569967\n",
            "iteration:  26 loss: 0.19303209\n",
            "iteration:  27 loss: 0.13882609\n",
            "iteration:  28 loss: 0.09199566\n",
            "iteration:  29 loss: 0.12382944\n",
            "iteration:  30 loss: 0.13780515\n",
            "iteration:  31 loss: 0.14065145\n",
            "iteration:  32 loss: 0.06449133\n",
            "iteration:  33 loss: 0.05178782\n",
            "iteration:  34 loss: 0.22951233\n",
            "iteration:  35 loss: 0.15756494\n",
            "iteration:  36 loss: 0.16393095\n",
            "iteration:  37 loss: 0.23179054\n",
            "iteration:  38 loss: 0.22809257\n",
            "iteration:  39 loss: 0.11235988\n",
            "iteration:  40 loss: 0.13658608\n",
            "iteration:  41 loss: 0.06996155\n",
            "iteration:  42 loss: 0.14501786\n",
            "iteration:  43 loss: 0.13539326\n",
            "iteration:  44 loss: 0.31258214\n",
            "iteration:  45 loss: 0.14846839\n",
            "iteration:  46 loss: 0.07948259\n",
            "iteration:  47 loss: 0.05822149\n",
            "iteration:  48 loss: 0.05971442\n",
            "iteration:  49 loss: 0.10149658\n",
            "iteration:  50 loss: 0.10110836\n",
            "iteration:  51 loss: 0.11027046\n",
            "iteration:  52 loss: 0.05927580\n",
            "iteration:  53 loss: 0.07933062\n",
            "iteration:  54 loss: 0.08500108\n",
            "iteration:  55 loss: 0.07055104\n",
            "iteration:  56 loss: 0.08953111\n",
            "iteration:  57 loss: 0.12071964\n",
            "iteration:  58 loss: 0.10753302\n",
            "iteration:  59 loss: 0.15479660\n",
            "iteration:  60 loss: 0.08633009\n",
            "iteration:  61 loss: 0.10554559\n",
            "iteration:  62 loss: 0.06815176\n",
            "iteration:  63 loss: 0.07356355\n",
            "iteration:  64 loss: 0.10400972\n",
            "iteration:  65 loss: 0.07517630\n",
            "iteration:  66 loss: 0.06473282\n",
            "iteration:  67 loss: 0.15227880\n",
            "iteration:  68 loss: 0.14116023\n",
            "iteration:  69 loss: 0.08779400\n",
            "iteration:  70 loss: 0.15185617\n",
            "iteration:  71 loss: 0.09137004\n",
            "iteration:  72 loss: 0.06841604\n",
            "iteration:  73 loss: 0.05082036\n",
            "iteration:  74 loss: 0.06284591\n",
            "iteration:  75 loss: 0.21253458\n",
            "iteration:  76 loss: 0.13663039\n",
            "iteration:  77 loss: 0.13876276\n",
            "iteration:  78 loss: 0.11808690\n",
            "iteration:  79 loss: 0.16768694\n",
            "iteration:  80 loss: 0.12628675\n",
            "iteration:  81 loss: 0.24999538\n",
            "iteration:  82 loss: 0.06623008\n",
            "iteration:  83 loss: 0.13122560\n",
            "iteration:  84 loss: 0.08724408\n",
            "iteration:  85 loss: 0.04826950\n",
            "iteration:  86 loss: 0.11459252\n",
            "iteration:  87 loss: 0.05682822\n",
            "iteration:  88 loss: 0.08406086\n",
            "iteration:  89 loss: 0.18389863\n",
            "iteration:  90 loss: 0.06937315\n",
            "iteration:  91 loss: 0.09581721\n",
            "iteration:  92 loss: 0.10477243\n",
            "iteration:  93 loss: 0.08809289\n",
            "iteration:  94 loss: 0.10727264\n",
            "iteration:  95 loss: 0.11634101\n",
            "iteration:  96 loss: 0.06498725\n",
            "iteration:  97 loss: 0.06992036\n",
            "iteration:  98 loss: 0.12273911\n",
            "iteration:  99 loss: 0.07241429\n",
            "iteration: 100 loss: 0.15394165\n",
            "iteration: 101 loss: 0.16642056\n",
            "iteration: 102 loss: 0.06977246\n",
            "iteration: 103 loss: 0.34400862\n",
            "iteration: 104 loss: 0.07639220\n",
            "iteration: 105 loss: 0.10258467\n",
            "iteration: 106 loss: 0.05753794\n",
            "iteration: 107 loss: 0.08633218\n",
            "iteration: 108 loss: 0.39657733\n",
            "iteration: 109 loss: 0.13621055\n",
            "iteration: 110 loss: 0.16566060\n",
            "iteration: 111 loss: 0.06139213\n",
            "iteration: 112 loss: 0.07548373\n",
            "iteration: 113 loss: 0.07664445\n",
            "iteration: 114 loss: 0.07304474\n",
            "iteration: 115 loss: 0.09695579\n",
            "iteration: 116 loss: 0.11976896\n",
            "iteration: 117 loss: 0.08998416\n",
            "iteration: 118 loss: 0.14273177\n",
            "iteration: 119 loss: 0.08316274\n",
            "iteration: 120 loss: 0.12737694\n",
            "iteration: 121 loss: 0.15617512\n",
            "iteration: 122 loss: 0.10388118\n",
            "iteration: 123 loss: 0.08494837\n",
            "iteration: 124 loss: 0.17256711\n",
            "iteration: 125 loss: 0.12147597\n",
            "iteration: 126 loss: 0.12147926\n",
            "iteration: 127 loss: 0.24722978\n",
            "iteration: 128 loss: 0.10626733\n",
            "iteration: 129 loss: 0.12080751\n",
            "iteration: 130 loss: 0.08442416\n",
            "iteration: 131 loss: 0.08543003\n",
            "iteration: 132 loss: 0.06819705\n",
            "iteration: 133 loss: 0.08477178\n",
            "iteration: 134 loss: 0.08477616\n",
            "iteration: 135 loss: 0.11233874\n",
            "iteration: 136 loss: 0.24985957\n",
            "iteration: 137 loss: 0.06524585\n",
            "iteration: 138 loss: 0.09085301\n",
            "iteration: 139 loss: 0.10736075\n",
            "iteration: 140 loss: 0.48457813\n",
            "iteration: 141 loss: 0.09889181\n",
            "iteration: 142 loss: 0.10832250\n",
            "iteration: 143 loss: 0.07447689\n",
            "iteration: 144 loss: 0.05985226\n",
            "iteration: 145 loss: 0.11673521\n",
            "iteration: 146 loss: 0.11650483\n",
            "iteration: 147 loss: 0.13964114\n",
            "iteration: 148 loss: 0.08604765\n",
            "iteration: 149 loss: 0.07586901\n",
            "iteration: 150 loss: 0.12013514\n",
            "iteration: 151 loss: 0.08818834\n",
            "iteration: 152 loss: 0.07791769\n",
            "iteration: 153 loss: 0.06552148\n",
            "iteration: 154 loss: 0.15002251\n",
            "iteration: 155 loss: 0.09239153\n",
            "iteration: 156 loss: 0.15265810\n",
            "iteration: 157 loss: 0.23472126\n",
            "iteration: 158 loss: 0.09521663\n",
            "iteration: 159 loss: 0.09070518\n",
            "iteration: 160 loss: 0.07681711\n",
            "iteration: 161 loss: 0.19756976\n",
            "iteration: 162 loss: 0.12959039\n",
            "iteration: 163 loss: 0.12721157\n",
            "iteration: 164 loss: 0.08302282\n",
            "iteration: 165 loss: 0.07747912\n",
            "iteration: 166 loss: 0.09141774\n",
            "iteration: 167 loss: 0.32022601\n",
            "iteration: 168 loss: 0.11937587\n",
            "iteration: 169 loss: 0.20061471\n",
            "iteration: 170 loss: 0.07173022\n",
            "iteration: 171 loss: 0.08661915\n",
            "iteration: 172 loss: 0.07373627\n",
            "iteration: 173 loss: 0.11148921\n",
            "iteration: 174 loss: 0.08740067\n",
            "iteration: 175 loss: 0.15561363\n",
            "iteration: 176 loss: 0.46640748\n",
            "iteration: 177 loss: 0.09761732\n",
            "iteration: 178 loss: 0.07619137\n",
            "iteration: 179 loss: 0.09354635\n",
            "iteration: 180 loss: 0.08729111\n",
            "iteration: 181 loss: 0.14282328\n",
            "iteration: 182 loss: 0.06644035\n",
            "iteration: 183 loss: 0.12501100\n",
            "iteration: 184 loss: 0.10277615\n",
            "iteration: 185 loss: 0.05636952\n",
            "iteration: 186 loss: 0.12703256\n",
            "iteration: 187 loss: 0.09643274\n",
            "iteration: 188 loss: 0.07586729\n",
            "iteration: 189 loss: 0.08705710\n",
            "iteration: 190 loss: 0.07516205\n",
            "iteration: 191 loss: 0.08882338\n",
            "iteration: 192 loss: 0.08508977\n",
            "iteration: 193 loss: 0.12057139\n",
            "iteration: 194 loss: 0.10482941\n",
            "iteration: 195 loss: 0.15293585\n",
            "iteration: 196 loss: 0.06556790\n",
            "iteration: 197 loss: 0.06664200\n",
            "iteration: 198 loss: 0.08327401\n",
            "iteration: 199 loss: 0.11212677\n",
            "epoch:  99 mean loss training: 0.12122818\n",
            "epoch:  99 mean loss validation: 1.17108083\n",
            "iteration:   0 loss: 0.19219008\n",
            "iteration:   1 loss: 0.08707951\n",
            "iteration:   2 loss: 0.10564730\n",
            "iteration:   3 loss: 0.16705772\n",
            "iteration:   4 loss: 0.11720646\n",
            "iteration:   5 loss: 0.08011517\n",
            "iteration:   6 loss: 0.12989891\n",
            "iteration:   7 loss: 0.05709750\n",
            "iteration:   8 loss: 0.13916203\n",
            "iteration:   9 loss: 0.13142601\n",
            "iteration:  10 loss: 0.14775358\n",
            "iteration:  11 loss: 0.11468156\n",
            "iteration:  12 loss: 0.07056762\n",
            "iteration:  13 loss: 0.10812228\n",
            "iteration:  14 loss: 0.13400036\n",
            "iteration:  15 loss: 0.07998073\n",
            "iteration:  16 loss: 0.04732927\n",
            "iteration:  17 loss: 0.12811255\n",
            "iteration:  18 loss: 0.06090304\n",
            "iteration:  19 loss: 0.06289203\n",
            "iteration:  20 loss: 0.08507873\n",
            "iteration:  21 loss: 0.06766515\n",
            "iteration:  22 loss: 0.13563797\n",
            "iteration:  23 loss: 0.07738397\n",
            "iteration:  24 loss: 0.09981862\n",
            "iteration:  25 loss: 0.15395796\n",
            "iteration:  26 loss: 0.08883691\n",
            "iteration:  27 loss: 0.10738891\n",
            "iteration:  28 loss: 0.09886502\n",
            "iteration:  29 loss: 0.06953889\n",
            "iteration:  30 loss: 0.20230724\n",
            "iteration:  31 loss: 0.30918455\n",
            "iteration:  32 loss: 0.09957416\n",
            "iteration:  33 loss: 0.21946603\n",
            "iteration:  34 loss: 0.07746464\n",
            "iteration:  35 loss: 0.07706799\n",
            "iteration:  36 loss: 0.12309344\n",
            "iteration:  37 loss: 0.13160686\n",
            "iteration:  38 loss: 0.05086878\n",
            "iteration:  39 loss: 0.09565954\n",
            "iteration:  40 loss: 0.13846323\n",
            "iteration:  41 loss: 0.22590148\n",
            "iteration:  42 loss: 0.11992265\n",
            "iteration:  43 loss: 0.04026537\n",
            "iteration:  44 loss: 0.09302708\n",
            "iteration:  45 loss: 0.08184718\n",
            "iteration:  46 loss: 0.11068309\n",
            "iteration:  47 loss: 0.13051686\n",
            "iteration:  48 loss: 0.16475338\n",
            "iteration:  49 loss: 0.06388699\n",
            "iteration:  50 loss: 0.14782265\n",
            "iteration:  51 loss: 0.23553362\n",
            "iteration:  52 loss: 0.16944583\n",
            "iteration:  53 loss: 0.09799948\n",
            "iteration:  54 loss: 0.09654609\n",
            "iteration:  55 loss: 0.06882869\n",
            "iteration:  56 loss: 0.06635410\n",
            "iteration:  57 loss: 0.05695774\n",
            "iteration:  58 loss: 0.09611396\n",
            "iteration:  59 loss: 0.09775396\n",
            "iteration:  60 loss: 0.10504134\n",
            "iteration:  61 loss: 0.09547763\n",
            "iteration:  62 loss: 0.07839374\n",
            "iteration:  63 loss: 0.08586158\n",
            "iteration:  64 loss: 0.06399556\n",
            "iteration:  65 loss: 0.11002627\n",
            "iteration:  66 loss: 0.23600559\n",
            "iteration:  67 loss: 0.15151791\n",
            "iteration:  68 loss: 0.07938337\n",
            "iteration:  69 loss: 0.10732047\n",
            "iteration:  70 loss: 0.21538471\n",
            "iteration:  71 loss: 0.04374641\n",
            "iteration:  72 loss: 0.06452215\n",
            "iteration:  73 loss: 0.10582437\n",
            "iteration:  74 loss: 0.18583305\n",
            "iteration:  75 loss: 0.15308809\n",
            "iteration:  76 loss: 0.08653916\n",
            "iteration:  77 loss: 0.09363715\n",
            "iteration:  78 loss: 0.08863688\n",
            "iteration:  79 loss: 0.09783624\n",
            "iteration:  80 loss: 0.23494516\n",
            "iteration:  81 loss: 0.07023343\n",
            "iteration:  82 loss: 0.06467090\n",
            "iteration:  83 loss: 0.08393478\n",
            "iteration:  84 loss: 0.13606137\n",
            "iteration:  85 loss: 0.13984005\n",
            "iteration:  86 loss: 0.09062469\n",
            "iteration:  87 loss: 0.16436493\n",
            "iteration:  88 loss: 0.10070872\n",
            "iteration:  89 loss: 0.10907974\n",
            "iteration:  90 loss: 0.47537240\n",
            "iteration:  91 loss: 0.08467659\n",
            "iteration:  92 loss: 0.09156083\n",
            "iteration:  93 loss: 0.09602477\n",
            "iteration:  94 loss: 0.07671145\n",
            "iteration:  95 loss: 0.09812304\n",
            "iteration:  96 loss: 0.24643803\n",
            "iteration:  97 loss: 0.11393945\n",
            "iteration:  98 loss: 0.14943160\n",
            "iteration:  99 loss: 0.08134204\n",
            "iteration: 100 loss: 0.08790985\n",
            "iteration: 101 loss: 0.10461301\n",
            "iteration: 102 loss: 0.06494780\n",
            "iteration: 103 loss: 0.08781835\n",
            "iteration: 104 loss: 0.09664616\n",
            "iteration: 105 loss: 0.08576461\n",
            "iteration: 106 loss: 0.08022293\n",
            "iteration: 107 loss: 0.09625123\n",
            "iteration: 108 loss: 0.14369380\n",
            "iteration: 109 loss: 0.26227891\n",
            "iteration: 110 loss: 0.16090994\n",
            "iteration: 111 loss: 0.07218644\n",
            "iteration: 112 loss: 0.07521196\n",
            "iteration: 113 loss: 0.06198125\n",
            "iteration: 114 loss: 0.10451518\n",
            "iteration: 115 loss: 0.14458729\n",
            "iteration: 116 loss: 0.27714902\n",
            "iteration: 117 loss: 0.12822042\n",
            "iteration: 118 loss: 0.28570122\n",
            "iteration: 119 loss: 0.07228373\n",
            "iteration: 120 loss: 0.13852076\n",
            "iteration: 121 loss: 0.10086714\n",
            "iteration: 122 loss: 0.53194362\n",
            "iteration: 123 loss: 0.09192427\n",
            "iteration: 124 loss: 0.06200422\n",
            "iteration: 125 loss: 0.09529807\n",
            "iteration: 126 loss: 0.12826851\n",
            "iteration: 127 loss: 0.08638988\n",
            "iteration: 128 loss: 0.06792673\n",
            "iteration: 129 loss: 0.11663064\n",
            "iteration: 130 loss: 0.34587651\n",
            "iteration: 131 loss: 0.06962682\n",
            "iteration: 132 loss: 0.09339619\n",
            "iteration: 133 loss: 0.06625985\n",
            "iteration: 134 loss: 0.08828440\n",
            "iteration: 135 loss: 0.21687891\n",
            "iteration: 136 loss: 0.25311798\n",
            "iteration: 137 loss: 0.22725728\n",
            "iteration: 138 loss: 0.16997710\n",
            "iteration: 139 loss: 0.08019091\n",
            "iteration: 140 loss: 0.21370395\n",
            "iteration: 141 loss: 0.08723949\n",
            "iteration: 142 loss: 0.26326048\n",
            "iteration: 143 loss: 0.05579877\n",
            "iteration: 144 loss: 0.11182121\n",
            "iteration: 145 loss: 0.09181920\n",
            "iteration: 146 loss: 0.05730120\n",
            "iteration: 147 loss: 0.11352856\n",
            "iteration: 148 loss: 0.04890276\n",
            "iteration: 149 loss: 0.11830486\n",
            "iteration: 150 loss: 0.08466735\n",
            "iteration: 151 loss: 0.12966608\n",
            "iteration: 152 loss: 0.08200108\n",
            "iteration: 153 loss: 0.23794506\n",
            "iteration: 154 loss: 0.10060696\n",
            "iteration: 155 loss: 0.11218861\n",
            "iteration: 156 loss: 0.15615049\n",
            "iteration: 157 loss: 0.35123187\n",
            "iteration: 158 loss: 0.10672808\n",
            "iteration: 159 loss: 0.33117712\n",
            "iteration: 160 loss: 0.07272203\n",
            "iteration: 161 loss: 0.15578273\n",
            "iteration: 162 loss: 0.14032021\n",
            "iteration: 163 loss: 0.09112135\n",
            "iteration: 164 loss: 0.06309907\n",
            "iteration: 165 loss: 0.14644325\n",
            "iteration: 166 loss: 0.06278065\n",
            "iteration: 167 loss: 0.09391509\n",
            "iteration: 168 loss: 0.10466761\n",
            "iteration: 169 loss: 0.12282766\n",
            "iteration: 170 loss: 0.19432555\n",
            "iteration: 171 loss: 0.07842878\n",
            "iteration: 172 loss: 0.14461982\n",
            "iteration: 173 loss: 0.06027929\n",
            "iteration: 174 loss: 0.30300352\n",
            "iteration: 175 loss: 0.13051319\n",
            "iteration: 176 loss: 0.22577800\n",
            "iteration: 177 loss: 0.21946035\n",
            "iteration: 178 loss: 0.07248761\n",
            "iteration: 179 loss: 0.11695137\n",
            "iteration: 180 loss: 0.08704963\n",
            "iteration: 181 loss: 0.06264216\n",
            "iteration: 182 loss: 0.10534002\n",
            "iteration: 183 loss: 0.06553724\n",
            "iteration: 184 loss: 0.10856565\n",
            "iteration: 185 loss: 0.05623385\n",
            "iteration: 186 loss: 0.10220656\n",
            "iteration: 187 loss: 0.24571200\n",
            "iteration: 188 loss: 0.10990776\n",
            "iteration: 189 loss: 0.08058705\n",
            "iteration: 190 loss: 0.06590380\n",
            "iteration: 191 loss: 0.18016000\n",
            "iteration: 192 loss: 0.10508417\n",
            "iteration: 193 loss: 0.15462846\n",
            "iteration: 194 loss: 0.13812444\n",
            "iteration: 195 loss: 0.03732228\n",
            "iteration: 196 loss: 0.10037440\n",
            "iteration: 197 loss: 0.06870744\n",
            "iteration: 198 loss: 0.09220339\n",
            "iteration: 199 loss: 0.14581025\n",
            "epoch: 100 mean loss training: 0.12439836\n",
            "epoch: 100 mean loss validation: 1.16410625\n"
          ]
        }
      ],
      "source": [
        "#Training the data\n",
        "epochs = 100 #The number of times the dataset will be used to train the model\n",
        "batch_size = 10 #Besaran kumpulan atau pecahan data dari dataset\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_valid = []\n",
        "\n",
        "mean_losses_train_autoencoder = []\n",
        "mean_losses_valid_autoencoder = []\n",
        "\n",
        "mean_losses_train_classification = []\n",
        "mean_losses_valid_classification = []\n",
        "\n",
        "best_loss_valid = np.inf\n",
        "\n",
        "#===========================================================================================================\n",
        "#autoencoder loss ratio is created following a function that decreases exponentially starting from 0.5.\n",
        "#The goal is that the longer it takes, the stronger the effect of loss classification becomes.\n",
        "#This ratio reduction is needed because the main purpose of the model is classification\n",
        "#while the autoencoder is only an addition to strengthen the classification ability\n",
        "#===========================================================================================================\n",
        "ae_ratio = 0.5\n",
        "ae_scale = 0.1\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    aggregated_losses_train = []\n",
        "    aggregated_losses_valid = []\n",
        "\n",
        "    aggregated_losses_autoencoder = []\n",
        "    aggregated_losses_valid_autoencoder = []\n",
        "    \n",
        "    aggregated_losses_classification = []\n",
        "    aggregated_losses_valid_classification = []\n",
        "    \n",
        "    mean_losses = []\n",
        "    i += 1\n",
        "#added random permutation for shuffle data training\n",
        "    idxs_train = np.random.permutation(train_records)\n",
        "    for j in range((train_records//batch_size)+1):\n",
        "        start_train = j*batch_size\n",
        "        end_train = start_train+batch_size\n",
        "        \n",
        "        idxs_batch_train = idxs_train[start_train:end_train] #for shuffle training dataset\n",
        "             \n",
        "        #input is replaced with categorical_train_data and numerical_train_data\n",
        "        train, weights1, weights2, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, train_embed = model(numerical_train_data[idxs_batch_train], categorical_train_data[idxs_batch_train])\n",
        "\n",
        "        #loss name is differentiated between train and validation, outputs is changed to train_outputs\n",
        "        classification_loss_train = loss_function(train_embed, train_outputs[idxs_batch_train])\n",
        "        \n",
        "        categorical_1_loss_train = loss_function_autoencoder(categorical_1_decoded, categorical_data[idxs_batch_train][:,0])\n",
        "        categorical_2_loss_train = loss_function_autoencoder(categorical_2_decoded, categorical_data[idxs_batch_train][:,1])\n",
        "        categorical_3_loss_train = loss_function_autoencoder(categorical_3_decoded, categorical_data[idxs_batch_train][:,2])\n",
        "        categorical_4_loss_train = loss_function_autoencoder(categorical_4_decoded, categorical_data[idxs_batch_train][:,3])\n",
        "        \n",
        "        numerical_loss_train = loss_function_mse(numerical_decoded, numerical_train_data[idxs_batch_train])\n",
        "   \n",
        "        autoencoder_loss_train = ae_scale*(categorical_1_loss_train + categorical_2_loss_train + categorical_3_loss_train + categorical_4_loss_train + numerical_loss_train)\n",
        "        train_loss = ((1-ae_ratio)*classification_loss_train) + (ae_ratio*autoencoder_loss_train)\n",
        "        \n",
        "        aggregated_losses_autoencoder.append(autoencoder_loss_train)\n",
        "        aggregated_losses_classification.append(classification_loss_train)\n",
        "        aggregated_losses_train.append(train_loss)\n",
        "\n",
        "        print(f'iteration: {j:3} loss: {train_loss.item():10.8f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    aggregated_losses_autoencoder = torch.stack(aggregated_losses_autoencoder)\n",
        "    aggregated_losses_classification = torch.stack(aggregated_losses_classification)\n",
        "    aggregated_losses_train = torch.stack(aggregated_losses_train)\n",
        "        \n",
        "    mean_losses_autoencoder = torch.mean(aggregated_losses_autoencoder)\n",
        "    mean_losses_classification = torch.mean(aggregated_losses_classification)\n",
        "    mean_losses = torch.mean(aggregated_losses_train)\n",
        "        \n",
        "    print(f'epoch: {i:3} mean loss training: {mean_losses.item():10.8f}')\n",
        "        \n",
        "    mean_losses_train_autoencoder.append(mean_losses_autoencoder)\n",
        "    mean_losses_train_classification.append(mean_losses_classification)\n",
        "    mean_losses_train.append(mean_losses)\n",
        "# ==============================================================================================\n",
        "# validation\n",
        "# ==============================================================================================\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "        idxs_valid = np.random.permutation(valid_records)\n",
        "        for k in range((valid_records//batch_size)+1):\n",
        "            start_valid = k*batch_size\n",
        "            end_valid = start_valid+batch_size\n",
        "\n",
        "            idxs_batch_valid = idxs_valid[start_valid:end_valid] #for shuffle validation dataset\n",
        "\n",
        "            #input is replaced with categorical_valid_data and numerical_valid_data\n",
        "            valid, weights1, weights2, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, valid_embed = model(numerical_valid_data[idxs_batch_valid], categorical_valid_data[idxs_batch_valid])\n",
        "\n",
        "            #loss name is differentiated between train and validation, outputs is changed to valid_outputs\n",
        "            classification_loss_valid = loss_function(valid_embed, valid_outputs[idxs_batch_valid])\n",
        " \n",
        "            categorical_1_loss_valid = loss_function_autoencoder(categorical_1_decoded, categorical_data[idxs_batch_valid][:,0])\n",
        "            categorical_2_loss_valid = loss_function_autoencoder(categorical_2_decoded, categorical_data[idxs_batch_valid][:,1])\n",
        "            categorical_3_loss_valid = loss_function_autoencoder(categorical_3_decoded, categorical_data[idxs_batch_valid][:,2])\n",
        "            categorical_4_loss_valid = loss_function_autoencoder(categorical_4_decoded, categorical_data[idxs_batch_valid][:,3])\n",
        "\n",
        "            numerical_loss_valid = loss_function_mse(numerical_decoded, numerical_valid_data[idxs_batch_valid])\n",
        "            \n",
        "            autoencoder_loss_valid = ae_scale*(categorical_1_loss_valid + categorical_2_loss_valid + categorical_3_loss_valid + categorical_4_loss_valid + numerical_loss_valid)\n",
        "            valid_loss = ((1-ae_ratio)*classification_loss_valid) + (ae_ratio*autoencoder_loss_valid)\n",
        "\n",
        "            aggregated_losses_valid_autoencoder.append(autoencoder_loss_valid)\n",
        "            aggregated_losses_valid_classification.append(classification_loss_valid)\n",
        "            aggregated_losses_valid.append(valid_loss)\n",
        "\n",
        "    mean_loss_valid = torch.mean(torch.stack(aggregated_losses_valid))\n",
        "    mean_loss_valid_autoencoder = torch.mean(torch.stack(aggregated_losses_valid_autoencoder))\n",
        "    mean_loss_valid_classification = torch.mean(torch.stack(aggregated_losses_valid_classification))\n",
        "    \n",
        "    print(f'epoch: {i:3} mean loss validation: {mean_loss_valid:.8f}')\n",
        "    \n",
        "#======================================================================\n",
        "#The model is saved when the loss is lowest not at the end of the epoch\n",
        "#======================================================================\n",
        "    if mean_loss_valid.cpu().numpy()[()] < best_loss_valid:\n",
        "        best_loss_valid = mean_loss_valid\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/SSL.pth\".format(churn_percentage))\n",
        "        best_epoch = i        \n",
        "    \n",
        "    mean_losses_valid.append(mean_loss_valid)\n",
        "    mean_losses_valid_autoencoder.append(mean_loss_valid_autoencoder)\n",
        "    mean_losses_valid_classification.append(mean_loss_valid_classification)\n",
        "    \n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DVvZMDdyfCF",
        "outputId": "db228e95-a9dd-43a8-d0b0-20f37ad6e3f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Load training model\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Degree and Master Study/S2 Binus/Semester 6/Thesis Deep Learning/Koding/Train Model/SSL.pth\".format(churn_percentage)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhDQIqVxyfCI",
        "outputId": "e0c00133-6543-410a-8f4b-f914ec7f399e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.37437657\n"
          ]
        }
      ],
      "source": [
        "#Creating predictions\n",
        "with torch.no_grad():\n",
        "    valid, weights1, weights2, categorical_1_decoded, categorical_2_decoded, categorical_3_decoded, categorical_4_decoded, numerical_decoded, valid_embed = model(numerical_valid_data, categorical_valid_data)\n",
        "    classification_loss_valid = loss_function(valid_embed, valid_outputs)\n",
        "    total_valid_loss = classification_loss_valid\n",
        "print(f'Loss: {total_valid_loss:.8f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZNR8HCwyfCJ",
        "outputId": "0292e47c-1f28-4457-88f3-d11fbfdd832b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[523  41]\n",
            " [ 36  66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.93       564\n",
            "           1       0.62      0.65      0.63       102\n",
            "\n",
            "    accuracy                           0.88       666\n",
            "   macro avg       0.78      0.79      0.78       666\n",
            "weighted avg       0.89      0.88      0.89       666\n",
            "\n",
            "Accuracy:  0.8843843843843844\n",
            "F1 Score:  0.7815063036040679\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================================\n",
        "#the main result seen is the F1 Score, because\n",
        "#the misleading accuracy metric is used for imbalance data\n",
        "# =============================================================================================\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "valid_val = np.argmax(valid_embed, axis=1)\n",
        "print(confusion_matrix(valid_outputs, valid_val))\n",
        "print(classification_report(valid_outputs, valid_val))\n",
        "print(\"Accuracy: \", accuracy_score(valid_outputs, valid_val))\n",
        "print(\"F1 Score: \", f1_score(valid_outputs, valid_val, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRtHw_uWlzkm"
      },
      "outputs": [],
      "source": [
        "# function for smoothing the loss plot by using exponential moving average\n",
        "# https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar\n",
        "def smooth(scalars, weight):\n",
        "    last = scalars[0]  # First value in the plot (first timestep)\n",
        "    smoothed = list()\n",
        "    for point in scalars:\n",
        "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
        "        smoothed.append(smoothed_val)                        # Save it\n",
        "        last = smoothed_val                                  # Anchor the last smoothed value\n",
        "\n",
        "    return smoothed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOB1FI0ykOcx",
        "outputId": "6a005a7f-e07e-4140-9e0d-ff7b5f057548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(0.53765684, dtype=float32), array(0.3902268, dtype=float32), array(0.3365135, dtype=float32), array(0.2973251, dtype=float32), array(0.26601258, dtype=float32), array(0.24863687, dtype=float32), array(0.22794582, dtype=float32), array(0.21597828, dtype=float32), array(0.20719822, dtype=float32), array(0.1892213, dtype=float32), array(0.18098766, dtype=float32), array(0.1743401, dtype=float32), array(0.17315397, dtype=float32), array(0.1747026, dtype=float32), array(0.16152312, dtype=float32), array(0.15953688, dtype=float32), array(0.15524024, dtype=float32), array(0.15054718, dtype=float32), array(0.1456182, dtype=float32), array(0.14785849, dtype=float32), array(0.14415269, dtype=float32), array(0.14168553, dtype=float32), array(0.13241167, dtype=float32), array(0.13242893, dtype=float32), array(0.13151735, dtype=float32), array(0.13720845, dtype=float32), array(0.1335363, dtype=float32), array(0.1340666, dtype=float32), array(0.13179044, dtype=float32), array(0.13255674, dtype=float32), array(0.13011712, dtype=float32), array(0.12060943, dtype=float32), array(0.12527233, dtype=float32), array(0.11995402, dtype=float32), array(0.1189735, dtype=float32), array(0.13062885, dtype=float32), array(0.1247478, dtype=float32), array(0.1251492, dtype=float32), array(0.12741397, dtype=float32), array(0.11966661, dtype=float32), array(0.12256327, dtype=float32), array(0.12223896, dtype=float32), array(0.12084903, dtype=float32), array(0.11550248, dtype=float32), array(0.12138894, dtype=float32), array(0.12718622, dtype=float32), array(0.11820548, dtype=float32), array(0.11701059, dtype=float32), array(0.12324698, dtype=float32), array(0.12607801, dtype=float32), array(0.12645325, dtype=float32), array(0.11368526, dtype=float32), array(0.1255276, dtype=float32), array(0.11628906, dtype=float32), array(0.12038612, dtype=float32), array(0.12431147, dtype=float32), array(0.1243163, dtype=float32), array(0.11863685, dtype=float32), array(0.12086675, dtype=float32), array(0.11777189, dtype=float32), array(0.12085554, dtype=float32), array(0.12626103, dtype=float32), array(0.12343358, dtype=float32), array(0.11795585, dtype=float32), array(0.11733369, dtype=float32), array(0.12394946, dtype=float32), array(0.12721053, dtype=float32), array(0.11392274, dtype=float32), array(0.12575601, dtype=float32), array(0.12154283, dtype=float32), array(0.1226812, dtype=float32), array(0.11806283, dtype=float32), array(0.12336376, dtype=float32), array(0.12413053, dtype=float32), array(0.12349036, dtype=float32), array(0.11990666, dtype=float32), array(0.11798253, dtype=float32), array(0.12209477, dtype=float32), array(0.11912815, dtype=float32), array(0.11943085, dtype=float32), array(0.12473276, dtype=float32), array(0.12176624, dtype=float32), array(0.12157971, dtype=float32), array(0.11526468, dtype=float32), array(0.12270269, dtype=float32), array(0.12306131, dtype=float32), array(0.12041372, dtype=float32), array(0.11693063, dtype=float32), array(0.11962098, dtype=float32), array(0.12302542, dtype=float32), array(0.11984757, dtype=float32), array(0.11849298, dtype=float32), array(0.12480205, dtype=float32), array(0.11988384, dtype=float32), array(0.12795965, dtype=float32), array(0.12352658, dtype=float32), array(0.11684914, dtype=float32), array(0.12340725, dtype=float32), array(0.12122818, dtype=float32), array(0.12439836, dtype=float32)]\n",
            "[tensor(0.5954), tensor(0.6608), tensor(0.7054), tensor(0.7595), tensor(0.7922), tensor(0.8502), tensor(0.8578), tensor(0.8996), tensor(0.9142), tensor(0.9554), tensor(0.9628), tensor(0.9830), tensor(0.9954), tensor(1.0162), tensor(1.0259), tensor(1.0768), tensor(1.0769), tensor(1.0799), tensor(1.0854), tensor(1.0842), tensor(1.1139), tensor(1.1028), tensor(1.1076), tensor(1.1230), tensor(1.1650), tensor(1.1271), tensor(1.1417), tensor(1.1420), tensor(1.1486), tensor(1.1267), tensor(1.1827), tensor(1.1490), tensor(1.1371), tensor(1.1444), tensor(1.1246), tensor(1.1476), tensor(1.1632), tensor(1.1500), tensor(1.1594), tensor(1.1611), tensor(1.1794), tensor(1.1724), tensor(1.1701), tensor(1.1621), tensor(1.1963), tensor(1.1988), tensor(1.1686), tensor(1.1421), tensor(1.1649), tensor(1.1847), tensor(1.1786), tensor(1.1758), tensor(1.1718), tensor(1.1817), tensor(1.1745), tensor(1.1504), tensor(1.1630), tensor(1.1547), tensor(1.1752), tensor(1.1804), tensor(1.1716), tensor(1.1766), tensor(1.1909), tensor(1.1703), tensor(1.1831), tensor(1.1669), tensor(1.1532), tensor(1.1522), tensor(1.1675), tensor(1.1680), tensor(1.1699), tensor(1.1585), tensor(1.1852), tensor(1.1807), tensor(1.1573), tensor(1.1455), tensor(1.1428), tensor(1.1498), tensor(1.1813), tensor(1.1634), tensor(1.1668), tensor(1.1777), tensor(1.1662), tensor(1.1844), tensor(1.1798), tensor(1.1932), tensor(1.2080), tensor(1.1691), tensor(1.1997), tensor(1.1891), tensor(1.1555), tensor(1.1813), tensor(1.1543), tensor(1.1903), tensor(1.1851), tensor(1.1560), tensor(1.1760), tensor(1.1895), tensor(1.1711), tensor(1.1641)]\n"
          ]
        }
      ],
      "source": [
        "me_losses_train = []\n",
        "for l in mean_losses_train:\n",
        "  me_losses_train.append(l.detach().numpy())\n",
        "\n",
        "\n",
        "print (me_losses_train)\n",
        "print (mean_losses_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "r7osUOA8nEIz",
        "outputId": "d18cffce-7b27-41bc-a9eb-7c3b8dec4b2a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "epochs_to_show = 100\n",
        "smoothing = 0.5\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(me_losses_train[0:epochs_to_show],smoothing), label = 'train')\n",
        "plt.plot(range(1,epochs_to_show+1), smooth(mean_losses_valid[0:epochs_to_show],smoothing), label = 'validation')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('loss graph')\n",
        "plt.show()\n",
        "print(best_epoch)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SSL_1st_Run_Train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}